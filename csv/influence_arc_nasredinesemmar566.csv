2006.bcs-1.9,1998.amta-tutorials.5,0,0.0295313,"lingual parallel corpora can be used as a translation memory in a computer-aided translation tool. 95 THE CHALLENGE OF ARABIC FOR NLP/MT Keywords: Cross-language information retrieval, linguistic analysis, sentence alignment, bilingual corpora, translation memory. 1. INTRODUCTION Information retrieval consists in providing relevant documents according to the submitted query, and locating as precisely as possible the most informational parts of these documents. The goal of cross-lingual information retrieval is to find relevant documents that are in a different language from that of the query (Grefenstette 1998). The query terms are translated using bilingual dictionaries. Sentence alignment consists in mapping sentences of the source language with their translations in the target language. A number of automatic sentence alignment approaches have been proposed (Gale and Church 1991) (Debili and Samouda 1992) (Gaussier 1995) (Planas 1998) (Fluhr 2000). In this paper, we present the LIC2M sentence aligner which is based on cross-language information retrieval techniques. The LIC2M sentence aligner was developed for aligning French-English parallel text, it is now ported to Arabic-French and ArabicEngli"
2006.bcs-1.9,P91-1023,0,0.0617943,"DUCTION Information retrieval consists in providing relevant documents according to the submitted query, and locating as precisely as possible the most informational parts of these documents. The goal of cross-lingual information retrieval is to find relevant documents that are in a different language from that of the query (Grefenstette 1998). The query terms are translated using bilingual dictionaries. Sentence alignment consists in mapping sentences of the source language with their translations in the target language. A number of automatic sentence alignment approaches have been proposed (Gale and Church 1991) (Debili and Samouda 1992) (Gaussier 1995) (Planas 1998) (Fluhr 2000). In this paper, we present the LIC2M sentence aligner which is based on cross-language information retrieval techniques. The LIC2M sentence aligner was developed for aligning French-English parallel text, it is now ported to Arabic-French and ArabicEnglish language pairs. We present in section 2 the main components of the LIC2M cross-lingual search engine, in particular, we will focus on the linguistic processing. In section 3, the prototype of our sentence aligner is described. We discuss in section 4 results obtained after"
2006.bcs-1.9,chiao-etal-2006-evaluation,1,0.867965,"Missing"
2006.jeptalnrecital-long.29,W02-0506,0,0.0393274,"Missing"
2006.jeptalnrecital-long.29,W05-0705,1,0.88865,"Missing"
2007.jeptalnrecital-long.38,chiao-etal-2006-evaluation,1,0.883371,"Missing"
2007.jeptalnrecital-long.38,P91-1023,0,0.278524,"Missing"
2007.jeptalnrecital-long.38,W96-0201,0,0.0737891,"Missing"
2010.tc-1.14,besancon-etal-2010-lima,1,0.674845,"Missing"
2010.tc-1.14,J96-1001,0,\N,Missing
2010.tc-1.14,J93-2003,0,\N,Missing
2010.tc-1.14,C94-1084,0,\N,Missing
2010.tc-1.14,W07-0810,1,\N,Missing
2010.tc-1.14,P08-2007,0,\N,Missing
2010.tc-1.14,2004.jeptalnrecital-recital.4,0,\N,Missing
2012.amta-caas14.6,W98-1005,0,0.611176,"work on Arabic, Chinese and English and (Shao and Ng, 2004) who use the information resulted from transliterations based on pronunciation. (Shao and Ng, 2004) combine the obtained information from the translation context and those generated from the Chinese and English transliteration. This technique allows processing some specific infrequent words. We can also find some other systems that assign for a given name only one transliteration such as the generative model for English words written in Japanese (Katakana) to Latin transcription (Knight and Graehl, 1997). This approach was adapted by (Stalls and Knight, 1998) to translate an English word written in Arabic to English. The system of transliteration generation is based on a training dictionary that considers the unknown and unlisted pronunciations within the system. In order to resolve this deficiency, some works have used statistical techniques. This is the case of the transliteration system of the English names to Arabic proposed by (AbdulJaleel and Larkey, 2003). However, this system has several limitations as it uses the computation of the most probable form, supposed to be the correct form but is not always valid in all the Arab countries and di"
2012.amta-caas14.6,P97-1017,0,0.58731,"on the Arabic-English alignment, (Tao et al., 2006) who work on Arabic, Chinese and English and (Shao and Ng, 2004) who use the information resulted from transliterations based on pronunciation. (Shao and Ng, 2004) combine the obtained information from the translation context and those generated from the Chinese and English transliteration. This technique allows processing some specific infrequent words. We can also find some other systems that assign for a given name only one transliteration such as the generative model for English words written in Japanese (Katakana) to Latin transcription (Knight and Graehl, 1997). This approach was adapted by (Stalls and Knight, 1998) to translate an English word written in Arabic to English. The system of transliteration generation is based on a training dictionary that considers the unknown and unlisted pronunciations within the system. In order to resolve this deficiency, some works have used statistical techniques. This is the case of the transliteration system of the English names to Arabic proposed by (AbdulJaleel and Larkey, 2003). However, this system has several limitations as it uses the computation of the most probable form, supposed to be the correct form"
2012.amta-caas14.6,P07-2045,0,0.0040495,"ulation rule Translation(A.B) = Translation(A).Translation(B) as follows: Translation(frais.inscription) = Translation(frais).Translation(inscription) = eUِbْSdَ .iْYَر. 5 Experimentation To evaluate the contribution of the transliteration on the alignment quality of single and compound words, we used two approaches: • A manual evaluation comparing the results of our word aligner with a reference alignment; • An automatic evaluation by integrating the results of our word aligner in the training corpus used to extract the translation model of the Moses statistical machine translation system (Koehn et al., 2007). Because the manual construction of the alignment reference is a difficult and time-consuming task, we conducted a small-scale evaluation based on 283 French-Arabic aligned sentences extracted from the corpus of the ARCADE II campaign. To evaluate the alignment quality, we followed the evaluation framework defined in the shared task on word alignment organized as part of the HLT/NAACL 2003 Workshop on building and using parallel corpora (Mihalcea and Pedersen, 2003). Table 7 summarizes the results of our word aligner in terms of precision and recall. The first line describes the performance o"
2012.amta-caas14.6,W03-0301,0,0.223007,"lts of our word aligner in the training corpus used to extract the translation model of the Moses statistical machine translation system (Koehn et al., 2007). Because the manual construction of the alignment reference is a difficult and time-consuming task, we conducted a small-scale evaluation based on 283 French-Arabic aligned sentences extracted from the corpus of the ARCADE II campaign. To evaluate the alignment quality, we followed the evaluation framework defined in the shared task on word alignment organized as part of the HLT/NAACL 2003 Workshop on building and using parallel corpora (Mihalcea and Pedersen, 2003). Table 7 summarizes the results of our word aligner in terms of precision and recall. The first line describes the performance of the word aligner when it does not integrate transliteration and the second line mentions its performance when it uses transliteration. As we can see, these results demonstrate that using transliteration improves both precision and recall of word alignment. Word alignment without using transliteration with the use of transliteration Precision Recall 0.85 0.80 F-measure 0.82 0.88 0.86 0.85 Table 7: Results of word alignment evaluation. Certainly, the insufficient siz"
2012.amta-caas14.6,W06-1630,0,0.186581,"e of a word alignment tool. 2 Related Work The transliteration problem has interested many linguists in different languages, and recently researchers in natural language processing due to the constant development and use of Internet. Many research works have focused on the automatic alignment of transliterations from a multilingual text corpus, in order to enrich bilingual lexicons, which play a vital role in machine translation (MT) and cross-language information retrieval. These include (Al-Onaizan and Knight, 2002) and (Sherif and Kondrak, 2007) who worked on the Arabic-English alignment, (Tao et al., 2006) who work on Arabic, Chinese and English and (Shao and Ng, 2004) who use the information resulted from transliterations based on pronunciation. (Shao and Ng, 2004) combine the obtained information from the translation context and those generated from the Chinese and English transliteration. This technique allows processing some specific infrequent words. We can also find some other systems that assign for a given name only one transliteration such as the generative model for English words written in Japanese (Katakana) to Latin transcription (Knight and Graehl, 1997). This approach was adapted"
2012.amta-caas14.6,P07-1109,0,0.303011,"her hand, the impact of using transliteration to improve the performance of a word alignment tool. 2 Related Work The transliteration problem has interested many linguists in different languages, and recently researchers in natural language processing due to the constant development and use of Internet. Many research works have focused on the automatic alignment of transliterations from a multilingual text corpus, in order to enrich bilingual lexicons, which play a vital role in machine translation (MT) and cross-language information retrieval. These include (Al-Onaizan and Knight, 2002) and (Sherif and Kondrak, 2007) who worked on the Arabic-English alignment, (Tao et al., 2006) who work on Arabic, Chinese and English and (Shao and Ng, 2004) who use the information resulted from transliterations based on pronunciation. (Shao and Ng, 2004) combine the obtained information from the translation context and those generated from the Chinese and English transliteration. This technique allows processing some specific infrequent words. We can also find some other systems that assign for a given name only one transliteration such as the generative model for English words written in Japanese (Katakana) to Latin tra"
2013.mtsummit-papers.18,C02-2020,1,0.872177,"article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. proach and previous works addressing the task of bilingual lexicon extraction from comparable corpora. In Section 3, we present our context vector disambiguation process. Before concluding in section 5, we describe the experimental protocol we followed and discuss the obtained results in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors ar"
2013.mtsummit-papers.18,P04-1067,0,0.0791038,"Missing"
2013.mtsummit-papers.18,P95-1050,0,0.0670506,"esults on four specialized French-English comparable corpora show that our method outperforms two state-of-the-art approaches. Introduction Bilingual lexicons play an important role in many natural language processing applications such as machine translation or cross-language information retrieval (Shi, 2009). Research on lexical extraction from multilingual corpora have largely focused on parallel corpora. The scarcity of such corpora in particular for specialized domains and for language pairs not involving English pushed researchers to investigate the use of comparable corpora (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003), in which texts are not exact translation of each other but share common features. The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based xcas pz@limsi.fr nasredine.semmar@cea.fr Abstract 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France on the characterization and comparison of lexical environments represented by context"
2013.mtsummit-papers.18,hazem-morin-2012-adaptive,0,0.0600701,"ng to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. Hazem and Morin (2012) filtered the entries of the bilingual dictionary on the basis of partof-speech tags and of domain relevance criteria but no improvement was demonstrated. Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. However, only small improvements are reported. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we con"
2013.mtsummit-papers.18,P99-1067,0,0.0539965,"of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dic"
2013.mtsummit-papers.18,C10-1070,0,0.0814639,"Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. proach and previous works addressing the task of bilingual lexicon extraction from comparable corpora. In Section 3, we present our context vector disambiguation process. Before concluding in section 5, we describe the experimental protocol we followed and discuss the obtained results in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target la"
2013.mtsummit-papers.18,W11-1205,0,0.1434,"lts in section 4. 2 Bilingual Lexicon extraction 2.1 Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is t"
2013.mtsummit-papers.18,2009.mtsummit-posters.14,0,0.0130285,"nslation of context vectors: To enable the comparison of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 2.2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dictionary to translate context vectors. Few works have however focused on the ambiguity pr"
2014.tc-1.4,besancon-etal-2010-lima,1,0.776609,"Missing"
2015.jeptalnrecital-court.32,C04-1053,0,0.785086,"Missing"
2015.jeptalnrecital-court.32,A00-1031,0,0.599263,"Missing"
2015.jeptalnrecital-court.32,W06-2920,0,0.218701,"Missing"
2015.jeptalnrecital-court.32,P11-1061,0,0.266815,"Missing"
2015.jeptalnrecital-court.32,P13-2112,0,0.68192,"Missing"
2015.jeptalnrecital-court.32,J07-3002,0,0.443875,"Missing"
2015.jeptalnrecital-court.32,2005.mtsummit-papers.11,0,0.0424531,"Missing"
2015.jeptalnrecital-court.32,P00-1056,0,0.757351,"Missing"
2015.jeptalnrecital-court.32,H05-1108,0,0.488272,"Missing"
2015.jeptalnrecital-court.32,P06-1146,0,0.0656778,"Missing"
2015.jeptalnrecital-court.32,petrov-etal-2012-universal,0,0.160827,"Missing"
2015.jeptalnrecital-court.32,Q13-1001,0,0.0284499,"Missing"
2015.jeptalnrecital-court.32,F14-1005,0,0.0350045,"Missing"
2015.jeptalnrecital-court.32,H01-1035,0,0.707797,"Missing"
2016.jeptalnrecital-long.21,2012.iwslt-evaluation.13,1,0.869457,"Missing"
2016.jeptalnrecital-long.21,W03-1022,0,0.120238,"Missing"
2016.jeptalnrecital-long.21,J05-1003,0,0.0406407,"Missing"
2016.jeptalnrecital-long.21,W99-0613,0,0.4842,"Missing"
2016.jeptalnrecital-long.21,P11-1061,0,0.0697689,"Missing"
2016.jeptalnrecital-long.21,P13-2112,0,0.0440674,"Missing"
2016.jeptalnrecital-long.21,J07-3002,0,0.0529836,"Missing"
2016.jeptalnrecital-long.21,N15-1157,0,0.022943,"Missing"
2016.jeptalnrecital-long.21,P04-1013,0,0.0197756,"Missing"
2016.jeptalnrecital-long.21,W08-0510,0,0.0166881,"Missing"
2016.jeptalnrecital-long.21,D11-1110,0,0.0525488,"Missing"
2016.jeptalnrecital-long.21,P12-1073,0,0.0332014,"Missing"
2016.jeptalnrecital-long.21,W15-1521,0,0.0202754,"Missing"
2016.jeptalnrecital-long.21,H93-1061,0,0.719631,"Missing"
2016.jeptalnrecital-long.21,P00-1056,0,0.220339,"Missing"
2016.jeptalnrecital-long.21,H05-1108,0,0.103874,"Missing"
2016.jeptalnrecital-long.21,petrov-etal-2012-universal,0,0.091347,"Missing"
2016.jeptalnrecital-long.21,C12-1146,0,0.0298235,"Missing"
2016.jeptalnrecital-long.21,N12-1052,0,0.0530419,"Missing"
2016.jeptalnrecital-long.21,P12-1068,0,0.064653,"Missing"
2016.jeptalnrecital-long.21,D14-1187,0,0.0220272,"Missing"
2016.jeptalnrecital-long.21,K15-1008,0,0.0477618,"Missing"
2016.jeptalnrecital-long.21,H01-1035,0,0.292025,"Missing"
2016.jeptalnrecital-long.21,S07-1051,0,0.0610886,"Missing"
2016.jeptalnrecital-long.21,Y15-1016,1,0.803477,"Missing"
2016.jeptalnrecital-long.21,2015.jeptalnrecital-court.32,1,0.8592,"Missing"
2016.jeptalnrecital-long.7,W09-0432,0,0.0782396,"Missing"
2016.jeptalnrecital-long.7,besancon-etal-2010-lima,1,0.862337,"Missing"
2016.jeptalnrecital-long.7,bouamor-etal-2012-identifying,1,0.896883,"Missing"
2016.jeptalnrecital-long.7,J07-3002,0,0.0926468,"Missing"
2016.jeptalnrecital-long.7,1998.amta-tutorials.5,0,0.20321,"Missing"
2016.jeptalnrecital-long.7,2005.eamt-1.19,0,0.0818074,"Missing"
2016.jeptalnrecital-long.7,W02-1405,0,0.0929,"Missing"
2016.jeptalnrecital-long.7,2011.eamt-1.40,0,0.0370532,"Missing"
2016.jeptalnrecital-long.7,W09-2907,0,0.0508374,"Missing"
2016.jeptalnrecital-long.7,Y15-2013,1,0.795986,"Missing"
2016.jeptalnrecital-long.7,W07-0728,0,0.0311305,"Missing"
2016.jeptalnrecital-long.7,D08-1090,0,0.0761274,"Missing"
2016.jeptalnrecital-long.7,tiedemann-2012-parallel,0,0.0652843,"Missing"
2017.jeptalnrecital-court.8,besancon-etal-2010-lima,1,0.845246,"Missing"
2017.jeptalnrecital-court.8,bouamor-etal-2012-identifying,1,0.823959,"Missing"
2017.jeptalnrecital-court.8,A94-1006,0,0.548069,"Missing"
2017.jeptalnrecital-court.8,P08-4006,0,0.0193252,"Missing"
2017.jeptalnrecital-court.8,E09-1057,0,0.0672738,"Missing"
2017.jeptalnrecital-court.8,P00-1056,0,0.498785,"Missing"
2017.jeptalnrecital-court.8,W10-4006,0,0.0384889,"Missing"
2017.jeptalnrecital-court.8,P02-1040,0,0.0975575,"Missing"
2017.jeptalnrecital-court.8,2007.jeptalnrecital-long.37,0,0.0847982,"Missing"
2017.jeptalnrecital-court.8,vintar-fiser-2008-harvesting,0,0.0395968,"Missing"
2019.jeptalnrecital-court.15,N18-1127,0,0.0285393,"Missing"
2019.jeptalnrecital-court.15,baumann-pierrehumbert-2014-using,0,0.0233076,"Missing"
2019.jeptalnrecital-court.15,P18-1246,0,0.0810145,"âches : MS tagging, PoS tagging et NE recognition. 1. Le PoS tagging consiste à étiqueter chaque mot dans la phrase avec sa partie de discours (Nom, Adjective, etc.). Pour le domaine source, nous utilisons la partie WSJ du PTB (Marcus et al., 1993), annotée avec l’ensemble d’étiquettes du PTB. Et nous évaluons le performances du fine-tuning sur trois corpus des Tweets : TPoS (Ritter et al., 2011), annoté également avec l’ensemble d’étiquettes du PTB ; ARK (Owoputi et al., 2013) annoté avec un ensemble de 25 classes conçues pour les textes des RS ; et TweeBank, récemment proposé par Liu et al. (2018) et annoté avec l’ensemble d’étiquettes universelles. 2. Le MS tagging consiste à étiqueter chaque mot de la phrase avec une étiquette morphosyntactique, où la partie de discours est enrichie avec les informations morphologiques du mot. Comme illustré sur la figure 2, chaque lettre de l’étiquette représente une catégorie. La première position représente la partie de discours et les autres positions représentent des catégories morphosyntactiques, comme le nombre, le genre, etc. Nous utilisons les données fournies par la compagne d’évaluation MTT du Vardial18 (Zampieri et al., 2018), contenant u"
2019.jeptalnrecital-court.15,W17-4418,0,0.0212492,"Missing"
2019.jeptalnrecital-court.15,D17-1256,0,0.0280633,"Missing"
2019.jeptalnrecital-court.15,N18-1088,0,0.0208919,"Missing"
2019.jeptalnrecital-court.15,P16-1101,0,0.0447356,"Missing"
2019.jeptalnrecital-court.15,J93-2004,0,0.0648395,"Missing"
2019.jeptalnrecital-court.15,L18-1446,1,0.858465,"Missing"
2019.jeptalnrecital-court.15,W18-3927,1,0.888953,"Missing"
2019.jeptalnrecital-court.15,N19-1416,1,0.786681,"Missing"
2019.jeptalnrecital-court.15,D16-1046,0,0.0385885,"Missing"
2019.jeptalnrecital-court.15,N13-1039,0,0.0197473,"Missing"
2019.jeptalnrecital-court.15,D14-1162,0,0.0783083,"Missing"
2019.jeptalnrecital-court.15,N18-1202,0,0.0707121,"Missing"
2019.jeptalnrecital-court.15,D11-1141,0,0.0951193,"Missing"
2019.jeptalnrecital-court.15,W03-0419,0,0.462222,"Missing"
2019.jeptalnrecital-court.15,W18-3904,0,0.0614354,"Missing"
2020.socialnlp-1.8,N19-1078,0,0.022729,"Missing"
2020.socialnlp-1.8,C18-1251,0,0.0172344,"al. (2019)). More specifically, we organised the four tasks from low-level to high-level, with each task being fed with a shared word embedding as well as the outputs of all the lower tasks. To construct that hierarchy of tasks, we followed some linguistic hints from the literature. Indeed, many works have shown that POS improves CK (Yang et al., 2017; Ruder12 et al., 2019); NER benefits from POS (Meftah and Semmar, 2018; Ruder, 2019) and CK (Collobert and Weston, 2008); and DP profits from POS and CK (Hashimoto et al., 2017). In simple terms, POS and CK are considered as “universal helpers” (Changpinyo et al., 2018). Thus, based on these linguistic hierarchy observations, we feed POS features to CK; then POS and CK features to both NER and DP. An illustration of our multi-task hierarchical model is given in Fig.1. More specifically, WRE is shared across all tasks, its output (namely xi = WREshared (wi )) is fed to all branches. The lower component of the POS tagging branch (FEpos ) is fed with the shared embedding and after processing, it outputs BiLSTMs features hpos . This is i then fed into the POS classifier Cpos to calculate predictions through: Figure 1: Illustrative scheme of our Hierarchical mult"
2020.socialnlp-1.8,D17-1070,0,0.0330129,"ets for the News and Tweets domains, by unifying the aforementioned task-independent datasets. 2 rates for task-specific parameters. While the second, modify the importance of each task statically or dynamically. e.g. Kiperwasser and Ballesteros (2018) proposed variable schedules that increasingly favour the principal task over batches and Jean et al. (2018) proposed adaptive schedules that vary according to the validation performance of each task during training. Multi-Task Pretraining and Fine-tuning: Multitask pretraining has been especially explored for learning universal representations (Conneau et al., 2017; Ahmad et al., 2018). Multi-task fine-tuning was recently explored to fine-tune BERT pretrained model in a multi-task fashion on multiple tasks (Liu et al., 2019). Furthermore, in term of using multi-task features for domain adaptation, Søgaard and Goldberg (2016) showed the benefit of multi-task learning for domain adaptation from News-domain to Weblogs-domain for CK task, when disposing CK’s supervision only for the source-domain, and lower-level POS supervision for the target-domain. Finally, in terms of unifying multi-task learning and fine-tuning, Kiperwasser and Ballesteros (2018) propo"
2020.socialnlp-1.8,W17-4418,0,0.0209646,"Missing"
2020.socialnlp-1.8,N19-1423,0,0.0132548,"d Multi-Task Learning. In the following, we briefly present the SOTA of each one. Then, we discuss some papers from the literature with a loosely close idea to multi-task pretraining and fine-tuning. Sequential Transfer Learning (STL) is a TL setting performed in two stages: Pretraining and Adaptation. The purpose behind using STL techniques for NLP can be divided into two main research areas, “universal representations” and “domain adaptation”. The former aims to build neural features transferable and beneficial to a wide range of NLP tasks and domains. e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. The second aims to harness the knowledge represented in features learned on a source domain (high-resourced in most cases) to improve learning a target domain (low-resourced in most cases) (Zennaki et al., 2016, 2019). The source and the target problems may differ on the task, the language or the domain. For instance, cross-lingual adaptation has been explored for sentiment analysis (Chen et al., 2016) and cross-domain adaptation has been applied for POS models adaptation from News to Tweets domain (Gui et al., 2017; Meftah et al., 2017, 2018). Our research falls into the second researc"
2020.socialnlp-1.8,N18-1088,0,0.0124463,"377 203,621 - 51,362 - 46,435 204,585 - 25,148 - 25,096 24,753 - 11,742 - 19,112 10,652 - 2,242 - 2,291 62,729 - 15,734 - 23,394 24,753 - 11,742 - 19,112 Table 1: Statistics of the datasets we used to train our multi-task learning models. Top: datasets of the source domain (called “EnglishAll”). Bottom: datasets of the target domain (called “TweetAll”). NER (Sang and De Meulder, 2003); CONLL2000 (Sang et al., 2000) for CK; and finally UD-EnglishEWT (Nivre et al., 2016) for DP. In the same vein, for the target-datasets, we used the Tweets domain and the following datasets: the recent TweeBank (Liu et al., 2018) for POS, annotated with the PTB universal tag-set; WNUT-17 from emerging entity detection shared task (Derczynski et al., 2017) for NER; TChunk (Ritter et al., 2011) for CK; and the data annotated with Universal dependency relations in the TweeBank dataset for DP1 . Detailed statistics of all the datasets are summarised in Table 1. To evaluate our models, we use the accuracy (acc) for POS, Exact-match F12 (Li et al., 2020) for NER and CK and labelled attachment score (LAS) (Nivre et al., 2004). 5.2 of a CNNs-based character-level representations followed by a 2-layer LSTMs. Thus, ELMo with th"
2020.socialnlp-1.8,N19-1249,0,0.0680556,"ck all the tasks following a constant ordering strategy “from lowerlevel to higher-level tasks” (Hashimoto et al., 2017): POS then CK then NER then DP. Thus, every 4 steps, the model sees all the tasks once and learns their corresponding parameters once. 4.2.2 Datasets Unification As mentioned in the previous section, we mostly face the heterogeneous multi-task learning scenario, where only one task-labels might be assigned to a dataset. In that case, the classical multi-task learning approach is not directly applicable, thus we propose to use the “Scheduling process” (Zaremoodi et al., 2018; Lu et al., 2019) (described in the following paragraph). However, since training with different datasets for each task remains difficult (Subramanian et al., 2018) ) we proposed “Dataset Unification” a much simpler and easy to learn method for that scenario. To overcome the intricacy of “tasks scheduling process”, we propose to construct a unified dataset by combining several sources of independent textual annotations. Furthermore, since we are interested in benefiting from pretraining and fine-tuning, we apply unification process on both, source and targetdomains. These datasets contain samples of a broad ra"
2020.socialnlp-1.8,P16-1101,0,0.0112188,"t al., 2018). The processing (training) order of examples from different tasks (datasets), also called Scheduling, is particularly studied in the literature. This could be implicit or explicit (Jean et al., 2018). The formal include, for instance, affecting different learning 3 3.1 Model Architecture Sequence Labelling Architecture Regarding the exact architecture of each task, POS, CK and NER tasks are Sequence Labelling (SL) tasks. Given an input sentence of n successive tokens [w1 , . . . , wn ], SL predicts the tag ci ∈ C of every wi , with C being the tag-set. We followed the literature (Ma and Hovy, 2016; Yang et al., 2018) and used a common SL architecture, including three main components: (i) a Word Representation Extractor (WRE), (ii) a Features Extractor (FE) and (iii) a Classifier (Cl). WRE computes, for each token wi , a word and a character-level biLSTMs encoderbased embeddings (respectively, wei =WE(wi ) and cei =CE(wi )), and concatenates them to get a final representation xi =(wei ,cei ). WRE’s out62 producing four distinct vectors for representing the word: (i) as a dependent seeking its head; (ii), as a head seeking all its dependants; (iii), as a dependent deciding on its relatio"
2020.socialnlp-1.8,K17-3002,0,0.0208397,"d seeking all its dependants; (iii), as a dependent deciding on its relation; and (iv), as a head deciding on the labels of its dependants. These representations are then passed to biAffine softmax classifiers. 3.3 As mentioned above, POS, CK, NER and DP are the four tasks considered in this work. As we aim to learn a multi-task model where the four tasks are learned jointly, the architecture of our model contains a common branch as well as four exits, one per task. Also, as the tasks are hierarchically related to each other, we adopted a hierarchical architecture (similar to Hashimoto et al. (2017) and Sanh et al. (2019)). More specifically, we organised the four tasks from low-level to high-level, with each task being fed with a shared word embedding as well as the outputs of all the lower tasks. To construct that hierarchy of tasks, we followed some linguistic hints from the literature. Indeed, many works have shown that POS improves CK (Yang et al., 2017; Ruder12 et al., 2019); NER benefits from POS (Meftah and Semmar, 2018; Ruder, 2019) and CK (Collobert and Weston, 2008); and DP profits from POS and CK (Hashimoto et al., 2017). In simple terms, POS and CK are considered as “univers"
2020.socialnlp-1.8,J93-2004,0,0.0723342,"selected randomly. Specifically, the base steps of “one task per mini-batch” scheduling process are as follows: 1) picking a mini-batch of samples from only one particular task and 2) updating only the parameters corresponding to the selected task, 5.1 Experiments Domains, Tasks and Datasets As mentioned above, we conducted experiments on four tasks: two low-level tasks (POS and CK) and two higher-level ones: (NER and DP). In terms of domain, data and annotations for the sourcedatasets, we used the standard English domain and chose the following datasets: The WSJ part of PennTree-Bank (PTB) (Marcus et al., 1993) for POS, annotated with the PTB tag-set; CONLL2003 for 65 Task POS: POS Tagging CK: Chunking NER: Named Entity Recognition DP: Dependency Parsing POS: POS Tagging CK: Chunking NER: Named Entity Recognition DP: Dependency Parsing Classes 36 22 4 51 17 18 6 51 Sources WSJ CONLL-2000 CONLL-2003 UD-English-EWT TweeBank TChunk WNUT TweeBank Eval. Metrics Top-1 Acc. Top-1 Exact-match F1. Top-1 Exact-match F1. Top-1 LAS. Top-1 Acc. Top-1 Exact-match F1. Top-1 Exact-match F1. Top-1 LAS. Splits (train - val - test) 912,344 - 131,768 - 129,654 211,727 - n/a - 47,377 203,621 - 51,362 - 46,435 204,585 -"
2020.socialnlp-1.8,L18-1446,1,0.851196,"common branch as well as four exits, one per task. Also, as the tasks are hierarchically related to each other, we adopted a hierarchical architecture (similar to Hashimoto et al. (2017) and Sanh et al. (2019)). More specifically, we organised the four tasks from low-level to high-level, with each task being fed with a shared word embedding as well as the outputs of all the lower tasks. To construct that hierarchy of tasks, we followed some linguistic hints from the literature. Indeed, many works have shown that POS improves CK (Yang et al., 2017; Ruder12 et al., 2019); NER benefits from POS (Meftah and Semmar, 2018; Ruder, 2019) and CK (Collobert and Weston, 2008); and DP profits from POS and CK (Hashimoto et al., 2017). In simple terms, POS and CK are considered as “universal helpers” (Changpinyo et al., 2018). Thus, based on these linguistic hierarchy observations, we feed POS features to CK; then POS and CK features to both NER and DP. An illustration of our multi-task hierarchical model is given in Fig.1. More specifically, WRE is shared across all tasks, its output (namely xi = WREshared (wi )) is fed to all branches. The lower component of the POS tagging branch (FEpos ) is fed with the shared emb"
2020.socialnlp-1.8,W18-3927,1,0.859709,"Missing"
2020.socialnlp-1.8,2020.findings-emnlp.282,0,0.0219462,"Missing"
2020.socialnlp-1.8,D17-1256,0,0.0822203,"ge of NLP tasks and domains. e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. The second aims to harness the knowledge represented in features learned on a source domain (high-resourced in most cases) to improve learning a target domain (low-resourced in most cases) (Zennaki et al., 2016, 2019). The source and the target problems may differ on the task, the language or the domain. For instance, cross-lingual adaptation has been explored for sentiment analysis (Chen et al., 2016) and cross-domain adaptation has been applied for POS models adaptation from News to Tweets domain (Gui et al., 2017; Meftah et al., 2017, 2018). Our research falls into the second research area, since we aim, as the last two works, to transfer the knowledge learned when training on the News-domain to improve the Tweets-domain’s training. Multi-Task Learning (MTL) consists in a joint learning of related tasks and thus leverages training signals generated by different tasks (Caruana, 1997). The advantage of using MTL over independent task learning has been shown in many NLP tasks and applications (Lin et al., 2018). The processing (training) order of examples from different tasks (datasets), also called Sche"
2020.socialnlp-1.8,N19-1416,1,0.907588,"18), consisting • BiAffine (Dozat and Manning, 2016): we report the LAS score for DP reported by Liu et al. (2018). Note that, in addition to wordlevel and character embeddings, which we use in our model to represent words, they use predicted POS labels and lemmas as input. 1 Note that TweeBank dataset is already anonymised. For TChuck and WNUT datasets, we used simple rules to anonymise usernames and URLs. 2 SeqEval package were used to calculate F1 metric. 3 https://allennlp.org/elmo • Flairs (Akbik et al., 2019): For NER, using 66 Method SOTA - BiAffine(Dozat et al., 2017) SOTA - PretRand (Meftah et al., 2019) SOTA - Flairs (Akbik et al., 2019) SOTA - MDMT (Mishra, 2019) SOTA - DA (LSTM) (Gu and Yu, 2020) SOTA - DA (BERTBASE ) (Gu and Yu, 2020) SOTA - DA (BERTLARGE ) (Gu and Yu, 2020) Best SOTA Mono-task Learning Multi-Task Learning ELMosmall ELMolarge Mono-Task Pre-Training∗ Mono-Task Pre-Training Adversarial Pre-Training MuTSPad (best) PreTraining POS (acc) DP (LAS) NER (F1) CK (F1) mNRG n/a n/a n/a n/a n/a n/a n/a n/a Adversarial n/a 94.95 n/a 92.44 n/a n/a n/a 94.95 91.58 91.98 92.51 94.02 n/a 93.33 93.47 77.7 n/a n/a n/a n/a n/a n/a 77.7 67.48 71.16 69.12 69.76 76.92 78.21 77.49 n/a n/a 49.59"
2020.socialnlp-1.8,D17-1206,0,0.14366,"d; (ii), as a head seeking all its dependants; (iii), as a dependent deciding on its relation; and (iv), as a head deciding on the labels of its dependants. These representations are then passed to biAffine softmax classifiers. 3.3 As mentioned above, POS, CK, NER and DP are the four tasks considered in this work. As we aim to learn a multi-task model where the four tasks are learned jointly, the architecture of our model contains a common branch as well as four exits, one per task. Also, as the tasks are hierarchically related to each other, we adopted a hierarchical architecture (similar to Hashimoto et al. (2017) and Sanh et al. (2019)). More specifically, we organised the four tasks from low-level to high-level, with each task being fed with a shared word embedding as well as the outputs of all the lower tasks. To construct that hierarchy of tasks, we followed some linguistic hints from the literature. Indeed, many works have shown that POS improves CK (Yang et al., 2017; Ruder12 et al., 2019); NER benefits from POS (Meftah and Semmar, 2018; Ruder, 2019) and CK (Collobert and Weston, 2008); and DP profits from POS and CK (Hashimoto et al., 2017). In simple terms, POS and CK are considered as “univers"
2020.socialnlp-1.8,L16-1262,0,0.0282557,"Missing"
2020.socialnlp-1.8,Q18-1017,0,0.123387,"ational Linguistics, pages 61–71 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ther for the News-domain or the Tweets-domain. Though, many early works had highlighted the intricacy of multi-task training from heterogeneous datasets (Subramanian et al., 2018). Thus, we propose to build multi-task datasets for the News and Tweets domains, by unifying the aforementioned task-independent datasets. 2 rates for task-specific parameters. While the second, modify the importance of each task statically or dynamically. e.g. Kiperwasser and Ballesteros (2018) proposed variable schedules that increasingly favour the principal task over batches and Jean et al. (2018) proposed adaptive schedules that vary according to the validation performance of each task during training. Multi-Task Pretraining and Fine-tuning: Multitask pretraining has been especially explored for learning universal representations (Conneau et al., 2017; Ahmad et al., 2018). Multi-task fine-tuning was recently explored to fine-tune BERT pretrained model in a multi-task fashion on multiple tasks (Liu et al., 2019). Furthermore, in term of using multi-task features for domain adapta"
2020.socialnlp-1.8,W04-2407,0,0.0316575,"n, for the target-datasets, we used the Tweets domain and the following datasets: the recent TweeBank (Liu et al., 2018) for POS, annotated with the PTB universal tag-set; WNUT-17 from emerging entity detection shared task (Derczynski et al., 2017) for NER; TChunk (Ritter et al., 2011) for CK; and the data annotated with Universal dependency relations in the TweeBank dataset for DP1 . Detailed statistics of all the datasets are summarised in Table 1. To evaluate our models, we use the accuracy (acc) for POS, Exact-match F12 (Li et al., 2020) for NER and CK and labelled attachment score (LAS) (Nivre et al., 2004). 5.2 of a CNNs-based character-level representations followed by a 2-layer LSTMs. Thus, ELMo with the randomly initialised FE and Cl are further trained on the target-domain tasks. Specifically, we run experiments with two ELMo models: 1) ELMosmall : the small pre-trained model (13.6M parameters) on 1 billion word benchmark. 2) ELMolarge : the big pre-trained model (93.6M parameters) on 5.5 billion word benchmark. Supervised pretraining on the source-domain of the network on each task independently then fine-tuning on the same task in the Tweets domain. This method is called Mono-Task Pre-Tra"
2020.socialnlp-1.8,P18-1074,0,0.0200349,"cross-domain adaptation has been applied for POS models adaptation from News to Tweets domain (Gui et al., 2017; Meftah et al., 2017, 2018). Our research falls into the second research area, since we aim, as the last two works, to transfer the knowledge learned when training on the News-domain to improve the Tweets-domain’s training. Multi-Task Learning (MTL) consists in a joint learning of related tasks and thus leverages training signals generated by different tasks (Caruana, 1997). The advantage of using MTL over independent task learning has been shown in many NLP tasks and applications (Lin et al., 2018). The processing (training) order of examples from different tasks (datasets), also called Scheduling, is particularly studied in the literature. This could be implicit or explicit (Jean et al., 2018). The formal include, for instance, affecting different learning 3 3.1 Model Architecture Sequence Labelling Architecture Regarding the exact architecture of each task, POS, CK and NER tasks are Sequence Labelling (SL) tasks. Given an input sentence of n successive tokens [w1 , . . . , wn ], SL predicts the tag ci ∈ C of every wi , with C being the tag-set. We followed the literature (Ma and Hovy,"
2020.socialnlp-1.8,D14-1162,0,0.0821001,"but not with the same order of improvement as for NER, which we mainly attribute to the fact that contextual representations pre-trained on language modelling capture more semantic features. Particularly, we find that DP gains the least from ELMo compared to the other syntactic tasks. Implementation details The hyper-parameters (HP) we used are as follows. For The task-agnostic WRE: The dimensions of character embedding = 50, hidden states of the character-level biLSTM = 100 and word-level embeddings (updated during training) = 300 (these latter are pre-loaded from Glove pre-trained vectors (Pennington et al., 2014) and fine-tuned during training). For Sequence labelling branches: we use a single-layer biLSTM Comparing MuTSPad to baselines: MuTSPad outperforms both TL methods, Multi-Task Learning and Mono-Task Fine-Tuning, on all data-sets, by ∼+26 and ∼+10, respectively, on mNRG (median 67 Method POS DP NER CK w/o unif. w/ source unif. w/ source+target unif. 94.08 94.36 94.53 79.17 79.67 80.12 43.34 43.21 40.65 84.87 85.77 85.71 Table 3: Impact of Datasets Unification on MuTSPad. Normalised Relative Gain; a well suited metric for multi-task (Tamaazousti et al., 2019)). Compared to unsupervised pretraini"
2020.socialnlp-1.8,P19-1441,0,0.0219989,"rtance of each task statically or dynamically. e.g. Kiperwasser and Ballesteros (2018) proposed variable schedules that increasingly favour the principal task over batches and Jean et al. (2018) proposed adaptive schedules that vary according to the validation performance of each task during training. Multi-Task Pretraining and Fine-tuning: Multitask pretraining has been especially explored for learning universal representations (Conneau et al., 2017; Ahmad et al., 2018). Multi-task fine-tuning was recently explored to fine-tune BERT pretrained model in a multi-task fashion on multiple tasks (Liu et al., 2019). Furthermore, in term of using multi-task features for domain adaptation, Søgaard and Goldberg (2016) showed the benefit of multi-task learning for domain adaptation from News-domain to Weblogs-domain for CK task, when disposing CK’s supervision only for the source-domain, and lower-level POS supervision for the target-domain. Finally, in terms of unifying multi-task learning and fine-tuning, Kiperwasser and Ballesteros (2018) proposed to improve machine translation with the help of POS and DEP tasks by scheduling tasks during training, starting with multi-tasking of the principal task with a"
2020.socialnlp-1.8,N18-1202,0,0.0918159,"uential Transfer Learning and Multi-Task Learning. In the following, we briefly present the SOTA of each one. Then, we discuss some papers from the literature with a loosely close idea to multi-task pretraining and fine-tuning. Sequential Transfer Learning (STL) is a TL setting performed in two stages: Pretraining and Adaptation. The purpose behind using STL techniques for NLP can be divided into two main research areas, “universal representations” and “domain adaptation”. The former aims to build neural features transferable and beneficial to a wide range of NLP tasks and domains. e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. The second aims to harness the knowledge represented in features learned on a source domain (high-resourced in most cases) to improve learning a target domain (low-resourced in most cases) (Zennaki et al., 2016, 2019). The source and the target problems may differ on the task, the language or the domain. For instance, cross-lingual adaptation has been explored for sentiment analysis (Chen et al., 2016) and cross-domain adaptation has been applied for POS models adaptation from News to Tweets domain (Gui et al., 2017; Meftah et al., 2017, 2018). Our research f"
2020.socialnlp-1.8,C16-1044,1,0.836303,"er Learning (STL) is a TL setting performed in two stages: Pretraining and Adaptation. The purpose behind using STL techniques for NLP can be divided into two main research areas, “universal representations” and “domain adaptation”. The former aims to build neural features transferable and beneficial to a wide range of NLP tasks and domains. e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. The second aims to harness the knowledge represented in features learned on a source domain (high-resourced in most cases) to improve learning a target domain (low-resourced in most cases) (Zennaki et al., 2016, 2019). The source and the target problems may differ on the task, the language or the domain. For instance, cross-lingual adaptation has been explored for sentiment analysis (Chen et al., 2016) and cross-domain adaptation has been applied for POS models adaptation from News to Tweets domain (Gui et al., 2017; Meftah et al., 2017, 2018). Our research falls into the second research area, since we aim, as the last two works, to transfer the knowledge learned when training on the News-domain to improve the Tweets-domain’s training. Multi-Task Learning (MTL) consists in a joint learning of relate"
2020.socialnlp-1.8,K18-2016,0,0.0216028,"rchitecture. puts [x1 , . . . , xn ] are fed into the FE that outputs a context sensitive representation for each token, consisting of a single biLSTMs layer which iteratively passes through the sentence in both directions. Finally, Cl consists of a fully-connected layer (denoted Ψ) that classifies every given xi following: yˆwi = (C ◦ FE ◦ WRE)(wi ). 3.2 Hierarchical Multi-Task Architecture (1) Dependency Parsing Architecture For the DP branch, a similar procedure is applied, except that, compared to previous tasks, DP is a a harder problem and thus requires a more complex model. We followed Qi et al. (2018) and used their “neural arc-factored graph-based dependency parser”, which is based on the “Deep biAffine parser” (Dozat and Manning, 2016)). Indeed, given an input sentence of n successive tokens [w1 , . . . , wn ], the goal of DP is two folds: 1) identifying, for each wi , its head wj ∈ S. The couple of tokens wi and wj are called the dependent and the head, respectively. Then, 2) predicting the dependency syntactic relation’s class rji ∈ Rdp relating each dependent-head pair, where Rdp being the dependency-relations set. More precisely, for each token wi we predict its out-going labelled ar"
2020.socialnlp-1.8,D11-1141,0,0.00924657,"Statistics of the datasets we used to train our multi-task learning models. Top: datasets of the source domain (called “EnglishAll”). Bottom: datasets of the target domain (called “TweetAll”). NER (Sang and De Meulder, 2003); CONLL2000 (Sang et al., 2000) for CK; and finally UD-EnglishEWT (Nivre et al., 2016) for DP. In the same vein, for the target-datasets, we used the Tweets domain and the following datasets: the recent TweeBank (Liu et al., 2018) for POS, annotated with the PTB universal tag-set; WNUT-17 from emerging entity detection shared task (Derczynski et al., 2017) for NER; TChunk (Ritter et al., 2011) for CK; and the data annotated with Universal dependency relations in the TweeBank dataset for DP1 . Detailed statistics of all the datasets are summarised in Table 1. To evaluate our models, we use the accuracy (acc) for POS, Exact-match F12 (Li et al., 2020) for NER and CK and labelled attachment score (LAS) (Nivre et al., 2004). 5.2 of a CNNs-based character-level representations followed by a 2-layer LSTMs. Thus, ELMo with the randomly initialised FE and Cl are further trained on the target-domain tasks. Specifically, we run experiments with two ELMo models: 1) ELMosmall : the small pre-t"
2020.socialnlp-1.8,N19-5004,0,0.0246369,"four exits, one per task. Also, as the tasks are hierarchically related to each other, we adopted a hierarchical architecture (similar to Hashimoto et al. (2017) and Sanh et al. (2019)). More specifically, we organised the four tasks from low-level to high-level, with each task being fed with a shared word embedding as well as the outputs of all the lower tasks. To construct that hierarchy of tasks, we followed some linguistic hints from the literature. Indeed, many works have shown that POS improves CK (Yang et al., 2017; Ruder12 et al., 2019); NER benefits from POS (Meftah and Semmar, 2018; Ruder, 2019) and CK (Collobert and Weston, 2008); and DP profits from POS and CK (Hashimoto et al., 2017). In simple terms, POS and CK are considered as “universal helpers” (Changpinyo et al., 2018). Thus, based on these linguistic hierarchy observations, we feed POS features to CK; then POS and CK features to both NER and DP. An illustration of our multi-task hierarchical model is given in Fig.1. More specifically, WRE is shared across all tasks, its output (namely xi = WREshared (wi )) is fed to all branches. The lower component of the POS tagging branch (FEpos ) is fed with the shared embedding and aft"
2020.socialnlp-1.8,W03-0419,0,0.582383,"Missing"
2020.socialnlp-1.8,W00-0726,0,0.147912,"cc. Top-1 Exact-match F1. Top-1 Exact-match F1. Top-1 LAS. Top-1 Acc. Top-1 Exact-match F1. Top-1 Exact-match F1. Top-1 LAS. Splits (train - val - test) 912,344 - 131,768 - 129,654 211,727 - n/a - 47,377 203,621 - 51,362 - 46,435 204,585 - 25,148 - 25,096 24,753 - 11,742 - 19,112 10,652 - 2,242 - 2,291 62,729 - 15,734 - 23,394 24,753 - 11,742 - 19,112 Table 1: Statistics of the datasets we used to train our multi-task learning models. Top: datasets of the source domain (called “EnglishAll”). Bottom: datasets of the target domain (called “TweetAll”). NER (Sang and De Meulder, 2003); CONLL2000 (Sang et al., 2000) for CK; and finally UD-EnglishEWT (Nivre et al., 2016) for DP. In the same vein, for the target-datasets, we used the Tweets domain and the following datasets: the recent TweeBank (Liu et al., 2018) for POS, annotated with the PTB universal tag-set; WNUT-17 from emerging entity detection shared task (Derczynski et al., 2017) for NER; TChunk (Ritter et al., 2011) for CK; and the data annotated with Universal dependency relations in the TweeBank dataset for DP1 . Detailed statistics of all the datasets are summarised in Table 1. To evaluate our models, we use the accuracy (acc) for POS, Exact-m"
2020.socialnlp-1.8,P16-2038,0,0.0228586,"sed variable schedules that increasingly favour the principal task over batches and Jean et al. (2018) proposed adaptive schedules that vary according to the validation performance of each task during training. Multi-Task Pretraining and Fine-tuning: Multitask pretraining has been especially explored for learning universal representations (Conneau et al., 2017; Ahmad et al., 2018). Multi-task fine-tuning was recently explored to fine-tune BERT pretrained model in a multi-task fashion on multiple tasks (Liu et al., 2019). Furthermore, in term of using multi-task features for domain adaptation, Søgaard and Goldberg (2016) showed the benefit of multi-task learning for domain adaptation from News-domain to Weblogs-domain for CK task, when disposing CK’s supervision only for the source-domain, and lower-level POS supervision for the target-domain. Finally, in terms of unifying multi-task learning and fine-tuning, Kiperwasser and Ballesteros (2018) proposed to improve machine translation with the help of POS and DEP tasks by scheduling tasks during training, starting with multi-tasking of the principal task with auxiliary lower-level tasks (POS and DEP), and as the training graduates, the model trains only to the"
2020.socialnlp-1.8,C18-1327,0,0.0159704,"rocessing (training) order of examples from different tasks (datasets), also called Scheduling, is particularly studied in the literature. This could be implicit or explicit (Jean et al., 2018). The formal include, for instance, affecting different learning 3 3.1 Model Architecture Sequence Labelling Architecture Regarding the exact architecture of each task, POS, CK and NER tasks are Sequence Labelling (SL) tasks. Given an input sentence of n successive tokens [w1 , . . . , wn ], SL predicts the tag ci ∈ C of every wi , with C being the tag-set. We followed the literature (Ma and Hovy, 2016; Yang et al., 2018) and used a common SL architecture, including three main components: (i) a Word Representation Extractor (WRE), (ii) a Features Extractor (FE) and (iii) a Classifier (Cl). WRE computes, for each token wi , a word and a character-level biLSTMs encoderbased embeddings (respectively, wei =WE(wi ) and cei =CE(wi )), and concatenates them to get a final representation xi =(wei ,cei ). WRE’s out62 producing four distinct vectors for representing the word: (i) as a dependent seeking its head; (ii), as a head seeking all its dependants; (iii), as a dependent deciding on its relation; and (iv), as a he"
2020.socialnlp-1.8,P18-2104,0,0.06227,"ined. We successively pick all the tasks following a constant ordering strategy “from lowerlevel to higher-level tasks” (Hashimoto et al., 2017): POS then CK then NER then DP. Thus, every 4 steps, the model sees all the tasks once and learns their corresponding parameters once. 4.2.2 Datasets Unification As mentioned in the previous section, we mostly face the heterogeneous multi-task learning scenario, where only one task-labels might be assigned to a dataset. In that case, the classical multi-task learning approach is not directly applicable, thus we propose to use the “Scheduling process” (Zaremoodi et al., 2018; Lu et al., 2019) (described in the following paragraph). However, since training with different datasets for each task remains difficult (Subramanian et al., 2018) ) we proposed “Dataset Unification” a much simpler and easy to learn method for that scenario. To overcome the intricacy of “tasks scheduling process”, we propose to construct a unified dataset by combining several sources of independent textual annotations. Furthermore, since we are interested in benefiting from pretraining and fine-tuning, we apply unification process on both, source and targetdomains. These datasets contain sam"
2021.adaptnlp-1.14,W17-4418,0,0.0493384,"Missing"
2021.adaptnlp-1.14,D18-1275,0,0.0345414,"Missing"
2021.adaptnlp-1.14,D17-1256,0,0.0185638,"et al., 2018; Chen et al., 2019; Wang et al., 2019; O’Neill, 2019) that, when source and target domains are less related (e.g. languages from different families), sequential transfer learning may lead to a negative effect on the performance, instead of improving it. This phenomenon is referred to as negative transfer. Precisely, negative transfer is considered when transfer learning is harmful for the target task/dataset, i.e. the performance when using transfer learning algorithm is lower than that with a solely supervised training on in-target data (Torrey and Shavlik, 2010). Several works (Gui et al., 2017, 2018b; Meftah et al., 2018a,b; M¨arz et al., 2019) have shown that sequential transfer learning from the News resource-rich domain to the Tweets low-resource domain enhances the performance of sequence labelling of Tweets. Hence, following the above definition of negative transfer, transfer learning from News to Tweets does not beget a negative transfer. Contrariwise, in this work, we rather consider the hidden negative transfer, i.e. the percentage of predictions which were correctly tagged by random initialisation, but using transfer learning falsified. In this work, we take a step towards"
2021.adaptnlp-1.14,N18-1088,0,0.0173792,"rce model, while the second set of parameters (Ψ) of the target model is randomly initialised. Then, 3) the target model is further fine-tuned on the small target data-set. 4 Experimental Settings 4.1 Data-sets We conduct experiments on TL from English News (source-domain) to English Tweets (target-domain) on three tasks (Datasets statistics are given in Table.1): • POS tagging: we use the Wall Street Journal (WSJ) part of Penn-Tree-Bank (PTB) as a source-dataset. Regarding the target-datasets, we used three Tweets datasets: TPoS (Ritter et al., 2011), ARK (Owoputi et al., 2013) and TweeBank (Liu et al., 2018). • CK: for the source dataset, we use the CONLL2000 shared task’s English data-set (Tjong Kim Sang and Buchholz, 2000). Regarding the target dataset, we use TChunk Tweets data-set (Ritter et al., 2011) (the same corpus as TPoS). • NER: regarding the source domain, we make use of the English newswire dataset CONLL03 from the CONLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). target domain, we conduct our experiments on WNUT2017 dataset (Derczynski et al., 2017). 4.2 Implementation Details In the standard word-level embeddings, tokens are converted to lower-case while the character-l"
2021.adaptnlp-1.14,N19-1345,0,0.0626851,"Missing"
2021.adaptnlp-1.14,L18-1446,1,0.833194,", 2019; Wang et al., 2019; O’Neill, 2019) that, when source and target domains are less related (e.g. languages from different families), sequential transfer learning may lead to a negative effect on the performance, instead of improving it. This phenomenon is referred to as negative transfer. Precisely, negative transfer is considered when transfer learning is harmful for the target task/dataset, i.e. the performance when using transfer learning algorithm is lower than that with a solely supervised training on in-target data (Torrey and Shavlik, 2010). Several works (Gui et al., 2017, 2018b; Meftah et al., 2018a,b; M¨arz et al., 2019) have shown that sequential transfer learning from the News resource-rich domain to the Tweets low-resource domain enhances the performance of sequence labelling of Tweets. Hence, following the above definition of negative transfer, transfer learning from News to Tweets does not beget a negative transfer. Contrariwise, in this work, we rather consider the hidden negative transfer, i.e. the percentage of predictions which were correctly tagged by random initialisation, but using transfer learning falsified. In this work, we take a step towards identifying and analysing t"
2021.adaptnlp-1.14,W18-3927,1,0.829422,", 2019; Wang et al., 2019; O’Neill, 2019) that, when source and target domains are less related (e.g. languages from different families), sequential transfer learning may lead to a negative effect on the performance, instead of improving it. This phenomenon is referred to as negative transfer. Precisely, negative transfer is considered when transfer learning is harmful for the target task/dataset, i.e. the performance when using transfer learning algorithm is lower than that with a solely supervised training on in-target data (Torrey and Shavlik, 2010). Several works (Gui et al., 2017, 2018b; Meftah et al., 2018a,b; M¨arz et al., 2019) have shown that sequential transfer learning from the News resource-rich domain to the Tweets low-resource domain enhances the performance of sequence labelling of Tweets. Hence, following the above definition of negative transfer, transfer learning from News to Tweets does not beget a negative transfer. Contrariwise, in this work, we rather consider the hidden negative transfer, i.e. the percentage of predictions which were correctly tagged by random initialisation, but using transfer learning falsified. In this work, we take a step towards identifying and analysing t"
2021.adaptnlp-1.14,N13-1039,0,0.064314,"Missing"
2021.adaptnlp-1.14,D14-1162,0,0.0818445,"WNUT2017 dataset (Derczynski et al., 2017). 4.2 Implementation Details In the standard word-level embeddings, tokens are converted to lower-case while the character-level component still retains access to the capitalisation information. We set the randomly initialised character embedding dimension at 50, the dimension of hidden states of the character-level biLSTM at 100 and used 300-dimensional word-level embeddings. Word-level embeddings were pre-loaded from publicly available GloVe vectors pre-trained on 42 billions words collected through web crawling and containing 1.9M different words (Pennington et al., 2014). These embeddings are also updated during training. For the FE component, we 141 Task POS: POS Tagging CK: Chunking NER: Named Entity Recognition POS: POS Tagging CK: Chunking NER: Named Entity Recognition Classes 36 22 4 17 18 6 Sources WSJ CONLL-2000 CONLL-2003 TweeBank TChunk WNUT Eval. Metrics Top-1 Acc. Top-1 Exact-match F1. Top-1 Exact-match F1. Top-1 Acc. Top-1 Exact-match F1. Top-1 Exact-match F1. Splits (train - val - test) 912,344 - 131,768 - 129,654 211,727 - n/a - 47,377 203,621 - 51,362 - 46,435 24,753 - 11,742 - 19,112 10,652 - 2,242 - 2,291 62,729 - 15,734 - 23,394 Table 1: Sta"
2021.adaptnlp-1.14,D11-1141,0,0.1434,"Missing"
2021.adaptnlp-1.14,N19-5004,0,0.0269228,"-art neural models on News (formal texts). The last few years have witnessed an escalated interest in studying Transfer Learning (TL) for neural networks to overcome the problem of the lack of annotated data. TL aims at performing a task on a target dataset using features learned from a source dataset (Pan and Yang, 2009). TL has been proven to be effective for a wide range of applications (Zamir et al., 2018; Long et al., 2015; Moon and Carbonell, 2017), especially for low-resourced domains. However, it has been shown in many works in the literature (Rosenstein et al., 2005; Ge et al., 2014; Ruder, 2019; Gui et al., 2018a; Cao et al., 2018; Chen et al., 2019; Wang et al., 2019; O’Neill, 2019) that, when source and target domains are less related (e.g. languages from different families), sequential transfer learning may lead to a negative effect on the performance, instead of improving it. This phenomenon is referred to as negative transfer. Precisely, negative transfer is considered when transfer learning is harmful for the target task/dataset, i.e. the performance when using transfer learning algorithm is lower than that with a solely supervised training on in-target data (Torrey and Shavli"
2021.adaptnlp-1.14,W00-0726,0,0.338545,"et model is further fine-tuned on the small target data-set. 4 Experimental Settings 4.1 Data-sets We conduct experiments on TL from English News (source-domain) to English Tweets (target-domain) on three tasks (Datasets statistics are given in Table.1): • POS tagging: we use the Wall Street Journal (WSJ) part of Penn-Tree-Bank (PTB) as a source-dataset. Regarding the target-datasets, we used three Tweets datasets: TPoS (Ritter et al., 2011), ARK (Owoputi et al., 2013) and TweeBank (Liu et al., 2018). • CK: for the source dataset, we use the CONLL2000 shared task’s English data-set (Tjong Kim Sang and Buchholz, 2000). Regarding the target dataset, we use TChunk Tweets data-set (Ritter et al., 2011) (the same corpus as TPoS). • NER: regarding the source domain, we make use of the English newswire dataset CONLL03 from the CONLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). target domain, we conduct our experiments on WNUT2017 dataset (Derczynski et al., 2017). 4.2 Implementation Details In the standard word-level embeddings, tokens are converted to lower-case while the character-level component still retains access to the capitalisation information. We set the randomly initialised character embedd"
2021.adaptnlp-1.14,W03-0419,0,0.170561,"Missing"
besancon-etal-2010-lima,carreras-etal-2004-freeling,0,\N,Missing
besancon-etal-2010-lima,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
besancon-etal-2010-lima,W99-0301,0,\N,Missing
besancon-etal-2010-lima,W09-1511,1,\N,Missing
besancon-etal-2010-lima,C94-1097,0,\N,Missing
besancon-etal-2010-lima,W05-0705,1,\N,Missing
besancon-etal-2010-lima,R09-1053,1,\N,Missing
besancon-etal-2010-lima,D09-1098,0,\N,Missing
bouamor-etal-2012-identifying,W10-4006,0,\N,Missing
bouamor-etal-2012-identifying,A94-1006,0,\N,Missing
bouamor-etal-2012-identifying,P02-1040,0,\N,Missing
bouamor-etal-2012-identifying,P93-1003,0,\N,Missing
bouamor-etal-2012-identifying,W09-2907,0,\N,Missing
bouamor-etal-2012-identifying,W11-0805,0,\N,Missing
bouamor-etal-2012-identifying,N03-1017,0,\N,Missing
bouamor-etal-2012-identifying,2005.mtsummit-papers.11,0,\N,Missing
bouamor-etal-2012-identifying,2005.mtsummit-posters.11,0,\N,Missing
bouamor-etal-2012-identifying,besancon-etal-2010-lima,1,\N,Missing
bouamor-etal-2012-identifying,vintar-fiser-2008-harvesting,0,\N,Missing
bouamor-etal-2012-identifying,2007.jeptalnrecital-long.37,0,\N,Missing
bouamor-etal-2012-identifying,boulaknadel-etal-2008-multi,0,\N,Missing
bouamor-etal-2012-identifying,2006.amta-papers.11,0,\N,Missing
bouamor-etal-2012-identifying,W11-2107,0,\N,Missing
C16-1044,W13-3520,0,0.0140378,"anguages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language depen"
C16-1044,C04-1053,0,0.255705,"of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these ap"
C16-1044,L16-1662,1,0.797549,"using the function f described as f (w) = arg max(PM 12 (t|w)) t 4 (5) Experiments Our models are evaluated on two labeling tasks: Cross-language Part-Of-speech (POS) tagging and Multilingual Super Sense Tagging (SST). 4.1 Multilingual POS Tagging We applied our method to build RNN POS taggers for four target languages - French, German, Greek and Spanish - with English as the source language. In order to determine the effectiveness of our common words representation described in section 3.2.1, we also investigated the use of state-of-the-art bilingual word embeddings (using MultiVec Toolkit (Bérard et al., 2016)) as input to our RNN. 4.1.1 Dataset For French as a target language, we used a training set of 10, 000 parallel sentences, a validation set of 1000 English sentences, and a test set of 1000 French sentences, all extracted from the ARCADE II English-French corpus (Veronis et al., 2008). The test set is tagged with the French TreeTagger (Schmid, 1995) and then manually checked. For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al"
C16-1044,2012.iwslt-evaluation.13,1,0.821481,".2.1 Dataset SemCor The SemCor (Miller et al., 1993) is a subset of the Brown Corpus (Kucera and Francis, 1979) labeled with the WordNet (Fellbaum, 1998) senses. MultiSemCor The English-Italian MultiSemcor (MSC-IT-1) corpus is a manual translation of the English SemCor to Italian (Bentivogli et al., 2004). As we already mentioned, we are also interested in measuring the impact of the parallel corpus quality on our method. For this we use two translation systems: (a) Google Translate to translate the English SemCor to Italian (MSC-IT-2) and French (MSCFR-2). (b) LIG machine translation system (Besacier et al., 2012) to translate the English SemCor to French (MSC-FR-1). Training corpus The SemCor was labeled with the WordNet synsets. However, because we train models for SST, we convert SemCor synsets annotations to super senses. We learn our models using the four different versions of MSC (MSC-IT-1,2 - MSC-FR-1,2), with modified Semcor on source side. Test Corpus To evaluate our models, we used the SemEval 2013 Task 12 (Multilingual Word Sense Disambiguation) (Navigli et al., 2013) test corpora, which are available in 5 languages (English, French, German, Spanish and Italian) and labeled with BabelNet (Na"
C16-1044,A00-1031,0,0.737789,"present the simple cross-lingual projection method, considered as our baseline in this work. 3.1 Baseline Cross-lingual Annotation Projection We use direct transfer as a baseline system which is similar to the method described in (Yarowsky et al., 2001). First we tag the source side of the parallel corpus using the available supervised tagger. Next, we align words in the parallel corpus to find out corresponding source and target words. Tags are then projected to the (resource-poor) target language. The target language tagger is trained using any machine learning approach (we use TnT tagger (Brants, 2000) in our experiments). 3.2 Proposed Approach We propose a method for learning multilingual sequence labeling tools based on RNN, as it can be seen in Figure 1. In our approach, a parallel or multi-parallel corpus between a resource-rich language and one or many under-resourced languages is used to extract common (multilingual) and agnostic words representations. These representations, which rely on sentence level alignment only, are used with the source side of the parallel/multi-parallel corpus to learn a neural network tagger in the source language. Since a common representation of source and"
C16-1044,W06-2920,0,0.0237114,"OV 78.9 73.0 70.3 68.8 76.1 76.4 77.5 76.6 77.6 77.8 81.5 77.0 81.9 77.1 82.1 78.7 82.8 — 85.4 — 84.8 — Greek All words OOV 77.5 72.8 71.1 65.4 75.7 70.7 77.2 71.0 77.9 75.3 78.3 74.6 79.2 75.0 79.9 78.5 82.5 — 80.4 — — — Spanish All words OOV 80.0 79.7 73.4 62.4 78.8 72.6 80.5 73.1 80.6 74.7 83.6 81.2 84.4 81.7 84.4 81.9 84.2 — 83.3 — 82.6 — Table 1: Token-level POS tagging accuracy for Simple Projection, SRNN using MultiVec bilingual word embeddings as input, RNN5 , Projection+RNN and methods of Das & Petrov (2011), Duong et al (2013) and Gouws & Søgaard (2015). tasks on dependency parsing (Buchholz and Marsi, 2006)). The evaluation metric (per-token accuracy) and the Petrov et al. (2012) universal tagset are used for evaluation. For training, the English (source) sides of the training corpora (ARCADE II and Europarl) and of the validation corpora are tagged with the English TreeTagger toolkit. Using the matching provided by Petrov et al. (2012), we map the TreeTagger and the CoNLL tagsets to the common Universal Tagset. In order to build our baseline unsupervised tagger (based on a Simple Cross-lingual Projection – see section 3.1), we also tag the target side of the training corpus, with tags projected"
C16-1044,W06-1670,0,0.0440975,"nnotations in the target language from sentencebased alignments only. While most NLP researches on RNN have focused on monolingual tasks1 and sequence labeling (Collobert et al., 2011; Graves, 2012), this paper, however, considers the problem of learning multilingual NLP tools using RNN. Contributions In this paper, we investigate the effectiveness of RNN architectures — Simple RNN (SRNN) and Bidirectional RNN (BRNN) — for multilingual sequence labeling tasks without using any word alignment information. Two NLP tasks are considered: Part-Of-Speech (POS) tagging and Super Sense (SST) tagging (Ciaramita and Altun, 2006). Our RNN architectures demonstrate very competitive results on unsupervised training for new target languages. In addition, we show that the integration of 1 Exceptions are the recent propositions on Neural Machine Translation (Cho et al., 2014; Sutskever et al., 2014) This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 450 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 450–460, Osaka, Japan, December 11-17 2016. POS information i"
C16-1044,W99-0613,0,0.0161153,"f-Speech tags) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers. 1 Introduction In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of the"
C16-1044,J81-4005,0,0.659115,"Missing"
C16-1044,P11-1061,0,0.55477,"e finally conclude the paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avo"
C16-1044,P13-2112,0,0.175601,"paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments. The resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-pro"
C16-1044,D12-1001,0,0.0607578,"en source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015)"
C16-1044,J07-3002,0,0.092707,"semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of these approaches. Our work is built upon these previous contributions and observations. We explore the possibility of using Recurrent Neural Networks (RNN) to build multilingual NLP tools for resource-poor languages analysis. The major difference with previous works is that we do not explicitly use word alignment information. Our only assumption is that parallel sentences (source-target) are available and that the source part is annotated. In other words, we try to infer annotations in the target language from sentencebased alignments only. While most NLP researches o"
C16-1044,N15-1157,0,0.0890659,"Missing"
C16-1044,2005.mtsummit-papers.11,0,0.0225251,"use of state-of-the-art bilingual word embeddings (using MultiVec Toolkit (Bérard et al., 2016)) as input to our RNN. 4.1.1 Dataset For French as a target language, we used a training set of 10, 000 parallel sentences, a validation set of 1000 English sentences, and a test set of 1000 French sentences, all extracted from the ARCADE II English-French corpus (Veronis et al., 2008). The test set is tagged with the French TreeTagger (Schmid, 1995) and then manually checked. For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al., 2013). This choice allows us to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015). The train data set contains 65, 000 bi-sentences ; a validation set of 10, 000 bi-sentences is also available. For testing, we use the same test corpora as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015) (bi-sentences from CoNLL shared 3 words which do not have a known vector representation 455 Lang. Model Simple Projection SRNN MultiVec SRNN BRNN BRNN - OO"
C16-1044,W15-1521,0,0.0320977,"are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckstr"
C16-1044,H93-1061,0,0.126466,"e labeling RNN is proposed. Methodology For training our multilingual RNN models, we just need as input a parallel (or multiparallel) corpus between a resource-rich language and one or many under-resourced languages. Such a parallel corpus can be manually obtained (clean corpus) or automatically obtained (noisy corpus). To show the potential of our approach, we investigate two sequence labeling tasks: cross-language POS tagging and multilingual Super Sense Tagging (SST). For the SST task, we measure the impact of the parallel corpus quality with manual or automatic translations of the SemCor (Miller et al., 1993) translated from English into Italian (manually and automatically) and French (automatically). Outline The remainder of the paper is organized as follows. Section 2 reviews related work. Section 3 describes our cross-language annotation projection approaches based on RNN. Section 4 presents the empirical study and associated results. We finally conclude the paper in Section 5. 2 Related Work Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor"
C16-1044,S13-2040,0,0.023609,"le Translate to translate the English SemCor to Italian (MSC-IT-2) and French (MSCFR-2). (b) LIG machine translation system (Besacier et al., 2012) to translate the English SemCor to French (MSC-FR-1). Training corpus The SemCor was labeled with the WordNet synsets. However, because we train models for SST, we convert SemCor synsets annotations to super senses. We learn our models using the four different versions of MSC (MSC-IT-1,2 - MSC-FR-1,2), with modified Semcor on source side. Test Corpus To evaluate our models, we used the SemEval 2013 Task 12 (Multilingual Word Sense Disambiguation) (Navigli et al., 2013) test corpora, which are available in 5 languages (English, French, German, Spanish and Italian) and labeled with BabelNet (Navigli and Ponzetto, 2012) senses. We map BabelNet senses to WordNet synsets, then WordNet synsets are mapped to super senses. 4.2.2 SST Systems Evaluated The goals of our SST experiments are twofold: first, to investigate the effectiveness of using POS information to build multilingual super sense tagger, secondly to measure the impact of the parallel corpus quality (manual or automatic translation) on our RNN models (SRNN, BRNN and our proposed variants). To summarize,"
C16-1044,P00-1056,0,0.170144,"resulting (noisy) annotations are used in conjunction with robust learning algorithms to build cheap unsupervised NLP tools (Padó and Lapata, 2009). This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al., 2004)). Cross-lingual projection requires a parallel corpus and word alignment between source and target languages. Many automatic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the i"
C16-1044,petrov-etal-2012-universal,0,0.343667,"ree new RNN variants to take into account low level (POS) information in a higher level (SST) annotation task. The question addressed here is: at which layer of the RNN this low level information should be included to improve SST performance? As specified in Figure 3, the POS information can be introduced either at input layer or at forward layer (forward and backward layers for BRNN) or at compression layer. In all these RNN variants, the POS of the current word is also represented with a vector (P OS(t)). Its dimension corresponds to the number of POS tags in the tagset (universal tagset of Petrov et al. (2012) is used). We propose one hot vector representation where only one value is set to 1 and corresponds to the index of current tag (all other values are 0). 3.2.3 Network Training The first step in our approach is to train the neural network, given a parallel corpus (training corpus), and a validation corpus (different from train data) in the source language. In typical applications, the source language is a resource-rich language (which already has an efficient tagger or manually tagged resources). Our RNN models are trained by stochastic gradient descent using usual back-propagation and back-p"
C16-1044,C12-1146,0,0.0410307,"Missing"
C16-1044,N12-1052,0,0.0310849,"rst, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckström et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and Søgaard (2015), who use bilingual lexicons, and unlike Luong et al. (2015) who use word alignments between the source and target languages2 o"
C16-1044,N13-1126,0,0.033775,"ic word alignment tools are available, such as GIZA++ which implements IBM models (Och and Ney, 2000). However, the noisy (non perfect) outputs of these methods is a serious limitation for the annotation projection based on word alignments (Fraser and Marcu, 2007). To deal with this limitation, recent studies based on cross-lingual representation learning methods have been proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; Täckström et al., 2013; Luong et al., 2015; Gouws and Søgaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett e"
C16-1044,P12-1068,0,0.125292,"duced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and Søgaard, 2015), cross-language named entity recognition (Täckström et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; Täckström et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and Søgaard (2015), who use bilingual lexicons, and unlike Luong et al. (2015) who use word alignments between the source and target languages2 our common multilingual representation is very agnostic. We use a simple (multilingual) vector representation based on the occurrence of source and target words in a parallel corpus and we let the RNN learn the best internal representations (c"
C16-1044,H01-1035,0,0.477814,"manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers. 1 Introduction In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010). For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001). The key idea of annotation projection can be summarized as follows: through word alignment in parallel text corpora, the annotations are transferred from the source (resource-rich) language to the target (under-resourced) language, and the resulting annotations are used for supervised training in the target language. However, automatic word alignment errors (Fraser and Marcu, 2007) limit the performance of these approaches. Our work is built upon these previous contributions and observations. We explore the possibility of using Recurrent Neural Networks (RNN) to build multilingual NLP tools"
C16-1044,Y15-1016,1,0.732491,"Missing"
C16-1044,2015.jeptalnrecital-court.32,1,0.76448,"Missing"
C16-1044,W14-4012,0,\N,Missing
chiao-etal-2006-evaluation,W05-0809,0,\N,Missing
chiao-etal-2006-evaluation,W03-0301,0,\N,Missing
D13-1046,C02-2020,1,0.915827,"Missing"
D13-1046,hazem-morin-2012-adaptive,0,0.0216078,"Missing"
D13-1046,C10-2055,0,0.354564,"Missing"
D13-1046,C10-1070,0,0.153346,"Missing"
D13-1046,W11-1205,0,0.0354002,"Missing"
D13-1046,J03-1002,0,0.00653281,"Missing"
D13-1046,2009.mtsummit-posters.14,0,0.445836,"Missing"
D13-1046,P95-1050,0,0.318963,"Missing"
D13-1046,P99-1067,0,0.574046,"Missing"
F12-2010,besancon-etal-2010-lima,1,0.794393,"Missing"
F12-2010,P97-1017,0,0.225241,"Missing"
F12-2010,P07-2045,0,0.0050176,"Missing"
F12-2010,D07-1091,0,0.0994484,"Missing"
F12-2010,W03-0301,0,0.111395,"Missing"
F12-2010,C04-1089,0,0.0790397,"Missing"
F12-2010,P07-1109,0,0.0743176,"Missing"
F12-2010,W98-1005,0,0.188972,"Missing"
F12-2010,W06-1630,0,0.0806565,"Missing"
F12-2010,P02-1051,0,0.0789045,"Missing"
F13-1024,C02-1166,0,0.0918764,"Missing"
F13-1024,P95-1032,0,0.357061,"Missing"
F13-1024,2007.mtsummit-papers.26,0,0.108017,"Missing"
F13-1024,P04-1067,0,0.0620838,"Missing"
F13-1024,hazem-morin-2012-adaptive,0,0.0346641,"Missing"
F13-1024,2009.mtsummit-posters.26,0,0.107232,"Missing"
F13-1024,C10-1070,0,0.0410905,"Missing"
F13-1024,C10-1073,0,0.0437096,"Missing"
F13-1024,W11-1205,0,0.0411106,"Missing"
F13-1024,2009.mtsummit-posters.14,0,0.0667486,"Missing"
F13-1024,P95-1050,0,0.288959,"Missing"
F13-1024,2010.jeptalnrecital-demonstration.6,0,0.11305,"Missing"
F13-1024,E12-1046,0,0.0262621,"Missing"
F14-1024,P02-1051,0,0.159642,"Missing"
F14-1024,besancon-etal-2010-lima,1,0.765885,"Missing"
F14-1024,bouamor-etal-2012-identifying,1,0.885843,"Missing"
F14-1024,J93-2003,0,0.0329818,"Missing"
F14-1024,C94-1084,0,0.369114,"Missing"
F14-1024,P08-4006,0,0.0521838,"Missing"
F14-1024,N04-1036,0,0.0787065,"Missing"
F14-1024,W03-1502,0,0.0723729,"Missing"
F14-1024,E09-2008,0,0.0294639,"Missing"
F14-1024,P07-2045,0,0.00479922,"Missing"
F14-1024,W04-3250,0,0.260247,"Missing"
F14-1024,P97-1017,0,0.334374,"Missing"
F14-1024,2005.mtsummit-papers.40,0,0.0894582,"Missing"
F14-1024,N03-2016,0,0.0703922,"Missing"
F14-1024,E09-1057,0,0.0606899,"Missing"
F14-1024,W03-0301,0,0.131217,"Missing"
F14-1024,P00-1056,0,0.391564,"Missing"
F14-1024,W10-4006,0,0.0347907,"Missing"
F14-1024,P02-1040,0,0.0890168,"Missing"
F14-1024,N09-4005,0,0.026047,"Missing"
F14-1024,P06-1001,0,0.0276607,"Missing"
F14-1024,2010.tc-1.14,1,0.898854,"Missing"
F14-1024,2007.jeptalnrecital-long.37,0,0.0524601,"Missing"
F14-1024,C04-1089,0,0.0881846,"Missing"
F14-1024,P07-1109,0,0.060834,"Missing"
F14-1024,W98-1005,0,0.197337,"Missing"
F14-1024,W06-1630,0,0.0529182,"Missing"
F14-1024,vintar-fiser-2008-harvesting,0,0.0713986,"Missing"
I13-1125,2009.mtsummit-posters.14,0,0.0207206,"the standard approach builds and compares context vectors for each word of the source and target languages. A particularity of this approach is that, to enable the comparison of context vectors, it requires the existence of a seed bilingual dictionary to translate source context vectors. The use of the bilingual dictionary is problematic when a word has several translations, whether they are synonymous or 2 Related Work Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) 952 International Joint Conference on Natural Language Processing, pages 952–956, Nagoya, Japan, 14-18 October 2013. Once translated into the target language, the context vectors disambiguation process intervenes. This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words. For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polysemous word’s translations senses. We hypothesize that a word is monosemic if it is associated to only one entry in the bilingual dictionary. We checked this assump"
I13-1125,P95-1050,0,0.200321,"ext vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The main work in this research area could be seen as an extension of Harris’s distributional hypothesis (Harris, 1954). It is based on the simple observation that a word and its translation are likely to appear in similar contexts across languages (Rapp, 1995). Based on this assumption, the alignment method, known as the standard approach builds and compares context vectors for each word of the source and target languages. A particularity of this approach is that, to enable the comparison of context vectors, it requires the existence of a seed bilingual dictiona"
I13-1125,P04-1067,0,0.082163,"Missing"
I13-1125,hazem-morin-2012-adaptive,0,0.0165341,"t prominent translations of each polysemous unit wp , an average similarity is computed for each translation wpj of wp : PN SemSim (wi , wpj ) j Ave Sim(wp ) = i=1 (3) N 953 Corpus Corpus French 396, 524 Romanian 22,539 English 524, 805 English 322,507 The resulting bilingual dictionary contains about 136,681 entries for Romanian-English with an average of 1 translation per word. 4.1.3 Evaluation list In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are usually composed of about 100 single terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created a reference list3 for each pair of language. The FrenchEnglish list contains 96 terms extracted from the French-English M E SH and the UMLS thesauri4 . The Romanian-English reference list was created by a native speaker and contains 38 pair of words. Note that reference terms pairs appear at least five times in each part of both comparable corpora. Table 1: Comparable corpora sizes in term of words. where N is the total number of monosemic words and SemSim is the similarity value of wpj and the ith monosemic word. Hence, according to average rel"
I13-1125,C10-1070,0,0.0173584,"ord. Hence, according to average relatedness values Ave Sim(wpj ), we obtain for each polysemous word wp an ordered list of translations wp1 . . . wpn . This allows us to select translations of words which are more salient than the others to represent the word to be translated. 4 Experiments and Results 4.2 4.1 Resources 4.1.1 Three other parameters need to be set up: (1) the window size, (2) the association measure and the (3) similarity measure. To define context vectors, we use a seven-word window as it approximates syntactic dependencies. Concerning the rest of the parameters, we followed Laroche and Langlais (2010) for their definition. The authors carried out a complete study of the influence of these parameters on the bilingual alignment and showed that the most effective configuration is to combine the Discounted Log-Odds ratio (equation 4) with the cosine similarity. The Discounted Log-Odds ratio is defined as follows: Comparable corpora We conducted our experiments on two FrenchEnglish and Romanian-English comparable corpora specialized on the breast cancer domain. Both corpora were extracted from Wikipedia1 . We consider the topic in the source language (for instance cancer du sein [breast cancer]"
I13-1125,C10-1073,0,0.0420861,"Missing"
I13-1125,C02-2020,1,\N,Missing
I13-1125,P94-1019,0,\N,Missing
I13-1139,P02-1051,0,0.338135,"Missing"
I13-1139,besancon-etal-2010-lima,1,0.844189,"Missing"
I13-1139,C94-1084,0,0.452931,"Missing"
I13-1139,P97-1017,0,0.296101,"Missing"
I13-1139,W04-3250,0,0.149914,"Missing"
I13-1139,W04-2207,0,0.026096,"ords alignment The word &quot;Kosovo&quot; was aligned using cognates matching after transliteration, the words &quot;condemn&quot;, &quot;human&quot;, &quot;affect&quot; and “group” were aligned using grammatical categories matching and the other single words exist in the EnglishArabic lexicon. The compound words “violation_human_right”, “َن2&quot;ْ _إ:َ;_”ِإ ْ&quot; ِ! َك, “human_right”, “َن2&quot;ْ _إ:َ;”, “ethnic_group” and “'<(ِ َ َ _'َ* ِ ” are first recognized by LIMA respectively from the source sentence and the target sentence, and then aligned using lexical and syntactic transfer rules between source and target languages (Ozdowska, 2004). 1024 5 Experimental Results and Evaluation The impact of using transliteration of proper names on the quality of alignment and machine translation has been evaluated according to the two following approaches: • A manual evaluation comparing the results of our word aligner with a reference alignment; • An automatic evaluation by integrating the results of our word aligner tool in the training corpus used to build the translation table of the statistical MT system Moses (Koehn et al., 2007). In order to evaluate the alignment quality manually, we used 500 English-Arabic aligned sentences extra"
I13-1139,2012.amta-caas14.6,1,0.75145,"Missing"
I13-1139,P11-1044,0,0.0358204,"Missing"
I13-1139,P07-1109,0,0.0688166,"Missing"
I13-1139,2010.tc-1.14,1,0.909548,"Missing"
I13-1139,W98-1005,0,0.199454,"Missing"
I13-1139,W06-1630,0,0.0673805,"Missing"
I13-1139,C04-1089,0,\N,Missing
I13-1139,P07-2045,0,\N,Missing
I13-1139,W03-0301,0,\N,Missing
L18-1047,W02-2001,0,0.126939,"Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical information. Tufis and Ion (2007) introduce a linguistic approach in which they claim that MWEs keep in most cases the same morpho-syntactic structure in the source and target languages. Statistical approaches also have proven to be useful in collecting bilingual MWEs"
L18-1047,bouamor-etal-2012-identifying,1,0.96447,"cast as an integer linear program. Marchand and Semmar (2011) used an approach which followed to some extent that of DeNero and Klein (2008) while they added two scoring functions based on co-occurrence and a seed single word bilingual dictionary. The second approach for extracting bilingual MWEs from parallel corpora, firstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cru"
L18-1047,boulaknadel-etal-2008-multi,0,0.0147257,"corpora. Kupiec (1993) introduced the use of machine learning algorithms such as the Expectation Maximization (EM) to extract MWEs. Similarly, Vintar and Fiser (2008) proposed to extract bilingual MWEs by translating MWEs from a well-known language (English) to a low resource language (Slovene) by using machine translation. They have shown that their translation-based approach performs better than using linguistic approaches. However, they did not combine these two kind of approaches. The combination of such approaches enables to extract finer MWEs. In this way, Wu and Chang (2004) and later Boulaknadel et al. (2008), proposed to use syntactic and statistical analysis to extract bilingual MWEs from a parallel corpus. The main aspect of their approach is a monolingual parsing to extract MWEs combined with statistical detection in each language, then, they confront candidates from each side to find bilingual MWEs. Other approaches proposed to use machine translation to translate MWEs candidates found with a syntactic analysis (Seretan and Wehrli, 2007). 4. Building Bilingual Lexicons of MWEs The process of building MWEs bilingual lexicons from parallel corpora is composed of the following two steps: 1. MWEs"
L18-1047,A94-1006,0,0.545797,"rocessed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical information. Tufis and Ion (2007) introduce a linguistic approach in which they claim that MWEs keep in most cases the same morpho-syntactic structure in the source and target languages. Statistical approaches also have proven to be useful in collecting bilingual MWEs from parallel corpora. Kupiec (1993) introduced the use of machine learning algorith"
L18-1047,C94-1084,0,0.682651,"d be collocations, compound words, named entities, idioms, etc. They constitute an important part of the lexicon of any natural language (Jackendoff, 1997). Bilingual lexicons of MWEs play a vital role in Machine Translation (MT) and CrossLanguage Information Retrieval (CLIR) because for a specific domain the specialized vocabulary is largely dominated by MWEs. The manual construction of these lexicons is costly and time-consuming. Word alignment approaches are often used to automatically construct bilingual lexicons from parallel corpora. Several word alignment approaches have been explored (Daille et al., 1994; Barbu, 2004) and many automatic word alignment tools are available, such as Giza++ (Och and Ney, 2000). However, most of these tools are efficient only to align single words (Fraser and Marcu, 2007). In this paper, we describe and evaluate a hybrid approach to automatically extract and align MWEs from an English-French parallel corpus. In contrast to traditional approaches for MWEs alignment which consist in firstly identifying monolingual MWEs candidates and secondly applying alignment to find bilingual correspondences, our approach extracts and aligns MWEs in a one-step process. The remain"
L18-1047,H05-1022,0,0.130651,"Missing"
L18-1047,P08-4006,0,0.0256427,"7). 5.1 Manual Evaluation Our hybrid approach for MWEs alignment and the baseline Giza++ (Och and Ney, 2000) have been evaluated using the evaluation metrics defined in (Mihalcea et al., 2003). The http://catalog.elra.info/product_info.php?products_id=666. 314 corpus used to evaluate the performance of the EnglishFrench MWE aligners is composed of a set of 1992 parallel sentences extracted from Europarl (European Parliament Proceedings). This parallel corpus is composed of 46265 English words and 49332 French words and has been used to build manually the reference alignment by the Yawat tool (Germann, 2008). Alignment with Giza++ was achieved in source–target and target–source directions and the results were merged using the union heuristic. At first glance, we can see that the combination of the scoring using co-occurrence and the scoring based on the bilingual dictionary with the filtering patterns provides the best performance of our MWEs alignment approach. It clearly appears that keeping only MWEs candidates that have equivalent morpho-syntactic patterns in source and target languages has had a significant impact on the precision of the alignment. This filtering step certainly has improved"
L18-1047,P07-2045,0,0.00685756,"Noun-Noun-Adj-Adj Adj-Noun-Noun-Noun Adj-Adj-Noun-Noun Table 1: Some English and French filtering morphosyntactic patterns (Adj refers to an Adjective, Prep to a Preposition, and Det to a Determiner). 5. Experimental Results The quality of alignment of MWEs and the impact of using MWEs on machine translation have been evaluated, firstly, manually, by comparing the results of our approach with a reference alignment; and secondly automatically by using the results of our MWEs alignment approach to build the translation model of the state-of-the-art statistical machine translation system Moses (Koehn et al., 2007). 5.1 Manual Evaluation Our hybrid approach for MWEs alignment and the baseline Giza++ (Och and Ney, 2000) have been evaluated using the evaluation metrics defined in (Mihalcea et al., 2003). The http://catalog.elra.info/product_info.php?products_id=666. 314 corpus used to evaluate the performance of the EnglishFrench MWE aligners is composed of a set of 1992 parallel sentences extracted from Europarl (European Parliament Proceedings). This parallel corpus is composed of 46265 English words and 49332 French words and has been used to build manually the reference alignment by the Yawat tool (Ge"
L18-1047,D07-1091,0,0.0779345,"y être décidé” is abandonned after the step of filtering because no morpho-syntactic pattern matches this expression. 5.2 Evaluation through a Translation Task The unavailability of a reference alignment of a significant size for MWEs does not allow us to achieve a large scale evaluation. That’s why we considered evaluating the impact of MWEs on the quality of translation by integrating the results of our MWEs alignment approach in the training corpus used to extract the translation model of the phrase based statistical machine translation system Moses. We used the factored translation model (Koehn and Hoang, 2007) as our baseline system. It is an extension of the phrase-based model which enables the use of additional linguistic information at the word level such as morphology and Part-Of-Speech. Note that in Moses translation models are produced by the word alignment tool Giza++. The factored translation model operates on lemmas instead of surface forms. The translation process is then broken up into the following mapping steps: 1. Translate the lemmas of the source language into lemmas in the target language. 2. Generate surface forms given the lemma and linguistic information (Morphology and Part-OfS"
L18-1047,P93-1003,0,0.671917,"da Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical information. Tufis and Ion (2007) introduce a linguistic approach in which they claim that MWEs keep in most cases the same morpho-syntactic structure in the source and target languages. Statistical approaches also have proven to be useful in collecting bilingual MWEs from parallel corpora. Kupiec (1993) introduced the use of machine learning algorithms such as the Expectation Maximization (EM) to extract MWEs. Similarly, Vintar and Fiser (2008) proposed to extract bilingual MWEs by translating MWEs from a well-known language (English) to a low resource language (Slovene) by using machine translation. They have shown that their translation-based approach performs better than using linguistic approaches. However, they did not combine these two kind of approaches. The combination of such approaches enables to extract finer MWEs. In this way, Wu and Chang (2004) and later Boulaknadel et al. (200"
L18-1047,E09-1057,0,0.0291597,"mal alignment between bilingual MWEs can be cast as an integer linear program. Marchand and Semmar (2011) used an approach which followed to some extent that of DeNero and Klein (2008) while they added two scoring functions based on co-occurrence and a seed single word bilingual dictionary. The second approach for extracting bilingual MWEs from parallel corpora, firstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties"
L18-1047,D08-1084,0,0.0403048,"roblem of finding an optimal alignment between bilingual MWEs can be cast as an integer linear program. Marchand and Semmar (2011) used an approach which followed to some extent that of DeNero and Klein (2008) while they added two scoring functions based on co-occurrence and a seed single word bilingual dictionary. The second approach for extracting bilingual MWEs from parallel corpora, firstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic a"
L18-1047,W03-0301,0,0.223822,"Missing"
L18-1047,P00-1056,0,0.677791,"Missing"
L18-1047,W10-4006,0,0.0261934,"ed two scoring functions based on co-occurrence and a seed single word bilingual dictionary. The second approach for extracting bilingual MWEs from parallel corpora, firstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extrac"
L18-1047,P02-1040,0,0.118972,"ce occurrence occurrence + Bilingual + Bilingual dictionary dictionary + Filtering patterns 32.69 32.71 32.72 33.88 33.89 33.91 34.30 34.32 34.33 22.96 23.30 24.55 Out-Of-Domain (Emea) CoCoCooccurrence occurrence occurrence + Bilingual + Bilingual dictionary dictionary + Filtering patterns 23.03 23.06 23.07 23.37 23.39 23.41 24.59 24.62 24.63 Table 5: BLEU scores of Moses for Out-Of-Domain texts. Table 3: Corpora details used to train Moses language and translation models (K refers to 1000) 5.2.2 Results and Discussion The performance of the SMT system Moses is evaluated using the BLEU score (Papineni et al., 2002) on the two test sets for the three runs described in the previous section. Note that we consider only one reference per sentence. The obtained results are reported in tables 4 and 5. As shown in tables 4 and 5, for In-Domain texts, Moses achieves a relatively high BLEU score and the scores of Moses when using the results of our MWEs alignment approach are better than those when we use the baseline (Giza++) in all the runs. Again, the best performance for both In-Domain and Out-Of-Domain texts is achieved using the combination of the scoring using co-occurrence and the scoring based on the bil"
L18-1047,pearce-2002-comparative,0,0.129352,"ngual MWEs from parallel corpora, firstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs"
L18-1047,2010.tc-1.14,1,0.870863,"Missing"
L18-1047,semmar-laib-2017-building,1,0.820522,"e machine translation to translate MWEs candidates found with a syntactic analysis (Seretan and Wehrli, 2007). 4. Building Bilingual Lexicons of MWEs The process of building MWEs bilingual lexicons from parallel corpora is composed of the following two steps: 1. MWEs extraction and alignment using scoring functions. 2. MWEs candidates filtering using morphosyntactic patterns. 4.1 Extraction and Alignment of MWEs In this section, we describe our approach to extract and align MWEs from an English-French parallel corpus in a one-step process (Marchand and Semmar, 2011; Semmar and Marchand, 2017; Semmar and Laib, 2017). This approach is hybrid because it considers the global task of identification and alignment of MWEs as an optimization 312 problem and it uses external linguistic resources: a seed single word bilingual dictionary and morpho-syntactic patterns. It handles MWEs which are composed of contiguous units. As the only restriction we made is the contiguity of MWEs, the alignment task is a NP-hard problem. We formalize, then, the alignment task as an integer linear programming problem to find an approximated optimal solution (DeNero and Klein, 2008; Marchand and Semmar, 2011). In this formalization,"
L18-1047,2007.jeptalnrecital-long.37,0,0.0268224,"ey did not combine these two kind of approaches. The combination of such approaches enables to extract finer MWEs. In this way, Wu and Chang (2004) and later Boulaknadel et al. (2008), proposed to use syntactic and statistical analysis to extract bilingual MWEs from a parallel corpus. The main aspect of their approach is a monolingual parsing to extract MWEs combined with statistical detection in each language, then, they confront candidates from each side to find bilingual MWEs. Other approaches proposed to use machine translation to translate MWEs candidates found with a syntactic analysis (Seretan and Wehrli, 2007). 4. Building Bilingual Lexicons of MWEs The process of building MWEs bilingual lexicons from parallel corpora is composed of the following two steps: 1. MWEs extraction and alignment using scoring functions. 2. MWEs candidates filtering using morphosyntactic patterns. 4.1 Extraction and Alignment of MWEs In this section, we describe our approach to extract and align MWEs from an English-French parallel corpus in a one-step process (Marchand and Semmar, 2011; Semmar and Marchand, 2017; Semmar and Laib, 2017). This approach is hybrid because it considers the global task of identification and al"
L18-1047,tiedemann-2012-parallel,0,0.0454028,"omatically using Giza++. In order to integrate into Moses the bilingual lexicon which is extracted automatically by our MWEs alignment approach, we add the extracted bilingual lexicon as a parallel corpus and retrain the translation model. 5.2.1 Data and Experimental Setup In order to study the impact of the bilingual lexicon of MWEs on the performance of Moses, we conducted our experiments on two English-French parallel corpora (Table 3): Europarl (European Parliament Proceedings) and Emea (European Medicines Agency Documents). These corpora were extracted from the open parallel corpus OPUS (Tiedemann, 2012). We achieved three runs and two test experiments for each run: In-Domain and Out-Of-Domain. For this, we randomly extracted 500 parallel sentences rom 315 Europarl as an In-Domain corpus and 500 pairs of sentences from Emea as Out-Of-Domain corpus. The domain vocabulary is represented in the case of the baseline (Giza++) by the specialized parallel corpus Emea which is added to the training data (Europarl). For our MWEs alignment approach, the domain vocabulary corresponds to the bilingual lexicon of MWEs extracted from the specialized corpus. This bilingual lexicon of MWEs is added to the tr"
L18-1047,W07-1104,0,0.0729235,"Missing"
L18-1047,D07-1110,0,0.0400527,"monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical information. Tufis and Ion"
L18-1047,vintar-fiser-2008-harvesting,0,0.0373755,"extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical information. Tufis and Ion (2007) introduce a linguistic approach in which they claim that MWEs keep in most cases the same morpho-syntactic structure in the source and target languages. Statistical approaches also have proven to be useful in collecting bilingual MWEs from parallel corpora. Kupiec (1993) introduced the use of machine learning algorithms such as the Expectation Maximization (EM) to extract MWEs. Similarly, Vintar and Fiser (2008) proposed to extract bilingual MWEs by translating MWEs from a well-known language (English) to a low resource language (Slovene) by using machine translation. They have shown that their translation-based approach performs better than using linguistic approaches. However, they did not combine these two kind of approaches. The combination of such approaches enables to extract finer MWEs. In this way, Wu and Chang (2004) and later Boulaknadel et al. (2008), proposed to use syntactic and statistical analysis to extract bilingual MWEs from a parallel corpus. The main aspect of their approach is a"
L18-1047,O04-2001,0,0.066822,"g bilingual MWEs from parallel corpora. Kupiec (1993) introduced the use of machine learning algorithms such as the Expectation Maximization (EM) to extract MWEs. Similarly, Vintar and Fiser (2008) proposed to extract bilingual MWEs by translating MWEs from a well-known language (English) to a low resource language (Slovene) by using machine translation. They have shown that their translation-based approach performs better than using linguistic approaches. However, they did not combine these two kind of approaches. The combination of such approaches enables to extract finer MWEs. In this way, Wu and Chang (2004) and later Boulaknadel et al. (2008), proposed to use syntactic and statistical analysis to extract bilingual MWEs from a parallel corpus. The main aspect of their approach is a monolingual parsing to extract MWEs combined with statistical detection in each language, then, they confront candidates from each side to find bilingual MWEs. Other approaches proposed to use machine translation to translate MWEs candidates found with a syntactic analysis (Seretan and Wehrli, 2007). 4. Building Bilingual Lexicons of MWEs The process of building MWEs bilingual lexicons from parallel corpora is composed"
L18-1047,W06-1206,0,0.0386283,"irstly, identifies monolingual MWEs candidates and then applies alignment techniques to find bilingual correspondences (Daille et al., 1994; Blank 2000; Barbu 2004; Deng et al., 2005; Samuelsson et al., 2007; MacCartney et al., 2008; Lefever et al., 2009; Semmar et al., 2011; Bouamor et al., 2012). In the second approach, MWEs extraction can be processed by using symbolic methods based on linguistic patterns (Dagan et al., 1994; Okita et al., 2010; Bouamor et al., 2012), or, through statistical approaches which use automatic measures to rank MWEs candidates (Pearce 2002; Evert and Krenn 2005; Zhang et al. 2006; Villavicencio et al. 2007; Vintar et al., 2008). Finally, MWEs extraction can be done by using hybrid approaches, which combine statistical information with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio 2002; Van de Cruys and Villada Moiron 2007; Caseli et al., 2010). Dagan and Church (1994) proposed to use syntactic analysis to extract terminology. MWEs are then extracted by grouping linguistically related terms. In the same way, Okita et al. (2010) proposed to link across two languages MWEs according to their syntactic and lexical"
L18-1446,W16-3905,0,0.0883696,"and syntactic information of words, each word from the input sequence is represented by a combination of two vectors of features, character-level and word-level embedding. Therefore, each word in the input sentence is represented by a combination of two vectors: 1. Pre-trained words embedding: We initialize wordlevel embedding with a concatenation of different preretrained words embedding (details in section 6.3.) to accurately capture words’ semantics. 2. Character level embedding: To learn orthographic features at the character level, we use a CNN architecture similar to that of Ma and Hovy (2016). As illustrated in figure 1. Each word is represented with a v × l dimensional matrix, next it’s embedded into a d × l dimensional matrix, where v is character’s vocabulary size, l is the maximal length of words and d is character embedding’s dimension. Then, we take the character embeddings and apply (30 × 3)-stacked convolutional layers, followed by a max-pooling operation. Finally, the result is passed to a fully-connected layer using a Rectifier Linear Unit (ReLU) activation function. 1 The model’s architecture is the same among all datasets and tasks. 2822 Figure 1: Convolutional Neural"
L18-1446,N16-1031,0,0.0574937,"Missing"
L18-1446,Y09-1013,0,0.0806624,"Missing"
L18-1446,R13-1026,0,0.71503,"er-level NLP applications such as semantic relations extraction, sentiment analysis, automatic summarization and machine translation. Most performing traditional POS tagging models for social media content are linear statistical models, including Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMMs), Conditional Random Fields (CRF) and linear classifiers like SVM-based taggers. There are two principal state-of-the-art works for English tweets POS tagging, both based on hand-crafted features, Ritter et al. (2011) published a set of 787 hand-annotated English tweets and proposed in (Derczynski et al., 2013) a model based on hidden Markov Models and a set of normalization rules, external dictionaries and lexical features. Gimpel et al. (2011) and Owoputi et al. (2013) constructed 1827 and 547 hand-annotated tweets, respectively, using the same tag-set. They proposed a model based on Firstorder maximum entropy Markov model (MEMM), engineered features like brown clustering and lexical features. Nooralahzadeh et al. (2014) proposed a POS tagging system for French Social Media content using Conditional random fields (CRFs) with a set of several hand-crafted features. These models rely heavily on hand"
L18-1446,P11-2008,0,0.255867,"Missing"
L18-1446,D17-1256,0,0.793665,"chine translation (Zoph et al., 2016). Two studies have been recently performed on TL for neural networks based models in sequence labeling: Yang et al. (2017) examined the effects of TL for deep hierarchical recurrent networks across domains, applications, and languages, and showed that significant improvement can be obtained. Lee et al. (2017) used cross-domain TL for Named Entity Recognition (NER) (specifically patient note de-identification), and showed that TL may be especially beneficial for a target dataset with small number of examples. 2.2. to the former works. The model proposed in (Gui et al., 2017) requires that labeled in-domain-data and labeled outof-domain data share the same tag-set (a mapping is necessary in case of tag-sets mismatch). 3. This work is built on the basis of the recently published paper (Meftah et al., 2017), where cross-domain TL was successfully used for English tweets POS tagging by exploiting available huge amounts of POS labeled corpora of a similar domain (standard English). The knowledge learned on the parent neural network trained on enough standard English labeled data was transferred to initialize the child network, further fine-tuned on small annotated Eng"
L18-1446,P16-1101,0,0.067697,"th semantic and syntactic information of words, each word from the input sequence is represented by a combination of two vectors of features, character-level and word-level embedding. Therefore, each word in the input sentence is represented by a combination of two vectors: 1. Pre-trained words embedding: We initialize wordlevel embedding with a concatenation of different preretrained words embedding (details in section 6.3.) to accurately capture words’ semantics. 2. Character level embedding: To learn orthographic features at the character level, we use a CNN architecture similar to that of Ma and Hovy (2016). As illustrated in figure 1. Each word is represented with a v × l dimensional matrix, next it’s embedded into a d × l dimensional matrix, where v is character’s vocabulary size, l is the maximal length of words and d is character embedding’s dimension. Then, we take the character embeddings and apply (30 × 3)-stacked convolutional layers, followed by a max-pooling operation. Finally, the result is passed to a fully-connected layer using a Rectifier Linear Unit (ReLU) activation function. 1 The model’s architecture is the same among all datasets and tasks. 2822 Figure 1: Convolutional Neural"
L18-1446,J93-2004,0,0.0752613,"h language, on which we perform our evaluations. The statistics of the datasets2 are described in table 1. 6.1.1. English As a source dataset for English experiments, we use a standard English corpus, the Wall Street Journal (WSJ) part of the PTB, annotated with 36 POS tags. We evaluate our approach on three target datasets. The NPS IRC Chat Corpus (Forsythand and Martell, 2007) of 10,567 posts gathered from various online chat services. And two Twitter datasets: • The T-PoS corpus of 787 hand-annotated English tweets, introduced by (Ritter et al., 2011), which uses the same tag-set as PTB’s (Marcus et al., 1993), plus four Twitter special tags: URL for urls, HT for hashtags, USR for username mentions and RT for retweet signifier (40 tags in total). For our experiments on TPoS, we use the same data splits used in (Derczynski et al., 2013); 70:15:15 into training, development and test sets named T-train, T-dev and T-eval. • The ARK corpus was published on two parts, the first, Oct27 of 1827 hand-annotated English tweets, published in (Gimpel et al., 2011) and the second, Daily547 of 547 tweets published by Owoputi et al. (2013), using a novel and coarse grained tag-set (25 tags). For example, its V tag"
L18-1446,D16-1046,0,0.0613431,"ach by a jointly training of source and target tasks (NER and POS). The training procedure for cross-task TL is as follows: At each epoch, we perform training on a batch from both datasets (source and target), and then, we perform the parameters optimization according to the loss function of the given task (The shared parameters are optimized to improve the performances of both tasks. However, each set of task’s parameters is optimized only to improve the corresponding task). Training on NER is stopped before the POS tagging in order to preserve more specific features of the POS tagging task. Mou et al. (2016) showed that the features represented by the lowest layers of neural networks are more general than topmost layers features in NLP applications. And the knowledge to transfer from the parent network to the child network depends on the relatedness of the source and the target tasks and data-sets. For this purpose, we followed the same experiments realized in (Meftah et al., 2017) to study the transferability of each layer of the neural network for each dataset, and to choose the set of layers to transfer from the parent problem to the child problem. 6.5. Training Settings All experiments descri"
L18-1446,C14-1166,0,0.0154623,"state-of-the-art works for English tweets POS tagging, both based on hand-crafted features, Ritter et al. (2011) published a set of 787 hand-annotated English tweets and proposed in (Derczynski et al., 2013) a model based on hidden Markov Models and a set of normalization rules, external dictionaries and lexical features. Gimpel et al. (2011) and Owoputi et al. (2013) constructed 1827 and 547 hand-annotated tweets, respectively, using the same tag-set. They proposed a model based on Firstorder maximum entropy Markov model (MEMM), engineered features like brown clustering and lexical features. Nooralahzadeh et al. (2014) proposed a POS tagging system for French Social Media content using Conditional random fields (CRFs) with a set of several hand-crafted features. These models rely heavily on hand-crafted features and task specific resources (morphological, orthographic and lexical features and external resources such as gazetteers or dictionaries). However, such task-specific knowledge is costly to develop and making sequence labelling models difficult to adapt to new tasks or new domains. Recently a neural network model for English tweets POS tagging was proposed by Gui et al. (2017) (TPANN), they used Adve"
L18-1446,N13-1039,0,0.46021,"Missing"
L18-1446,D14-1162,0,0.0842606,"Missing"
L18-1446,D11-1141,0,0.877667,".) in the sentential context in which the word is used. This information is useful for higher-level NLP applications such as semantic relations extraction, sentiment analysis, automatic summarization and machine translation. Most performing traditional POS tagging models for social media content are linear statistical models, including Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMMs), Conditional Random Fields (CRF) and linear classifiers like SVM-based taggers. There are two principal state-of-the-art works for English tweets POS tagging, both based on hand-crafted features, Ritter et al. (2011) published a set of 787 hand-annotated English tweets and proposed in (Derczynski et al., 2013) a model based on hidden Markov Models and a set of normalization rules, external dictionaries and lexical features. Gimpel et al. (2011) and Owoputi et al. (2013) constructed 1827 and 547 hand-annotated tweets, respectively, using the same tag-set. They proposed a model based on Firstorder maximum entropy Markov model (MEMM), engineered features like brown clustering and lexical features. Nooralahzadeh et al. (2014) proposed a POS tagging system for French Social Media content using Conditional rand"
L18-1446,C12-1149,0,0.0232771,"source problem’s task is NER and the target’s is POS tagging) to exploit the underlying similarities of the two tasks. As illustrated in the figure 4, the parent neural network and the child network share the same first set of parameters (The feature extractor) : θp1 = θc1 = θ1 . θ1 are jointly optimized by the two tasks, while task specific parameters θc2 and θp2 are trained for each task separately. 2823 al., 2003), a POS-annotated French newspaper corpus. We evaluate our approach on two publicly available POSlabeled User Generated (UG) French content datasets: • The French web 2.0 (Fr2.0) (Seddah et al., 2012) is a set of 1700 sentences extracted from various types of French Web. (1) Micro-blogging: Facebook and Twitter. (2) web forums: French health forum DOCTISSIMO3 and video games website JEUXVIDEOS4 . The tag-set includes 28 POS tags from FTB, plus combined tags for contracted tokens. For instance, the non-standard French contraction tes (widely used by French web’s users), which stands for tu es, would have been tagged CLS and V (subject clitic and finite verb) in FTB. The non-standard contracted token tes is then tagged CLS+V. And specific tags to social media, including HT and RT. Twitter at"
L18-1446,N03-1033,0,0.301191,"Missing"
L18-1446,D16-1163,0,0.029988,"rticular, TL was largely exploited in computer vision using pre-trained CNNs to generate representations for 2821 novel tasks; some of the parameters learned on the source dataset are used to initialize the corresponding parameters of the CNNs for the target dataset. In the past few years, few studies have been conducted on TL for neural based models in the field of NLP. It consists in performing a task on a low-resource target problem using features learned from a high-resource source problem. For instance, TL has been successfully applied in neural speech processing and machine translation (Zoph et al., 2016). Two studies have been recently performed on TL for neural networks based models in sequence labeling: Yang et al. (2017) examined the effects of TL for deep hierarchical recurrent networks across domains, applications, and languages, and showed that significant improvement can be obtained. Lee et al. (2017) used cross-domain TL for Named Entity Recognition (NER) (specifically patient note de-identification), and showed that TL may be especially beneficial for a target dataset with small number of examples. 2.2. to the former works. The model proposed in (Gui et al., 2017) requires that label"
L18-1575,W16-4809,1,0.807382,"of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, there are few tools for automatically processing Arabizi. Works presented in (Darwish, 2014) and (Eskander et al., 2014), aimed at distinguishing English from Arabizi, resulting in a transliteration of texts from Arabizi to Arabic, which allows to process these texts with NLP systems dedicated to Arabic. (Adouane et al., 2016) considered the task of automatic identification of dialects as a classification problem and used supervised machine learning techniques to recognize Arabized Berber and Arabic dialects. A review of methods and obtained results for the processing of Arabic dialects was presented in (Shoufan and AlAmeri, 2015). Four types of tasks are described: basic analyzes, resource building, semantic analysis and identification of dialects. We can see that the approaches are generally divided into two main categories: dialectal systems built from dedicated resources and systems made by adaptation of availa"
L18-1575,al-sabbagh-girju-2012-yadac,0,0.0252355,"elated Work The creation of resources and development of methods to deal with Arabic dialects have attracted the attention of many researchers in the last few years. The aim is to compensate the lack of resources for dialectal Arabic, which are crucial for the development of adequate NLP tools. In (Zaidan and Callison-Burch, 2011) the authors collected a corpus based on texts available on the web, from three Arabic newspapers of Levantine, Gulf and Egyptian dialects. Articles and their comments were extracted to build the corpus. We can find several other corpora for Arabic dialects, such as (Al-Sabbagh and Girju, 2012) who created an annotated corpus of Egyptian, but only a small subset of it was manually annotated to build a classifier, the rest of the corpus being automatically annotated. Other initiatives aimed to create a dialectal Arabic dataset to address the lack of dedicated resources such as (Cotterell and Callison-Burch, 2014), where the authors collected a significant amount of dialectal data from comments, online journals and Twitter for Egyptian, Gulf, Levantine, Algerian and Iraqi. 3639 The work presented in (Elfardy and Diab, 2012b) suggests guidelines for the foundation of large corpora of m"
L18-1575,I13-1048,0,0.0283062,"same schema. For example, the  - inside  &apos; P rbaH  word l&apos;.QK l - yirbaH “to win” follows the .  . JºK - I. J» ktab- yakALG –pattern-I-aa; the word I tab, “to write” follows the ALG-I-aa.We remark that the two verbs came from the same class but they have not the same imperfect marks. This is why we propose to extend the ALG patternI-au in order to define for the pattern-I more  -sub patterns. To get this goal, we attribute to l&apos;.QK l&apos;.P  I. J» ktab- ya-ktab the scheme ALG-I-aa-a.  I. JºK - This approach was also followed by (Shaalan et al., 2007) for Egyptian dialect and (Boujelbane et al., 2013) for Tunisian dialect. Table 2 gives some statistics about the final lexicons per dialect. 5. 5.1. EG 34859 • Lang2: Dialect word &lt;AD&gt; in Arabic or Arabizi script and information for Arabic text: &lt;DZ&gt; (Algerian),&lt;TN&gt; (Tunisian), &lt;MA&gt; (Moroccan) or &lt;EG&gt; (Egyptian). This information is added in the arabizi script after their transliteration. • Pattern’s second consonant vowel oriented classification: In Arabic, the vowel of the pattern’s second consonant is one of the basic deterministic elements of the verbal morphology. According to (Ouerhani, 2009), this vowel is considered as the first crite"
L18-1575,cotterell-callison-burch-2014-multi,0,0.0157564,"son-Burch, 2011) the authors collected a corpus based on texts available on the web, from three Arabic newspapers of Levantine, Gulf and Egyptian dialects. Articles and their comments were extracted to build the corpus. We can find several other corpora for Arabic dialects, such as (Al-Sabbagh and Girju, 2012) who created an annotated corpus of Egyptian, but only a small subset of it was manually annotated to build a classifier, the rest of the corpus being automatically annotated. Other initiatives aimed to create a dialectal Arabic dataset to address the lack of dedicated resources such as (Cotterell and Callison-Burch, 2014), where the authors collected a significant amount of dialectal data from comments, online journals and Twitter for Egyptian, Gulf, Levantine, Algerian and Iraqi. 3639 The work presented in (Elfardy and Diab, 2012b) suggests guidelines for the foundation of large corpora of mixed Arabic resources with switching code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Di"
L18-1575,W14-3629,0,0.0206647,"sources with switching code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, there are few tools for automatically processing Arabizi. Works presented in (Darwish, 2014) and (Eskander et al., 2014), aimed at distinguishing English from Arabizi, resulting in a transliteration of texts from Arabizi to Arabic, which allows to process these texts with NLP systems dedicated to Arabic. (Adouane et al., 2016) considered the task of automatic identification of dialects as a classification problem and used supervised machine learning techniques to recognize Arabized Berber and Arabic dialects. A review of methods and obtained results for the processing of Arabic dialects was presented in (Shoufan and AlAmeri, 2015). Four types of tasks are described: basic analyzes, r"
L18-1575,2012.eamt-1.18,0,0.039558,"find several other corpora for Arabic dialects, such as (Al-Sabbagh and Girju, 2012) who created an annotated corpus of Egyptian, but only a small subset of it was manually annotated to build a classifier, the rest of the corpus being automatically annotated. Other initiatives aimed to create a dialectal Arabic dataset to address the lack of dedicated resources such as (Cotterell and Callison-Burch, 2014), where the authors collected a significant amount of dialectal data from comments, online journals and Twitter for Egyptian, Gulf, Levantine, Algerian and Iraqi. 3639 The work presented in (Elfardy and Diab, 2012b) suggests guidelines for the foundation of large corpora of mixed Arabic resources with switching code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, t"
L18-1575,elfardy-diab-2012-simplified,0,0.0321153,"find several other corpora for Arabic dialects, such as (Al-Sabbagh and Girju, 2012) who created an annotated corpus of Egyptian, but only a small subset of it was manually annotated to build a classifier, the rest of the corpus being automatically annotated. Other initiatives aimed to create a dialectal Arabic dataset to address the lack of dedicated resources such as (Cotterell and Callison-Burch, 2014), where the authors collected a significant amount of dialectal data from comments, online journals and Twitter for Egyptian, Gulf, Levantine, Algerian and Iraqi. 3639 The work presented in (Elfardy and Diab, 2012b) suggests guidelines for the foundation of large corpora of mixed Arabic resources with switching code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, t"
L18-1575,P13-2081,0,0.0175971,"n-Burch, 2014), where the authors collected a significant amount of dialectal data from comments, online journals and Twitter for Egyptian, Gulf, Levantine, Algerian and Iraqi. 3639 The work presented in (Elfardy and Diab, 2012b) suggests guidelines for the foundation of large corpora of mixed Arabic resources with switching code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, there are few tools for automatically processing Arabizi. Works presented in (Darwish, 2014) and (Eskander et al., 2014), aimed at distinguishing English from Arabizi, resulting in a transliteration of texts from Arabizi to Arabic, which allows to process these texts with NLP systems dedicated to Arabic. (Adouane et al., 2016) considered the task of automatic identification of dialects as a cla"
L18-1575,W14-3911,0,0.017151,"ic Hadi 3afsa chaba bezzef Dialect DZ,MA,TN DZ MSA,DZ DZ ø X Aë é ® « éK . A • Combination: The combining step is used to aggregate multiple components, including dictionaries of named entities and language templates, in order to perform the recognition and the identification of named entities and language. Each word of the input sentence can be tagged with different labels from the previous steps. Thereby the combining step, based on the generated labels, uses a set of decision rules to assign the final tag to each word in the analyzed sentence. The decision rules used are presented in (Elfardy et al., 2014; Saˆadane, 2015), and summarized as follows: ¬@ Q K . Table 3: Filtering of the best candidate using a morphological analyze The generated list is then filtered using a morphological analyzer to predict whether the word belongs to one of the studied dialects (Table 3). The conversion of Arabizi into Arabic script is an important step, but this article focuses on the identification of dialects and due to lack of space we cannot detail this part of the processing. Note, however, that this process adds a crucial information to identify the dialect: the presence of vowels in Arabizi makes it pos"
L18-1575,W14-3901,0,0.0169306,"ng code. In addition to the former work, an identification, interpretation and classification system for dialects was introduced in (Elfardy and Diab, 2012a) called AIDA (Automatic Identification and Glossing of Dialectal Arabic). In the continuation of AIDA, the authors of (Elfardy and Diab, 2013) presented a supervised approach for the identification of dialectal sentences. They also studied the effects of preprocessing techniques on the accuracy of the developed classifiers. As far as we know, there are few tools for automatically processing Arabizi. Works presented in (Darwish, 2014) and (Eskander et al., 2014), aimed at distinguishing English from Arabizi, resulting in a transliteration of texts from Arabizi to Arabic, which allows to process these texts with NLP systems dedicated to Arabic. (Adouane et al., 2016) considered the task of automatic identification of dialects as a classification problem and used supervised machine learning techniques to recognize Arabized Berber and Arabic dialects. A review of methods and obtained results for the processing of Arabic dialects was presented in (Shoufan and AlAmeri, 2015). Four types of tasks are described: basic analyzes, resource building, semantic a"
L18-1575,habash-etal-2012-conventional,0,0.137238,"asis of many methods of dialect detection. • Orthographic: Unlike MSA, dialects do not have an orthographic standard. We find many orthographic variations in the writing of words. These variations are mainly due to phonological differences between MSA and Arabic dialects . In some cases, phonology or underlying morphology results in regular phonological YªK. man bac¸d assimilation writing, for example, áÓ “after” also written Õ× YªK. mam bac¸d. To remedy this lack of norm, work has been carried out to propose a Conventional Orthography for Dialectal Arabic (CODA), first proposed for Egyptian (Habash et al., 2012), then extended to other dialects such as Tunisian (Zribi et al., 2014), Algerian (Saˆadane and Habash, 2015), Maghrebi Arabic (Turki et al., 2016) and Palestinian (Habash et al., 2016). • Morphology: There are many morphological differences between dialects and MSA. These differences can be seen through several aspects. One of these aspects is the future particle which appears as +  sa sawfa in MSA, which is expressed in: (i) + ¬ñ h+ Ha + or hP raH in the Levantine dialects, (ii) è+ ha + in Egyptian dialect, (iii) + ka + in Moroccan + or 1 Arabic transliteration is presented in the Habash-"
L18-1575,W15-3208,1,0.908253,"Missing"
L18-1575,W15-3205,0,0.0281412,"Missing"
L18-1575,D13-1007,0,0.0208059,"other hand, a large number of dialects are used as mother tongues for many Arabic-speaking populations. In fact, they are the main communication tool spoken in everyday life through informal conversations, exchanges on SMS, forums and social networks, even in e-mails. These dialects vary from one country to another, one region to another, or even from one city to another. In addition, they differ from each other by important phonological, morphological, lexical and syntactic characteristics. The processing of informal texts has become an extremely popular field of research among researchers (Yang and Eisenstein, 2013). For Arabic NLP, the identification of dialects is very important and considered as a prepossessing step for any natural language application dealing with Arabic language, such as machine translation, information retrieval for social media. It is sometimes considered as a difficult case of language identification where, according to (Zaidan and Callison-Burch, 2011) it is applied to a group of closely related languages that share a common character set. This identification is made even more complex by the absence of orthographic conventions, by the transliteration of Arabic dialects into Lati"
L18-1575,P11-2007,0,0.467975,"to another. In addition, they differ from each other by important phonological, morphological, lexical and syntactic characteristics. The processing of informal texts has become an extremely popular field of research among researchers (Yang and Eisenstein, 2013). For Arabic NLP, the identification of dialects is very important and considered as a prepossessing step for any natural language application dealing with Arabic language, such as machine translation, information retrieval for social media. It is sometimes considered as a difficult case of language identification where, according to (Zaidan and Callison-Burch, 2011) it is applied to a group of closely related languages that share a common character set. This identification is made even more complex by the absence of orthographic conventions, by the transliteration of Arabic dialects into Latin script (Arabizi) and also the use of code-switching. Recent works have proposed both supervised and unsupervised statistical approaches to language identification. However, current methods are based on the assumption that dedicated resources exist, such as large corpora and dictionaries. Unfortunately, these resources are rarely available for certain languages and"
L18-1575,zribi-etal-2014-conventional,0,0.0236344,"ialects do not have an orthographic standard. We find many orthographic variations in the writing of words. These variations are mainly due to phonological differences between MSA and Arabic dialects . In some cases, phonology or underlying morphology results in regular phonological YªK. man bac¸d assimilation writing, for example, áÓ “after” also written Õ× YªK. mam bac¸d. To remedy this lack of norm, work has been carried out to propose a Conventional Orthography for Dialectal Arabic (CODA), first proposed for Egyptian (Habash et al., 2012), then extended to other dialects such as Tunisian (Zribi et al., 2014), Algerian (Saˆadane and Habash, 2015), Maghrebi Arabic (Turki et al., 2016) and Palestinian (Habash et al., 2016). • Morphology: There are many morphological differences between dialects and MSA. These differences can be seen through several aspects. One of these aspects is the future particle which appears as +  sa sawfa in MSA, which is expressed in: (i) + ¬ñ h+ Ha + or hP raH in the Levantine dialects, (ii) è+ ha + in Egyptian dialect, (iii) + ka + in Moroccan + or 1 Arabic transliteration is presented in the Habash-SoudiBuckwalter (HSB) scheme (Habash et al., 2007).  dilwaqtiy, øñK t"
N19-1416,R13-1026,0,0.0293853,"assess the POS tagging performances of our PretRand model, we compared it to 5 baselines: Random-200 and Random-400: randomly initialised neural model with 200 and 400 biLSTM’s units; Fine-tuning: pre-trained neural model, fine-tuned with the standard scheme; Ensemble (2 rand): averaging the predictions of two base models randomly initialised and learned independently (with different random initialisation) on Tweets datasets; and Ensemble (1 pret + 1 rand): same as the previous but with one pre-trained on WSJ and the other randomly initialised. We also compared it to the 3 best SOTA methods: Derczynski et al. (2013) (GATE) is a model based on HMMs with a set of normalisation rules, external dictionaries and lexical features. They experiment it on TPoS, with WSJ and 32K tokens from the NPS IRC corpus. They also used 1.5M additional training tokens annotated by vote-constrained bootstrapping (GATEbootstrap). Owoputi et al. (2013) proposed a model based on first-order Maximum Entropy 4109 Method #params GATE (Derczynski et al., 2013) GATE-bootstrap (Derczynski et al., 2013) ARK (Owoputi et al., 2013) TPANN (Gui et al., 2017) Random-200 Random-400 Standard fine-tuning Ensemble Model (2 rand) Ensemble Model ("
N19-1416,D17-1256,0,0.455903,"Missing"
N19-1416,N18-1088,0,0.315089,"ation, we used `2 -norm. Finally, in all experiments, training was performed using SGD with momentum and mini-batches of 8 sentences. Evidently, all the hyperparameters have been cross-validated. 3.2 Datasets For the source-dataset, we used the Wall Street Journal (WSJ) part of Penn-Tree-Bank (PTB), a large English dataset containing 1.2M+ tokens from the newswire domain annotated with the PTB tag-set. Regarding the target-datasets, we used three Tweets datasets: TPoS (Ritter et al., 2011), annotated with 40 tags ; ARK (Owoputi et al., 2013) containing 25 coarse tags; and the recent TweeBank (Liu et al., 2018) containing 17 tags (PTB universal tag-set). The number of tokens in the datasets are given in Table 1. 3.3 Comparison Methods To assess the POS tagging performances of our PretRand model, we compared it to 5 baselines: Random-200 and Random-400: randomly initialised neural model with 200 and 400 biLSTM’s units; Fine-tuning: pre-trained neural model, fine-tuned with the standard scheme; Ensemble (2 rand): averaging the predictions of two base models randomly initialised and learned independently (with different random initialisation) on Tweets datasets; and Ensemble (1 pret + 1 rand): same as"
N19-1416,L18-1446,1,0.893228,"n POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets. 1 Introduction POS tagging is a sequence labelling problem, that consists on assigning to each sentence’ word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used. Such information is useful for higher-level applications, such as machine-translation (Niehues and Cho, 2017) or cross-lingual information retrieval (Semmar et al., 2006, 2008). One of the best approaches for POS tagging of social media text (Meftah et al., 2018a), is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest (target-dataset). While this approach is known to be very effective (Zennaki et al., 2019), because it takes benefit from pre-trained neurons, it has one main drawback by design. Indeed, it has been shown in computervision (Zhou et al., 2018a) that, when fine-tuning on scenes a model pre-trained on objects, it is the neuron firing on the white dog object that became highly sensitive to the white waterfall scene. Simply said, pre-t"
N19-1416,W18-3927,1,0.68001,"n POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets. 1 Introduction POS tagging is a sequence labelling problem, that consists on assigning to each sentence’ word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used. Such information is useful for higher-level applications, such as machine-translation (Niehues and Cho, 2017) or cross-lingual information retrieval (Semmar et al., 2006, 2008). One of the best approaches for POS tagging of social media text (Meftah et al., 2018a), is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest (target-dataset). While this approach is known to be very effective (Zennaki et al., 2019), because it takes benefit from pre-trained neurons, it has one main drawback by design. Indeed, it has been shown in computervision (Zhou et al., 2018a) that, when fine-tuning on scenes a model pre-trained on objects, it is the neuron firing on the white dog object that became highly sensitive to the white waterfall scene. Simply said, pre-t"
N19-1416,D16-1046,0,0.057989,"by minimising a Softmax CrossEntropy (SCE) loss using the SGD algorithm. 2.2 Adding Random Branch As mentioned in the introduction, pre-trained neurons are biased by design, thus limited. This motivated our proposal to augment the pre-trained branch with additional random units (as illustrated in Fig. 1). To do so, theoretically one can add the new units in any layer of the base model. However in practice, we have to make a trade-off between performances and the number of parameters (model complexity). Thus, given that deep layers are more task-specific than shallow ones (Peters et al., 2018; Mou et al., 2016), and that word embeddings (shallow layers) contain the majority of parameters, we choose to expand only the top layers. With this choice, we desirably increase the complexity of the model only by 1.02× compared to the base one. In terms of the layers expanded, we specifically add k units to Φ resulting in an extra biLSTM layer: Φr (r for rand); and C units in Ψ resulting in an extra FC layer: Ψr . Hence, for every wi , the additional random branch predicts r = Ψ ◦ Φ (x ) class-probabilities following: yˆw r r i i (with xi = Υ(wi )). Note that, having two FC layers obviously outputs two predic"
N19-1416,W17-4708,0,0.0296247,"k with normalised, weighted and randomly initialised units that beget a better adaptation while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets. 1 Introduction POS tagging is a sequence labelling problem, that consists on assigning to each sentence’ word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used. Such information is useful for higher-level applications, such as machine-translation (Niehues and Cho, 2017) or cross-lingual information retrieval (Semmar et al., 2006, 2008). One of the best approaches for POS tagging of social media text (Meftah et al., 2018a), is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest (target-dataset). While this approach is known to be very effective (Zennaki et al., 2019), because it takes benefit from pre-trained neurons, it has one main drawback by design. Indeed, it has been shown in computervision (Zhou et al., 2018a) that, when fine-tuning on scenes a mo"
N19-1416,N13-1039,0,0.124324,"Missing"
N19-1416,D14-1162,0,0.0820529,"y SGD. Formally, the final predictions are computed folp r ). lowing: yˆwi = u Np (ˆ yw yw i ) ⊕ v Np (ˆ i 3 Experiments 3.1 Implementation Details In the word-level embeddings, tokens are lowercased while the character-level component still retains access to the capitalisation information. We set the character embedding dimension at 50, the dimension of hidden states of the character-level biLSTM at 100 and used 300-dimensional wordlevel embeddings. The latter were pre-loaded from publicly available Glove pre-trained vectors on 42 billions words from a web crawling and containing 1.9M words (Pennington et al., 2014). Note that, these embeddings are also updated during fine-tuning. For biLSTM (token-level feature extractor), we set the number of units of the pretrained branch to 200 and experimented our approach with k added random-units, with k ∈ {50, 100, 150, 200}. For the normalisation, we used `2 -norm. Finally, in all experiments, training was performed using SGD with momentum and mini-batches of 8 sentences. Evidently, all the hyperparameters have been cross-validated. 3.2 Datasets For the source-dataset, we used the Wall Street Journal (WSJ) part of Penn-Tree-Bank (PTB), a large English dataset co"
N19-1416,N18-1202,0,0.0496107,"d on the target-task by minimising a Softmax CrossEntropy (SCE) loss using the SGD algorithm. 2.2 Adding Random Branch As mentioned in the introduction, pre-trained neurons are biased by design, thus limited. This motivated our proposal to augment the pre-trained branch with additional random units (as illustrated in Fig. 1). To do so, theoretically one can add the new units in any layer of the base model. However in practice, we have to make a trade-off between performances and the number of parameters (model complexity). Thus, given that deep layers are more task-specific than shallow ones (Peters et al., 2018; Mou et al., 2016), and that word embeddings (shallow layers) contain the majority of parameters, we choose to expand only the top layers. With this choice, we desirably increase the complexity of the model only by 1.02× compared to the base one. In terms of the layers expanded, we specifically add k units to Φ resulting in an extra biLSTM layer: Φr (r for rand); and C units in Ψ resulting in an extra FC layer: Ψr . Hence, for every wi , the additional random branch predicts r = Ψ ◦ Φ (x ) class-probabilities following: yˆw r r i i (with xi = Υ(wi )). Note that, having two FC layers obviously"
N19-1416,D11-1141,0,0.185039,"Missing"
N19-1416,semmar-etal-2006-deep,1,0.34182,"beget a better adaptation while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets. 1 Introduction POS tagging is a sequence labelling problem, that consists on assigning to each sentence’ word, its disambiguated POS tag (e.g., Pronoun, Noun) in the phrasal context in which the word is used. Such information is useful for higher-level applications, such as machine-translation (Niehues and Cho, 2017) or cross-lingual information retrieval (Semmar et al., 2006, 2008). One of the best approaches for POS tagging of social media text (Meftah et al., 2018a), is transfer-learning, which relies on a neuralnetwork learned on a source-dataset with sufficient annotated data, then further adapted to the problem of interest (target-dataset). While this approach is known to be very effective (Zennaki et al., 2019), because it takes benefit from pre-trained neurons, it has one main drawback by design. Indeed, it has been shown in computervision (Zhou et al., 2018a) that, when fine-tuning on scenes a model pre-trained on objects, it is the neuron firing on the w"
P13-2133,C02-2020,1,0.736512,"rd approach is the bilingual dictionary. Its use is problematic when a word has several translations, whether they are synonymous or polysemous. For instance, the French 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements ha"
P13-2133,P04-1067,0,0.877152,"Missing"
P13-2133,hazem-morin-2012-adaptive,0,0.0535024,"epresentative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standar"
P13-2133,C10-1070,0,0.247056,"terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains. The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6 . The second list contains 79 terms extracted from the FrenchEnglish MESH and the UMLS thesauri7 . Note that reference terms pairs appear more than five times in each part of both comparable corpora. Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. We followed (Laroche and Langlais, 2010) to define these parameters. They carried out a complete study of the influence of these parameters on the bilingual alignment. The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7. (2) where N is the total number of monosemic words in each context vector and SemSim is the similarity value of wpj and the ith monosemic word. Hence, according to average similarity values Ave Sim(wpj ), we obtain for each polysemous word wp an ordered list of translations wp1 . . . wpn . 4 Experiments and Results 4.1"
P13-2133,C10-1073,0,0.284,"Missing"
P13-2133,W11-1205,0,0.13144,"seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standard approach but weigh the different translations according to their frequency in the target corpus. Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 3.1 path-based semantic similarity measures denoted PATH,W UP (Wu and Palmer, 1994) and L EA COCK (Leacock and Chodorow, 1998). PATH is a baseline that is equal to the inverse of the shortest path between two words. W UP finds the dep"
P13-2133,2009.mtsummit-posters.14,0,0.332928,"oblematic when a word has several translations, whether they are synonymous or polysemous. For instance, the French 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attem"
P13-2133,P95-1050,0,0.284557,"n from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source and target vec"
P13-2133,P94-1019,0,\N,Missing
R15-1075,2010.amta-papers.16,0,0.427691,"Missing"
R15-1075,W09-0432,0,0.1426,"error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domainspecific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. However, this work did not report the impact of generic data on domain translation accuracy. Daumé"
R15-1075,besancon-etal-2010-lima,1,0.834396,"Missing"
R15-1075,W12-5108,1,0.611993,"tracted 500 parallel sentences from Europarl as an InDomain corpus and 500 pairs of sentences from Emea as an Out-Of-Domain corpus. These experiments are done to show the impact of the domain vocabulary on the translation results. The domain vocabulary is represented in the case of Moses by the specialized parallel corpus (Emea) which is added to the training data (Europarl). In the case of the EBMT prototype, the domain vocabulary is identified by a bilingual lexicon which is extracted automatically from the specialized parallel corpus (Emea) using a word alignment tool (Semmar et al., 2010; Bouamor et al., 2012). This specialized bilingual lexicon is added to the English-French lexicon which is used by the cross-language search engine and the bilingual reformulator. First, both corpora have been normalized through the following preprocessing tools provided by the open source SMT toolkit Moses (Khoen et al., 2007): Tokenization, True-casing (the initial words in each sentence are converted to their most probable casing) and Cleaning (long sentences –more than 80 characters- and empty sentences are removed). To evaluate the performance of our approach, we used Moses (Koehn et al., 2007) as a baseline,"
R15-1075,J93-2003,0,0.0348015,"Missing"
R15-1075,2006.amta-papers.3,0,0.0396926,"quire manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based MT approaches are effective only when large amounts of parallel corpora are available. Recently, several strategies have been proposed to combine the strengths of rule-based and corpus-based MT approaches or to add deep linguistic knowledge into statistical machine translation. Examples include Part-Of-Speech and morphological information (Koehn et al., 2010), word sense disambiguation models (Carpuat and Wu, 2007) and semantic role labels (Wu and Fung, 2009). Carbonell et al. (2006) described a new paradigm for corpus based translation that does not require parallel text. They called this paradigm Context-Based Machine Translation (CBMT) which relies on a lightweight translation model utilizing a full-form bilingual lexicon and a decoder using long-range context via long ngrams and cascaded overlapping. The authors evaluated their approach in Spanish-English translation using Spanish newswire text. This approach achieves a BLEU score of 0.64. The results showed that quality increases above the reported score as the target corpus size increases and as dictionary coverage"
R15-1075,D07-1007,0,0.0433576,"rder to build the language model. Rule-Based MT (RBMT) approaches require manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based MT approaches are effective only when large amounts of parallel corpora are available. Recently, several strategies have been proposed to combine the strengths of rule-based and corpus-based MT approaches or to add deep linguistic knowledge into statistical machine translation. Examples include Part-Of-Speech and morphological information (Koehn et al., 2010), word sense disambiguation models (Carpuat and Wu, 2007) and semantic role labels (Wu and Fung, 2009). Carbonell et al. (2006) described a new paradigm for corpus based translation that does not require parallel text. They called this paradigm Context-Based Machine Translation (CBMT) which relies on a lightweight translation model utilizing a full-form bilingual lexicon and a decoder using long-range context via long ngrams and cascaded overlapping. The authors evaluated their approach in Spanish-English translation using Spanish newswire text. This approach achieves a BLEU score of 0.64. The results showed that quality increases above the reported"
R15-1075,W07-0722,0,0.290994,"gnificant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domainspecific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. However, this work did not report the impact of generic data on d"
R15-1075,P11-2071,0,0.330166,"Missing"
R15-1075,1998.amta-tutorials.5,0,0.76596,"nsfer rules and bilingual lexicons. In order to illustrate the translation process of the EBMT prototype, we indexed a textual database composed of 1127 French sentences extracted from the ARCADE II corpus 1 and we considered the input source sentence &quot;Social security funds in Greece encourage investment in innovation.&quot; as the sentence to translate. 3.1 The Cross-language Search Engine The purpose of Cross-Language Information Retrieval (CLIR) is to find similar or relevant documents for a given query where the documents and the query are written in different languages (Davis and Ogden, 1997; Grefenstette, 1998). In our use of CLIR in machine translation, a document corresponds to a sentence. The role of the cross-language search engine is to retrieve for each user’s query (which is introduced as a sentence in natural language) translation candidates from an indexed monolingual corpus. The crosslanguage search engine used in the EBMT prototype is based on a deep linguistic analysis (Besançon et al., 2010) of the query and the monolingual corpus to be indexed and uses a weighted vector space model in which sentences to be indexed are grouped into classes characterized by the same set of words (Salton"
R15-1075,2005.eamt-1.19,0,0.295161,"designed for one domain to work in another, several ideas have been explored and implemented (Bungum and Gambäck, 2011). Langlais (2002) integrated domain-specific lexicons in the translation model of a SMT engine which yields a significant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domainspecific data."
R15-1075,P07-2045,0,0.00634064,"et al., 2010; Bouamor et al., 2012). This specialized bilingual lexicon is added to the English-French lexicon which is used by the cross-language search engine and the bilingual reformulator. First, both corpora have been normalized through the following preprocessing tools provided by the open source SMT toolkit Moses (Khoen et al., 2007): Tokenization, True-casing (the initial words in each sentence are converted to their most probable casing) and Cleaning (long sentences –more than 80 characters- and empty sentences are removed). To evaluate the performance of our approach, we used Moses (Koehn et al., 2007) as a baseline, and the BLEU score as an automatic evaluation metric (Papineni et al; 2002). The first observation is that, when the test set is In-Domain, we achieve a relatively high score BLEU for both the two systems and the score of Moses is better in all the runs. For the Out-OfDomain test corpus, the EBMT prototype performs better than Moses in all the runs and in particular Moses has obtained a very low BLEU score in the first run. This result can be explained by the fact that the test corpus has a vocabulary which is different from the entries of the translation table. Furthermore, it"
R15-1075,W10-1715,0,0.312745,"translation model and strings succession (n-grams) in order to build the language model. Rule-Based MT (RBMT) approaches require manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based MT approaches are effective only when large amounts of parallel corpora are available. Recently, several strategies have been proposed to combine the strengths of rule-based and corpus-based MT approaches or to add deep linguistic knowledge into statistical machine translation. Examples include Part-Of-Speech and morphological information (Koehn et al., 2010), word sense disambiguation models (Carpuat and Wu, 2007) and semantic role labels (Wu and Fung, 2009). Carbonell et al. (2006) described a new paradigm for corpus based translation that does not require parallel text. They called this paradigm Context-Based Machine Translation (CBMT) which relies on a lightweight translation model utilizing a full-form bilingual lexicon and a decoder using long-range context via long ngrams and cascaded overlapping. The authors evaluated their approach in Spanish-English translation using Spanish newswire text. This approach achieves a BLEU score of 0.64. The"
R15-1075,W02-1405,0,0.824384,"anish newswire text. This approach achieves a BLEU score of 0.64. The results showed that quality increases above the reported score as the target corpus size increases and as dictionary coverage of source words and phrases becomes more complete. As regards domain adaptation in MT, most previous works addressing this task have proven that a statistical machine translation system trained on general texts, has poor performance on specific domains. In order to adapt MT systems designed for one domain to work in another, several ideas have been explored and implemented (Bungum and Gambäck, 2011). Langlais (2002) integrated domain-specific lexicons in the translation model of a SMT engine which yields a significant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples t"
R15-1075,lewis-etal-2010-achieving,0,0.180352,"pus size increases and as dictionary coverage of source words and phrases becomes more complete. As regards domain adaptation in MT, most previous works addressing this task have proven that a statistical machine translation system trained on general texts, has poor performance on specific domains. In order to adapt MT systems designed for one domain to work in another, several ideas have been explored and implemented (Bungum and Gambäck, 2011). Langlais (2002) integrated domain-specific lexicons in the translation model of a SMT engine which yields a significant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) us"
R15-1075,J03-1002,0,0.00752441,"ing Moses’s translation results for Out-Of-Domain sentences, we noted that most of errors are related to vocabulary. For example, Moses proposes the compound word “glycémie artérielle” as a translation for the expression “fasting blood glucose” which is not correct. In SMT systems such as Moses, phrase tables are the main knowledge source for the machine translation decoder. The decoder consults these tables to figure out how to translate an input sentence from the source language into the target language. These tables are built automatically using the open-source word alignment tool GIZA++5 (Och and Ney, 2003). However, this tool could produce errors in particular when it aligns multiword expressions. As a conclusion to this study, even if the comparison between the results of the two MT systems is not completely adequate since the EBMT prototype includes several components that require additional training data (Part-Of-Speech tagger), handwritten rules (Syntactic analyzer, Bilingual reformulator), monolingual and bilingual lexicons (Morphological analyzer, Bilingual reformulator), and Moses is trained on a small amount of the Emea corpus, the experiments show that the EBMT prototype performs bette"
R15-1075,P02-1040,0,0.0913452,"Missing"
R15-1075,2011.eamt-1.40,0,0.319235,"his data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. However, this work did not report the impact of generic data on domain translation accuracy. Daumé III and Jagarlamudi (2011) used dictionary mining techniques to find translations for unseen words from comparable corpora and they integrated these translations into a statistical phrase-based translation system. They reported improvements in translation quality (between 0.5 and 1.5 BLEU points) on four domains and two language pairs. Pecina et al. (2011) exploited domain-specific data acquired by domain-focused web-crawling to adapt general-domain SMT systems to new domains. They observed that even small amounts of in-domain parallel data are more important for translation quality than large amounts of indomain monolingual data. Wang et al. (2012) used a single translation model and generalized a single-domain decoder to deal with different domains. They used this method to adapt largescale generic SMT systems for 20 language pairs in order to translate patents. The authors reported a gain of 0.35 BLEU points for patent translation and a loss"
R15-1075,2014.tc-1.4,1,0.774794,"ss-language search engine. For each sentence to translate (query in natural language), the cross-language search engine is used to provide a set of sentences in the target language. These sentences are combined with translation hypotheses provided by a finite-state transducer. The result of this combination is evaluated against a statistical language model learned from the target language corpus in order to produce the n-best translations. We believe that this is the first application of cross-language information retrieval in machine translation (Semmar and Bouamor 2011; Semmar et al., 2011; Semmar et al., 2014). The remainder of this paper is organized as follows: Section 2 describes the main approaches used in machine translation and presents previous works addressing the task of domain adaptation in statistical machine translation. Section 3 introduces the hybrid approach used to implement the EBMT prototype and presents its architecture. In section 4 we discuss results obtained after translating two types of texts in general and specialized domains. Section 5 concludes our study and presents our future work. 2 Related Work Machine translation systems are indispensable tools in a globalizing world"
R15-1075,D08-1090,0,0.275669,"SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on “in-domain” monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domainspecific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. However, this work did not report the impact of generic data on domain translation accuracy. Daumé III and Jagarlamudi (2011) used dictionary mining"
R15-1075,tiedemann-2012-parallel,0,0.0302586,"ation 1 les caisses de la sécurité sociale en Grèce encouragent l’investissement dans l’innovation. 2 les fonds de la sécurité sociale en Grèce encouragent l’investissement en l’innovation. Table 3: The two first translations for the English sentence “Social security funds in Greece encourage investment in innovation.”. 4 Experimental Results 4.1 Data and Experimental Setup We conducted our experiments on two EnglishFrench parallel corpora: Europarl (European Parliament Proceedings) and Emea (European Medicines Agency Documents). Both corpora were extracted from the open parallel corpus OPUS (Tiedemann, 2012). Table 4 lists corpora details. Run n°. 1 2 3 4 Training (# sentences) 150000 (Europarl) 150000+10000 (Europarl+Emea) 150000+20000 (Europarl+Emea) 150000+30000 (Europarl+Emea) Tuning (# sentences) 3750 (Europarl) 1500 (Europarl) 1500 (Europarl) 1500 (Europarl) 2 http://catalog.elra.info/product_info.php?products_id=666. FSM Library is available from AT&T for non-commercial use as executable binary programs. 4 http://wing.comp.nus.edu.sg/~forecite/services/parscit100401/crfpp/CRF++-0.51/doc/. 3 Table 4: Corpora details used to train Moses and to build the database of the cross-language search"
R15-1075,2012.amta-papers.18,0,0.185154,"ary mining techniques to find translations for unseen words from comparable corpora and they integrated these translations into a statistical phrase-based translation system. They reported improvements in translation quality (between 0.5 and 1.5 BLEU points) on four domains and two language pairs. Pecina et al. (2011) exploited domain-specific data acquired by domain-focused web-crawling to adapt general-domain SMT systems to new domains. They observed that even small amounts of in-domain parallel data are more important for translation quality than large amounts of indomain monolingual data. Wang et al. (2012) used a single translation model and generalized a single-domain decoder to deal with different domains. They used this method to adapt largescale generic SMT systems for 20 language pairs in order to translate patents. The authors reported a gain of 0.35 BLEU points for patent translation and a loss of only 0.18 BLEU points for generic translation. 580 • A generator of translations which consists in assembling the results returned by the cross-language search engine and the bilingual reformulator, and in choosing the n-best translations according to a statistical language model learned from t"
R15-1075,N09-2004,0,0.0231514,"(RBMT) approaches require manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based MT approaches are effective only when large amounts of parallel corpora are available. Recently, several strategies have been proposed to combine the strengths of rule-based and corpus-based MT approaches or to add deep linguistic knowledge into statistical machine translation. Examples include Part-Of-Speech and morphological information (Koehn et al., 2010), word sense disambiguation models (Carpuat and Wu, 2007) and semantic role labels (Wu and Fung, 2009). Carbonell et al. (2006) described a new paradigm for corpus based translation that does not require parallel text. They called this paradigm Context-Based Machine Translation (CBMT) which relies on a lightweight translation model utilizing a full-form bilingual lexicon and a decoder using long-range context via long ngrams and cascaded overlapping. The authors evaluated their approach in Spanish-English translation using Spanish newswire text. This approach achieves a BLEU score of 0.64. The results showed that quality increases above the reported score as the target corpus size increases an"
W05-0705,W02-0506,0,0.392777,"ach valid combination of clitics. There are 77 and 65 entries respectively in each dictionary. The clitic stemmer proceeds as follows on tokens unrecognized after step 1c: • Several vowel form normalizations are performed (َ ً ُ ٌ ِ ٍ are removed,  إ أ are replaced by  اand final  ي ئ ؤor ة are replaced by  ى ىء وءor ). • All clitic possibilities are computed by using proclitics and enclitics dictionaries. A radical, computed by removing these clitics, is checked against the full form lexicon. If it does not exist in the full form lexicon, re-write rules (such as those described in Darwish (2002)) are applied, and the altered form is checked against the full form dictionary. For example, consider the token  وهاهand the included clitics (و, )ه, the computed radical  هاdoes not exist in the full form lexicon but after applying one of the dozen re-write rules, the modified radical  هىis found the dictionary and the input token is segmented into root and clitics as:  ه+  هى+ وهاه = و. The compatibility of the morpho-syntactic tags of the three components (proclitic, radical, enclitic) is then checked. Only valid segmentations are kept and added into the word graph"
W05-0705,W98-1002,0,0.137409,"graph. 34 tified as such because it is found in the lexicon; the name (&رد,H (Lampard) is not in the lexicon and incorrectly stemmed as (&رد, + H (plural of the noun (د, (grater)); the name ( إی!ورEidur) is incorrectly tagged as a verb; and +<)&دیB (Gudjohnsen), which is not in the dictionary and for which the clitic stemmer does not produce any solutions receives the default tags adjective, noun, proper noun and verb, to be decided by the part-of-speech tagger. To improve this performance, we plan to enrich the Arabic lexicon with more proper names, using either name recognition (Maloney and Niv, 1998) or a back translation approach after name recognition in English texts (Al-Onaizan and Knight, 2002). Processing Steps: Part-of-speech analysis For the succeeding steps involving part-of-speech tagging, named entity recognition, division into nominal and verbal chains, and dependency extraction no changes were necessary for treating Arabic. After morphological analysis, as input to step 2a, part-of-speech tagging, we have the same type of word graph for Arabic text as for European text: each node is annotated with the surface form, a lemma and a part-of-speech in the graph. If a word is ambig"
W05-0705,P84-1108,0,0.0414101,"rent forms (corresponding to different surface forms) for the same entity. This minor problem was solved by storing the fully voweled forms of the entities (for application such as information retrieval as shown below) rather than the surface form. After named entity recognition, our methods of verbal and nominal chain recognition and dependency extraction did not require any modifications for Arabic. But since the sentence graphs, as mentioned above, are currently large, we have restricted the chains recognized to simple noun and verb chunks (Abney, 1991) rather than the more complex chains (Marsh, 1984) we recognize for European languages. Likewise, the only dependency relations that we extract for the moment are relations between nominal elements. We expect that the reduction in sentence graph once lemmas are all collected in the same word node will allow us to treat more complex dependency relations. 4 Integration in a CLIR application The results of the NLP steps produce, for all languages we treat, a set of normalized lemmas, a set of named entities and a set of nominal compounds (as well as other dependency relations for some 35 languages). These results can be used for any natural lang"
W05-0705,C96-1017,0,\N,Missing
W05-0705,W02-0505,0,\N,Missing
W05-0705,W04-1808,0,\N,Missing
W07-0810,P91-1022,0,0.219029,"ments to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands 1 Introduction Sentence alignment consists in mapping sentences of the source language with their translations in the target language. Automatic sentence alignment approaches face two kinds of difficulties: robustness and accuracy. A number of automatic sentence alignment techniques have been proposed (Kay and Röscheisen, 1993; Gale and Church, 1991; Brown et al., 1991; Debili and Samouda, 1992; Papageorgiou et al., 1994; Gaussier, 1995; Melamed, 1996; Fluhr et al., 2000). 73 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73–80, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics The method proposed in (Kay and Röscheisen, (Besançon et al., 2003). It is composed of a 1993) is based on the assumption that in order for linguistic analyzer, a statistical analyzer, a the sentences in a translation to correspond, the reformulator and a comparator (Figure 1): words in them must correspond. In other words, al"
W07-0810,chiao-etal-2006-evaluation,1,0.87343,"Missing"
W07-0810,W02-0506,0,0.0166723,"ach word in a general dictionary (Debili and Zouari, 1985). If this word is found, it will be associated with its lemma and all its morpho-syntactic tags. If the word is not found in the general dictionary, it is given a default set of morpho-syntactic tags based on its typography. For Arabic, we added to the morphological analyzer a new processing step: a Clitic stemmer (Larkey et al., 2002) which splits agglutinated words into proclitics, simple forms 75 • • and enclitics. If the simple form computed by the clitic stemmer does not exist in the general dictionary, re-write rules are applied (Darwish, 2002). For example, consider the token “” (with their ballon) and the included clitics “( ”بwith) and “”ه (their), the computed simple form “”آت does not exist in the general dictionary but after applying one of the dozen re-write rules, the modified simple form “”آة (ballon) is found in the general dictionary and the input token is segmented as:   =  ب+  آ ة+ ه. An Idiomatic Expressions recognizer which detects idiomatic expressions and considers them as single words for the rest of the processing. Idiomatic expressions are phrases or compound nouns that are listed i"
W07-0810,P91-1023,0,0.110942,"lyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands 1 Introduction Sentence alignment consists in mapping sentences of the source language with their translations in the target language. Automatic sentence alignment approaches face two kinds of difficulties: robustness and accuracy. A number of automatic sentence alignment techniques have been proposed (Kay and Röscheisen, 1993; Gale and Church, 1991; Brown et al., 1991; Debili and Samouda, 1992; Papageorgiou et al., 1994; Gaussier, 1995; Melamed, 1996; Fluhr et al., 2000). 73 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73–80, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics The method proposed in (Kay and Röscheisen, (Besançon et al., 2003). It is composed of a 1993) is based on the assumption that in order for linguistic analyzer, a statistical analyzer, a the sentences in a translation to correspond, the reformulator and a comparator (Figure 1): words in them must correspond"
W07-0810,W05-0705,1,0.906751,"Missing"
W07-0810,W96-0201,0,0.020304,"computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands 1 Introduction Sentence alignment consists in mapping sentences of the source language with their translations in the target language. Automatic sentence alignment approaches face two kinds of difficulties: robustness and accuracy. A number of automatic sentence alignment techniques have been proposed (Kay and Röscheisen, 1993; Gale and Church, 1991; Brown et al., 1991; Debili and Samouda, 1992; Papageorgiou et al., 1994; Gaussier, 1995; Melamed, 1996; Fluhr et al., 2000). 73 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73–80, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics The method proposed in (Kay and Röscheisen, (Besançon et al., 2003). It is composed of a 1993) is based on the assumption that in order for linguistic analyzer, a statistical analyzer, a the sentences in a translation to correspond, the reformulator and a comparator (Figure 1): words in them must correspond. In other words, all necessary information (and in particular, lexical Documents to be mapping) is deri"
W07-0810,P94-1051,0,0.0691835,"Missing"
W07-0810,J93-1004,0,\N,Missing
W07-0810,1998.amta-tutorials.5,0,\N,Missing
W07-0810,J93-1006,0,\N,Missing
W12-5108,D09-1050,0,0.0301563,"Missing"
W12-5108,besancon-etal-2010-lima,1,0.845855,"Missing"
W12-5108,boulaknadel-etal-2008-multi,0,0.0421588,"Missing"
W12-5108,N10-1029,0,0.124822,"Missing"
W12-5108,A94-1006,0,0.419027,"Missing"
W12-5108,W04-3250,0,0.263946,"Missing"
W12-5108,2005.mtsummit-papers.11,0,0.0448373,"Missing"
W12-5108,N03-1017,0,0.0276194,"Missing"
W12-5108,P93-1003,0,0.0392538,"Missing"
W12-5108,2005.mtsummit-posters.11,0,0.304752,"Missing"
W12-5108,W06-2402,0,0.0536135,"Missing"
W12-5108,E09-1057,0,0.0406034,"Missing"
W12-5108,2006.amta-papers.11,0,0.0308064,"Missing"
W12-5108,P03-1021,0,0.0303012,"Missing"
W12-5108,W10-4006,0,0.0245231,"Missing"
W12-5108,P02-1040,0,0.0840765,"Missing"
W12-5108,W09-2907,0,0.364864,"Missing"
W12-5108,2007.jeptalnrecital-long.37,0,0.0306627,"Missing"
W12-5108,2006.amta-papers.25,0,0.0550019,"Missing"
W12-5108,vintar-fiser-2008-harvesting,0,0.0542712,"Missing"
W12-5108,O04-2001,0,0.0554197,"Missing"
W13-2503,C02-2020,1,0.839754,"teration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is suffic"
W13-2503,P04-1067,0,0.0882668,"Missing"
W13-2503,hazem-morin-2012-adaptive,0,0.0527117,"Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as ‘anchor points’. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the sour"
W13-2503,C10-1070,0,0.0902848,"(2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilingual lexicon extraction 2.1 Related Work Standard Approach Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Formally, this approach is composed of the following three steps: Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only tra"
W13-2503,C10-1073,0,0.0383228,"Missing"
W13-2503,W11-1205,0,0.0127304,"ilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors. 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated S in a window of N words. Generally, an association measure like the mutual information (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. A large number of Word Sense Disambiguation WSD techniques were previously proposed in the literature. The most popular ones are those that compute semantic similarity with the help of existing thesauri such as WordNet (Fellbaum, 1998). This resource groups English words into sets of synonyms called synsets, provides short, general definitions and records various semantic relations (hypernymy, meronymy, etc.) between these synonym sets. This thesaurus has been applied to many tasks relying on"
W13-2503,J03-1002,0,0.00648395,"rd approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure. The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. On two specialized French-English comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 1 Pierre Zweigenbaum LIMSI-CNRS, F-91403 Orsay CEDEX France Introduction Bilingual lexicons play a vital role in many Natural Language Processing applications such as Machine Translation (Och and Ney, 2003) or CrossLanguage Information Retrieval (Shi, 2009). Research on lexical extraction from multilingual corpora have largely focused on parallel corpora. The scarcity of such corpora in particular for specialized domains and for language pairs not involving English pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora are comprised of texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The main work in this research area could be seen as an extension of Harris"
W13-2503,2009.mtsummit-posters.14,0,0.0925898,"ainder of the paper is organized as follows: Section 2 presents the standard approach and recalls in some details previous work addressing the task of bilingual lexicon extraction from comparable corpora. In section 3 we present our context disambiguation process. Before concluding and presenting directions for future work, we describe in section 4 the experimental protocol we followed and discuss the obtained results. 2 Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as ‘anchor points’. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar`es (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Bilin"
W13-2503,P95-1050,0,0.14801,"Missing"
W13-2503,P99-1067,0,0.0734271,"of source and target vectors, source terms vectors are translated in the target language by using a seed bilingual dictionary. Whenever it provides several translations for an element, all proposed translations are considered. Words not included in the bilingual dictionary are simply ignored. 3. Comparison of source and target vectors: Translated vectors are compared to target ones using a similarity measure. The most widely used is the cosine similarity, but many authors have studied alternative metrics such as the Weighted Jaccard index (Prochasson et al., 2009) or the City-Block distance (Rapp, 1999). According to similarity values, a ranked list of translations for S is obtained. 17 Word to be translated (source language) Bilingual Dictionary WordNet Context Vectors (Target language) Building Context Vector Context vector Disambiguated Context vector Translated Context vector Figure 1: Overall architecture of the lexical extraction approach 3 Context Vector Disambiguation context vector. There is a relatively large number of word-to-word similarity metrics that were previously proposed in the literature, ranging from path-length measures computed on semantic networks, to metrics based on"
W13-2503,P94-1019,0,\N,Missing
W16-4807,W14-5316,0,0.0649194,"Missing"
W16-4807,P98-1002,0,0.0156317,"a set, we removed 500 documents (around 6,000 words) for each language to be used in training and evaluating our system. We used the rest of the data to compile lexicons, for each language, by extracting the unique vocabulary using a script. We also used external lexicon for RB. 10 A standard metric used to evaluate the quality of a set of annotations in classification tasks. This term refers to the use of more than one language in a single interaction. The classic code-switching framework does not always apply to Arabic for many complex reasons which are out of our scope. Researchers like D. Sankoff (1998) suggested to classify the use of mixed languages in Arabic as a separate phenomenon and not code-switching. Others like Davies et al. (2013) called it ’mixed Arabic’. We will use ’language mixing’ to refer to both code-switching and borrowing. 12 Documents containing vocabulary of different languages. In our case, Arabic written in Latin script plus Berber, English, French, German, Spanish and Swedish words. 13 Including Algerian, Egyptian, Gulf, Kuwaiti/Iraqi, Levantine, Moroccan and Tunisian Arabic. 14 Being familiar with north African Arabic dialects, we have noticed that Maltese is much c"
W16-4807,W14-3629,0,0.0673557,"Missing"
W16-4807,P12-3005,0,0.096452,"Missing"
W16-4807,P11-2007,0,0.0807122,"Missing"
W16-4807,W14-3901,0,0.266654,"Missing"
W16-4809,L16-1284,0,0.0241158,"Missing"
W16-4809,W14-5904,0,0.111254,"Missing"
W16-4809,P13-2081,0,0.201674,"Missing"
W16-4809,bouamor-etal-2014-multidialectal,0,0.0991641,"Missing"
W16-4809,J96-2004,0,0.0914454,"Missing"
W16-4809,J14-1006,0,0.116132,"Missing"
W16-4809,W16-4801,0,0.0496792,"Missing"
W16-4809,W15-5410,1,0.828429,"Missing"
W16-4821,P13-2081,0,0.0320962,"ALI have been applied to the DSL and DLV tasks for some languages. Goutte et al. (2016) give a comprehensive bibliography of the recently published papers dealing with these tasks. Discriminating between Arabic varieties is also an active research area although limited work has been done, so far, to distinguish between written Arabic varieties. The main reason is the lack of annotated data (Benajiba and Diab, 2010). Zaidan (2012) in his PhD distinguished between four Arabic varieties (Modern Standard Arabic (MSA), Egyptian, Gulf and Levantine dialects) using character and word n-gram models. Elfardy and Diab (2013) identified MSA from Egyptian at a sentence level, Tillmann et al. (2014) proposed an approach to improve classifying Egyptian and MSA at a sentence level, and Saˆadane (2015) in her PhD distinguished between Maghrebi Arabic (Algerian, Moroccan and Tunisian dialects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program re"
W16-4821,W15-5413,0,0.297764,"Missing"
W16-4821,L16-1284,0,0.139644,"Missing"
W16-4821,W15-5407,0,0.0680657,"a exists in the form of phone conversations and television program recordings, but, in general, dialectal Arabic data sets are hard to come by” (Zaidan and Callison-Burch, 2014). Akbacak et al. (2009), Akbacak et al. (2011), Lei and Hansen (2011), Boril et al. (2012), and Zhang et al. (2013) are some examples of work done to distinguish between spoken Arabic varieties. Similarly to Goutte and L´eger (2015), we experimented with both character-based and word-based ngrams as features. However, we used only one prediction step instead of two for both sub-tasks. Compared to the system proposed by Malmasi and Dras (2015), we used the same set of features with only one SVM classifier instead of an ensemble of SVM classifiers. 3 Methodology and Data We used a supervised machine learning approach where we trained a linear SVM classifier (LinearSVC) as implemented in the Scikit-learn package1 . In sub-task 1, we submitted two runs (run1 and run2). We experimented with different character-based and word-based n-grams and different combinations as features, and we reported only the best scoring features for each run. In run1: we used character-based 4-grams as features using TF-IDF weighting scheme. In run2: we use"
W16-4821,W16-4801,0,0.0403558,"Missing"
W16-4821,C12-1160,0,0.0717191,"Missing"
W16-4821,W14-5313,0,0.123659,"et al. (2016) give a comprehensive bibliography of the recently published papers dealing with these tasks. Discriminating between Arabic varieties is also an active research area although limited work has been done, so far, to distinguish between written Arabic varieties. The main reason is the lack of annotated data (Benajiba and Diab, 2010). Zaidan (2012) in his PhD distinguished between four Arabic varieties (Modern Standard Arabic (MSA), Egyptian, Gulf and Levantine dialects) using character and word n-gram models. Elfardy and Diab (2013) identified MSA from Egyptian at a sentence level, Tillmann et al. (2014) proposed an approach to improve classifying Egyptian and MSA at a sentence level, and Saˆadane (2015) in her PhD distinguished between Maghrebi Arabic (Algerian, Moroccan and Tunisian dialects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program recordings which are easy to transcribed. “The problem is somewhat mitigate"
W16-4821,J14-1006,0,0.0223516,"lects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program recordings which are easy to transcribed. “The problem is somewhat mitigated in the speech domain, since dialectal data exists in the form of phone conversations and television program recordings, but, in general, dialectal Arabic data sets are hard to come by” (Zaidan and Callison-Burch, 2014). Akbacak et al. (2009), Akbacak et al. (2011), Lei and Hansen (2011), Boril et al. (2012), and Zhang et al. (2013) are some examples of work done to distinguish between spoken Arabic varieties. Similarly to Goutte and L´eger (2015), we experimented with both character-based and word-based ngrams as features. However, we used only one prediction step instead of two for both sub-tasks. Compared to the system proposed by Malmasi and Dras (2015), we used the same set of features with only one SVM classifier instead of an ensemble of SVM classifiers. 3 Methodology and Data We used a supervised mac"
W18-1203,C12-2029,0,0.0278964,"ck and Meechan (1998) defined borrowing as a morphological or a phonological adaptation of a word from one language to another and code-switching as the use of a foreign word, as it is in its original language, to express something in another lan1.1 Related Work There has been some interesting work in detecting code mixing for a couple of languages/language varieties, mostly using traditional sequence labelling algorithms like Conditional Random Field (CRF), Hidden Markov Model (HMM), linear kernel Support Vector Machines (SVMs) and a combination of different methods and linguistic resources (Elfardy and Diab, 2012; Elfardy et al., 2013; Barman et al., 2014b,a; Diab et al., 2016; Samih and Maier, 2016; Adouane and Dobnik, 2017). Prior work that is most closely related to our work using neural networks and related languages, Samih et al. (2016) used supervised 22 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 22–31 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics ture is often referred to, somewhat mistakenly as Algerian Arabic. In this paper, we use the term Algerian language to refer to a mixture of languages and language varieties spok"
W18-1203,W16-4809,1,0.893646,"itionally show the (incorrect) translation proposed by Google translate e., where words in black are additional words not appearing in the original sentence. (1)  Ég úÎK ñJÊJ a. @P ñÓ H. AJ.Ë Qº ð é¯A¢Ë@ . b. [muræk ælbæb sekkær wu ætQaqæ èæl si:ltupli:] c. Please open the window and close the door behind you d. French Algerian Berber MSA Berber MSA Algerian e. SELTOPLEY POWER SOLUTION AND SUGAR FOR MORAK PAPER Linguistic Situation in Algeria The linguistic landscape in Algeria consists of several languages which are used in different social and geographic contexts to different degrees (Adouane et al., 2016a): local Arabic varieties (ALG), Modern Standard Arabic (MSA) which is the only standardised Arabic variety, Berber which is an Afro-Asiatic language different from Arabic and widely spoken in North Africa, and other non-Arabic languages such as French, English, Spanish, Turkish, etc. A typical text consists of a mixture of these languages, and this mixAll the words in different languages are normally written in the Arabic script, which causes high degree of lexical ambiguity and therefore even if we had dictionaries (only available for MSA) it would be hard to disambiguate word senses this w"
W18-1203,E17-1005,0,0.0184835,"sed tasks assuming that linguistic structures are predictive of the labels used in these tasks. Approaches like this are known as transfer learning or multi-task learning (MTL) and are classified as a semi-supervised approaches (with no bootstrapping) (Zhou et al., 2004). There is an increasing interest in evaluating different frameworks (Ando and Zhang, 2005; Pan and Yang, 2010) and comparing neural network models (Cho et al., 2014; Yosinski et al., 2014). Some studies have shown that MTL is useful for certain tasks (Sutton et al., 2007) while others reported that it is not always effective (Alonso and Plank, 2017). Bootstrapping (Nigam et al., 2000) is a genExcept for MSA, Arabic varieties are neither well-documented nor well-studied, and they are classified as under-resourced languages. Furthermore, social media are the only source of written texts for Algerian Arabic. The work in NLP on Algerian Arabic and other Arabic varieties also suffers severely from the lack of labelled (and even unlabelled) data that would allow any kind of supervised training. Another challenge is that we have to deal with all the complications present in social media domain, namely the use of short texts, spelling and word s"
W18-1203,W01-0501,0,0.033832,"organized as follows: in Section 2, we give a brief overview of methods for levering learning from limited datasets. In Section 3, we describe the data. In Section 4, we present the 24 croblogs, forums, community media sites and user reviews.1 eral and commonly used method of countering the limits of labelled datasets for learning. It is a semi-supervised method where a well-performing model is used to automatically label new data which is subsequently used as a training data for another model. This helps to enhance supervised learning. However, this is also not always effective. For example, Pierce and Cardie (2001) and Ando and Zhang (2005) show that bootstrapping degraded the performance of some classifiers. 3 The new raw corpus contains mainly short nonedited texts which require further processing before useful information can be extracted from them. We cleaned and pre-processed the corpus following the pre-processing and normalisation methods described by Adouane and Dobnik (2017). The data pre-processing and normalisation is based on applying certain linguistic rules, including: 1. Removal of non-linguistic words like punctuation and emoticons (indeed emoticons and inconsistent punctuation are abund"
W18-1203,W14-3902,0,0.123222,"orphological or a phonological adaptation of a word from one language to another and code-switching as the use of a foreign word, as it is in its original language, to express something in another lan1.1 Related Work There has been some interesting work in detecting code mixing for a couple of languages/language varieties, mostly using traditional sequence labelling algorithms like Conditional Random Field (CRF), Hidden Markov Model (HMM), linear kernel Support Vector Machines (SVMs) and a combination of different methods and linguistic resources (Elfardy and Diab, 2012; Elfardy et al., 2013; Barman et al., 2014b,a; Diab et al., 2016; Samih and Maier, 2016; Adouane and Dobnik, 2017). Prior work that is most closely related to our work using neural networks and related languages, Samih et al. (2016) used supervised 22 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 22–31 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics ture is often referred to, somewhat mistakenly as Algerian Arabic. In this paper, we use the term Algerian language to refer to a mixture of languages and language varieties spoken in Algeria, and the term Algerian variet"
W18-1203,W14-3915,0,0.532369,"orphological or a phonological adaptation of a word from one language to another and code-switching as the use of a foreign word, as it is in its original language, to express something in another lan1.1 Related Work There has been some interesting work in detecting code mixing for a couple of languages/language varieties, mostly using traditional sequence labelling algorithms like Conditional Random Field (CRF), Hidden Markov Model (HMM), linear kernel Support Vector Machines (SVMs) and a combination of different methods and linguistic resources (Elfardy and Diab, 2012; Elfardy et al., 2013; Barman et al., 2014b,a; Diab et al., 2016; Samih and Maier, 2016; Adouane and Dobnik, 2017). Prior work that is most closely related to our work using neural networks and related languages, Samih et al. (2016) used supervised 22 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 22–31 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics ture is often referred to, somewhat mistakenly as Algerian Arabic. In this paper, we use the term Algerian language to refer to a mixture of languages and language varieties spoken in Algeria, and the term Algerian variet"
W18-1203,W16-5806,0,0.115098,"nother lan1.1 Related Work There has been some interesting work in detecting code mixing for a couple of languages/language varieties, mostly using traditional sequence labelling algorithms like Conditional Random Field (CRF), Hidden Markov Model (HMM), linear kernel Support Vector Machines (SVMs) and a combination of different methods and linguistic resources (Elfardy and Diab, 2012; Elfardy et al., 2013; Barman et al., 2014b,a; Diab et al., 2016; Samih and Maier, 2016; Adouane and Dobnik, 2017). Prior work that is most closely related to our work using neural networks and related languages, Samih et al. (2016) used supervised 22 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 22–31 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics ture is often referred to, somewhat mistakenly as Algerian Arabic. In this paper, we use the term Algerian language to refer to a mixture of languages and language varieties spoken in Algeria, and the term Algerian variety (ALG) to refer to the local variety of Arabic, which is used alongside other languages such as, for example Berber (BER). deep neural networks (LSTM) and a CRF classifier on the top of it"
W18-3927,W13-2408,0,0.0757231,"Missing"
W18-3927,W14-0405,0,0.136476,"Missing"
W18-3927,P16-1101,0,0.140689,"Missing"
W18-3927,L18-1446,1,0.615195,"data by significantly limiting the necessary data and instead extrapolating the relevant knowledge from another, related domain. This contribution generalizes previous results for POS tagging of user generated content This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 235 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 235–243 Santa Fe, New Mexico, USA, August 20, 2018. in social media for five languages: English, French, Italian, German and Spanish (Meftah et al., 2018), by applying our approach on three Twitter corpora of South-Slavic languages: Slovene, Croatian, and Serbian. Figure 1: Example of a morphologically-tagged sentence in Slovene: To ni nobena novost (”This is not a novelty” in English) . 2 The Model In this section, we introduce the model we experimented for MS tagging of South-Slavic languages. The model takes as input a tweet T , separated into a succession of n tokens wi , such as T = {w1 , w2 , ..., wn }. The objective is to predict the morpho-syntactic tag yˆi for each token wi of the tweet. 2.1 System Architecture We use a similar archite"
W18-3927,D16-1046,0,0.0845226,"Missing"
W18-3927,W18-3901,0,0.0586752,"Missing"
Y15-1016,W13-3520,0,0.0434952,"rojected labels with partially supervised monolingual information in order to filter out invalid label sequences. For example, Li et al. (2012), T¨ackstr¨om et al. (2013b) and Wisniewski et al. (2014) have proposed to improve projection performance by using a dictionary of valid tags for each word (coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named en"
Y15-1016,C04-1053,0,0.523267,"daptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the poor accuracy of word-alignment algo134 rithms, and also to the weak or incomplete inherent match between the two sides of a bilingual c"
Y15-1016,W06-2920,0,0.050027,"lied our method to build RNN POS taggers for three more target languages — German, Greek and Spanish — with English as the source language, in order to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). Our training and validation (English) data extracted from the Europarl corpus (Koehn, 2005) are a subset of the training data of (Das and Petrov, 2011; Duong et al., 2013). The sizes of the data sets are: 65, 000 (train) and 10, 000 (dev) bi-sentences. For testing, we used the same test corpora (from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006)) as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). The evaluation metric (per-token accuracy) and the Universal Tagset are the same as before. The source sides of the training corpora (ARCADE II and Europarl) and the validation corpora are tagged with the English TreeTagger Toolkit. Using the matching provided by Petrov et 4 For RNN a single system is used for German, Greek and Spanish 138 al. (2012) we map the TreeTagger and the CoNLL tagsets to a common Universal Tagset. In order to build our unsupervised tagger based on a Simple Cross-lingual Projection (Algorithm 1)"
Y15-1016,A00-1031,0,0.103079,"ct annotations from a source language to a target language, to build unsupervised POS taggers. The algorithm is shortly recalled below. Algorithm 1 : Simple POS Tagger 1: Tag source side of the parallel corpus. 2: Word align the parallel corpus with Giza++ (Och and Ney, 2000) or other word alignment tools. 3: Project tags directly for 1-to-1 alignments. 4: For many-to-one mappings project the tag of the middle word. 5: The unaligned words (target) are tagged with their most frequent associated tag in the corpus. 6: Learn POS tagger on target side of the bi-text with, for instance, TNT tagger (Brants, 2000). 3.2 Unsupervised POS Tagger Based on Recurrent Neural Network There are two major architectures of neural networks: Feedforward (Bengio et al., 2006) and Recurrent Neural Networks (RNN) (Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language models based on recurrent architecture achieve better performance than language models based on feedforward architecture. This is due to the fact that recurrent neural networks do not use a context of limited size. This property led us to use, in our experiments, a simple recurrent architecture (Elman, 1990). PACLIC 29 In this section, we"
Y15-1016,P11-1061,0,0.617034,"h. Secondly, the performance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the"
Y15-1016,P13-2112,0,0.526186,"rmance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these approaches is due to the poor accuracy of wor"
Y15-1016,D12-1001,0,0.09708,"lid tags for each word (coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role lab"
Y15-1016,P04-1013,0,0.0584824,"f the parallel corpus, and to use it for tagging target text. Before describing our bilingual neural network POS tagger, we present the simple crosslingual projection method, considered as our baseline in this work. 3.1 Figure 1: Architecture of the recurrent neural network. labeled text and apply it to tag target language text. We also show that the architecture proposed is well suited for lightly supervised training (adaptation). Finally, several works have investigated how to apply neural networks to NLP applications (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Henderson, 2004; Mikolov et al., 2010; Federici and Pirrelli, 1993). While Federici and Pirrelli (1993) was one of the earliest attempts to develop a part-of-speech tagger based on a special type of neural network, Bengio et al. (2006) and Mikolov et al. (2010) applied neural networks to build language models. Collobert and Weston (2008) and Collobert et al. (2011) employed a deep learning framework for multi-task learning including part-of-speech tagging, chunking, namedentity recognition, language modelling and semantic role-labeling. Henderson (2004) proposed training methods for learning a statistical pa"
Y15-1016,N15-1157,0,0.430244,"(coming from Wiktionary 2 ). In another vein, various studies based on crosslingual representation learning methods have proposed to avoid using such pre-processed and noisy alignments for label projection. First, these approaches learn language-independent features, across many different languages (Al-Rfou et al., 2013). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. To induce interlingual features, several resources have been used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klement"
Y15-1016,D11-1110,0,0.409403,"Missing"
Y15-1016,P12-1073,0,0.172818,"Missing"
Y15-1016,D12-1127,0,0.0598002,"Missing"
Y15-1016,P00-1056,0,0.30196,"results to state-ofthe-art unsupervised POS taggers. 135 Unsupervised POS Tagger Based on a Simple Cross-lingual Projection Our simple POS tagger (described by Algorithm 1) is close to the approach introduced in Yarowsky et al. (2001). These authors were the first to use automatic word alignments (from a bilingual parallel corpus) to project annotations from a source language to a target language, to build unsupervised POS taggers. The algorithm is shortly recalled below. Algorithm 1 : Simple POS Tagger 1: Tag source side of the parallel corpus. 2: Word align the parallel corpus with Giza++ (Och and Ney, 2000) or other word alignment tools. 3: Project tags directly for 1-to-1 alignments. 4: For many-to-one mappings project the tag of the middle word. 5: The unaligned words (target) are tagged with their most frequent associated tag in the corpus. 6: Learn POS tagger on target side of the bi-text with, for instance, TNT tagger (Brants, 2000). 3.2 Unsupervised POS Tagger Based on Recurrent Neural Network There are two major architectures of neural networks: Feedforward (Bengio et al., 2006) and Recurrent Neural Networks (RNN) (Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language mode"
Y15-1016,petrov-etal-2012-universal,0,0.0989177,"can then use the RNN POS tagger, initially trained on source side, to tag the target side (because of our common vector representation). We also use two hidden layers (our preliminary experiments have shown better performance than one hidden layer), with variable sizes (usually 80-1024 neurons) and sigmoid activation function. These hidden layers inherently capture word alignment information. The output layer of our model contains 12 neurons, this number is determined by the POS tagset size. To deal with the potential mismatch in the POS tagsets of source and target languages, we adopted the Petrov et al. (2012) universal tagset (12 tags common for most languages): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), . (punctuation marks) and X (all other categories, e.g., foreign words, abbreviations). 136 Therefore, each output neuron corresponds to one POS tag in the tagset. The softmax activation function is used to normalize the values of output neurons to sum up to 1. Finally, the current word w (in input) is tagged with most probable output tag. 3"
Y15-1016,2005.mtsummit-papers.11,0,0.0783864,"from the train set) tagged with the French TreeTagger Toolkit (Schmid, 1995) and manually checked. Encouraged by the results obtained on the English–French language pair, and in order to confirm our results, we run additional experiments on other languages, we applied our method to build RNN POS taggers for three more target languages — German, Greek and Spanish — with English as the source language, in order to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). Our training and validation (English) data extracted from the Europarl corpus (Koehn, 2005) are a subset of the training data of (Das and Petrov, 2011; Duong et al., 2013). The sizes of the data sets are: 65, 000 (train) and 10, 000 (dev) bi-sentences. For testing, we used the same test corpora (from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006)) as (Das and Petrov, 2011; Duong et al., 2013; Gouws and Søgaard, 2015a). The evaluation metric (per-token accuracy) and the Universal Tagset are the same as before. The source sides of the training corpora (ARCADE II and Europarl) and the validation corpora are tagged with the English TreeTagger Toolkit. Using the matc"
Y15-1016,N12-1052,0,0.241524,"Missing"
Y15-1016,N13-1126,0,0.0891296,"Missing"
Y15-1016,Q13-1001,0,0.0364857,"Missing"
Y15-1016,P12-1068,0,0.140177,"and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to learn a common language-independent feature space. Our common (multilingual) representation is based on the occurrence of source and target words in a parallel corpus. Using this representation, we learn a cross-lingual POS tagger (multilingual POS tagger if a multilingual parallel corpus is used) based on a recurrent neural network (RNN) on the source 2 http://www.wiktionary.org/ PACLIC 29 Our approach is the following: we assume that we have a POS tagger in the source language and a parallel corpus. The"
Y15-1016,F14-1005,0,0.0265144,"Missing"
Y15-1016,D14-1187,0,0.124403,"Missing"
Y15-1016,W14-1613,0,0.0137828,"used, including bilingual lexicon (Durrett et al., 2012; Gouws and Søgaard, 2015a) and parallel corpora (T¨ackstr¨om et al., 2013a; Gouws et al., 2015b). Cross-lingual representation learning have achieved good results in different NLP applications such as cross-language POS tagging and cross-language super sense (SuS) tagging (Gouws and Søgaard, 2015a), cross-language named entity recognition (T¨ackstr¨om et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015b), cross language dependency parsing (Durrett et al., 2012; T¨ackstr¨om et al., 2013a; Xiao and Guo, 2014) and cross language semantic role labeling ( Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to learn a common language-independent feature space. Our common (multilingual) representation is based on the occurrence of source and target words in a parallel corpus. Using this representation, we learn a cross-lingual POS tagger (multilingual POS tagger if a multilingual parallel corpus is used) based on a recurrent neural network (RNN) on the source 2 http://www.wiktionary.org/ PACLIC 29 Our approach is the following: we assume tha"
Y15-1016,H01-1035,0,0.848938,"erman, Greek and Spanish. Secondly, the performance of our approach is evaluated for German in a weakly supervised context, using several amounts of target adaptation data (Section 5). Finally, Section 6 concludes our study and presents our future work. 2 Related Work Several studies have used cross-lingual projection to transfer linguistic annotations from a resourcerich language to a resource-poor language in order to train NLP tools for the target language. The projection approach has been successfully used to transfer several linguistic annotations between languages. Examples include POS (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity (Kim and Lee, 2012), syntactic constituent (Jiang et al., 2011), word senses (Bentivogli et al., 2004; Van der Plas and Apidianaki, 2014), and semantic role labeling (Pad´o , 2007; Annesi and Basili, 2010). In these approaches, the source language is tagged, and tags are projected from the source language to the target language through the use of word alignments in parallel corpora. Then, these partial noisy annotations can be used in conjunction with robust learning algorithms to build unsupervised NLP tools. One limitation of these ap"
Y15-2013,2010.amta-papers.16,0,0.0393121,"Missing"
Y15-2013,W09-0432,0,0.152747,"domain monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. 106 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 106 - 115 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Nasredine Semmar, Othman Zennaki and Meriama Laib PACLIC 29 (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domain-specific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. Daumé III and Jagarlamudi (2011) used dictionary mining techniques to find translations for unseen"
Y15-2013,besancon-etal-2010-lima,1,0.656741,"Missing"
Y15-2013,bouamor-etal-2012-identifying,1,0.948328,"of the domain vocabulary on the translation results. 3 FSM Library is available from AT&T for non-commercial use as executable binary programs. 4 http://wing.comp.nus.edu.sg/~forecite/services/parscit100401/crfpp/CRF++-0.51/doc/. 110 The domain vocabulary is represented in the case of Moses by the specialized parallel corpus (Emea) which is added to the training data (Europarl). In the case of the EBMT system, the domain vocabulary is identified by a bilingual lexicon which is extracted automatically from the specialized parallel corpus (Emea) using a word alignment tool (Semmar et al., 2010; Bouamor et al., 2012). This specialized bilingual lexicon is added to the English-French lexicon which is used jointly by the cross-language search engine and the bilingual reformulator. To evaluate the performance of the EBMT system and Moses, we used the BLEU score (Papineni et al; 2002). Run n°. 1 2 3 4 5 6 7 8 Training (# sentences) 150K (Europarl) 150K+10K (Europarl+Emea) 150K+20K (Europarl+Emea) 150K+30K (Europarl+Emea) 500K (Europarl) 500K+10K (Europarl+Emea) Tuning (# sentences) 3.75K (Europarl) 1.5K (Europarl) 1.5K (Europarl) 1.5K (Europarl) 2.5K (Europarl) 2K+0.5K (Europarl+Emea) 500K+20K (Europarl+Emea)"
Y15-2013,W07-0722,0,0.025275,"cific language models on in-domain monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. 106 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 106 - 115 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Nasredine Semmar, Othman Zennaki and Meriama Laib PACLIC 29 (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domain-specific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. Daumé III and Jagarlamudi (2011) used dictionary mining techniqu"
Y15-2013,P11-2071,0,0.0322289,"Missing"
Y15-2013,C14-1192,0,0.0277724,"Missing"
Y15-2013,1998.amta-tutorials.5,0,0.68909,"of translations. In order to illustrate the functioning of the EBMT system, we indexed a small textual database composed of 1127 French sentences extracted from the ARCADE II corpus (Veronis et al., 2008) and we considered the input source sentence &quot;Social security funds in Greece encourage investment in innovation.&quot; as the sentence to translate. 3.1 The Cross-language Search Engine The role of the cross-language search engine is to extract for each sentence to translate (user’s query) sentences or sub-sentences from an indexed monolingual corpus in the target language (Davis and Ogden, 1997; Grefenstette, 1998; Baeza-Yates and Ribeiro-Neto, 1999). These sentences or subsentences correspond to a total or a partial translation of the sentence to translate. The crosslanguage search engine used in the EBMT system is based on a deep linguistic analysis of the query and the monolingual corpus to be indexed and uses a weighted vector space model (Salton and McGill, 1986; Besançon et al., 2003; Semmar et al., 2006). This cross-language search engine is composed of the following modules: • A linguistic analyzer based on the open source multilingual platform LIMA 1 (Besançon et al., 2010) which includes a mo"
Y15-2013,2005.eamt-1.19,0,0.504913,"Missing"
Y15-2013,P07-2045,0,0.00677593,"Missing"
Y15-2013,W10-1715,0,0.0242075,"LIC 29 For these experiments, we used only EnglishFrench training corpora of runs 2, 3 and 4 to build Moses’s translation and language models. We measure the translation quality on the same test sets of the previous experiments (500 parallel sentences extracted randomly from Europarl for the In-Domain test and 500 pairs of sentences extracted randomly from Emea for the Out-OfDomain test). Because the bilingual lexicon which is extracted automatically from the specialized parallel corpus is composed of entries in their normalized forms (lemmas), we used the factored translation model of Moses (Koehn et al., 2010). This model accepts the use of additional annotations at the word level and operates on lemmas instead of surface forms. The translation process consists, first, in translating lemmas of words from the source language into the target language, and second, in generating the inflected forms for each lemma. Tables 8 and 9 present respectively the Moses’s results for the In-Domain and the Out-Of-Domain sentences when using the three integration strategies. The first important point to mention here is that there is improvement of the BLEU score in all the integration methods for the Out-Of-Domain"
Y15-2013,W02-1405,0,0.71827,"ed (Trujillo, 1999; Hutchins, 2003). Rule-Based MT (RBMT) approaches require manually made bilingual lexicons and linguistic rules, which can be costly, and not generalized to other languages. Corpus-based machine translation approaches are effective only when large amounts of parallel corpora are available. However, parallel corpora are only available for a limited number of language Related Work Domain adaptation consists in adapting MT systems designed for one domain to work in another. Several ideas have been explored and implemented in domain adaptation of SMT (Bungum and Gambäck, 2011). Langlais (2002) integrated domain-specific lexicons in the translation model of a SMT engine which yields a significant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on in-domain monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. 106 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 106 - 115 Shanghai, China, Oct"
Y15-2013,lewis-etal-2010-achieving,0,0.0310693,"y, and not generalized to other languages. Corpus-based machine translation approaches are effective only when large amounts of parallel corpora are available. However, parallel corpora are only available for a limited number of language Related Work Domain adaptation consists in adapting MT systems designed for one domain to work in another. Several ideas have been explored and implemented in domain adaptation of SMT (Bungum and Gambäck, 2011). Langlais (2002) integrated domain-specific lexicons in the translation model of a SMT engine which yields a significant reduction in word error rate. Lewis et al. (2010) developed domain specific SMT by pooling all training data into one large data pool, including as much in-domain parallel data as possible. They trained highly specific language models on in-domain monolingual data in order to reduce the dampening effect of heterogeneous data on quality within the domain. Hildebrand et al. 106 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 106 - 115 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Nasredine Semmar, Othman Zennaki and Meriama Laib PACLIC 29 (2005) used an approach which consisted essential"
Y15-2013,P02-1038,0,0.418433,"Missing"
Y15-2013,J03-1002,0,0.00406052,"my in the bilingual lexicon. To handle the first two issues, we proposed to take into account translation candidates returned by the cross-language search engine even if these translations correspond only to a part of the sentence to translate. However, for the presence of the polysemy in the bilingual lexicon, the EBMT system has no specific treatment. This can explain partially why the EBMT system is outperformed by Moses when translating InDomain sentences. It seems that translation table probabilities which are computed during the word alignment process with Giza++ (Och and Ney, 112 2002; Och and Ney, 2003) have contributed to choose the right translation. On the other hand, we noted that most of Moses’s translation errors for Out-Of-Domain sentences are related to vocabulary. For example, Moses proposes the compound word “glycémie artérielle” as a translation for the expression “fasting blood glucose” in run 4 which is not correct. In SMT systems such as Moses, phrase tables are the main knowledge source for the machine translation decoder. The decoder consults these tables to figure out how to translate an input sentence from the source language into the target language. These tables are built"
Y15-2013,P02-1040,0,0.0920305,"Missing"
Y15-2013,2011.eamt-1.40,0,0.435681,"o separate domain models. Each model is trained from small amounts of domain-specific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. Daumé III and Jagarlamudi (2011) used dictionary mining techniques to find translations for unseen words from comparable corpora and they integrated these translations into a statistical phrase-based translation system. They reported improvements in translation quality (between 0.5 and 1.5 BLEU points) on four domains and two language pairs. Pecina et al. (2011) exploited domain-specific data acquired by domain-focused web-crawling to adapt generaldomain SMT systems to new domains. They observed that even small amounts of in-domain parallel data are more important for translation quality than large amounts of in-domain monolingual data. Wang et al. (2012) used a single translation model and generalized a single-domain decoder to deal with different domains. They used this method to adapt large-scale generic SMT systems for 20 language pairs in order to translate patents. The authors reported a gain of 0.35 BLEU points for patent translation and a los"
Y15-2013,W09-2907,0,0.0910463,"r example, Moses proposes the compound word “glycémie artérielle” as a translation for the expression “fasting blood glucose” in run 4 which is not correct. In SMT systems such as Moses, phrase tables are the main knowledge source for the machine translation decoder. The decoder consults these tables to figure out how to translate an input sentence from the source language into the target language. These tables are built automatically using the open source word alignment tool Giza++. However, Giza++ could produce errors in particular when it aligns multiword expressions. (Bouamor et al, 2012; Ren et al., 2009) showed that the integration of multiword expressions in Moses’s translation model improves the translation quality. Multiword expressions include a large list of categories such as collocations, compound words, idiomatic expressions, named entities and domain-specific terms (Baldwin and Kim, 2010). To reduce word alignment errors with Giza++, we propose the following three methods to integrate into Moses the bilingual lexicon which is extracted automatically by our word alignment tool from the specialized parallel corpus (Emea): • MosesCORPUS: In this method, we add the extracted bilingual le"
Y15-2013,semmar-etal-2006-deep,1,0.771159,"ross-language search engine is to extract for each sentence to translate (user’s query) sentences or sub-sentences from an indexed monolingual corpus in the target language (Davis and Ogden, 1997; Grefenstette, 1998; Baeza-Yates and Ribeiro-Neto, 1999). These sentences or subsentences correspond to a total or a partial translation of the sentence to translate. The crosslanguage search engine used in the EBMT system is based on a deep linguistic analysis of the query and the monolingual corpus to be indexed and uses a weighted vector space model (Salton and McGill, 1986; Besançon et al., 2003; Semmar et al., 2006). This cross-language search engine is composed of the following modules: • A linguistic analyzer based on the open source multilingual platform LIMA 1 (Besançon et al., 2010) which includes a morphological analyzer, a Part-Of-Speech tagger and a syntactic analyzer. This analyzer processes both sentences to be indexed in the target language and the sentence to translate in order to produce a set of normalized lemmas with their linguistic information (Part-Of-Speech, gender, number, etc.). The syntactic analyzer implements a dependency grammar to produce syntactic dependencies relations (used t"
Y15-2013,R15-1075,1,0.259413,"EU points for generic translation. 3 The Translation Process of the ExampleBased Machine Translation System The translation process of the EBMT system consists of several steps: retrieving translation candidates from a monolingual corpus using a cross-language search engine, producing translation hypotheses using a transducer, using word lattices to represent the combination of translation candidates and translation hypotheses, and choosing the n-best translations according to a statistical language model learned from a target 107 language corpus (Semmar and Bouamor 2011; Semmar et al., 2011; Semmar et al., 2015). This process uses a cross-language search engine, a bilingual reformulator (transducer) and a generator of translations. In order to illustrate the functioning of the EBMT system, we indexed a small textual database composed of 1127 French sentences extracted from the ARCADE II corpus (Veronis et al., 2008) and we considered the input source sentence &quot;Social security funds in Greece encourage investment in innovation.&quot; as the sentence to translate. 3.1 The Cross-language Search Engine The role of the cross-language search engine is to extract for each sentence to translate (user’s query) sen"
Y15-2013,D08-1090,0,0.185153,"ffect of heterogeneous data on quality within the domain. Hildebrand et al. 106 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 106 - 115 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Nasredine Semmar, Othman Zennaki and Meriama Laib PACLIC 29 (2005) used an approach which consisted essentially in performing test-set relativization (choosing training samples that look most like the test data) to improve the translation quality when changing the domain. Civera and Juan (2007), and Bertoldi and Federico (2009) used monolingual corpora and Snover et al. (2008) used comparable corpora to adapt MT systems designed for Parliament domain to work in News domain. The obtained results showed significant gains in performance. Banerjee et al. (2010) combined two separate domain models. Each model is trained from small amounts of domain-specific data. This data is gathered from a single corporate website. The authors used document filtering and classification techniques to realize the automatic domain detection. Daumé III and Jagarlamudi (2011) used dictionary mining techniques to find translations for unseen words from comparable corpora and they integrated"
Y15-2013,tiedemann-2012-parallel,0,0.0545233,"th another statistical language model learned from texts of the target language containing words in inflected forms. The CRF++ toolkit is used to select the nbest translations in inflected forms. 4 Experiments and Results 4.1 Data and Experimental Setup In order to study the impact of using a domainspecific bilingual lexicon on the performance of the EBMT system, we conducted our experiments on two English-French parallel corpora (Table 4): Europarl (European Parliament Proceedings) and Emea (European Medicines Agency Documents). Both corpora were extracted from the open parallel corpus OPUS (Tiedemann, 2012). Evaluation consists in comparing translation results produced by the open source SMT system Moses (Khoen et al., 2007) and the EBMT system on in-domain and out-of-domain texts. The English-French training corpus is used to build Moses’s translation and language models. The French sentences of this training corpus are used to create the indexed database of the cross-language search engine integrated in the EBMT system. We conducted eight runs and two test experiments for each run: In-Domain and Out-Of-Domain. For this, we randomly extracted 500 parallel sentences from Europarl as an In-Domain"
Y15-2013,2012.amta-papers.18,0,0.0247856,"ary mining techniques to find translations for unseen words from comparable corpora and they integrated these translations into a statistical phrase-based translation system. They reported improvements in translation quality (between 0.5 and 1.5 BLEU points) on four domains and two language pairs. Pecina et al. (2011) exploited domain-specific data acquired by domain-focused web-crawling to adapt generaldomain SMT systems to new domains. They observed that even small amounts of in-domain parallel data are more important for translation quality than large amounts of in-domain monolingual data. Wang et al. (2012) used a single translation model and generalized a single-domain decoder to deal with different domains. They used this method to adapt large-scale generic SMT systems for 20 language pairs in order to translate patents. The authors reported a gain of 0.35 BLEU points for patent translation and a loss of only 0.18 BLEU points for generic translation. 3 The Translation Process of the ExampleBased Machine Translation System The translation process of the EBMT system consists of several steps: retrieving translation candidates from a monolingual corpus using a cross-language search engine, produc"
Y15-2013,W12-5108,1,\N,Missing
