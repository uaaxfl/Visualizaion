2005.eamt-1.26,P04-1041,1,0.881757,"Missing"
2005.eamt-1.26,P03-1046,0,0.115152,"Missing"
2005.eamt-1.26,H94-1020,0,0.395048,"Missing"
2005.eamt-1.26,P02-1040,0,0.102728,"Missing"
2005.eamt-1.26,2001.mtsummit-teach.7,0,0.290509,"Missing"
2005.eamt-1.26,P02-1035,0,0.027175,"Missing"
2005.eamt-1.26,J03-3004,1,0.836852,"Missing"
2005.mtsummit-papers.38,P04-1041,1,0.867266,"Missing"
2005.mtsummit-papers.38,P03-1046,0,0.147484,"ites In TransBooster, it is essential to be able to distinguish between required elements and optional material, as adjuncts can safely be omitted from the simplified string that we submit to the MT system. This can clearly be seen in (7), where the apposition a long-time rival can freely be omitted. This simple method considerably reduces the complexity of the source strings. More complex cases involve the replacement of constituents by syntactically similar material (see next section). The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The nodes we label as arguments include all the nodes Hockenmaier labels as arguments together with some of the nodes (e.g. VP children of S where S is headed by a modal verb; quantitative adjectives) which she describes as adjuncts. In ongoing research, we wish to compare this procedure with the annotation of Penn-II nodes with LFG functional information (Cahill et al., 2004). 3.1.4 Skeletons and Substitution Variables The simplified source strings such as (5) are submitted to the MT systems, and these output target strings of the form TL in (8): (8) TL: [ARG01 ] [ADJ01 ]. . . [ARG0L ] [ADJ"
2005.mtsummit-papers.38,2005.eamt-1.26,1,0.640124,"Missing"
2005.mtsummit-papers.38,P02-1040,0,0.100158,"Missing"
2005.mtsummit-papers.38,2001.mtsummit-teach.7,0,0.032325,"Missing"
2005.mtsummit-papers.38,2004.tmi-1.9,0,0.308159,"Missing"
2006.amta-papers.13,J93-2003,0,0.00545117,"into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majority voting and a N-gram language model. Since edit distance only focuses on insertions, deletions and substitutions, the model cannot handle translation hypotheses with a significantly different word order. ("
2006.amta-papers.13,A00-2018,0,0.0922949,"e presented in (Mellebeek et al., 2005). In the following subsections, we will explain the decomposition of the input sentence, the translation of the input chunks, the calculation of the best output chunk and the composition of the output sentence. We will also demonstrate the approach with a worked example. 3.1 Decomposition of Input Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2002) in section 4. In a first step, the input sentence is decomposed into a number of syntactically meaningful chunks as in (1). (1) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = 111 2003). The result of this first step on a worked example can be seen in (5). In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to"
2006.amta-papers.13,A94-1016,0,0.504325,"the Americas give a short overview of the most relevant MEMT techniques. We explain our approach in section 3 and demonstrate it with a worked example. Section 4 contains the description, results and analysis of our experiments. We give avenues for future research in section 5 and summarize our findings in section 6. ously known MEMT proposals operate on MT output for complete input sentences. In the research presented here, we pursue a different approach: we decompose MT input into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniqu"
2006.amta-papers.13,P03-1046,0,0.0207593,"Missing"
2006.amta-papers.13,P05-3026,0,0.0508826,"to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majority voting and a N-gram language model. Since edit distance only focuses on insertions, deletions and substitutions, the model cannot handle translation hypotheses with a significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem by introducing a method that can find nonmonotone alignments. They compose a consensus from these alignments by using a language model and confidence score specific to each MT engine. (van Zaanen and Somers, 2005) present ‘Democrat’, a ‘plug-and-play’ MEMT system that relies solely on a simple edit distance-based alignment of the translation hypotheses and does not use additional heuristics to compute the consensus translation. Finally, (Matusov et al., 2006) use well-established techniques from the Statistical MT community to produce alignments of hypotheses base"
2006.amta-papers.13,E06-1005,0,0.0522155,"titutions, the model cannot handle translation hypotheses with a significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem by introducing a method that can find nonmonotone alignments. They compose a consensus from these alignments by using a language model and confidence score specific to each MT engine. (van Zaanen and Somers, 2005) present ‘Democrat’, a ‘plug-and-play’ MEMT system that relies solely on a simple edit distance-based alignment of the translation hypotheses and does not use additional heuristics to compute the consensus translation. Finally, (Matusov et al., 2006) use well-established techniques from the Statistical MT community to produce alignments of hypotheses based on pairwise word alignments in an entire corpus instead of at the sentence level. To date, to the best of our knowledge, all previDescription of the Algorithm Given N different MT engines (E1 . . . EN ), the proposed method recursively decomposes an input sentence S into M syntactically meaningful chunks C1 . . . CM . Each chunk Ci (1 &lt; i &lt; M ) is embedded in a minimal necessary context and translated by all MT engines. For each chunk Ci , the translated output candidates Ci1 − CiN are"
2006.amta-papers.13,2005.mtsummit-papers.38,1,0.873363,"Missing"
2006.amta-papers.13,P04-1063,0,0.118926,"ummarize our findings in section 6. ously known MEMT proposals operate on MT output for complete input sentences. In the research presented here, we pursue a different approach: we decompose MT input into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majorit"
2006.amta-papers.13,P02-1040,0,0.0717701,"Missing"
2006.amta-papers.13,P98-2186,0,0.0307195,"(English→Spanish) carried out on an 800sentence test set extracted from the PennII Treebank. 1 Introduction In this paper, we present a novel approach to combine the outputs of multiple MT engines into a consensus translation. Multi-Engine Machine Translation (MEMT) is a term coined by (Frederking and Nirenburg, 1994), who were the first to apply the idea of a multiengine approach in Natural Language Processing to Machine Translation (MT). Researchers in other areas of language technology such as Speech Recognition (Fiscus, 1997), Text Categorization (Larkey and Croft, 1996) and POS Tagging (Roth and Zelenko, 1998) have also experimented with multisystem approaches. Since then, several researchers in the MT community have come up with different techniques to calculate consensus translations from multiple MT engines (cf. section 2). All these previously proposed techniques share one important characteristic: they translate the entire input sentence as is and operate on the resulting target language sentences to calculate a consensus output. Their main difference lies in the method they use to compute word alignments between the multiple output sentences. In contrast to previous MEMT approaches, the techn"
2006.amta-papers.13,2003.mtsummit-papers.51,0,0.061623,"Missing"
2006.amta-papers.13,2005.mtsummit-papers.23,0,0.427265,"Missing"
2006.amta-papers.13,2004.tmi-1.9,0,0.10647,"Missing"
2006.amta-papers.13,P95-1037,0,0.0133401,"d leaving out the adjuncts in (1), we obtain the skeleton in (2) Input Decomposition ... C1 E1 C1_1 C1_N CM ... ... EN CM_1 CM_N Selection C1_best ... CM_best Composition Output Figure 1: A flow chart of the entire MEMT system, with Ci the ith input chunk (1 &lt; i &lt; M ), Ej the j th MT engine (1 &lt; j &lt; N ) and Ci j the translation of Ci by Ej . argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. In order to determine the pivot, we compute the head of the local tree by adapting the headlexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of the head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. The procedure used for argument/adjunct identification is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, (2) [VARG1 ] . . . [VARGL ] pivot [VARGL+1 ] . . . [VARGL+R ] where VARGi is the simpler string substituting ARGi By matching the previously established translations of the Substitution Variables VARGi (1 &lt;= i &lt;= L + R) in the translation of the skeleton in (2), we are able to (i) extract the translation of t"
2006.amta-papers.13,H94-1020,0,0.0107646,"get and the final composition of the output are based on the TransBooster architecture presented in (Mellebeek et al., 2005). In the following subsections, we will explain the decomposition of the input sentence, the translation of the input chunks, the calculation of the best output chunk and the composition of the output sentence. We will also demonstrate the approach with a worked example. 3.1 Decomposition of Input Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2002) in section 4. In a first step, the input sentence is decomposed into a number of syntactically meaningful chunks as in (1). (1) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = 111 2003). The result of this first step on a worked example can be seen in (5). In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) t"
2006.amta-papers.13,2005.eamt-1.20,0,\N,Missing
2006.amta-papers.13,C98-2181,0,\N,Missing
2006.amta-papers.17,1999.tmi-1.3,0,0.0308549,"selección. 2. Determining the sub-sentential translation links in those retrieved examples. 3. Recombining relevant parts of the target translation links to derive the translation. In order to determine a similarity metric during the search for relevant matches, word co-occurrence, part-of-speech labels, generalised templates and bilingual dictionaries are often used. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrase-structure trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 2.1 Taking into account marker tag information (label, and relative sentence position), and lexical similarity (via mutual information), the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <PRON> You click : <PRON> Usted cliquea b. <PREP> on the red button : <PREP> en el botón rojo c. <PREP> to view : <PREP> para ver d. <DET> the effect : <DET> el efecto e. <PREP> of the selection : <PREP> de la selección Marker-based EBMT One approach in EBMT is to use a set of closedclass words to segment aligned source and ta"
2006.amta-papers.17,2004.tmi-1.11,1,0.848034,"ce of a marker word (e.g. determiners, quantifiers, conjunctions etc.), and together with cognate matches and mutual information scores, aligned marker chunks are derived. In order to describe this resource creation in more detail, consider the English–Spanish example in (1): (1) You click on the red button to view the effect of the selection. -> Usted cliquea en el botón rojo para ver el efecto de la selección. In our experiments our marker set consisted of determiners, prepositions, conjunctions, personal pronouns, possessive pronouns, quantifiers and wh-adverbs, following (Gough, 2005; and Gough and Way, 2004). We also made use of auxiliary verbs, such as has and is in English and their Spanish counterparts ha and es, in addition to punctuation, which acted as chunk-final, rather than chunk-initial markers. 3 TransBooster: Architecture TransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to an MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. 149 Our approach presupposes the existence of some sort of syntactic analysis of the input sent"
2006.amta-papers.17,2003.mtsummit-papers.22,1,0.837819,"cliquea <PREP> en <DET> el botón rojo <PREP> para ver <DET> el efecto <PREP> de <DET> la selección. 2. Determining the sub-sentential translation links in those retrieved examples. 3. Recombining relevant parts of the target translation links to derive the translation. In order to determine a similarity metric during the search for relevant matches, word co-occurrence, part-of-speech labels, generalised templates and bilingual dictionaries are often used. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrase-structure trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 2.1 Taking into account marker tag information (label, and relative sentence position), and lexical similarity (via mutual information), the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <PRON> You click : <PRON> Usted cliquea b. <PREP> on the red button : <PREP> en el botón rojo c. <PREP> to view : <PREP> para ver d. <DET> the effect : <DET> el efecto e. <PREP> of the selection : <PREP> de la selección Marker-based EBMT One approac"
2006.amta-papers.17,P03-1046,0,0.0197715,"tation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (4), the pivot is likes. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier's algorithm for CCG (Hockenmaier, 2003). The result of this first step on the example sentence (4) can be seen in (7): (7) [The chairman, a longtime rival of Bill Gates,]ARG1 [likes]pivot [fast and confidential deals]ARG2 . 3.2 Skeletons and Substitution Variables In the next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to an improved translation of the pivot; (ii) to help keep track of the location of the translation of the arguments in target. In choo"
2006.amta-papers.17,koen-2004-pharaoh,0,0.0182006,"to determine subsentential links between sentence pairs. In addition to these chunk alignments, we used statistical techniques to extract a high quality word-level lexicon (which in turn was used during the chunk alignment process). Following the refined alignment method of (Och and Ney, 2003), we used the GIZA++ statistical word alignment tool2 to perform source-target and target-source word alignment. The resulting ‘refined’ word alignment set was then passed along with the chunk database to the system decoder (for the results reported in this paper we used the Pharaoh phrase-based decoder (Koehn, 2004)). For training the EBMT system we made use of a subsection of the English-Spanish section of the Europarl corpus (Koehn, 2005). The corpus was filtered based on sentence length (maximum sentence length set at 40 words for Spanish and English) and relative sentence length ratio (a relative sentence length ratio of 1.5 was used), resulting in 958K English-Spanish sentence pairs. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank3; the second set consists of ran"
2006.amta-papers.17,2005.mtsummit-papers.11,0,0.0159209,"to extract a high quality word-level lexicon (which in turn was used during the chunk alignment process). Following the refined alignment method of (Och and Ney, 2003), we used the GIZA++ statistical word alignment tool2 to perform source-target and target-source word alignment. The resulting ‘refined’ word alignment set was then passed along with the chunk database to the system decoder (for the results reported in this paper we used the Pharaoh phrase-based decoder (Koehn, 2004)). For training the EBMT system we made use of a subsection of the English-Spanish section of the Europarl corpus (Koehn, 2005). The corpus was filtered based on sentence length (maximum sentence length set at 40 words for Spanish and English) and relative sentence length ratio (a relative sentence length ratio of 1.5 was used), resulting in 958K English-Spanish sentence pairs. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank3; the second set consists of randomly extracted sentences from the test section of the Europarl corpus, which had been parsed with (Bikel, 2002). We decided to"
2006.amta-papers.17,N03-1017,0,0.0242043,"Missing"
2006.amta-papers.17,P95-1037,0,0.0770656,"entence is decomposed into a number of syntactically meaningful chunks as in (6): (6) [ARG1] [ADJ1]. . . [ARGL] [ADJl] pivot [ARGL+1] [ADJl+1]. . . [ARGL+R] [ADJl+r] where pivot = the nucleus of the sentence, ARG = argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. The pivot is the part of the string that must remain unaltered during decomposition in order to avoid an incorrect translation. In order to determine the pivot, we compute the head of the local tree by adapting the head-lexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (4), the pivot is likes. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier's algorithm for CCG (Hockenmaier, 2003). The result of"
2006.amta-papers.17,H94-1020,0,0.0544782,"h counterparts ha and es, in addition to punctuation, which acted as chunk-final, rather than chunk-initial markers. 3 TransBooster: Architecture TransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to an MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. 149 Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parseannotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of a state-of-the-art statistical parser (Bikel, 2002) in Section 5. Essentially, each TransBooster run from a parsed input string to a translated output string consists of the following 5 steps. 1. Finding the Pivot. 2. Locating Arguments and Adjuncts (‘Satellites’) in the source language. 3. Creating and Translating Skeletons and Substitution Variables. 4. Translating Satellites. 5. Combining the translation of Satellites into the output string. We briefly explain each of these steps by processing the following simple example sentence: (4) The chairman, a long-time rival"
2006.amta-papers.17,2005.mtsummit-papers.38,1,0.895422,"Missing"
2006.amta-papers.17,2005.eamt-1.26,1,0.895742,"Missing"
2006.amta-papers.17,2006.eamt-1.24,1,0.761114,"Missing"
2006.amta-papers.17,J03-1002,0,\N,Missing
2006.eamt-1.24,2003.mtsummit-papers.6,0,0.0521718,"Missing"
2006.eamt-1.24,P05-1033,0,0.217935,"Missing"
2006.eamt-1.24,P03-1046,0,0.0471658,"tion scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (1), the pivot is ‘likes’. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The result of this first step on a the example sentence (1) can be seen in (4). (4) 3.2 [The chairman, a long-time rival of Bill Gates,]ARG1 [likes]pivot [fast and confidential deals]ARG2 . Skeletons Variables and Substitution In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to an improved translation of the pivot; (ii) to help keep track of the location of the translation of the arguments in target. In cho"
2006.eamt-1.24,koen-2004-pharaoh,0,0.0643615,"ule-based and statistical MT systems tend to perform better at translating shorter sentences than longer ones. TransBooster decomposes source language sentences into syntactically simpler and shorter chunks, sends the chunks to a baseline MT system and recomposes the translated output into target language sentences. It has already proved successful in experiments with rule-based MT systems (Mellebeek, Khasin, Owczarzak, Van Genabith, & Way, 2005). In this paper we apply the TransBooster wrapper technology to a state-of-the-art phrase-based English → Spanish SMT model constructed with Pharaoh (Koehn, 2004) and we report a statistically significant improvement in BLEU and NIST score. The paper is organised as follows. In section 2, we give a short overview of the most relevant methods that incorporate syntactic knowledge in SMT models. We explain our approach in section 3 and demonstrate it with a worked example. Sections 4 and 5 contain the description, results and analysis of our experiments. We summarize our findings in section 6. 2 Related Research One of the major difficulties SMT faces is its inability to model long-distance dependencies and correct word order for many language pairs. In t"
2006.eamt-1.24,2005.mtsummit-papers.11,0,0.0839355,"ess leads to an improvement in translation quality compared to the original output by Systran in (2), as is shown in (10): (10) 4 El presidente, un rival de largo plazo de Bill Gates, tiene gusto de repartos r´apidos y confidenciales. Experimental Setup For our experiments, the phrase-based SMT system (English → Spanish) was constructed using the Pharaoh phrase-based SMT decoder, and the SRI Language Modeling toolkit.1 We used an interpolated trigram language model with Kneser-Ney discounting. The data used to train the system was taken from the English-Spanish section of the Europarl corpus (Koehn, 2005). From this data, 501K sentence pairs were randomly extracted from the designated training section of the corpus and lowercased. Sentence length was limited to a maximum of 40 words for both Spanish and English, 1 http://www.speech.sri.com/projects/srilm/ with sentence pairs having a maximum relative sentence length ratio of 1.5. From this data we used the method of (Och & Ney, 2003) to extract phrase correspondences from GIZA++ word alignments. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the"
2006.eamt-1.24,N03-1017,0,0.0303935,"Missing"
2006.eamt-1.24,P95-1037,0,0.101654,"e is decomposed into a number of syntactically meaningful chunks as in (3). (3) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. The pivot is the part of the string that must remain unaltered during decomposition in order to ensure a correct translation. In order to determine the pivot, we compute the head of the local tree by adapting the head-lexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (1), the pivot is ‘likes’. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The result"
2006.eamt-1.24,H94-1020,0,0.118183,"or assumptions. It therefore does not make use of linguistically motivated syntax, in contrast to TransBooster. 3 TransBooster: ture ArchitecTransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to a baseline MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of a state-of-the-art statistical parser (Charniak, 2000) in section 5. Essentially, each TransBooster cycle from a parsed input string to a translated output string consists of the following 5 steps: 1. Finding the Pivot. 2. Locating Arguments and Adjuncts (‘Satellites’) in the source language. 3. Creating and Translating Skeletons and Substitution Variables. 4. Translating Satellites. 5. Combining the translation of Satellites into the output string. We briefly explain each of these steps by processing the following simple example sentence. (1) The chairman, a long-time r"
2006.eamt-1.24,2005.eamt-1.26,1,0.899862,"Missing"
2006.eamt-1.24,J03-1002,0,0.00360759,"and the SRI Language Modeling toolkit.1 We used an interpolated trigram language model with Kneser-Ney discounting. The data used to train the system was taken from the English-Spanish section of the Europarl corpus (Koehn, 2005). From this data, 501K sentence pairs were randomly extracted from the designated training section of the corpus and lowercased. Sentence length was limited to a maximum of 40 words for both Spanish and English, 1 http://www.speech.sri.com/projects/srilm/ with sentence pairs having a maximum relative sentence length ratio of 1.5. From this data we used the method of (Och & Ney, 2003) to extract phrase correspondences from GIZA++ word alignments. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank; the second set consists of randomly extracted sentences from the test section of the Europarl corpus, which had been parsed with (Bikel, 2002). We decided to use two different sets of test data instead of one because we are faced with two ‘out-of-domain’ phenomena that have an influence on the scores, one affecting the TransBooster algorithm, the"
2006.eamt-1.24,P02-1040,0,0.0767457,"e scores, one affecting the TransBooster algorithm, the other the phrase-based SMT system. On the one hand, the TransBooster decomposition algorithm performs better on ‘perfectly’ parse-annotated sentences from the Penn Treebank than on the output produced by a statistical parser as (Bikel, 2002), which introduces a certain amount of noise. On the other hand, Pharaoh was trained on data from the Europarl corpus, so it performs much better on translating Europarl data than out-of-domain Wall Street Journal text. 5 Results and Evaluation We present results of an automatic evaluation using BLEU (Papineni, Roukos, Ward, & Zhu, 2002) and NIST (Doddington, 2002) against the 800-sentence test sets mentioned in section 4. In each case, the statistical significance of the results was tested by using the BLEU/NIST resampling toolkit described in (Zhang & Vogel, 2004).2 We also conduct a manual evaluation of the first 200 sentences in the Europarl test set. Finally, we analyse the differences between the output of Pharaoh and TransBooster, and provide a number of example translations. 2 http://projectile.is.cs.cmu.edu/research/public/ tools/bootStrap/tutorial.htm 5.1 Automatic Evaluation 5.1.1 Europarl English→Spanish Pharaoh"
2006.eamt-1.24,P01-1067,0,0.215018,"Missing"
2006.eamt-1.24,P02-1039,0,0.0437931,"Missing"
2006.eamt-1.24,2004.tmi-1.9,0,0.100205,"Missing"
2006.eamt-1.24,A00-2018,0,\N,Missing
2006.eamt-1.24,2005.mtsummit-papers.38,1,\N,Missing
2007.mtsummit-ucnlg.2,W05-0909,0,0.00921405,"sfer-based MT systems where the transfer rules have been automatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation compo"
2007.mtsummit-ucnlg.2,P04-1041,1,0.898456,"Missing"
2007.mtsummit-ucnlg.2,P06-1130,1,0.893763,"Missing"
2007.mtsummit-ucnlg.2,P05-1022,0,0.0417968,"only evaluation as results are not necessarily for the same set of sentences. 4.1 Evaluation of Automatically Induced Resources Using the methodology described in detail in Section 3, we evaluated the history-based statistical generator of Hogan et al. (2007). This generator, an extension of the work presented in (Cahill and van Genabith, 2006), generates sentences from LFG fstructures and achieves state-of-the-art results when tested on input generated from Penn Treebank gold standard trees. In order to generate f-structure inputs in a completely automated fashion for evaluation, we used the Charniak and Johnson (2005) reranking parser to parse the original test sentences into Penn Treebank style trees 4 . The f-structure annotation algorithm of Cahill et al. (2004) was then applied to the parser-generated trees to create a set of f-structures for testing. Section 23 (2416 sentences) Input NIST BLEU From Gold-standard Trees 13.29 0.6680 From Parser Trees 13.01 0.6511 Table 1: Entire Test Set Results on Section 23 of the Penn Treebank for the generator of (Hogan et al., 2007) on input automatically generated from gold standard and from parser generated trees. Section 23 (2416 sentences) Input NIST BLEU Cover"
2007.mtsummit-ucnlg.2,W07-2204,1,0.890256,"Missing"
2007.mtsummit-ucnlg.2,P06-1043,0,0.0223686,"using automatic string comparison metrics such as NIST and BLEU. Testing on previously unseen sections of the Penn Treebank demonstrates to what degree a generator has achieved broad coverage and high accuracy (according to BLEU and NIST scores). However, if we wish to take into account how the generator might fare as 6 Figure 1: Translation of Source Language Sentence to Target Language a component of a machine translation system, this evaluation methodology is unrealistic in two main respects. Firstly, there is the problem of domain adaptation (well documented for parsing, see for example (McClosky et al., 2006; Foster et al., 2007)). Domain adaptation is particularly relevant for the systems of (Langkilde-Geary, 2002; Nakanishi et al., 2005; Cahill and van Genabith, 2006) which are trained on sections of the Wall Street Journal Penn treebank. It is highly likely that were these generators tested in an MT setting, the testing domain would change1 and, as in statistical parsing, testing on a domain which differs to the domain of the training data would lead to a deterioration in generation results. In addition, the inputs to the generators are constructed from gold standard (hence near perfect) trees"
2007.mtsummit-ucnlg.2,W01-1406,0,0.0215501,"pproaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they give no indication of how well th"
2007.mtsummit-ucnlg.2,P98-2139,0,0.0226045,"tence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they giv"
2007.mtsummit-ucnlg.2,W07-0411,1,0.870141,"Missing"
2007.mtsummit-ucnlg.2,P02-1035,0,0.0984101,"would not be the intended use of our evaluation method, the abstract structure that is given to the generator in our evaluation method should only contain information that is possible to be produced by the transfer component. Such information, like the actual surface form and word order in the target language would not be possible to have in the target language structure if it adheres to the transfer-based architecture that we describe in Section 3.1. 8 4 Experimental Results We conducted two sets of experiments to evaluate the quality of hand-crafted LFG parsing and generation technologies (Riezler et al., 2002) and the treebank-induced LFG parsing (Cahill et al., 2004) and generation (Cahill and van Genabith, 2006; Hogan et al., 2007) technologies using our new method. Both the automatically-induced technologies and the hand-crafted technologies are currently being used as part of transfer-based MT systems. Overall evaluation results for one of these MT systems are available and we include these results in Section 4.2 (Riezler and Maxwell, 2006). We evaluate the parsing/generation technologies on the English language components of three different MT bitext test sets: the 1755 English sentences of le"
2007.mtsummit-ucnlg.2,1992.tmi-1.12,0,0.178277,"ystem with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together."
2007.mtsummit-ucnlg.2,N06-1032,0,0.200528,"stical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they give no indication of how well the generation and parsing tec"
2007.mtsummit-ucnlg.2,2006.amta-papers.25,0,0.0133449,"tomatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 resu"
2007.mtsummit-ucnlg.2,W02-1506,0,0.0621942,"Missing"
2007.mtsummit-ucnlg.2,2003.mtsummit-papers.51,0,0.040552,"transfer rules have been automatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understa"
2007.mtsummit-ucnlg.2,N03-1017,0,0.00563435,"Missing"
2007.mtsummit-ucnlg.2,W02-2103,0,0.116365,"sults and, finally, section 6 gives some conclusions of the work presented in this paper. 2 Existing Generation and Parsing Evaluation Methods There is a considerable body of work on sentence realisation from abstract linguistic representation where evaluation has been carried out on standalone generators and independent of any MT system. However, an oft-cited future application for such generators is as the generation component of an MT system. Recently there has been an increasing amount of work in the area of robust, broad coverage sentence generation, tested on newswire text, for example (Langkilde-Geary, 2002; Callaway, 2003; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Abstract semantic/syntactic inputs to these generators were automatically constructed from sections of the hand-crafted Penn Treebank. Sentences were then generated for these inputs and compared to the original sentences, using automatic string comparison metrics such as NIST and BLEU. Testing on previously unseen sections of the Penn Treebank demonstrates to what degree a generator has achieved broad coverage and high accuracy (according to BLEU and NIST scores). However, if we wish to take into account how the generato"
2007.mtsummit-ucnlg.2,D07-1028,1,\N,Missing
2007.mtsummit-ucnlg.2,2001.mtsummit-ebmt.4,0,\N,Missing
2007.mtsummit-ucnlg.2,W05-1510,0,\N,Missing
2007.mtsummit-ucnlg.2,P02-1040,0,\N,Missing
2007.mtsummit-ucnlg.2,C98-2134,0,\N,Missing
2008.eamt-1.10,W08-0319,0,0.0225558,"Missing"
2008.eamt-1.10,W02-1503,0,0.0208418,"e set of solutions of the algorithm retrieves all unpacked rules from the packed representation. 5 Experimental Evaluation In order to evaluate the effects of using the packed rule representation on the space required to store transfer rules, we ran an automatic transfer rule induction algorithm on sentences of the Europarl corpus. We restricted the test corpus to German-English sentences within the length range of 5 to 15 words. This resulted in 219,666 sentence pairs. We reserved 2000 of these sentences as a development set. Each side of the corpus was parsed with a monolingual LFG grammar (Butt et al., 2002; Riezler et al., 63 2002). The automatic rule induction algorithm used a bilingual dictionary (Richter, 2007) and Giza++ word alignments (Och and Ney, 2000) to align local f-structures. A packed transfer representation for each input f-structure pair was induced. All of the rules were then unpacked and counted. Our rule induction algorithm induced 5,148,874 transfer rules from the training data f-structure pairs. This resulted in an average of 23.65 rules being induced from each aligned f-structure pair 11 . The total time taken for the rule extraction algorithm was approximately 3.5 hours ru"
2008.eamt-1.10,2003.mtsummit-papers.6,0,0.0120417,"o load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Max"
2008.eamt-1.10,D07-1079,0,0.0252786,"Missing"
2008.eamt-1.10,P03-2041,0,0.0133095,"ransfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Maxwell (2006) use feature structures of the Lexical Functional Grammar (LFG) formalism (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) for deep transfer. They impose a limit of a maximum of three primitive rules to construct a complex rule 1"
2008.eamt-1.10,W02-1506,0,0.0372394,"Missing"
2008.eamt-1.10,N03-1017,0,0.0131535,"Missing"
2008.eamt-1.10,D07-1091,0,0.100734,"as a high impact on the amount of time taken to load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of trans"
2008.eamt-1.10,P00-1056,0,0.00655862,"of using the packed rule representation on the space required to store transfer rules, we ran an automatic transfer rule induction algorithm on sentences of the Europarl corpus. We restricted the test corpus to German-English sentences within the length range of 5 to 15 words. This resulted in 219,666 sentence pairs. We reserved 2000 of these sentences as a development set. Each side of the corpus was parsed with a monolingual LFG grammar (Butt et al., 2002; Riezler et al., 63 2002). The automatic rule induction algorithm used a bilingual dictionary (Richter, 2007) and Giza++ word alignments (Och and Ney, 2000) to align local f-structures. A packed transfer representation for each input f-structure pair was induced. All of the rules were then unpacked and counted. Our rule induction algorithm induced 5,148,874 transfer rules from the training data f-structure pairs. This resulted in an average of 23.65 rules being induced from each aligned f-structure pair 11 . The total time taken for the rule extraction algorithm was approximately 3.5 hours running the algorithm on 8 parallel processors (28 CPU hours). In order to determine the effect of the packed representation we randomly selected 10 sets of 10"
2008.eamt-1.10,P05-1034,0,0.0149854,"epresentation also has a high impact on the amount of time taken to load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much l"
2008.eamt-1.10,P02-1035,0,0.0339934,"Missing"
2008.eamt-1.10,N06-1032,0,0.0956483,"es to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Maxwell (2006) use feature structures of th"
2008.eamt-1.10,W99-0604,0,\N,Missing
2008.eamt-1.10,P05-1067,0,\N,Missing
2010.amta-papers.10,P04-1041,1,0.880073,"Missing"
2010.amta-papers.10,C92-2101,0,0.0233149,"structure is not a tree, but a directed acyclic graph (DAG). While the algorithm of Meyers et al. aligns two trees with n nodes and maximum degree d in O(n2 d2 ) time, we see no straightforward way of adapting it to DAGs without increasing complexity substantially. Another issue not to be ignored is that the output of f-structure parsers(Kaplan et al., 2002; Cahill et al., 2004) is often fragmented. Unlike incorrect parses, fragmented parses do carry useful information and their exclusion is undesirable. The extensive existing work on phrase-structure tree alignment, starting with the work by Kaji et al. (1992) and proceeding to a number of more recent approaches (Ambati and Lavie, 2008; Zhechev, 2009), is also not straightforward to reuse, as LFG f-structures represent sentences in a way quite different from phrase-structure trees, in particular having all internal nodes and not only leaves (potentially) lexicalized; not to mention again that, in general, fstructures are graphs, and not necessarily trees.1 Therefore we decided to design a new algorithm. For the sake of computational speed, we keep the structure-related part of the algorithm as simple as possible. As measures of “structural closenes"
2010.amta-papers.10,W02-1506,0,0.448708,"A Josef van Genabith CNGL, School of Computing, Dublin City University josef@computing.dcu.ie are aligned to the descendant nodes of B. The main reason we did not adapt this algorithm for our task are the problems with generalization. In the general case an f-structure is not a tree, but a directed acyclic graph (DAG). While the algorithm of Meyers et al. aligns two trees with n nodes and maximum degree d in O(n2 d2 ) time, we see no straightforward way of adapting it to DAGs without increasing complexity substantially. Another issue not to be ignored is that the output of f-structure parsers(Kaplan et al., 2002; Cahill et al., 2004) is often fragmented. Unlike incorrect parses, fragmented parses do carry useful information and their exclusion is undesirable. The extensive existing work on phrase-structure tree alignment, starting with the work by Kaji et al. (1992) and proceeding to a number of more recent approaches (Ambati and Lavie, 2008; Zhechev, 2009), is also not straightforward to reuse, as LFG f-structures represent sentences in a way quite different from phrase-structure trees, in particular having all internal nodes and not only leaves (potentially) lexicalized; not to mention again that,"
2010.amta-papers.10,2005.mtsummit-papers.11,0,0.0821709,"ctionary; their method checks how well the neighbors of a word much the neighbors of each of its candidate counterparts. We use a very simple bilexical dictionary unit (a plain cooccurrence counter on training bitext) in this version of the software, but nothing prevents using more elaborate dictionary units within the same architecture. The software supports the output formats of two different LFG pasers, namely XLE (Kaplan et al., 2002) and DCU (Cahill et al., 2004). The evaluation of the tool presented in the paper is inevitably limited. We use sentence-aligned English and German Europarl (Koehn, 2005) and SMULTRON 2.0 (Volk et al., 2009) data. To the best of our knowledge, there is no relevant gold standard, so we produced a small gold standard set ourselves, nodealigning 20 f-structure pairs created from SMULTRON data, using the word-alignment information contained in SMULTRON for reference. Another possibility would be to evaluate the method within an SMT system, but the available LFG-based SMT software is currently not accurate enough for the alignment to be reliably evaluated by the final translation scores. In addition to that, we manually examined and analysed some sentences from the"
2010.amta-papers.10,P98-2139,0,0.891258,"lignment techniques as a prerequisite for transfer rule induction. The existing research (Riezler and Maxwell, 2006; Avramidis and Kuhn, 2009; Graham and van Genabith, 2009) uses general-purpose word-alignment tools such as GIZA++ (Och et al., 1999) for aligning the f-structures. Such tools, however, take no advantage of the dependencies, which are made explicit in the f-structures, thus actually ignoring a lot of useful information readily available in the f-structure annotated data. This paper focuses on a way of making use of precisely this information. A relevant algorithm was proposed by Meyers et al. (1998), who represent sentences with trees similar to f-structures. Their algorithm aligns a pair of trees in a single recursive bottom-up procedure, ensuring that, somewhat simplifying, if a node A is aligned to a node B, the descendant nodes of A Josef van Genabith CNGL, School of Computing, Dublin City University josef@computing.dcu.ie are aligned to the descendant nodes of B. The main reason we did not adapt this algorithm for our task are the problems with generalization. In the general case an f-structure is not a tree, but a directed acyclic graph (DAG). While the algorithm of Meyers et al. a"
2010.amta-papers.10,W99-0604,0,0.0586674,"igning f-structures directly, and plain word alignment is used for this purpose. In this way no use is made of the structural information contained in f-structures. We present the first version of a specialized f-structure alignment open-source software. 1 Introduction The use of LFG f-structures in transfer-based statistical machine translation naturally requires alignment techniques as a prerequisite for transfer rule induction. The existing research (Riezler and Maxwell, 2006; Avramidis and Kuhn, 2009; Graham and van Genabith, 2009) uses general-purpose word-alignment tools such as GIZA++ (Och et al., 1999) for aligning the f-structures. Such tools, however, take no advantage of the dependencies, which are made explicit in the f-structures, thus actually ignoring a lot of useful information readily available in the f-structure annotated data. This paper focuses on a way of making use of precisely this information. A relevant algorithm was proposed by Meyers et al. (1998), who represent sentences with trees similar to f-structures. Their algorithm aligns a pair of trees in a single recursive bottom-up procedure, ensuring that, somewhat simplifying, if a node A is aligned to a node B, the descenda"
2010.amta-papers.10,N06-1032,0,0.0128512,"years as an intermediate data representation for statistical machine translation. So far, however, there are no alignment tools capable of aligning f-structures directly, and plain word alignment is used for this purpose. In this way no use is made of the structural information contained in f-structures. We present the first version of a specialized f-structure alignment open-source software. 1 Introduction The use of LFG f-structures in transfer-based statistical machine translation naturally requires alignment techniques as a prerequisite for transfer rule induction. The existing research (Riezler and Maxwell, 2006; Avramidis and Kuhn, 2009; Graham and van Genabith, 2009) uses general-purpose word-alignment tools such as GIZA++ (Och et al., 1999) for aligning the f-structures. Such tools, however, take no advantage of the dependencies, which are made explicit in the f-structures, thus actually ignoring a lot of useful information readily available in the f-structure annotated data. This paper focuses on a way of making use of precisely this information. A relevant algorithm was proposed by Meyers et al. (1998), who represent sentences with trees similar to f-structures. Their algorithm aligns a pair of"
2010.amta-papers.10,C00-2131,0,0.0239027,", on one hand, simple and efficient to calculate, allowing a greedy alignment of two DAGs, irrespective of 1 Phrase-structure tress are used in another layer of LFG, namely in c-structure. whether they are fragmented or not, to be built in O(n2 d2 ) + O(n2 ln n) time, which is comparable to the complexity of Meyers et al. algorithm for trees. On the other hand, these measures are sufficient for resolving simple ambiguities, such as several occurrences of the same word in the same sentence (a very frequent situation, if we take function words into account). A similar idea underlies the work by Watanabe et al. (2000) on resolving the alignment ambiguities which arise when using a translation dictionary; their method checks how well the neighbors of a word much the neighbors of each of its candidate counterparts. We use a very simple bilexical dictionary unit (a plain cooccurrence counter on training bitext) in this version of the software, but nothing prevents using more elaborate dictionary units within the same architecture. The software supports the output formats of two different LFG pasers, namely XLE (Kaplan et al., 2002) and DCU (Cahill et al., 2004). The evaluation of the tool presented in the pap"
2010.amta-papers.10,C98-2134,0,\N,Missing
2010.amta-papers.16,W09-0432,0,0.184409,"Missing"
2010.amta-papers.16,eck-etal-2004-language,0,0.25838,"he classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in term"
2010.amta-papers.16,Y09-2027,1,0.845331,"Missing"
2010.amta-papers.16,2005.eamt-1.17,0,0.0567223,"is (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to"
2010.amta-papers.16,2005.eamt-1.19,0,0.298303,"Missing"
2010.amta-papers.16,P07-2045,0,0.0121647,"Missing"
2010.amta-papers.16,W07-0733,0,0.569288,"n and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific traini"
2010.amta-papers.16,W02-1405,0,0.521618,"e combined training model. The remainder of the paper is organized as follows. In Section 2, we present previous research in the field of Domain Adaptation in SMT. Section 3 describes details of the classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and"
2010.amta-papers.16,W08-0320,0,0.404527,"d each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve d"
2010.amta-papers.16,J03-1002,0,0.00407199,"for too short sentences (Papineni et al., 2002). Usually this score reflects the fluency of the translated sentence. On the other hand NIST scores are quite similar to BLEU 4 http://www.openmatrex.org/ scores, but use an arithmetic average rather than a geometric one (Doddington, 2002). NIST weighs more informative n-grams higher than the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mi"
2010.amta-papers.16,P03-1021,0,0.00737576,"the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mix of sentences coming from multiple domains, we tested all our translation models using a combined test set from both domains. Moreover, we also tested the same models using domain-specific test sets to get a clear understanding of the effect of domain-specific data. 4.3 Domain Adaptation Experiments In this section we describ"
2010.amta-papers.16,2007.mtsummit-papers.68,0,0.656756,"o translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve domain adaptation also achieved considerable improvements. They used domain-specific baseline systems to translate in-domain monolingua"
2010.amta-papers.16,C04-1059,0,0.0344827,"he results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include"
2010.amta-papers.16,P02-1040,0,\N,Missing
2010.amta-papers.16,2006.iwslt-evaluation.4,1,\N,Missing
2010.amta-papers.16,D07-1054,0,\N,Missing
2010.amta-papers.16,W07-0717,0,\N,Missing
2010.amta-papers.19,2009.mtsummit-posters.2,0,0.0117948,"ways have a fixed limit on reordering distance that tends to be relatively low to allow efficient computation. The alignments produced by a sub-tree alignment model are also precision-oriented, rather than recall-oriented (cf. Tinsley, 2010). This is important in our case, where we want to only extract those parts of the translation suggested by the TM for which we are most certain that they are good translations. Out of the three currently available open-source sub-tree alignment systems, two can only operate when at least one language-side of the data that needs to be aligned is pre-parsed (Ambati et al., 2009, Tiedemann, 2010) and one of them needs a hand-crafted parallel treebank as training data (Tiedemann, 2010). As these requirements necessitate the acquisition of human-annotated data besides the data available in the TM, we decided to use the system described in (Zhechev, 2010) instead. It can produce aligned phrase-based-tree pairs from unannotated (i.e. unparsed) data. It can also function fully automatically without the need for any training data. The only resource necessary for the operation of this system is a probabilistic bilingual dictionary covering the data that needs to be aligned."
2010.amta-papers.19,W05-0909,0,0.0606173,"Missing"
2010.amta-papers.19,2005.eamt-1.18,0,0.0227381,"ifficulties Feiliang et al. come across are with the handling of insertions, where they need a complex system of states to decide at what position to insert a word in the translation and how to handle the lexical choice properly so that the resulting translation is fluent. The problem is exacerbated by the fact that they only use simple dictionary lookup for the translation of mismatched fragments. The system from (Feiliang et al., 2007) uses a purpose-built algorithm for finding the best TM match for the pair Chinese–Japanese, which makes it difficult to integrate with an existing TM system. Hewavitharana et al. (2005) and Simard and Isabelle (2009) present systems akin to our xml approach. They, however, do not work with an existing SMT system that can handle pre-translated fragments and need to implement the functionality themselves. Therefore and due to the complexity of the translation task, Hewavitharana et al. (2005) only evaluate their system on exact TM matches and TM matches where only one word differs from the input sentence. Simard and Isabelle (2009) also concentrate in their evaluation on the case where the TM match is used only if it is a perfect match and otherwise the MT output is used. Howe"
2010.amta-papers.19,1996.eamt-1.12,0,0.149485,"ntly, the most widely used method to enhance TMs is to employ Example-Based Machine Translation (EBMT) techniques to suggest translations for new data by combining parts of sentences from the TM database, rather than simply looking for (almost) exact matches.1 With recent advances in the performance and quality of Statistical Machine Translation (SMT) systems, many commercial TM systems offer the user the option to obtain SMT-generated translations for new data. Such translations, however, are usually only obtained for cases where the TM system could not produce a good-enough translation (cf. Heyn, 1996). Given that the SMT system used is presented with the “hard” translation cases (strings not seen in the TM) and is usually trained only on the data available in the TM, it tends to have only few examples from which to construct the translation, thus often producing fairly low quality output. Because of this, and since translators are used to TMs as an integral part of their working environment but less so to MT, SMT output is still often scorned upon by professional translators. Another major problem with the TM-SMT approach is the fact that, unlike the Fuzzy-Match With the steadily increasin"
2010.amta-papers.19,kranias-samiotou-2004-automatic,0,0.0307507,"ross all ranges, as its output is generated by supplying the SMT backend with good pre-translated fragments. The Inverse FScore graph suggests that this is due to worse lexical choice, but only manual evaluation can provide us with clues for solving the issue. The discrepancy between the results in the Inverse F-Score graph and the other metrics suggests that the biggest problem for our system is producing output in the expected word-order. 5. Related Research In this section, we look at earlier proposals for the use of MT (or MT techniques) to modify TM output to produce better translations. Kranias and Samiotou (2004) present research similar to the comb approach discussed in our paper. Using automatically established alignments between the input sentence and the TM match, as well as between the TM match and the TM translation, they identify the transformations that need to be performed on the TM translation to obtain a translation of the source sentence. The operations are Insertion (during which an input-sentence segment is translated using MT and inserted in the translation), Deletion (during which words are deleted from the translation) and Replacement (during which an input-sentence segment is transla"
2010.amta-papers.19,P02-1040,0,0.0791838,"Missing"
2010.amta-papers.19,2009.mtsummit-papers.14,0,0.376546,"anslators. Current efforts in the localisation industry are mostly directed at the reduction of the amount of See e.g. the Déjà Vu TM system (http://www.atril.com/overview), as well as Chapter 3 in (Carl and Way, 2003). For an indepth comparative review of TM systems and EBMT, see (Somers and Fernández Díaz, 2004). 1 Scores (FMS) provided by TM systems, currently there is no reliable way to automatically ascertain the quality of SMT-generated translations, so that the user could at a glance make a judgement as to the amount of effort that might be needed to postedit the suggested translation (Simard and Isabelle, 2009). Not having such automatic quality metrics also has the side effect of it being impossible for a Translation-Services Provider (TSP) company to reliably determine in advance the increase in translator productivity due to the use of MT and to adjust their resources-allocation and cost models correspondingly. We present a new system implementing a different type of TM/MT integration. The system incorporates TM, SMT and automatic Sub-Tree Alignment (STA) backends. When a new sentence needs to be translated, first a Fuzzy-Match Score (FMS) is obtained from the TM backend, together with the sugges"
2010.amta-papers.19,2006.amta-papers.25,0,0.110338,"Missing"
2010.amta-papers.19,tiedemann-2010-lingua,0,0.131306,"it on reordering distance that tends to be relatively low to allow efficient computation. The alignments produced by a sub-tree alignment model are also precision-oriented, rather than recall-oriented (cf. Tinsley, 2010). This is important in our case, where we want to only extract those parts of the translation suggested by the TM for which we are most certain that they are good translations. Out of the three currently available open-source sub-tree alignment systems, two can only operate when at least one language-side of the data that needs to be aligned is pre-parsed (Ambati et al., 2009, Tiedemann, 2010) and one of them needs a hand-crafted parallel treebank as training data (Tiedemann, 2010). As these requirements necessitate the acquisition of human-annotated data besides the data available in the TM, we decided to use the system described in (Zhechev, 2010) instead. It can produce aligned phrase-based-tree pairs from unannotated (i.e. unparsed) data. It can also function fully automatically without the need for any training data. The only resource necessary for the operation of this system is a probabilistic bilingual dictionary covering the data that needs to be aligned. For the bilingual"
2010.amta-papers.19,C08-1139,1,\N,Missing
2010.amta-papers.19,H05-1095,0,\N,Missing
2010.amta-papers.19,P07-2045,0,\N,Missing
2010.amta-papers.19,J03-1002,0,\N,Missing
2010.amta-papers.27,J93-2003,0,0.0108662,"in (3): 3.3.3 System-Independent Features (He et al., 2010) use several features that are independent of the translation system, which are useful when a third-party translation service is used or the MT system is simply treated as a black-box: P r(y = 1|x) ≈ PA,B (f ) ≡ 1 1 + exp (Af + B) • Source-Side Language Model Score and Perplexity • Target-Side Language Model Perplexity • The Pseudo-Source Fuzzy Match Score: they translate the output back to obtain a pseudo source sentence. They compute the fuzzy match score between the original source sentence and this pseudo-source • The IBM Model 1 (Brown et al., 1993) scores in both directions 4 Evaluation Methodology We conduct a human evaluation on TM–MT integration with professional post-editors. In this section we introduce the evaluation data we use, the posteditors, the evaluation environment and the questionnaire which we give to the post-editors after they have completed the evaluation. 4.1 Data Our raw data set is an English–French translation memory which consists of 51K sentence pairs of technical translation from Symantec. We randomly selected 43K to train an SMT system and used this system to translate the English side of the remaining 8K sent"
2010.amta-papers.27,D09-1030,0,0.0229881,"ognized as TM outputs by one post-editor, which shows both the potential and the necessity for TM–MT integration. This work can be extended in several ways. First of all, in this paper we concentrated on proprietary data and professional post-editors, according to the major paradigm in the localization industry. However, at the same time this limits the number of annotators we can hire, as well as the types of evaluations we can perform. We can obtain more comprehensive results by experimenting on open-domain data sets, and applying crowd-sourcing technologies such as Amazon Mechanical Turk2 (Callison-Burch, 2009). Secondly, during the evaluation we were able to collect a number of human judgements for training a new translation recommendation system. We plan to train a new recommendation model and to compare the difference with models trained on automatic metric scores, when we have collected more humanannotated data. Finally, this experiment can also be extended by measuring the actual post-editing time instead of the judgement time, which can lead to a more precise approximation of reduced post-editing effort when using translation recommendation to integrate MT outputs into a TM system. Acknowledge"
2010.amta-papers.27,2009.mtsummit-btm.7,0,0.05994,"help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our work differs in that this paper measures how the integration of TM and MT systems can help post-editors, not how post-editors perform using separate TM or MT systems. 3 The Translation Recommendation System In this section we briefly review the translation recommendation system presented by (He et al., 2010). They use an SVM binary classifier to predict the relative quality of the SMT outp"
2010.amta-papers.27,P10-1064,1,0.442255,"Missing"
2010.amta-papers.27,2009.mtsummit-papers.8,0,0.042498,"riendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well a"
2010.amta-papers.27,N03-1017,0,0.0141447,"Missing"
2010.amta-papers.27,P07-2045,0,0.0163488,"ving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our w"
2010.amta-papers.27,P02-1038,0,0.0595401,"ompleted the evaluation. 4.1 Data Our raw data set is an English–French translation memory which consists of 51K sentence pairs of technical translation from Symantec. We randomly selected 43K to train an SMT system and used this system to translate the English side of the remaining 8K sentence pairs as recommendation candidates. We train SVM translation recommendation models with 4-fold cross validation on these 8K sentence pairs, and randomly select 300 from the cross validation test sets for human evaluation. More specifically, for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the target side of the training data, and Moses (Koehn et al., 2007) to decode. For the translation recommendation model, we output a confidence level using the method in Section 3.2 and all the features in Section 3.3. 4.2 The Post-editors Five professional post-editors help us to complete this study. Four"
2010.amta-papers.27,P03-1021,0,0.0158949,"d 43K to train an SMT system and used this system to translate the English side of the remaining 8K sentence pairs as recommendation candidates. We train SVM translation recommendation models with 4-fold cross validation on these 8K sentence pairs, and randomly select 300 from the cross validation test sets for human evaluation. More specifically, for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the target side of the training data, and Moses (Koehn et al., 2007) to decode. For the translation recommendation model, we output a confidence level using the method in Section 3.2 and all the features in Section 3.3. 4.2 The Post-editors Five professional post-editors help us to complete this study. Four of them are full-time post-editors, and one is a part-time post-editor. All of the editors are hired through the localization vendors of the IT security company and have experien"
2010.amta-papers.27,2009.mtsummit-papers.14,0,0.301696,"several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs."
2010.amta-papers.27,2006.amta-papers.25,0,0.192937,"chines (SVMs: (Cortes and Vapnik, 1995)) max-margin binary classifiers, perform Radial Basis Function (RBF) kernel parameter optimization to find the optimal meta-parameters for the classifier, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TMand system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort. However, the evaluation in (He et al., 2010) suffers from lack of human-annotated data. Instead they use the TER automatic evaluation metric (Snover et al., 2006) to approximate human judgement. Despite the fact that the correlations between automatic evaluation metrics and human judgements are improving, professional post-editors are the ones that hold the final verdict over the quality of MT/TM integration. In order to draw grounded conclusions on the performance of the (He et al., 2010) recommendation framework, it is essential to conduct user studies to show whether or not systems developed using automatic evaluation metrics are confirmed by human judgements. Our experimental results support validation of the approach to approximate post-editing ef"
2010.amta-papers.27,2009.eamt-1.5,0,0.0399007,"f recommendation performance and user behaviour in Sections 5 and 6, respectively. Section 7 concludes and points out avenues for future research. 2 Related Work The translation recommendation system we experiment with is an implementation of the translation recommendation model proposed in (He et al., 2010), which we review in more detail in Section 3. Besides the translation recommendation model, there are several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment"
2010.amta-papers.27,2009.mtsummit-papers.16,0,0.0527645,"f recommendation performance and user behaviour in Sections 5 and 6, respectively. Section 7 concludes and points out avenues for future research. 2 Related Work The translation recommendation system we experiment with is an implementation of the translation recommendation model proposed in (He et al., 2010), which we review in more detail in Section 3. Besides the translation recommendation model, there are several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment"
2010.amta-papers.27,2009.mtsummit-posters.20,0,0.0706455,"esents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our work differs in that this paper measures how the integration of TM and MT systems can help post-editors, not how post-editors perform using separate TM or MT systems. 3 The Translation Recommendation System In this section we briefly review the translation recommendation system presented by (He et al., 2010). They use an SVM binary classifier to predict the relative quality of the SMT output to make a recommendation. The SVM classifier uses features from the SMT system, the TM and additional linguis"
2011.iwslt-evaluation.4,2011.iwslt-evaluation.1,0,0.040356,"Missing"
2011.iwslt-evaluation.4,N03-1017,0,0.236145,"provides a brief description of the different SMT models and adaptation techniques used in our experiments. Section 3 details our experimental setup with descriptions on the specific toolsets and data used. Section 4 provides the results of each set of experiments as well as analyses, followed by conclusion and future work in Section 5. 2. Translation Systems This section focuses on the different translation techniques used in the experiments. 2 http://www.ted.com/talks 1 http://iwslt2011.org 3 http://www.euromatrixplus.eu/downloads/35 41 2.1. Phrase-based SMT Systems Phrase-based SMT systems [2] are the most commonly used technique in statistical machine translation nowadays. In this approach, source and target phrase pairs consistent with the word alignment are extracted from the parallel training data. Phrases in PBSMT are just contiguous chunks of text, and are not linguistically motivated. The extracted source-target phrase pairs along with their translation probabilities (computed from the same training data) are stored in a structure known as the ‘phrase table’. During translation, an input sentence is split up into phrases and their corresponding translations are looked up fro"
2011.iwslt-evaluation.4,P05-1033,0,0.443143,"used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus. HPB SMT is based on phrases extracted according to the PB model [2]. Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model. HPB SMT uses hierarchical rules as a translation unit. These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form: X →&lt; α, β, ∼&gt; (7) where X is a non-terminal, α and β"
2011.iwslt-evaluation.4,P07-2045,0,0.0348805,"n the training corpora. This mixture model was used to combine the ‘in-domain’ language model with an ‘out-of-domain’ one, with the mixture weights being estimated on the ‘in-domain’ training data by applying a cross-validation scheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the siz"
2011.iwslt-evaluation.4,P03-1021,0,0.0301456,"cheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw"
2011.iwslt-evaluation.4,J03-1002,0,0.00250058,"ammaticality of the output. Our experiments will show the effects of this trade-off between label accuracy and sparsity. 3. Experimental Setups This section details the setup for the different experiments. We also provide a brief account of the different tools and datasets used along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of ou"
2011.iwslt-evaluation.4,P02-1040,0,0.0815708,"d along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,6"
2011.iwslt-evaluation.4,2011.mtsummit-papers.32,1,0.751983,"e components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Gra"
2011.iwslt-evaluation.4,W07-0734,0,0.068226,"t on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 d"
2011.iwslt-evaluation.4,2006.amta-papers.25,0,0.0278312,"alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 development an"
2011.iwslt-evaluation.4,W06-3119,0,0.0256821,"sted in the translation table recursively from longer phrases and replacing them with the non-terminal symbol X. Non-terminals in hierarchical rules act as placeholders that are replaced with other phrases during translation in a bottom-up fashion. Hierarchical rules are extracted from the training corpus without using any syntactic information. As the resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a de"
2011.iwslt-evaluation.4,2010.iwslt-papers.1,1,0.902746,"resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a description of the approach of extracting non-terminal labels using the same. 2.4.1. Combinatory Categorial Grammar CCG [5] is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that sp"
2011.iwslt-evaluation.4,J99-2004,0,0.024692,"s in using statistically extracted phrases which do not necessarily correspond to syntactic constituents. Secondly, CCG categories reflect rich information about the syntactic structure to which the word/phrase belongs at the lexical level without the need to build a full parse tree for the sentence. Thirdly, CCG parsing is more efficient in comparison to phrase-structure grammar parsing. Because most of the CCG grammar is contained in the lexicon, the process of supertagging, which is to assign supertags (i.e. complex CCG categories) to the words in a sentence, is considered “almost parsing” [13]. After supertagging, the CCG parser is only required to combine the supertags using CCG simple combinatory operators. For the aforementioned reasons, CCG is considered more suitable to be used in SMT than phrase-structure grammar. Attaching CCG categories to non-terminals in hierarchical rules is done in a way similar to that of SAMT approach: • First, each target-side sentence from the parallel corpus is supertagged by assigning the best sequence of CCG supertags to its words. • Next, phrase pairs are extracted from the parallel corpus according to the PBSMT phrase extraction method [2]. • T"
2011.iwslt-evaluation.4,2011.eamt-1.38,1,0.804863,"thermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information. For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq]. All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem. In order to address this problem, we simplify CCG non-terminal labels by reducing the amount of the information represented in them using the following approaches [14]: • Feature-dropped CCG labels: these labels are extracted from CCG categories by dropping the syntactic features attached to atomic categories from the label representation. For example, if a phrase has a CCG category S[dcl]/NP, then its feature-dropped CCG label is S/NP. • CCG Contextual Labels: in a CCG contextual label, only left and right argument categories are used in the label representation whereas the resulting category (i.e. the functor) is dropped from the label representation. The resulting CCG contextual label takes the form L R. If any of the argument categories is missing, an X"
2011.iwslt-evaluation.4,Y09-2027,1,0.83049,"we perform case restoration and detokenization for the English data. Case restoration, or truecasing is treated as a translation task. A simple phrase-based translation model is trained on aligned lower-case and truecase data to successfully achieve the task of true-casing. 3.3. PBSMT based Language Model Adaptation Experiments As shown in Table 1, the size of the ‘in-domain’ TED training data is much smaller than the ‘out-of-domain’ Multi-UN training data. Since adding a significant amount of out-ofdomain data to an in-domain corpus reduces the quality of translation for in-domain sentences [23], we decided to use only a part of the out-of-domain data to enhance the translation quality. In order to achieve this, we constructed a language model on the TED monolingual data and computed sentence-level perplexity score for all the sentences in MultiUN, with respect to the TED language model. After sorting the sentences in the ascending order of the perplexity values, only sentences below a specific threshold were selected. This method provided us with the most ‘TED-like’ sentences from the Multi-UN corpora. In order to decide which specific threshold gives us the best possible translatio"
2011.iwslt-evaluation.4,2009.iwslt-papers.4,0,0.0154921,"ents Figure 1 shows the variation of BLEU scores for different adapted language models pertaining to different thresholds. According to our experiments, the best cut-off thresholds were 43.00 and 53.00 for Zh–En and Ar–En language pairs, respectively. For Ar–En language pair, the best BLEU 45 score is achieved for multiple thresholds, and we select the one with the maximum number of sentences in it. The number of Multi-UN sentences thus selected were 55,841 and 89,310 for Zh–En and Ar–En language pairs, respectively. 3.4. HPB Experiments We built our HPB baseline using the Moses Chart Decoder [24]. Continuous phrases are extracted according to the phrase based system settings explained in Section 3.1. Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only monotone concatenation of phrases is used. Rules extracted contain up to 2 non-terminals. Adjacent non-terminals on the source side are not allowed. 3.5. CCG-augmented HPB Experiments We built our CCG-augmented HPB system using the Moses Chart Decoder, which has an option to extract syntaxaugmented rules from an annotated corpus. We used"
2011.iwslt-evaluation.4,W04-3250,0,0.160582,"Missing"
2011.mtsummit-papers.32,N09-1025,0,0.0602809,"Missing"
2011.mtsummit-papers.32,W07-0722,0,0.0178866,"oved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and evaluation. While Foster and Kuhn (2007) used test and development sets which were essentially a combination of data from different training genres, in our case test data (user forum) are inherently different from"
2011.mtsummit-papers.32,eck-etal-2004-language,0,0.126201,"tensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT s"
2011.mtsummit-papers.32,2010.amta-commercial.5,0,0.224215,"Introduction In recent years, Statistical Machine Translation (SMT) technology has been used in many online applications, concentrating on professionally edited enterprise quality online content. At the same time, very little research has gone into adapting ∗ Work done while at CNGL, School of Computing, DCU 285 SMT technology to the translation of user-generated content on the web. While translation of online chats (Flournoy and Callison-Burch, 2000) has received some attention, there is surprisingly little work on translation of online user forum data, despite growing interest in the area (Flournoy and Rueppel, 2010). In this paper we describe our efforts in building a system to address this particular application area. Our experiments are conducted on data collected from online forums on Symantec Security tools and services.1 For a multinational company like Symantec, the primary motivation behind translation of user forum data is to enable access across language barriers to information in the forums. Forum posts are rich in information about issues and problems with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are ev"
2011.mtsummit-papers.32,W04-3250,0,0.0274837,"Missing"
2011.mtsummit-papers.32,2005.mtsummit-papers.11,0,0.0155595,"ms with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are even aware of them. The major challenge in developing MT systems for user forum data concerns the lack of proper parallel training material. Forum data is monolingual and hence cannot be used directly to train SMT systems. We use parallel training data in the form of Symantec Enterprise Translation Memories (TMs) from different product and service domains to train the SMT models. As an auxiliary source, we also used portions of the Europarl dataset2 (Koehn, 2005), selected according to their similarity with the forum data (Section 3.2), to supplement the TMbased training data. Symantec TM data, being a part of enterprise documentation, is professionally 1 2 http://community.norton.com/ http://www.statmt.org/europarl/ edited and by and large conforms to the Symantec controlled language guidelines, and is signiﬁcantly different in nature from the user forum data, which is loosely moderated and does not use controlled language at all. In contrast Europarl data is outof-domain with respect to the forum data. The differences between available training and"
2011.mtsummit-papers.32,P07-2045,0,0.0156353,"Missing"
2011.mtsummit-papers.32,W07-0733,0,0.0321071,"retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and eval"
2011.mtsummit-papers.32,W02-1405,0,0.0297662,"ts, followed by conclusions and future work in Section 6. 2 Related Work Mixture Modelling (Hastie et al., 2001), a wellestablished technique for combining multiple mod286 els, has been extensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain"
2011.mtsummit-papers.32,J03-1002,0,0.00718605,"Missing"
2011.mtsummit-papers.32,P03-1021,0,0.0354487,"Missing"
2011.mtsummit-papers.32,C08-1125,0,0.0125168,"nbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word a"
2011.mtsummit-papers.35,W09-0401,0,0.0230036,"o Target GIZA++ word alignment strengths to filter context information, using the word alignment levels of ≥0.6, ≥0.7, ≥0.8 and ≥0.9 as thresholds: that is for each threshold, source 3 This was suggested to us by Ondrej Bojar following an early presentation of the material. 4 Note that the source context information suffix #e filtering is crucially different from the alignment strength based context thresholding developed below. 311 We ran the same set of experiments for the other translation direction, French to English, which is often considered the easier of the two translation directions (Callison-Burch et al., 2009). 3.2.1 Experimental Results Using SPE Simple PE results on the test set show that for our data set a simple second-stage PB-SMT system is able to improve on the first-stage PB-SMT system in a pure PB-SMT post-editing pipeline, with a small increase in BLEU of 0.65 absolute over baseline. This result is statistically significant at the p ≤ 0.05 level. Compared to baseline and PE, BLEU scores deteriorate for the context-aware post-editing pipeline PE-C, as any beneficial impact of the postediting pipeline is swamped by data sparseness and OOV items in the output of the second stage PE-C system."
2011.mtsummit-papers.35,W07-0732,0,0.0557783,"“mono-lingual” SPE system. Simard et al. (2007a) present experiments using Human Resources and Social Development (HRSDC) Job Bank1 French and English parallel data and found that in combination, the RBMT system post-edited 1 www.jobbank.gc.ca 308 SPE was also applied in an attempt to improve Japanese to English patent translations. Teramusa (2007) uses RBMT to translate patent texts, which tend to be difficult to translate without syntactic analysis. Combining RBMT with SPE in the post-editing phase produced an improved score on the NIST evaluation compared to that of the RBMT system alone. Dugast et al. (2007) report research on combining SYSTRAN with PB-SMT systems Moses and Portage. Comparison between raw SYSTRAN output and SYSTRAN+SPE output shows significant improvements in terms of lexical choice, but almost no improvement in word order or grammaticality. Dugast et al. (2009) trained a similar post-editing system with some additional treatment to prevent the loss of entities such as dates and numbers. Oflazer and El-Kahlout (2007) explore selective segmentation based models for English to Turkish translation. As part of their experiments they present a short section towards the end of the pape"
2011.mtsummit-papers.35,W09-0419,0,0.368337,"attempt to improve Japanese to English patent translations. Teramusa (2007) uses RBMT to translate patent texts, which tend to be difficult to translate without syntactic analysis. Combining RBMT with SPE in the post-editing phase produced an improved score on the NIST evaluation compared to that of the RBMT system alone. Dugast et al. (2007) report research on combining SYSTRAN with PB-SMT systems Moses and Portage. Comparison between raw SYSTRAN output and SYSTRAN+SPE output shows significant improvements in terms of lexical choice, but almost no improvement in word order or grammaticality. Dugast et al. (2009) trained a similar post-editing system with some additional treatment to prevent the loss of entities such as dates and numbers. Oflazer and El-Kahlout (2007) explore selective segmentation based models for English to Turkish translation. As part of their experiments they present a short section towards the end of the paper on statistical post-editing of an SMT system, which they call model iteration. They train a post-editing SMT model on the training set decoded by the first stage SMT model and iterate the approach, post-editing the output of the post-editing system. BLEU results show positi"
2011.mtsummit-papers.35,2007.mtsummit-wpt.4,0,0.159837,"Missing"
2011.mtsummit-papers.35,N03-1017,0,0.0202641,"Missing"
2011.mtsummit-papers.35,P07-2045,0,0.0604729,"t (2007), our experiments use PB-SMT systems throughout both stages. The objective is to investigate in more detail whether and to what extent state-of-the-art PBSMT technology can be used to post-edit itself, i.e. its own output. 2 Methodology In our experiments we focus on English and French as these are the languages considered in the original research by Simard et al.(Simard et al., 2007a). As SPE on the output of RBMT systems is already a commercial reality,2 we use industry translation memories (TMs) from the technical computing domain as our data. We use a standard Moses PB-SMT set-up (Koehn et al., 2007). 2.1 Data The data for our experiments come from an English– French Translation Memory from an IT company (Symantec). The domain of the data is technical software user help information. The translation memory was preprocessed to remove all TMX markup and meta-data information. The translation memory contains 55,322 unique segments. From this we randomly extract a training set of 52,383 English– French segment pairs, between 1 and 98 words in length for English, and 1 to 100 for French. The average segment length in the training set is 13 words for English and 15 words for French. The training"
2011.mtsummit-papers.35,P06-1096,0,0.0717622,"Missing"
2011.mtsummit-papers.35,P11-1124,1,0.850096,"Missing"
2011.mtsummit-papers.35,J03-1002,0,0.0103913,"the second stage PBSMT system. Note that this (subtlety) applies to the generation of the (source side of the) second stage training data only, and that in all the experiments, unseen test data are always translated using first and second stage PB-SMT systems trained on the full training sets E and F (for the stage one system) and F’ and F (for the stage two post-editing system). 2.3 MT System We use a standard Moses PB-SMT system (Koehn et al., 2007), 5-gram language models with KneserNey smoothing trained with SRILM (Stolcke, 2002), the G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described in Koehn et al. (2003). All systems are tuned using minimum error rate training (MERT) (Och, 2003) on the development set. During the decoding phase, the stack size was limited to 500 hypotheses. We use approximate randomisation methods (Noreen, 1989) as implemented in FAST MTE VAL (Stroppa and Owczarzak, 2007) to test for statistical significance. 2.4 Contextual SPE In our basic SPE pipeline (PE) – translating, say, from English to French (E–F) – the second-stage SPE system is trained on the output (F’) of the (10fold cross valid"
2011.mtsummit-papers.35,P03-1021,0,0.0712321,"ments, unseen test data are always translated using first and second stage PB-SMT systems trained on the full training sets E and F (for the stage one system) and F’ and F (for the stage two post-editing system). 2.3 MT System We use a standard Moses PB-SMT system (Koehn et al., 2007), 5-gram language models with KneserNey smoothing trained with SRILM (Stolcke, 2002), the G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described in Koehn et al. (2003). All systems are tuned using minimum error rate training (MERT) (Och, 2003) on the development set. During the decoding phase, the stack size was limited to 500 hypotheses. We use approximate randomisation methods (Noreen, 1989) as implemented in FAST MTE VAL (Stroppa and Owczarzak, 2007) to test for statistical significance. 2.4 Contextual SPE In our basic SPE pipeline (PE) – translating, say, from English to French (E–F) – the second-stage SPE system is trained on the output (F’) of the (10fold cross validation version of the) first-stage MT system, effectively resulting in a “mono-lingual” SPE system (F’-F). In as sense, however, the secondstage SPE system has los"
2011.mtsummit-papers.35,W07-0704,0,0.745293,"ate without syntactic analysis. Combining RBMT with SPE in the post-editing phase produced an improved score on the NIST evaluation compared to that of the RBMT system alone. Dugast et al. (2007) report research on combining SYSTRAN with PB-SMT systems Moses and Portage. Comparison between raw SYSTRAN output and SYSTRAN+SPE output shows significant improvements in terms of lexical choice, but almost no improvement in word order or grammaticality. Dugast et al. (2009) trained a similar post-editing system with some additional treatment to prevent the loss of entities such as dates and numbers. Oflazer and El-Kahlout (2007) explore selective segmentation based models for English to Turkish translation. As part of their experiments they present a short section towards the end of the paper on statistical post-editing of an SMT system, which they call model iteration. They train a post-editing SMT model on the training set decoded by the first stage SMT model and iterate the approach, post-editing the output of the post-editing system. BLEU results show positive improvements, with a cumulative 0.46 increase after 2 model iterations. It is not clear whether the result is statistically significant. Our experiments fo"
2011.mtsummit-papers.35,N07-1064,0,0.464819,"side of the training set of the PB-SMT system and the corresponding human translated reference. A complete translation pipeline consists of a rule-based first-stage system, whose output on some (unseen) test set, in turn, is translated by the second-stage “mono-lingual” SPE system. Simard et al. (2007a) present experiments using Human Resources and Social Development (HRSDC) Job Bank1 French and English parallel data and found that in combination, the RBMT system post-edited 1 www.jobbank.gc.ca 308 SPE was also applied in an attempt to improve Japanese to English patent translations. Teramusa (2007) uses RBMT to translate patent texts, which tend to be difficult to translate without syntactic analysis. Combining RBMT with SPE in the post-editing phase produced an improved score on the NIST evaluation compared to that of the RBMT system alone. Dugast et al. (2007) report research on combining SYSTRAN with PB-SMT systems Moses and Portage. Comparison between raw SYSTRAN output and SYSTRAN+SPE output shows significant improvements in terms of lexical choice, but almost no improvement in word order or grammaticality. Dugast et al. (2009) trained a similar post-editing system with some additi"
2011.mtsummit-papers.35,2007.mtsummit-papers.34,0,0.625947,"for a Statistical MT System Hanna B´echara† Yanjun Ma ‡ Josef van Genabith† † Centre for Next Generation Localisation School of Computing, Dublin City University {hbechara, josef}@computing.dcu.ie ‡ Baidu Inc., Beijing China yma@baidu.com Abstract by the PB-SMT system performed significantly better than each of the individual systems on their own. Simard et al. (2007a) also tested the SPE technique with Portage PB-SMT both as first-stage MT and as second stage SPE system (i.e. Portage post-editing its own output) and reported that nothing could be gained. In a number of follow-up experiments, Simard et al. (2007b) used an SPE system to adapt RBMT-systems to a specific domain, once again using Portage in the SPE phase. Adding the SPE system produced BLEU score increases of about 20 points over the original RBMT baseline. Statistical post-editing (SPE) techniques have been successfully applied to the output of Rule Based MT (RBMT) systems. In this paper we investigate the impact of SPE on a standard Phrase-Based Statistical Machine Translation (PB-SMT) system, using PB-SMT both for the first-stage MT and the second stage SPE system. Our results show that, while a naive approach to using SPE in a PB-SMT"
2011.mtsummit-papers.35,2006.amta-papers.25,0,0.219364,"the relative merits of each system, and can be used to view the overall strength and weakness of our post-editing systems. Analysis and Discussion In order to obtain a better understanding of the translation quality gains in French to English translation and respectively the quality loss in English to French translation, we performed both automatic and manual sentence-level evaluation in a bid to reveal the advantages and disadvantages of our statistical post-editing systems under different translation conditions. Firstly, we report edit statistics for each sentence sorted by TER edit types (Snover et al., 2006). As a reasonable approximation to human post-editing efforts, these statistics can be of help in gauging the applicability of our post-editing systems in real world localisation work-flows. Secondly, we perform automatic sentence-level evaluation using sentence-level BLEU (SBLEU) (Liang 312 Table 5 shows the number of average edits per sentence, based on the TER edit types i.e. insertion (Ins), substitution (Sub), deletion (Del) and shift and the number of errors (N-Err) for our French to English experiments. These numbers, like all of the numbers in this section, have been normalised using s"
2011.mtsummit-papers.35,2007.tmi-papers.27,0,0.190507,"editing system). 2.3 MT System We use a standard Moses PB-SMT system (Koehn et al., 2007), 5-gram language models with KneserNey smoothing trained with SRILM (Stolcke, 2002), the G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described in Koehn et al. (2003). All systems are tuned using minimum error rate training (MERT) (Och, 2003) on the development set. During the decoding phase, the stack size was limited to 500 hypotheses. We use approximate randomisation methods (Noreen, 1989) as implemented in FAST MTE VAL (Stroppa and Owczarzak, 2007) to test for statistical significance. 2.4 Contextual SPE In our basic SPE pipeline (PE) – translating, say, from English to French (E–F) – the second-stage SPE system is trained on the output (F’) of the (10fold cross validation version of the) first-stage MT system, effectively resulting in a “mono-lingual” SPE system (F’-F). In as sense, however, the secondstage SPE system has lost the connection to the original source data: ideally we would like to be able to be in a position to distinguish between situations where f’ is a good translation of some source word (or phrase) e, and situations"
2011.mtsummit-papers.52,W05-0909,0,0.13897,"Missing"
2011.mtsummit-papers.52,P05-1033,0,0.244672,"Missing"
2011.mtsummit-papers.52,2010.jec-1.4,0,0.114664,"s”). When presented with fuzzy matches, translators can avail of useful complete matching sub-segments in previous translations while composing the translation of a new segment. This improves the consistency of translation, as new translations produced by translators are based on the target side of the fuzzy match they have consulted, and translators will build their translations around terminologies already used in the TM. It is, therefore, natural to resort to TMs for consistent translation, and to incorporate fully matching sub-segments from fuzzy match examples into the SMT pipeline (cf. (Koehn and Senellart, 2010), (Zhechev and van Genabith, 2010), and (Ma et al., 2011)). Although these methods have led to improved translations, they only use very simple features (such as a threshold on the fuzzy match score of the complete TM segment) to determine whether matching sub-segments from the fuzzy match are suitable for use in the SMT pipeline. Here, we propose a rich set of linguistic features to select TM fuzzy matches that contain useful sub-segments that improve translation consistency in an SMT pipeline.1 We assume that many factors are rele1 In Ma et al. (2011), we considered a richer set of features"
2011.mtsummit-papers.52,P11-1124,1,0.678615,"Missing"
2011.mtsummit-papers.52,J03-1002,0,0.00594353,"bed in Ma et al. (2011). If a classifier predicts that the markup will lead to improved translation quality and translation, the consistent phrase translation will be reused directly in the translation process. Below we explain how consistent phrase pairs are defined, and how markup classification is performed. Based on these, we discuss why linguistic features are essential for this task. 3.1 Consistent Phrase Pair Identification We use the method of Ma et al. (2011) to extract consistent phrase pairs: extracted phrase pairs are the intersections of bidirectional GIZA++ posterior alignments (Och and Ney, 2003) between the source and the target side of the TM fuzzy match. We use the intersected word alignment to minimize the noise introduced by word alignment in one direction only, in order to ensure translation consistency. 3.2 Markup Classification Following (Ma et al., 2011), we use Support Vector Machines (SVMs, (Cortes and Vapnik, 1995)) to determine whether constraining translation with our consistent phrase pairs can help improve translation quality. We treat constrained translation as a binary classification problem, and use the SVM classifier to decide whether we should mark up a segment or"
2011.mtsummit-papers.52,J04-4002,0,0.0207764,"ation from Symantec, consisting of 87K segment pairs. The average segment length of the English training set is 13.3 words and the size of the training set is comparable to the larger TMs used in the industry. We obtain training samples using the cross-fold translation technique in Ma et al. (2011), so the word aligner, the translation models, and the classifier are all trained on the same training corpus. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. As for SVM parameters, we set c = 2.0 and γ = 0.125. We conducted experiments using a standard log-linear PB-SMT (Och and Ney, 2004) system Moses,8 which is capable of handling user-specified translations for portions of the input during decoding. The maximum phrase length is set to 7. 5.1 Evaluation The performance of the phrase-based SMT system is measured by B LEU score (Papineni et al., 2002) and T ER (Snover et al., 2006). Significance testing is carried out using approximate randomization (Noreen, 1989). We also measure the quality of the classification using precision and recall. Let A be the set of predicted markup input segments, and B be the set of input segments where the markup version has a lower T ER score th"
2011.mtsummit-papers.52,J05-1004,0,0.00720046,"ad POSi =1 iff the first word of the source segment is marked up and has the POS tag POSi . Dependency Features We use dependency relations (obtained using the Stanford parser) to establish the roles of matched parts in the input sentence in terms of syntactic dependencies. The dependency features include DEP Coverage, DEP Position, and DEP Consistency, all of which follow the definitions in Ma et al. (2011). Semantic Role Features Our semantic role labels are obtained using the Suda SRL labeler,6 with constituent trees produced by the Stanford parser as input. The labels follow the PropBank (Palmer et al., 2005) annotation. Following POS features, for each predicate identified in a segment, we define SEMi (¯ em ) as the number of words in the input segment e having the role SEMi that are marked up 4 http://snowball.tartarus.org/ algorithms/english/stop.txt 5 http://nlp.stanford.edu/software/ lex-parser.shtml 6 http://nlp.suda.edu.cn/˜jhli/ with translations from TM, and define #SEMi (e) as the number of words in e that have the SEM role SEMi . The features include SEM Partial Coverage which calculates the marked-up percentage for each argument label,7 SEM Complete Coverage – a binary feature that fir"
2011.mtsummit-papers.52,P02-1040,0,0.0900314,"lation technique in Ma et al. (2011), so the word aligner, the translation models, and the classifier are all trained on the same training corpus. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. As for SVM parameters, we set c = 2.0 and γ = 0.125. We conducted experiments using a standard log-linear PB-SMT (Och and Ney, 2004) system Moses,8 which is capable of handling user-specified translations for portions of the input during decoding. The maximum phrase length is set to 7. 5.1 Evaluation The performance of the phrase-based SMT system is measured by B LEU score (Papineni et al., 2002) and T ER (Snover et al., 2006). Significance testing is carried out using approximate randomization (Noreen, 1989). We also measure the quality of the classification using precision and recall. Let A be the set of predicted markup input segments, and B be the set of input segments where the markup version has a lower T ER score than the plain version. We stan7 If more than one predicate is identified, the value of the feature is averaged among argument labels for each predicate. 8 http://www.statmt.org/moses/ 459 dardly define precision P and recall R as in (3): |A B| |A B| P = ,R= |A| |B| (3"
2011.mtsummit-papers.52,2009.mtsummit-papers.14,0,0.275606,"s the line of research proposed by Ma et al. (2011), which improves the consistency of translations in PB-SMT systems by constraining the SMT system with consistent phrase pairs induced from TMs. Whether the consistent phrase pairs should be used is determined through discriminative learning. As the research in this paper builds on this previous work of ours, we review it in detail in Section 3. Prior to Ma et al. (2011), several proposals used translation information derived from TM fuzzy matches, such as (i) adding such translations into a phrase table as in Bic¸ici and Dymetman (2008)2 and Simard and Isabelle (2009), or (ii) marking up the input segment using the relevant sub-segment translations in the fuzzy match, and using an MT system to translate the parts that are not marked up, as in Smith and Clark (2009), Koehn to dependency labels. 2 Note that discontinuous phrase pairs are used in Bic¸ici and Dymetman (2008), whereas we use continuous phrase pairs here. 457 and Senellart (2010), and Zhechev and van Genabith (2010). However, these do not include a classification step that determines whether consistent phrase pairs should be used. 3 Constrained Translation via Markup Classification Ma et al. (20"
2011.mtsummit-papers.52,2006.amta-papers.25,0,0.378349,". We use the intersected word alignment to minimize the noise introduced by word alignment in one direction only, in order to ensure translation consistency. 3.2 Markup Classification Following (Ma et al., 2011), we use Support Vector Machines (SVMs, (Cortes and Vapnik, 1995)) to determine whether constraining translation with our consistent phrase pairs can help improve translation quality. We treat constrained translation as a binary classification problem, and use the SVM classifier to decide whether we should mark up a segment or not. We label training data using the automatic T ER score (Snover et al., 2006), as in (1).  +1 if TER(w. markup) < TER(w/o markup) y= −1 if TER(w/o markup) ≥ TER(w. markup) (1) Each data point is associated with a set of features which are discussed in more detail in Section 4. We perform our experiments with the Radial Basis Function (RBF) kernel, and use Platt’s method (Platt, 1999) (as improved by (Lin et al., 2007)) to fit the SVM output to a sigmoid function, to obtain probabilistic outputs from the SVM. 3.3 Rich Linguistic Features for Markup Classification A close look at the markup classification procedure shows that the accuracy of classification (and ultimate"
2011.mtsummit-papers.52,W10-3806,1,0.899614,"Missing"
2011.mtsummit-tutorials.2,quirk-2004-training,0,\N,Missing
2011.mtsummit-tutorials.2,E06-1032,0,\N,Missing
2011.mtsummit-tutorials.2,C10-2043,1,\N,Missing
2011.mtsummit-tutorials.2,J99-4005,0,\N,Missing
2011.mtsummit-tutorials.2,C10-1035,0,\N,Missing
2011.mtsummit-tutorials.2,N04-1023,0,\N,Missing
2011.mtsummit-tutorials.2,2003.mtsummit-papers.52,0,\N,Missing
2011.mtsummit-tutorials.2,U05-1019,0,\N,Missing
2011.mtsummit-tutorials.2,W09-0402,0,\N,Missing
2011.mtsummit-tutorials.2,J93-2003,0,\N,Missing
2011.mtsummit-tutorials.2,C08-1064,0,\N,Missing
2011.mtsummit-tutorials.2,W07-0714,1,\N,Missing
2011.mtsummit-tutorials.2,W08-0332,0,\N,Missing
2011.mtsummit-tutorials.2,P02-1040,0,\N,Missing
2011.mtsummit-tutorials.2,P04-1077,0,\N,Missing
2011.mtsummit-tutorials.2,N10-1080,0,\N,Missing
2011.mtsummit-tutorials.2,P07-1108,0,\N,Missing
2011.mtsummit-tutorials.2,W06-1607,0,\N,Missing
2011.mtsummit-tutorials.2,D09-1133,0,\N,Missing
2011.mtsummit-tutorials.2,W05-0909,0,\N,Missing
2011.mtsummit-tutorials.2,P04-1041,1,\N,Missing
2011.mtsummit-tutorials.2,P07-2045,0,\N,Missing
2011.mtsummit-tutorials.2,P09-1034,0,\N,Missing
2011.mtsummit-tutorials.2,P08-1007,0,\N,Missing
2011.mtsummit-tutorials.2,P11-1124,1,\N,Missing
2011.mtsummit-tutorials.2,P09-1035,0,\N,Missing
2011.mtsummit-tutorials.2,N09-1025,0,\N,Missing
2011.mtsummit-tutorials.2,D08-1064,0,\N,Missing
2011.mtsummit-tutorials.2,D09-1030,0,\N,Missing
2011.mtsummit-tutorials.2,W07-0732,0,\N,Missing
2011.mtsummit-tutorials.2,C04-1046,0,\N,Missing
2011.mtsummit-tutorials.2,P10-1064,1,\N,Missing
2011.mtsummit-tutorials.2,W07-0718,0,\N,Missing
2011.mtsummit-tutorials.2,N10-1031,0,\N,Missing
2011.mtsummit-tutorials.2,J02-3001,0,\N,Missing
2011.mtsummit-tutorials.2,P05-1033,0,\N,Missing
2011.mtsummit-tutorials.2,N03-1017,0,\N,Missing
2011.mtsummit-tutorials.2,P02-1038,0,\N,Missing
2011.mtsummit-tutorials.2,J03-1002,0,\N,Missing
2011.mtsummit-tutorials.2,J05-1004,0,\N,Missing
2011.mtsummit-tutorials.2,2009.eamt-1.5,0,\N,Missing
2011.mtsummit-tutorials.2,W05-0904,0,\N,Missing
2011.mtsummit-tutorials.2,W08-0309,0,\N,Missing
2011.mtsummit-tutorials.2,2005.mtsummit-papers.11,0,\N,Missing
2011.mtsummit-tutorials.2,W07-0736,0,\N,Missing
2011.mtsummit-tutorials.2,2009.mtsummit-papers.8,0,\N,Missing
2011.mtsummit-tutorials.2,2009.mtsummit-btm.7,0,\N,Missing
2011.mtsummit-tutorials.2,W10-3806,1,\N,Missing
2011.mtsummit-tutorials.2,2010.jec-1.4,0,\N,Missing
2011.mtsummit-tutorials.2,D07-1080,0,\N,Missing
2012.eamt-1.38,W05-0909,0,0.00939362,"s confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089 3,333 test 2,000 62,953 4,022 66,770 7,056 English–French EN–EL / env 53.49 34.15 3.00 5.09 4.28 Sys"
2012.eamt-1.38,2010.amta-papers.16,1,0.890714,"Missing"
2012.eamt-1.38,2008.tc-1.1,0,0.351485,"proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation mo"
2012.eamt-1.38,eck-etal-2004-language,0,0.0421934,"st of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as"
2012.eamt-1.38,W08-0334,0,0.0350418,"an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from different domains or even from the web, which is also the case in our work. 3 Resources and their acquisition In this"
2012.eamt-1.38,2009.mtsummit-commercial.5,0,0.0940661,"Missing"
2012.eamt-1.38,2005.eamt-1.19,0,0.0499412,"those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-d"
2012.eamt-1.38,P05-1058,0,0.0700041,"Missing"
2012.eamt-1.38,W07-0733,0,0.108639,"2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to"
2012.eamt-1.38,P07-2045,0,0.0439871,"cally contain vocabulary that is not likely to be found in texts from other domains (Banerjee et al., 2010). Other problems can be caused by divergence in style or genre where the difference is not only in lexis but also in grammar. In order to achieve optimal performance, an SMT system should be trained on data from the same domain, genre, and style as it is applied to. For many domains, though, in-domain data of a size sufficient to train a full system is hard to find. Recent experiments have shown that even small amounts of such data can be used to adapt a system to the domain of interest (Koehn et al., 2007). In this work, we present a strategy for automatic web-crawling and cleaning of domain-specific data. Further, our exhaustive experiments, carried out for the Natural Environment (env) and Labour Legislation (lab) domains and English– French (EN–FR) and English–Greek (EN–EL) language pairs (in both directions), demonstrate how the crawled data improves SMT quality. After an overview of related work, we discuss the possibility of adapting a general-domain SMT system by using various types of in-domain data. Then, we present our web-crawling procedure followed by a description of a series of ex"
2012.eamt-1.38,2005.mtsummit-papers.11,0,0.192466,"Missing"
2012.eamt-1.38,W02-1405,0,0.111907,"that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn"
2012.eamt-1.38,J05-4003,0,0.0461806,"er to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and decl"
2012.eamt-1.38,W08-0320,0,0.0403881,". Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from dif"
2012.eamt-1.38,P03-1021,0,0.009744,"th length ratio within the interval h0.11,9.0i. The maximum 149 English–Greek category 1. perfect translation 2. minor corrections done 3. major corrections needed 4. misaligned sentence pair 5. wrong domain Table 6: Details of the in-domain parallel data sets obtained by web-crawling and manual correction: sentence pairs (sents), source (L1 ) and target (L2 ) tokens and vocabulary size (voc). length of aligned phrases is set to 7 and the reordering models are generated using parameters: distance, orientation-bidirectional-fe. The model parameters are optimized by Minimum Error Rate Training (Och, 2003, MERT) on development sets. For decoding, test sentences are tokenized, lowercased, and translated by the tuned system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4.2 Using out-of-domain test data A number of previous experiments (Wu et al., 2008; Banerjee et al., 2010, e.g.) showed significant degradation of translation quality if an SMT system was applied to out-of-domain data. In order to verify this observation we trained and tuned our system on general-domain data and compared its pe"
2012.eamt-1.38,P02-1040,0,0.0968791,"ected for corrections were used as training sets. See further statistics in Table 6. The correctors confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089"
2012.eamt-1.38,2011.eamt-1.40,1,0.250717,"Missing"
2012.eamt-1.38,J03-3002,0,0.202473,"ent as relevant to a domain or not also affects the acquisition of domain-specific resources, on the assumption that relevant pages are more likely to contain links to more pages in the same domain. Qi and Davison (2009) review features and algorithms used in web page classification. In most of the algorithms reviewed, on-page features (i.e. textual content and HTML tags) are used to construct a corresponding feature vector and then, several machine-learning approaches, such as SVMs, Decision Trees, and Neural Networks, are employed (Yu et al., 2004). Considering the Web as a parallel corpus, Resnik and Smith (2003) proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs"
2012.eamt-1.38,C08-1125,0,0.0633981,"Missing"
2012.eamt-1.38,wu-wang-2004-improving-domain,0,\N,Missing
2012.eamt-1.38,W04-3250,0,\N,Missing
2012.eamt-1.38,W10-1720,1,\N,Missing
2012.eamt-1.38,2006.eamt-1.31,0,\N,Missing
2012.eamt-1.38,2010.amta-papers.27,1,\N,Missing
2012.eamt-1.41,2011.mtsummit-papers.32,1,0.894345,"Missing"
2012.eamt-1.41,P11-2071,0,0.0421364,"Missing"
2012.eamt-1.41,2005.eamt-1.19,0,0.0387288,"briefly reviews relevant related work. Section 3 provides a detailed discussion on the normalization techniques as well as the acquisition of supplementary training material. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III"
2012.eamt-1.41,W04-3250,0,0.147683,"Missing"
2012.eamt-1.41,2005.mtsummit-papers.11,0,0.038138,"y the spell checker were replaced with the highest ranking suggestion from the spell checker. As in Section 3.4, the spelling corrections were applied only to the test sets to ensure a reduction in the number of spelling error-based OOVs. 3.6 2. Supplementary Data Selection To take care of the VAL tokens which are valid words but absent in the training data, we explored techniques of mining supplementary data to improve the chances of successfully translating these tokens. We used the following freely available parallel data collections as potential sources of supplementary data: 1. Europarl (Koehn, 2005): Parallel corpus comprising of the proceedings of the European 172 Parliament. News Commentary Corpus: Released as a part of the WMT 2011 Translation Task.3 OpenOffice Corpus: Parallel documentation of the Office package from OpenOffice.org, released as part of the OPUS corpus (Tiedemann, 2009). KDE4 Corpus: A parallel corpus of the KDE4 localization files released as part of OPUS. PHP Corpus: Parallel corpus generated from multilingual PHP manuals also released as part of OPUS. OpenSubtitles2011 Corpus:4 A collection of documents released as part of OPUS. EMEA Corpus: A parallel corpus from"
2012.eamt-1.41,J03-1002,0,0.00436446,"lders, the line number and the actual token replaced. This mapping file is used later in the post-processing step to substitute the actual tokens in the position of the unique placeholders. For target sentences having multiple placeholders of the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task w"
2012.eamt-1.41,P03-1021,0,0.0207458,"the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell"
2012.eamt-1.41,eck-etal-2004-language,0,0.354691,"Missing"
2012.eamt-1.41,P02-1040,0,0.084038,"kens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell checking results. However, the ‘in-domain’ adaptation"
2012.eamt-1.41,2011.mtsummit-papers.27,1,0.836761,"12 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to any publication quality guidelines. Hence despite being from the same IT domain, there is a significant difference in s"
2012.eamt-1.41,2010.amta-commercial.5,0,0.0905654,"provide an easy source of information and a viable alternative to traditional customer service options. Being a c 2012 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to a"
2012.eamt-1.41,P08-2015,0,0.31078,"aterial. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III and Jagarlamudi (2011) and that of Habash (2008). In our case, however, the target domain (web-forum) is different from the training data (Symantec TMs) mo"
2012.eamt-1.41,P07-2045,0,\N,Missing
2013.mtsummit-papers.13,D11-1033,0,0.549609,"Automatic quality estimation is used to identify such poorly translated sentences in the target domain. Our experiments reveal that this approach provides statistically significant improvements over the unadapted baseline and achieves comparable scores to that of conventional data selection approaches with significantly smaller amounts of selected data. 1 Introduction The quality of translations generated by a statistical machine translation (SMT) system depends heavily on the amount of available parallel training data, as well as on the domain-specificity of the training and target datasets (Axelrod et al., 2011). Real-life translation tasks are usually domain-specific in nature and require large volumes of in-domain parallel training data. However, such domain-specific parallel training data is often sparse or completely unavailable. In such scenarios, domain adaptation techniques are necessary to effectively leverage available out-ofdomain or related-domain parallel data. Supplementary data selection (Hildebrand et al., 2005; Axelrod et al., 2011) is one such popular technique which uses out-of-domain parallel data to supplement sparse in-domain data. However, combining lots of out-of-domain data wi"
2013.mtsummit-papers.13,W05-0909,0,0.023065,"selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free open source implementation of the technology, for all our classification/regression model training and predictions. In order to tune the features of the SVMbased classification and regression models the grid search functionality associated with LibSVM is used. The process of feature extraction is performed using an inhouse tool. http://ww"
2013.mtsummit-papers.13,C12-1010,1,0.840576,"Missing"
2013.mtsummit-papers.13,W12-3102,0,0.185957,"propriate use-case for our approach. The rest of paper is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005)"
2013.mtsummit-papers.13,W07-0717,0,0.425072,", 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005) to perplexity on ‘in-domain’ datasets (Foster and Kuhn, 2007). Axelrod et al. (2011) presented a technique using the bilingual difference of cross-entropy on ‘in-domain’ and ‘out-of-domain’ language models for ranking and selection by thresholding, which outperformed the monolingual perplexity based techniques. More recently, Banerjee et al. (2012) presented a novel translation-quality evaluation (rather than prediction) based data selection technique using an incremental translation model merging approach. While all these approaches select data with respect to the entire available target domain data, our approach uses only a sub-part of the same compri"
2013.mtsummit-papers.13,2005.eamt-1.19,0,0.211678,"tical machine translation (SMT) system depends heavily on the amount of available parallel training data, as well as on the domain-specificity of the training and target datasets (Axelrod et al., 2011). Real-life translation tasks are usually domain-specific in nature and require large volumes of in-domain parallel training data. However, such domain-specific parallel training data is often sparse or completely unavailable. In such scenarios, domain adaptation techniques are necessary to effectively leverage available out-ofdomain or related-domain parallel data. Supplementary data selection (Hildebrand et al., 2005; Axelrod et al., 2011) is one such popular technique which uses out-of-domain parallel data to supplement sparse in-domain data. However, combining lots of out-of-domain data with small amounts of in-domain data might negatively affect translation quality by overwhelming the in-domain characteristics. Hence relevant data selection is used, where only a sub-part of the out-of-domain data, relevant to the target domain, supplements the sparse indomain training data. Conventionally, the data selection process is guided by all available monolingual (or bilingual) target-domain data. Sentence pair"
2013.mtsummit-papers.13,P07-1034,0,0.0456762,"rc00 (s)] + [P Pitrg (s) − P Potrg (s)] (3) where P Pisrc0 indicates the perplexity on the indomain LM trained only on the source-side of the poorly translated sentences while P Posrc +isrc00 refers to the LM trained on the remaining targetdomain data and out-of-domain data. Note that the target side of the scoring remains the same, as there is no notion of good or bad translations in the target side of the bitext data. 3.3 Data Combination Multiple techniques exist in the SMT literature to combine out-of-domain data with in-domain data. The combination could be done using instance weighting (Jiang and Zhai, 2007), or by linearly interpolating the phrase tables (Foster and Kuhn, 2007). Considering the success of linear interpolation outperforming the other techniques (Sennrich, 2012), we choose this technique to combine the two datasets. In order to learn the interpolation weights, LMs are constructed on the target side of the in-domain training set and the selected supplementary data. 1 As cross-entropy and perplexity are monotonically related, they produce the same ranking. These LMs are then interpolated using expectation maximisation on the target side of the devset to learn the optimal mixture wei"
2013.mtsummit-papers.13,P07-2045,0,0.00407217,"et. Figure 1 shows the variation of F1 scores for different values of the prediction threshold, and our choice of threshold value of 0.4 corresponding to the best F1 score. Accuracy/F1 Score data selection and in determining the set of potentially poorly translated sentences in the targetdomain. The dev and test sets are randomly selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free ope"
2013.mtsummit-papers.13,W04-3250,0,0.361562,"Missing"
2013.mtsummit-papers.13,2005.mtsummit-papers.11,0,0.071804,"hts are subsequently used to combine the individual feature values for every phrase pair from two phrase-tables using a weighted linear interpolation scheme. For the LMs, individual models trained on the in-domain and selected out-ofdomain datasets are interpolated in a similar fashion with interpolation weights set on the devset. 4 Experimental Setup 4.1 Datasets and Tools The primary in-domain training data for our baseline systems comprises En–Fr bilingual datasets from Symantec TMs. Considering the wider vocabulary of the forum content, we use the freely available Europarl (EP) version 6 (Koehn, 2005) and News Commentary (NC)2 datasets in combination with the Symantec TMs to create a stronger second baseline model. We then use the following two freely available parallel datasets from the web, as the supplementary resources for data selection experiments: • OpenSubtitles2011 (OPS) Corpus3 . • MultiUN (UN) Parallel Corpus4 Data Set Line Cnt. En. Token Fr. Token Symantec TM 3,659,455 72,604,817 82,046,300 Bi- Europarl 1,924,594 52,139,148 57,837,037 text News-Comm. 134,757 3,338,552 3,917,982 1,692 22,661 25,840 Data Dev Test 1,032 13,160 15,164 Supp.MultiUN 9,010,933 227,085,145 263,051,365"
2013.mtsummit-papers.13,P03-1021,0,0.0102979,"hreshold, and our choice of threshold value of 0.4 corresponding to the best F1 score. Accuracy/F1 Score data selection and in determining the set of potentially poorly translated sentences in the targetdomain. The dev and test sets are randomly selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free open source implementation of the technology, for all our classification/regress"
2013.mtsummit-papers.13,W12-5706,1,0.884336,"Missing"
2013.mtsummit-papers.13,2012.eamt-1.55,1,0.89222,"Missing"
2013.mtsummit-papers.13,2012.eamt-1.43,0,0.0179014,"osrc +isrc00 refers to the LM trained on the remaining targetdomain data and out-of-domain data. Note that the target side of the scoring remains the same, as there is no notion of good or bad translations in the target side of the bitext data. 3.3 Data Combination Multiple techniques exist in the SMT literature to combine out-of-domain data with in-domain data. The combination could be done using instance weighting (Jiang and Zhai, 2007), or by linearly interpolating the phrase tables (Foster and Kuhn, 2007). Considering the success of linear interpolation outperforming the other techniques (Sennrich, 2012), we choose this technique to combine the two datasets. In order to learn the interpolation weights, LMs are constructed on the target side of the in-domain training set and the selected supplementary data. 1 As cross-entropy and perplexity are monotonically related, they produce the same ranking. These LMs are then interpolated using expectation maximisation on the target side of the devset to learn the optimal mixture weights. These weights are subsequently used to combine the individual feature values for every phrase pair from two phrase-tables using a weighted linear interpolation scheme."
2013.mtsummit-papers.13,2006.amta-papers.25,0,0.466665,"hree individual components involved in our approach. 3.1 Automatic Quality Estimation To distinguish between the good and the bad translations of the target-domain (English forum data in our context), we experimented with both classification as well as regression-based QE approaches. For both sets of experiments, we extract 17 features similar to the baseline QE setup 102 suggested by the organisers of the WMT12 shared task (Callison-Burch et al., 2012), which were shown to perform well on a post-editing effort prediction task. In our study, we want to predict the Translation Edit Rate (TER) (Snover et al., 2006) to spot bad translations. Given the TER scores for a set of translations, identifying the bad translations requires a threshold value, such that all sentences having TER scores above this threshold would be labelled as bad translation. However, a translation with a low TER score may still be considered bad since TER does not incorporate the notion of semantic equivalence (Snover et al., 2006). To set the value of this threshold, we selected two sets of 50 sentences randomly from our QE En–Fr training data such that there was an overlap of 10 sentences in each. These sentences along with their"
2013.mtsummit-papers.13,2011.mtsummit-papers.58,0,0.0250412,"is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005) to perplexity on ‘in-domain’ datasets (Foster"
2013.mtsummit-papers.13,2003.mtsummit-papers.52,0,0.0406122,"ata is often noisy, usergenerated and has a wider vocabulary and colloquialisms. This difference between the training and target datasets necessitates the use of supplementary data for adaptation, thus making this an appropriate use-case for our approach. The rest of paper is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data sel"
2015.eamt-1.17,2012.eamt-1.33,1,0.857982,"Missing"
2015.eamt-1.17,C04-1046,0,0.11191,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
2015.eamt-1.17,W12-3156,0,0.102075,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
2015.eamt-1.17,P14-1065,0,0.0530227,"Missing"
2015.eamt-1.17,P10-1064,1,0.914336,"Missing"
2015.eamt-1.17,P14-2047,0,0.0164777,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
2015.eamt-1.17,W13-3303,0,0.0260842,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
2015.eamt-1.17,P02-1040,0,0.0918502,"and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
2015.eamt-1.17,P09-2004,0,0.0370577,"omly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (filter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further filtered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (filter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each filter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also ran"
2015.eamt-1.17,potet-etal-2012-collection,0,0.122026,"Missing"
2015.eamt-1.17,2014.eamt-1.21,1,0.877315,"onsider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
2015.eamt-1.17,2006.amta-papers.25,0,0.41877,"s on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the final version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quality"
2015.eamt-1.17,P10-1063,0,0.608392,"han sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
2015.eamt-1.17,2009.eamt-1.5,1,0.876811,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
2015.eamt-1.17,D14-1025,0,0.053694,"Missing"
2015.eamt-1.17,C14-2028,0,\N,Missing
2015.eamt-1.17,W13-2201,1,\N,Missing
2015.eamt-1.22,W14-3351,0,0.0597704,"uality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is HTER (Snover et al., 2006) which produces targeted ref"
2015.eamt-1.22,2012.iwslt-papers.5,0,0.341338,"h ranking of the five MT outputs has the potential to produce 10 ranking pairs. Before applying the corresponding formulas on the data, the ranking pairs from all evaluators and for all systems are collected in a matrix like the one in Table 1. The matrix records the number of times system Si was ranked better than Sj and vice-versa. For example, if we look at the two systems S1 and S3 in the matrix, we can see that S3 was ranked 2 times higher (from the left triangle) and 4 times lower (from the right triangle) than system S1 . From the matrix, the final score for each system - as defined by Koehn (2012) and applied in WMT2013 - can be computed. From the matrix in Table 1 the score for system S1 is computed by counting for each pair of systems (S1 , S2 ), (S1 , S3 ), (S1 , S4 ), (S1 , S5 ) the number of times S1 was ranked higher than the other system divided by the total number of rankings for each pair. The results for each pair of systems including S1 are then 2 The implementation of a new tool was motivated by the accessibility of a server for the evaluators. This way each evaluator had his own evaluation set containing both the tool and the data set. Based on Koehn’s (2012) formula each"
2015.eamt-1.22,P11-1023,0,0.0769274,", 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT output. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by postediting MT output. Human evaluation can also be performed by measuring post-editing time, or by asking evaluators to assess the fluency and adequacy of a hypothesis translation on a Likert sc"
2015.eamt-1.22,W12-3129,0,0.0797414,"vie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT output. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by postediting MT output. Human evaluation can also be performed by measuring post-editing time, or by asking evaluators to assess the fluency and adequacy of a hypothesis translation on a Likert scale. Another popular human evaluation method is ranking: o"
2015.eamt-1.22,P02-1040,0,0.10171,"Machine translation evaluation is an important element in the process of building MT systems. The Workshop for Statistical Machine Translation (WMT) compares new techniques for MT through human and automatic MT evaluation and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and"
2015.eamt-1.22,2006.amta-papers.25,0,0.118516,"on and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is"
2015.eamt-1.22,W09-0441,0,0.0639153,"evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is HTER (Snover et al., 2006) whi"
2015.eamt-1.22,W14-3347,0,0.0568241,"post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role fillers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT o"
2015.eamt-1.22,W14-3348,0,0.0612756,"the process of building MT systems. The Workshop for Statistical Machine Translation (WMT) compares new techniques for MT through human and automatic MT evaluation and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al.,"
2015.eamt-1.6,P02-1040,0,0.106419,"ch use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects"
2015.eamt-1.6,P06-1055,0,0.00717483,"t we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of G"
2015.eamt-1.6,1999.mtsummit-1.48,0,0.178112,"om scratch when an exact match is not available. However, this retrieval process is still limited to editdistance based measures operating on surface form c 2015 The authors. This article is licensed under a Creative  Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 35 Several researchers have used semantic or syntactic information in TMs, but their evaluations were shallow and most of the time limited to subjective evaluation carried out by the authors. This makes it hard to judge how much a semantically informed TM matching system can benefit a translator. Existing research (Planas and Furuse, 1999; Hod´asz and Pohl, 2005; Pekar and Mitkov, 2007; Mitkov, 2008) pointed out the need for similarity 1 http://www.omegat.org calculations in TMs beyond surface form comparisons. Both Planas and Furuse (1999) and Hodasz and Pohl (2005) proposed to use lemma and parts of speech along with surface form comparison. Hodasz and Pohl (2005) also extend the matching process to a sentence skeleton where noun phrases are either tagged by a translator or by a heuristic NP aligner developed for English-Hungarian translation. Planas and Furuse (1999) tested a prototype model on 50 sentences from the softwar"
2015.eamt-1.6,aziz-etal-2012-pet,0,0.0163664,"ificantly improves the retrieval results. We have also observed that there are different paraphrases used to bring about this improvement. In the interval [70, 85), 169 different paraphrases are used to retrieve 98 additional segments. To check the quality of the retrieved segments human evaluations are carried out. The sets’ distribution for human evaluation is given in the Table 3. The sets contain randomly selected segments from the additionally retrieved segments using paraphrasing which changed their top ranking.2 TH Set1 Set2 Total 4.1 Familiarisation with the Tool We used the PET tool (Aziz et al., 2012) for all our human experiments. However, settings were changed depending on the experiment. To familiarise translators with the PET tool we carried out a pilot experiment before the actual experiment with the Europarl corpus. This experiment was 100 117 16 13.67 9 24 14 100 2 5 7 [85, 100) 6 4 10 [70, 85) 6 7 13 Total 14 16 30 Table 3: Test Sets for Human Experiments 2 The sets are constructed so that a translator can post-edit a file in one sitting. There is no differentiation between the evaluations based on sets and all evaluations are carried out in both sets in a similar fashion with diff"
2015.eamt-1.6,2012.amta-papers.26,0,0.0678513,"M matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in p"
2015.eamt-1.6,W14-3348,0,0.0285348,"Missing"
2015.eamt-1.6,2006.amta-papers.25,0,0.165174,"Missing"
2015.eamt-1.6,N13-1092,0,0.0373631,"s better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of German with English language level C1, in the age group of 21 to 40 years with a majority of female"
2015.eamt-1.6,R11-1014,0,0.0138269,"on metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding"
2015.eamt-1.6,2014.eamt-1.2,1,0.60298,"Missing"
2015.eamt-1.6,2011.mtsummit-papers.37,0,0.166221,"ed was considered usable if less than half of the words required editing to match the input sentence. The authors concluded that the approach gives more usable results compared to Trados Workbench used as a baseline. Hodasz and Pohl (2005) claimed that their approach stores simplified patterns and hence makes it more probable to find a match in the TM. Pekar and Mitkov (2007) presented an approach based on syntactic transformation rules. On evaluation of the prototype model using a query sentence, the authors found that the syntactic rules help in retrieving better segments. Recently, work by Utiyama et al. (2011) and Gupta and Or˘asan (2014) presented approaches which use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated"
2015.eamt-1.6,2005.mtsummit-papers.11,0,0.039766,"etter. 17 translators participated in this experiment. Finally, the decision of whether ‘ED is better’ or ‘PP is better’ is made on the basis of how many translators choose one over the other. 3.3 Subjective Evaluation with Three Options (SE3) This evaluation is similar to Evaluation SE2 except that we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing"
2015.eamt-1.6,2012.amta-wptp.2,0,0.0130456,"hine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into dif"
2015.eamt-1.6,W14-0314,1,0.730688,"es highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding paraphrase. Using this approach, the fuzzy match score between segments can be calculated in polynomial time despite the inclusion of paraphrases. The method uses dynamic programming along with greedy approximation. The me"
2015.eamt-1.6,2012.eamt-1.31,0,\N,Missing
2015.eamt-1.6,2012.tc-1.5,0,\N,Missing
2015.tc-1.3,W15-5202,0,0.0466142,"cant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memories. 2.3 Incorporation of Language Technology in Translation Memories Translation memories are among the most successfully used tools by professional translators. However, most of these tools rely on little language processing when they match and retrieve segments. Research carried out in the EXPERT project shows that even incorporation of simple language processing such as paraphrasing can help translators (Gupta and Or˘asan, 2014). Rather than expanding the segments stored in a translation memory with all the"
2015.tc-1.3,2014.tc-1.6,1,0.723752,"metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memorie"
2015.tc-1.3,D14-1062,1,0.857312,"Missing"
2015.tc-1.3,C14-1182,1,0.796863,"Missing"
2015.tc-1.3,N15-1043,1,0.871728,"Missing"
2015.tc-1.3,2015.mtsummit-papers.22,1,0.879605,"Missing"
2015.tc-1.3,2014.eamt-1.2,0,0.0541452,"Missing"
2015.tc-1.3,W15-4905,1,0.824476,"Missing"
2015.tc-1.3,2014.amta-researchers.19,1,0.706099,"professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT, showing that for English-Chinese and English-French the proposed methods lead to better translations. Translation into morphological rich languages poses challenges to current methods in statistical machine translation. For this problem, Daiber and Sima’an (2015) propose a method which consists of two steps: first the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that 20 matches the provided morphologic"
2015.tc-1.3,W15-4907,1,0.836766,"by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT,"
2015.tc-1.3,P02-1040,0,0.0939051,"uggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible"
2015.tc-1.3,W15-4107,0,0.0154388,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-wptp.4,0,0.0124165,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-papers.11,0,0.0130963,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,W15-5201,0,0.0277695,"largely due to the fact that in many cases the real needs of translators were not considered when designing these tools. To this end, a survey with professional translators was carried out in order to find out their views and requirements regarding various technologies, and their current work practices. Thanks to the help of the commercial partners in the project, the survey received 736 complete responses, from a total of over 1300 responses, which is more than in other similar surveys. A first analysis of the data is presented in (Zaretskaya et al., 2015) with more analyses underway. Parra Escartín (2015) carried out another study with professional translators in an attempt to find out “missing functionalities” of translation memories that could potentially improve their productivity. An interesting feature suggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently test"
2015.tc-1.3,2014.eamt-1.21,1,0.710839,"em in translation memories and statistical machine translation. 2.4 The Human Translator in the Loop Post-editing is one of the most promising ways of integrating the output of machine translation methods in the workflows used by translation companies. Quality estimation methods are used to decide whether a sentence should be translated from scratch or it is good enough to be given to a post-editor. Most of the existing methods focus on estimating the quality of sentences, but in some cases it is necessary to estimate the quality of the translation of a whole document. The work carried out by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015)"
2015.tc-1.3,W14-3323,0,0.0605642,"Missing"
2015.tc-1.3,2015.eamt-1.6,1,\N,Missing
2020.acl-demos.37,2014.amta-wptp.2,0,0.825123,"Missing"
2020.acl-demos.37,2020.acl-main.155,1,0.648575,"Missing"
2020.acl-demos.37,2014.eamt-1.18,0,0.31272,"Missing"
2020.acl-demos.37,W14-0315,0,0.384798,"Missing"
2020.acl-demos.37,2014.eamt-1.33,0,0.261952,"Missing"
2020.acl-demos.37,W19-6702,1,0.429507,"Missing"
2020.acl-demos.37,W14-0314,0,0.470262,"Missing"
2020.acl-main.155,2014.amta-wptp.2,0,0.676593,"Missing"
2020.acl-main.155,2015.tc-1.15,0,0.245536,"Missing"
2020.acl-main.155,C14-2028,0,0.442182,"Missing"
2020.acl-main.155,2014.amta-wptp.5,0,0.514741,"Missing"
2020.acl-main.155,D14-1130,0,0.536666,"Missing"
2020.acl-main.155,2020.acl-demos.37,1,0.648575,"Missing"
2020.acl-main.155,W12-3123,0,0.415474,"Missing"
2020.acl-main.155,2009.mtsummit-btm.8,0,0.911165,"Missing"
2020.acl-main.155,2013.mtsummit-wptp.10,0,0.275859,"Missing"
2020.acl-main.155,2014.eamt-1.18,0,0.377279,". 2.2 Multi-Modal Approaches The results of an elicitation study by Herbig et al. (2019a) indicate that pen, touch, and speech interaction should be combined with mouse and keyboard to improve PE of MT. In contrast, other modalities like eye tracking or gestures were seen as less promising. Dictating translations dates back to the time when secretaries transcribed dictaphone content on a typewriter (Theologitis, 1998); however, the use of automatic speech recognition also has a long history for translation (Dymetman et al., 1994; Brousseau et al., 1995). A more recent approach, called SEECAT (Martinez et al., 2014), investigates the use of automatic speech recognition (ASR) in PE and argues that its combination with typing could boost productivity. A survey regarding speech usage with PE trainees (Mesa-Lao, 2014) finds that they have a positive attitude towards In summary, previous research suggests that professional translators should switch to PE to increase productivity and reduce errors; however, translators themselves are not always eager to do so. It has been argued that the PE process might be better supported by using different modalities in addition to the common mouse and keyboard approaches,"
2020.acl-main.155,W14-0315,0,0.357216,"trast, other modalities like eye tracking or gestures were seen as less promising. Dictating translations dates back to the time when secretaries transcribed dictaphone content on a typewriter (Theologitis, 1998); however, the use of automatic speech recognition also has a long history for translation (Dymetman et al., 1994; Brousseau et al., 1995). A more recent approach, called SEECAT (Martinez et al., 2014), investigates the use of automatic speech recognition (ASR) in PE and argues that its combination with typing could boost productivity. A survey regarding speech usage with PE trainees (Mesa-Lao, 2014) finds that they have a positive attitude towards In summary, previous research suggests that professional translators should switch to PE to increase productivity and reduce errors; however, translators themselves are not always eager to do so. It has been argued that the PE process might be better supported by using different modalities in addition to the common mouse and keyboard approaches, and an elicitation study suggests concrete modalities that should be well suited for various editing tasks. A few of these modalities have already been explored in practice, showing promising results. H"
2020.acl-main.155,2016.eamt-2.6,0,0.351388,"Missing"
2020.acl-main.155,W19-6702,1,0.719118,"Missing"
2020.acl-main.155,W15-4910,0,0.308189,"Missing"
2020.acl-main.155,2014.eamt-1.33,0,0.29896,"Missing"
2020.acl-main.155,W14-0314,0,0.476867,"Missing"
2020.acl-main.155,2015.mtsummit-papers.15,0,0.752889,"Missing"
2020.acl-main.155,1998.tc-1.5,0,0.355325,"achieve within their implementation. In contrast, integrating dictation functionality using speech was shown to be quite useful and even preferred to mouse and keyboard by half of the participants. 2.2 Multi-Modal Approaches The results of an elicitation study by Herbig et al. (2019a) indicate that pen, touch, and speech interaction should be combined with mouse and keyboard to improve PE of MT. In contrast, other modalities like eye tracking or gestures were seen as less promising. Dictating translations dates back to the time when secretaries transcribed dictaphone content on a typewriter (Theologitis, 1998); however, the use of automatic speech recognition also has a long history for translation (Dymetman et al., 1994; Brousseau et al., 1995). A more recent approach, called SEECAT (Martinez et al., 2014), investigates the use of automatic speech recognition (ASR) in PE and argues that its combination with typing could boost productivity. A survey regarding speech usage with PE trainees (Mesa-Lao, 2014) finds that they have a positive attitude towards In summary, previous research suggests that professional translators should switch to PE to increase productivity and reduce errors; however, trans"
2020.acl-main.323,W04-3250,0,0.0998954,"ur approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied our approach to the training of the Transformer, and to compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks on 2 GTX 1080Ti GPUs. Hyper parameters were tuned on the development set (newstest 2012 and 2013). We followed all settings of Vaswani et al. (2017) except for the batch size. We used a beam size of 4 for decoding, and evaluated case-sensitive tokenized BLEU5 with significance test (Koehn, 2004). We used an α of 1.1 to determine the fluctuation of gradient direction by default. We regarded each encoder/decoder layer as a parameter group, and used a β of 3 for the parameter group selection. 3.1 ∆ak is positive, but after adding Gumble noise, there is a small possibility that it turns negative. In our case, negative values only occur very few times. Performance We compared the results of our dynamic batch size approach to two fixed batch size baselines, the 25k 4 We use pk as the probability to sample the kth group, and β is a hyper-parameter to sharpen the probability distribution. We"
2020.acl-main.323,W18-6301,0,0.0479447,"n Genabith1,2 Deyi Xiong3 Qiuhui Liu4∗ Saarland University / Saarland, Germany 2 German Research Center for Artificial Intelligence / Saarland, Germany 3 Tianjin University / Tianjin, China 4 China Mobile Online Services / Henan, China hfxunlp@foxmail.com, Josef.Van Genabith@dfki.de, dyxiong@tju.edu.cn, liuqhano@foxmail.com 1 Abstract Specifically, it has been shown that the performance of the Transformer model (Vaswani et al., 2017) for Neural Machine Translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) relies heavily on the batch size (Popel and Bojar, 2018; Ott et al., 2018; Abdou et al., 2017; Zhang et al., 2019a). The influence of batch size on performance raises the question, how to dynamically find proper and efficient batch sizes during training? In this paper, we investigate the relationship between the batch size and gradients, and propose a dynamic batch size approach by monitoring gradient direction changes. Our contributions are as follows: The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducin"
2020.acl-main.323,D19-1083,0,0.114094,"Missing"
2020.acl-main.323,P19-1426,0,0.066836,"Missing"
2020.acl-main.323,W17-4780,0,\N,Missing
2020.acl-main.37,J93-2003,0,0.107473,"Missing"
2020.acl-main.37,P05-1033,0,0.250398,"esentations for Neural Machine Translation Hongfei Xu1,2 Josef van Genabith1,2∗ Deyi Xiong3 Qiuhui Liu4 Jingyi Zhang2 1 Saarland University / Saarland, Germany 2 German Research Center for Artificial Intelligence / Saarland, Germany 3 Tianjin University / Tianjin, China 4 China Mobile Online Services / Henan, China hfxunlp@foxmail.com, Josef.Van Genabith@dfki.de, dyxiong@tju.edu.cn, liuqhano@foxmail.com, Jingyi.Zhang@dfki.de Abstract years (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Compared to plain SMT (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), a neural language model decoder (Sutskever et al., 2014) is better at long-distance re-ordering, and attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017) have been proven effective in modeling longdistance dependencies, while these two issues were both challenging for SMT. The Transformer (Vaswani et al., 2017), which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2015; Gehring et al., 2017), is based on multi-layer multi-head attention networks and can be trained in parallel very efficiently. Though attentional networks can connect distant words v"
2020.acl-main.37,D14-1179,0,0.111908,"Missing"
2020.acl-main.37,D17-1148,0,0.0767553,"results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better. 2 Background and Related Work In this section, we first review previous work which utilizes phrases in recurrent sequence-tosequence models, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step of NMT to address the coverage i"
2020.acl-main.37,N19-1423,0,0.0319857,"move ST from T ; while mt &gt; 0 do Find the adjacent sub-tree STA of depth dst with nsta tokens from the right side of T ; if STA exists and nsta ≤ mt then Insert the token sequence of STA to the beginning of p; Remove STA from T ; mt = mt − nsta; else Break; end if end while Append p to S; end while Reverse S; return S Rekall = Fglance (Rekt , ..., Rektm ) 1 neural models have been proven good at learning competitively effective representations with gate or attention mechanism even without modeling linguistic structures (Cho et al., 2014; Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017; Devlin et al., 2019). In our experiments we also explore phrases extracted from the Stanford Parser (Socher et al., 2013) as as an alternative to our simple segmentation strategy. The maximum number of tokens allowed is consistent with the simple segmentation approach, and we try to use the tokens from the largest sub-tree that complies with the maximum token limitation or from several adjacent sub-trees of the same depth as a phrase for efficiency. Our algorithm to extract phrases from parse trees is shown in Algorithm 1. To efficiently parallelize parser-based phrases of various length in a batch of data, we pa"
2020.acl-main.37,D18-1457,0,0.0182939,"tion in the computation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2"
2020.acl-main.37,D18-1338,0,0.0185689,"djacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2 • Though we employ a 2-layer neural network, it only has one linear transformation and a vector to calculate attention weights, which contains fewer parameters than the multi-head attention model that has 4 linear transformations. d X w j Ri Incorporating Phrase Representation into NMT After the phrase representation sequence for each encoder layer"
2020.acl-main.37,D19-1082,0,0.0193695,"h token vectors. Previous work does not use SMT phrases in this way. and values. It first projects queries, keys and values with 3 independent linear transformations, then splits the transformed key, query and value embeddings into several chunks of dk dimension vectors, each chunk is called a head,1 and scaled dotproduct attention is independently applied in each head: QK T Attn(Q, K, V ) = softmax( √ )V dk Comparison with Previous Works In more recent work, Wang et al. (2019) augment self attention with structural position representations to model the latent structure of the input sentence; Hao et al. (2019) propose multi-granularity self-attention which performs phrase-level attention with several attention heads. (1) 3 where Q, K and V stand for the query vectors, key vectors and value vectors. Finally, the network concatenates the outputs of all heads and transforms it into the target space with another linear layer. The self-attention network uses the query sequence also as the key sequence and the value sequence in computation, while the cross-attention feeds another vector sequence to attend as queries and values. Comparing the computation of the attentional network with RNNs, it is obvious"
2020.acl-main.37,W04-3250,0,0.0744435,"k which was 0.3. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively following Vaswani et al. (2017). The other settings were the same as (Vaswani et al., 2017) except that we did not bind the embedding between the encoder and the decoder for efficiency. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU 3 with the averaged model of the last 5 checkpoints for Transformer Base and 20 checkpoints for Transformer Big saved with an interval of 1, 500 training steps (Vaswani et al., 2017). We also conducted significance tests (Koehn, 2004). 4.2 Main Results We applied our approach to both the Transformer Base setting and the Transformer Big setting, and conducted experiments on both tasks to validate the effectiveness of our approach. Since parsing a large training set (specifically, the En-Fr dataset) is slow, we did not use phrases from parse results in this experiment (reported in Table 1). Results are shown in Table 1. † indicates p &lt; 0.01 compared to the baseline for the significance test. Table 1 shows that modeling phrase representation can bring consistent and significant improvements on both tasks, and benefit both the"
2020.acl-main.37,N03-1017,0,0.241009,"Missing"
2020.acl-main.37,N18-1202,0,0.0519183,"rallel. Each encoder layer will produce a vector sequence as the phrase representation. We do not use the multi-head attention in the computation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encod"
2020.acl-main.37,E17-2060,0,0.0413728,"sequences which are shorter than corresponding token sequences can help the model capture longdistance dependencies better, and modeling phrase representations for the Transformer can enhance its performance on long sequences. 4.5 Subject-Verb Agreement Analysis Intuitively, in translating longer sentences we should encounter more long-distance dependencies than in short sentences. To verify whether our method can improve the capability of the NMT model to capture long-distance dependencies, we also conducted a linguistically-informed verb-subject agreement analysis on the Lingeval97 dataset (Sennrich, 2017) following Tang et al. (2018). In German, subjects and verbs must agree with one another in grammatical number and person. In Lingeval97, each contrastive translation pair consists of a correct reference translation, and a contrastive example that has been minimally modified to introduce one translation error. The accuracy of a model is the number of times it assigns a higher score to the reference translation than to the contrastive one, relative to the total number of predictions. Results are shown in Figure 4. Figure 4 shows that our approach can improve the accuracy of long-distance subjec"
2020.acl-main.37,P16-1162,0,0.0724125,"sentation is produced inside the Transformer model and utilized as the input of layers, and all related computations are differentiable, the attentive phrase representation model is simply trained as part of the whole model through backpropagation effectively. 4 Experiments To compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks. 4.1 Settings We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations on both data sets to address the unknown word problem. We only kept sentences with a maximum of 256 subword tokens for training. Training sets were randomly shuffled in every training epoch. The concatenation of newstest 2012 and newstest 2013 was used for Models En-De En-Fr Transformer Base +PR 27.38 28.67† 39.34 40.71† Transformer Big +PR 28.49 29.60† 41.36 42.45† Table 1: Results on WMT 14 En-De and En-Fr. validation and newstest 2014 as test sets for both tasks. The number of warm-up steps was set to 8k, and each training batch contained at least 25k target token"
2020.acl-main.37,N18-1117,0,0.0268791,"Missing"
2020.acl-main.37,D18-1458,0,0.040503,"Missing"
2020.acl-main.37,P16-1008,0,0.375053,"ith our approach, and the results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better. 2 Background and Related Work In this section, we first review previous work which utilizes phrases in recurrent sequence-tosequence models, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step"
2020.acl-main.37,C18-1255,0,0.0165755,"ation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2 • Though we employ"
2020.acl-main.37,D19-1145,0,0.0596896,"dual connection and Layer normalization are omitted for simplicity. • We iteratively and dynamically generate phrase representations with token vectors. Previous work does not use SMT phrases in this way. and values. It first projects queries, keys and values with 3 independent linear transformations, then splits the transformed key, query and value embeddings into several chunks of dk dimension vectors, each chunk is called a head,1 and scaled dotproduct attention is independently applied in each head: QK T Attn(Q, K, V ) = softmax( √ )V dk Comparison with Previous Works In more recent work, Wang et al. (2019) augment self attention with structural position representations to model the latent structure of the input sentence; Hao et al. (2019) propose multi-granularity self-attention which performs phrase-level attention with several attention heads. (1) 3 where Q, K and V stand for the query vectors, key vectors and value vectors. Finally, the network concatenates the outputs of all heads and transforms it into the target space with another linear layer. The self-attention network uses the query sequence also as the key sequence and the value sequence in computation, while the cross-attention feeds"
2020.acl-main.37,D17-1149,1,0.930498,"els, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step of NMT to address the coverage issue and the unknown word issue of NMT. Wang et al. (2017b) suggested that phrases play a vital role in machine translation, and proposed to translate phrases in NMT by integrating target phrases from an SMT system with a phrase memory given that it is hard to integrate phrases into NMT which reads and generates sentences in a tok"
2020.acl-main.38,D18-1338,0,0.360185,"ess in the last few years (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). The Transformer (Vaswani et al., 2017), which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2015; Gehring et al., 2017), is based on multi-layer self-attention networks and can be trained very efficiently. The ∗ multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; Bapna et al., 2018). In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016). However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mec"
2020.acl-main.38,P18-1008,0,0.216615,"an task and the WMT 15 Czech to English task; • We further investigate deep decoders for the Transformer in addition to the deep encoders studied in previous works, and show that deep decoders can also benefit the Transformer. 2 Convergence of Different Computation Orders In this paper we focus on the convergence of the training of deep transformers. To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016) are adopted. 2.1 ure 1 b) seems better for harder-to-learn models.1 Even though several studies (Chen et al., 2018; Domhan, 2018) have mentioned this change and although Wang et al. (2019) analyze the difference between the two computation orders during backpropagation, and Zhang et al. (2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al."
2020.acl-main.38,P18-1167,0,0.0211259,"15 Czech to English task; • We further investigate deep decoders for the Transformer in addition to the deep encoders studied in previous works, and show that deep decoders can also benefit the Transformer. 2 Convergence of Different Computation Orders In this paper we focus on the convergence of the training of deep transformers. To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016) are adopted. 2.1 ure 1 b) seems better for harder-to-learn models.1 Even though several studies (Chen et al., 2018; Domhan, 2018) have mentioned this change and although Wang et al. (2019) analyze the difference between the two computation orders during backpropagation, and Zhang et al. (2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used"
2020.acl-main.38,E17-3017,0,0.0263175,"ion between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transformer uses a different computation order (Figure 1 b) compared to the published version (Vaswani et al., 2017) (Figure 1 a), since it (Fig398 https://github.com/tensorflow/ tensor2tensor/blob/v1.6.5/tensor2tensor/ layers/common_hparams.py#L110-L112. 2 initialize matrices between qUniformly q [− 6 ,+ (isize+osize) 6 ], ("
2020.acl-main.38,P17-4012,0,0.030273,"fect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transformer uses a different computation order (Figure 1 b) compared to the published version (Vaswani et al., 2017) (Figure 1 a), since it (Fig398 https://github.com/tensorflow/ tensor2tensor/blob/v1.6.5/tensor2tensor/ layers/common_hparams.py#L110-L112. 2 initialize matrices between qUniformly q [− 6 ,+"
2020.acl-main.38,P16-1162,0,0.029063,"(2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transfor"
2020.acl-main.38,W18-1819,0,0.435436,"nsformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s official implementation (Vaswani et al., 2018) which changes the computation order of residual connection and layer normalization can effectively ease its optimization; • We deeply analyze how the subtle difference of computation order affects convergence in 397 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 397–402 c July 5 - 10, 2020. 2020 Association for Computational Linguistics inres inres outLN/res outLN/res Input/Norm Process Dropout inmodel + Norm Process outLN/res Dropout + inmodel Norm Output (a) inres outres Norm Input inres outres Process Dropout inmodel + outLN Norm Process Drop"
2020.acl-main.38,P19-1176,0,0.472114,"lization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016). However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which combines outputs of all encoder layers into a weighted encoded representation. Wang et al. (2019) find that deep Transformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s offici"
2020.acl-main.38,P19-1558,0,0.470099,", even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which combines outputs of all encoder layers into a weighted encoded representation. Wang et al. (2019) find that deep Transformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s official implementation (Vaswani et al., 2018) which changes the computation order of residual connection and layer normalization can effectively ease its optimization; • We dee"
2020.acl-main.38,D19-1083,0,0.168042,"Missing"
2020.amta-pemdt.7,2014.amta-wptp.2,0,0.0680404,"Missing"
2020.amta-pemdt.7,C14-2028,0,0.0610674,"Missing"
2020.amta-pemdt.7,2014.amta-wptp.5,0,0.0755146,"Missing"
2020.amta-pemdt.7,D14-1130,0,0.0739649,"Missing"
2020.amta-pemdt.7,2020.acl-main.155,1,0.329549,"Missing"
2020.amta-pemdt.7,2020.acl-demos.37,1,0.842769,"Missing"
2020.amta-pemdt.7,W12-3123,0,0.35611,"Missing"
2020.amta-pemdt.7,2009.mtsummit-btm.8,0,0.477679,"Missing"
2020.amta-pemdt.7,2013.mtsummit-wptp.10,0,0.0605326,"Missing"
2020.amta-pemdt.7,2014.eamt-1.18,0,0.0692014,"o mouse and keyboard, have been explored for PE, as we discuss in the next section. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 94 2.2 Multi-Modal Approaches Using automatic speech recognition has a long history for traditional translation from scratch (Dymetman et al., 1994; Brousseau et al., 1995), and dictating translations that are then manually transcribed by secretaries dates back even further (Theologitis, 1998). For PE, the more recent study of the SEECAT (Martinez et al., 2014) environment supporting automatic speech recognition (ASR) argues that its combination with typing could boost productivity. According to a survey by Mesa-Lao (2014), PE trainees have a positive attitude towards speech input and would consider adopting it, but only as a complement to other modalities. In a smallscale study, Zapata et al. (2017) found that ASR for PE was faster than ASR for translation from scratch. Nowadays, commercial CAT tools like memoQ and MateCat are also beginning to integrate ASR. The CASMACAT tool (Alabau et al., 2013) further allows users to input text by handwriting"
2020.amta-pemdt.7,W14-0315,0,0.0194339,"ricas October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 94 2.2 Multi-Modal Approaches Using automatic speech recognition has a long history for traditional translation from scratch (Dymetman et al., 1994; Brousseau et al., 1995), and dictating translations that are then manually transcribed by secretaries dates back even further (Theologitis, 1998). For PE, the more recent study of the SEECAT (Martinez et al., 2014) environment supporting automatic speech recognition (ASR) argues that its combination with typing could boost productivity. According to a survey by Mesa-Lao (2014), PE trainees have a positive attitude towards speech input and would consider adopting it, but only as a complement to other modalities. In a smallscale study, Zapata et al. (2017) found that ASR for PE was faster than ASR for translation from scratch. Nowadays, commercial CAT tools like memoQ and MateCat are also beginning to integrate ASR. The CASMACAT tool (Alabau et al., 2013) further allows users to input text by handwriting with an e-pen in a separate area, where the handwriting is recognized and placed at the cursor position. A vision paper by Alabau and Casacuberta (2012) proposes to"
2020.amta-pemdt.7,W15-4910,0,0.0436714,"Missing"
2020.amta-pemdt.7,2014.eamt-1.33,0,0.264769,"Missing"
2020.amta-pemdt.7,2015.mtsummit-papers.15,0,0.0915969,"Missing"
2020.amta-pemdt.7,1998.tc-1.5,0,0.0276983,"; Green et al., 2013). Therefore, other modalities, in addition to mouse and keyboard, have been explored for PE, as we discuss in the next section. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 94 2.2 Multi-Modal Approaches Using automatic speech recognition has a long history for traditional translation from scratch (Dymetman et al., 1994; Brousseau et al., 1995), and dictating translations that are then manually transcribed by secretaries dates back even further (Theologitis, 1998). For PE, the more recent study of the SEECAT (Martinez et al., 2014) environment supporting automatic speech recognition (ASR) argues that its combination with typing could boost productivity. According to a survey by Mesa-Lao (2014), PE trainees have a positive attitude towards speech input and would consider adopting it, but only as a complement to other modalities. In a smallscale study, Zapata et al. (2017) found that ASR for PE was faster than ASR for translation from scratch. Nowadays, commercial CAT tools like memoQ and MateCat are also beginning to integrate ASR. The CASMACAT tool (Al"
2020.amta-pemdt.7,2015.tc-1.15,0,0.428987,"Missing"
2020.amta-pemdt.7,2016.eamt-2.6,0,0.0213142,"Missing"
2020.amta-pemdt.7,W19-6702,1,0.807873,"Missing"
2020.amta-pemdt.7,W14-0314,0,0.025744,"Missing"
2020.coling-main.524,W19-5402,0,0.0122306,"E task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single-encoder architecture, however, using the concatenated src and mt text as input ({src + mt} → pe) (Niehues et al., 2016). 5966 4.1 Data For our experiments, we use the English–German WMT 2016 (Bojar et al., 2016), 2017 (Bojar et al., 2017), 2018 (Chatterjee et al., 2018) and 2019 (Chatterjee et al., 2019) APE task data. All these released APE datasets consist of English–German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a 1st -stage MT system, and the corresponding human-post-edited version (pe). The sizes of the datasets (train; dev; test), in terms of number of sentences, are (12,000; 1,000; 2,000), (11,000; 0; 2,000), and (13,442; 1,000; 1,023), for the 2016 PBSMT, the 2017 PBSMT, and the 2018 NMT data, respectively. The 2019 version of the APE dataset released in WMT is the same as the WMT 2018 NMT data. It is to be note"
2020.coling-main.524,P18-1167,0,0.135825,"tion of encsrc→mt to produce the final translation). The paper makes the following contributions: (i) we propose a multi-encoder model for APE that consists only of standard transformer encoding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a representation of both src and mt for the decoder, allowing it to better capture errors in mt originating from src; this advances Junczys-Dowmunt and Grundkiewicz (2018) – the WMT 2018 best system (wmt18smt best ) in terms of BLEU and TER, (iii), we analyze the effect of varying the number of encoder and decoder layers (Domhan, 2018), indicating that the encoders contribute more than decoders in neural APE, and (iv) we present and evaluate an APE architecture inspired by a two-step approach professional translators often use during post-editing. In comparison to the shared task system description paper (Pal et al., 2019), this paper (i) provides more detailed explanations and reformation of different components of the transference architecture, (ii) compares it to a single encoder based transformer architecture where only mt or src concatenated with mt are used as an input, (iii) analyzes results when swapping mt and src"
2020.coling-main.524,W16-2378,0,0.11533,"yzes results when swapping mt and src in the multi-encoder setup, and (iv) investigates the importance of encoder and decoder by varying the amount of layers. The rest of the paper is organized as follows. In §2, we survey existing literature on APE. In §3, we describe the multi-encoder architecture. §4 describes our experimental setup. §5 reports the results of our approach against a number of baselines. Finally, §6 concludes the paper with future directions. 2 Related Research Recent advances in APE research are directed towards neural APE, which was first proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016) for the single-source APE scenario which does not consider src, i.e. mt → pe. Junczys-Dowmunt and Grundkiewicz (2016) also generated a large synthetic training dataset, which we also use as additional training data. Exploiting source information as an additional input can help neural APE to disambiguate corrections applied at each time step; this naturally leads to multi-source APE ({src, mt} → pe). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src an"
2020.coling-main.524,W18-6467,0,0.245569,"n suggestion (similar to what our encsrc→mt is doing), then corrections to the MT output are applied based on the encountered errors (in the same way that our decpe uses the encoded representation of encsrc→mt to produce the final translation). The paper makes the following contributions: (i) we propose a multi-encoder model for APE that consists only of standard transformer encoding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a representation of both src and mt for the decoder, allowing it to better capture errors in mt originating from src; this advances Junczys-Dowmunt and Grundkiewicz (2018) – the WMT 2018 best system (wmt18smt best ) in terms of BLEU and TER, (iii), we analyze the effect of varying the number of encoder and decoder layers (Domhan, 2018), indicating that the encoders contribute more than decoders in neural APE, and (iv) we present and evaluate an APE architecture inspired by a two-step approach professional translators often use during post-editing. In comparison to the shared task system description paper (Pal et al., 2019), this paper (i) provides more detailed explanations and reformation of different components of the transference architecture, (ii) compares"
2020.coling-main.524,W19-5412,0,0.0268686,"stacks an additional cross-attention component for src → pe above the previous cross-attention for mt → pe. In contrast to other multi-encoder based approaches and Libovick´y et al. (2018)’s approach, where the authors focused on cross-attention of two encoders with respect to the decoder within the transformer architecture, we propose a novel architecture where the second encoder block is similar to the transformer decoder block but without masking. In the latest edition of WMT (2019), the submissions are mostly multi-source models extending the transformer implementation (Pal et al., 2019; Lee et al., 2019; Xu et al., 2019) and adapting BERT (Devlin et al., 2018) to the transformer-based framework (Lopes et al., 2019). The winner system (Lopes et al., 2019) (wmt19nmt best ) uses a single pre-trained BERT encoder that receives both the source src and mt strings and applies a BERT-based encoder-decoder model. Additionally, they add a conservativeness penalty factor during beam decoding to avoid over-corrections in APE. Our method outperforms the WMT 2016, 2017, and 2018 winners by 1 BLEU point, and yields comparable performance to the WMT 2019 winner, however, without using a BERT-based architect"
2020.coling-main.524,W16-2361,0,0.0519407,"Missing"
2020.coling-main.524,W18-6326,0,0.0506587,"Missing"
2020.coling-main.524,W19-5413,0,0.0245094,"Missing"
2020.coling-main.524,L18-1004,0,0.0133383,"The 2019 version of the APE dataset released in WMT is the same as the WMT 2018 NMT data. It is to be noted that for WMT 2018, we carried out experiments only for the NMT sub-task and ignored the data for the PBSMT task. Since the WMT APE datasets are small in size, we use ‘artificial training data’ (Junczys-Dowmunt and Grundkiewicz, 2016) containing 4.5M sentences as additional resources, 4M of which are weakly similar to the WMT 2016 training data, while 500K are very similar according to TER statistics. For experimenting on the NMT data, we additionally use the synthetic eScape APE corpus (Negri et al., 2018), consisting of ∼7M triples. For cleaning this noisy eScape dataset containing many unrelated language words (e.g. Chinese), (i) we use the cleaning process described in Tebbifakhr et al. (2018), and (ii) we use the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then use the Moses tokenizer (Koehn et al., 2007) to tokenize the eScape corpus with ‘no-escape’ option. Finally, we apply true-casing. The cleaned version of the eScape corpus contains ∼6.5M triplets."
2020.coling-main.524,C16-1172,0,0.0545641,"Missing"
2020.coling-main.524,C16-1241,1,0.89551,"Missing"
2020.coling-main.524,P16-2046,1,0.904948,"Missing"
2020.coling-main.524,W18-6468,1,0.880295,"Missing"
2020.coling-main.524,W19-5414,1,0.451809,"Missing"
2020.coling-main.524,P02-1040,0,0.107046,"T (we refer as 1st -stage MT) system to which APE is applied is either a phrase-based statistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st stage PBSMT, the best-performing systems from WMT APE 2018 (wmt18smt best ), which are a single model and an ensemble model by Junczys-Dowmunt and Grundkiewicz (2018), as well as a transformer directly translating from src to pe (Transformer (src → pe)), thus performing translation instead of APE. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). For the NMT task, we consider three baselines: the raw NMT output provided by the 1st -stage NMT system, the best-performing system from the WMT 2018 (wmt18nmt best ) (Tebbifakhr et al., 2018) and nmt WMT 2019 (wmt19best ) (Lopes et al., 2019) NMT APE task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single"
2020.coling-main.524,P16-1162,0,0.0126887,"e ) ft with {src, mt}smt tr → pe . Last, we analyze the importance of our second encoder (encsrc→mt ), compared to the source encoder 5967 (encsrc ) and the decoder (decpe ), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for Nsrc -Nmt -Npe (cf. Figure 1). We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. To handle out-of-vocabulary words and reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the src, mt and pe, we define BPE tokens by jointly processing all triplets. Thus, src, mt and pe derive a single BPE vocabulary. Since mt and pe belong to the same language (German) and src is a close language (English), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size to 28k. We implemented our approach based on the Neutron implementation of the Transformer (Xu and Liu, 2019)1 . 4.3 Hyper-parameter Setup We follow a similar hyper-paramete"
2020.coling-main.524,W18-6470,0,0.0167713,"f src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. Shin and Lee (2018) proposed a multi-source transformer where on the decoder side, they added two additional multi-head attention 5964 layers for src → mt and src → pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should remain in pe. The APE PBSMT-subtask winner of WMT 2018 (wmt18smt best ) (Junczys-Dowmunt and Grundkiewicz, 2018) also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for src → pe above the previous cross-attention for mt → p"
2020.coling-main.524,2006.amta-papers.25,0,0.0324129,"stem to which APE is applied is either a phrase-based statistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st stage PBSMT, the best-performing systems from WMT APE 2018 (wmt18smt best ), which are a single model and an ensemble model by Junczys-Dowmunt and Grundkiewicz (2018), as well as a transformer directly translating from src to pe (Transformer (src → pe)), thus performing translation instead of APE. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). For the NMT task, we consider three baselines: the raw NMT output provided by the 1st -stage NMT system, the best-performing system from the WMT 2018 (wmt18nmt best ) (Tebbifakhr et al., 2018) and nmt WMT 2019 (wmt19best ) (Lopes et al., 2019) NMT APE task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single-encoder architecture, however"
2020.coling-main.524,W18-6471,0,0.29825,"multi-source models (Libovick´y et al., 2016) by means of concatenating both weighted contexts of encoded src and mt. Varis and Bojar (2017) compared two multi-source models, one using a single encoder with the concatenation of src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. Shin and Lee (2018) proposed a multi-source transformer where on the decoder side, they added two additional multi-head attention 5964 layers for src → mt and src → pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should remain in pe. The APE PBSMT-subtask winner of WMT 2018 (wmt18smt best ) (Junczys-Do"
2020.coling-main.524,W17-4777,0,0.0128738,"src and mt and passing the concatenation of both encoders’ final states to the decoder (Libovick´y et al., 2016). A few approaches to multi-source neural APE were proposed in the WMT 2017 APE shared task. Junczys-Dowmunt and Grundkiewicz (2017) combine both mt and src in a single neural architecture, exploring different combinations of attention mechanisms including soft attention and hard monotonic attention. Chatterjee et al. (2017) built upon the two-encoder architecture of multi-source models (Libovick´y et al., 2016) by means of concatenating both weighted contexts of encoded src and mt. Varis and Bojar (2017) compared two multi-source models, one using a single encoder with the concatenation of src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid expo"
2020.coling-main.524,W19-5417,1,0.887678,"Missing"
2020.coling-main.532,E17-2039,0,0.0558051,"Missing"
2020.coling-main.532,C16-1333,0,0.0130792,"16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, Polish and Bulgarian) into English and English original text. Abstractions. In addition to using raw word tokens, we create multiple views of the data at the morphological (PoS), lexical semantic (ST) and conceptual-semantic (SS) levels. We use the spaCy tagger (Honnibal and Johnson, 2015) with the OntoNotes 5 version (Weischedel et al., 2013) of the Penn Treebank PoS tag set. For ST (Bjerva et al., 2016; Abzianidze et al., 2017), we use the best model of Brants 1 Since these views represent diversified and complementary information of the same data, we refer to them as multi-view representations. 6057 Feature Annotated Output Vocabulary PoS DT NN VBD DT NN IN NN 37 ST DEF CON EPS DIS CON REL CON 57 Synset ministry.n.04 send.v.02 answer.n.04 inquiry.n.0.1 1667 Raw the ministry sent an answer to inquiry 6739 Table 1: Examples of the level of abstraction with the vocabulary size. (2000) which works directly on the words as input, and determines formal lexical semantics. Their implementation ach"
2020.coling-main.532,J19-2006,0,0.219248,"ion 3 introduces our experimental setup. The distance measure is described in Section 4. We present our results and analysis in Section 5, followed by conclusions in Section 6. 2 Phylogenetics and Shining-through Historical comparative linguistics determines genetic relationships between languages using concept lists of words that share a common origin, similar meaning and pronunciation across multiple languages (Swadesh, 1952; Dyen et al., 1992). By contrast, computational analysis methods aim to reconstruct language phylogeny based on measurable linguistic patterns (Rabinovich et al., 2017; Bjerva et al., 2019). Rabinovich et al. (2017) showed that source language interference is visible in translation. Specifically, they leverage interference (PoS trigrams and function words) and translation universal features (cohesive markers) to construct phylogenetic trees. Agglomerative clustering with variance minimisation (Ward Jr, 1963) is used as linkage procedure to cluster the data. The result is compared to the pruned gold tree of Serva and Petroni (2008) (henceforth referred to as SP08) used as the linguistic phylogenetic gold standard tree. Their comparison metric, which is based on the L2 norm, is ba"
2020.coling-main.532,Q17-1010,0,0.0766292,"Missing"
2020.coling-main.532,A00-1031,0,0.299709,"Missing"
2020.coling-main.532,D15-1162,0,0.0119336,"s Lj ’s, where j = 1, 2, ..., n; and to originally written text in English as Le . We select the subset of translations from 16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, Polish and Bulgarian) into English and English original text. Abstractions. In addition to using raw word tokens, we create multiple views of the data at the morphological (PoS), lexical semantic (ST) and conceptual-semantic (SS) levels. We use the spaCy tagger (Honnibal and Johnson, 2015) with the OntoNotes 5 version (Weischedel et al., 2013) of the Penn Treebank PoS tag set. For ST (Bjerva et al., 2016; Abzianidze et al., 2017), we use the best model of Brants 1 Since these views represent diversified and complementary information of the same data, we refer to them as multi-view representations. 6057 Feature Annotated Output Vocabulary PoS DT NN VBD DT NN IN NN 37 ST DEF CON EPS DIS CON REL CON 57 Synset ministry.n.04 send.v.02 answer.n.04 inquiry.n.0.1 1667 Raw the ministry sent an answer to inquiry 6739 Table 1: Examples of the level of abstraction with the vocabulary size."
2020.coling-main.532,2005.mtsummit-papers.11,0,0.030569,"ently, Bjerva et al. (2019) built on this work and compared different languages based on distance metrics computed on phrase structure trees and dependency relations. They claimed that such language representations correlate better with structural family distances between languages than genetic similarities. These examples show that phylogenetic reconstruction approaches and in particular, the evaluation of generated trees remains a highly debated topic in the history of linguistics and is beyond the scope of this study. 3 Experimental Settings Data. We use the comparable portion of Europarl (Koehn, 2005) with translations from 21 European Union languages into English to minimise the impact of domain difference. The amount of tokens per language varies, ranging from 67 k tokens for Maltese to 7.2 M for German. We refer to the multiple translations into English as Lj ’s, where j = 1, 2, ..., n; and to originally written text in English as Le . We select the subset of translations from 16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, P"
2020.coling-main.532,P11-1132,0,0.0360795,"target text” (Toury, 2012). Prominent evidence for shining-through as a translationese effect is found in the work of Rabinovich et al. (2017), who show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In the similar vein, a significant amount of work has gone into training classifiers to distinguish between translations and originally authored text and then investigating the contributions of individual features to the result of the classification (Baroni and Bernardini, 2005; Koppel and Ordan, 2011; Volansky et al., 2015; Avner et al., 2016). Features that contribute strongly to classification are interpreted as indicating important dimensions of translationese. In contrast, in this work, we leverage departures from isomorphism between embedding-based semantic spaces to detect translationese. We construct embedding spaces from original English (O) data and translations into English (T ) from comparable data in a number of languages. We hypothesize that the closer the source language is to English, the more isomorphic the embedding spaces are. In other words, departure from isomorphism i"
2020.coling-main.532,P18-2036,0,0.0239579,"(Bojanowski et al., 2017). Embeddings have 300 dimensions and only words with more than 5 occurrences are retained for training. We use skip-gram with negative sampling (Mikolov et al., 2013) and standard hyper-parameters. 4 Measuring Isomorphism An empirical measure of semantic proximity between two languages is often computed using the degree of isomorphism, that is, how similar the structures of two languages are in topological space (Søgaard et al., 2018). Research in cross-lingual transfer tasks shows that linguistic differences across languages often make spaces depart from isomorphism (Nakashole and Flauger, 2018; Søgaard et al., 2018; Patra et al., 2019; Vuli´c et al., 2020). While this degrades the quality of bilingual embeddings, it is a desired characteristic in our case: since our task involves processing of (multi-view) representations of monolingual text, departures from isomorphism indicate diversity in the source that generates them. To quantify isomorphism, we compute embeddings on a corpus in language L. Embeddings reflect distributional properties in the data: words in similar contexts have similar meanings (Harris, 1954) and should be close in embedding space. We then view the points repr"
2020.coling-main.532,2020.emnlp-main.187,0,0.0650884,"o the pruned gold tree of Serva and Petroni (2008) (henceforth referred to as SP08) used as the linguistic phylogenetic gold standard tree. Their comparison metric, which is based on the L2 norm, is basically the sum of squared deviations between each pair’s gold-tree distance g and computed distance P : Dist(P, g) = X (DP (li , lj ) − Dg (li , lj ))2 (1) i,j SP08 was constructed by computing the Levenshtein (edit) distance between words from an open cross-lingual list (Dyen et al., 1992) to compare linguistic divergence through time and thus partially encodes lexical similarity of languages (Oncevay et al., 2020). Rabinovich et al. (2017) also acknowledges that SP08 has been disputed and researchers have not yet agreed on a commonly accepted tree of the Indo-European languages (Ringe et al., 2002). More recently, Bjerva et al. (2019) built on this work and compared different languages based on distance metrics computed on phrase structure trees and dependency relations. They claimed that such language representations correlate better with structural family distances between languages than genetic similarities. These examples show that phylogenetic reconstruction approaches and in particular, the evalu"
2020.coling-main.532,P19-1018,0,0.0173548,"nsions and only words with more than 5 occurrences are retained for training. We use skip-gram with negative sampling (Mikolov et al., 2013) and standard hyper-parameters. 4 Measuring Isomorphism An empirical measure of semantic proximity between two languages is often computed using the degree of isomorphism, that is, how similar the structures of two languages are in topological space (Søgaard et al., 2018). Research in cross-lingual transfer tasks shows that linguistic differences across languages often make spaces depart from isomorphism (Nakashole and Flauger, 2018; Søgaard et al., 2018; Patra et al., 2019; Vuli´c et al., 2020). While this degrades the quality of bilingual embeddings, it is a desired characteristic in our case: since our task involves processing of (multi-view) representations of monolingual text, departures from isomorphism indicate diversity in the source that generates them. To quantify isomorphism, we compute embeddings on a corpus in language L. Embeddings reflect distributional properties in the data: words in similar contexts have similar meanings (Harris, 1954) and should be close in embedding space. We then view the points representing words or tags in the resulting hy"
2020.coling-main.532,P17-1049,0,0.454526,"nguage of the translation, in the same genre and style (Gellerstam, 1986; Baker and others, 1993; Baroni and Bernardini, 2005; Volansky et al., 2015). Characteristics such as simplification, over-adherence to conventions of the target language, and explicitation can occur as a communicative process itself. This is contrasted with “interference” or “shining-through” (Teich, 2003), described as “phenomena pertaining to the make-up of the source text tend to be transferred to the target text” (Toury, 2012). Prominent evidence for shining-through as a translationese effect is found in the work of Rabinovich et al. (2017), who show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In the similar vein, a significant amount of work has gone into training classifiers to distinguish between translations and originally authored text and then investigating the contributions of individual features to the result of the classification (Baroni and Bernardini, 2005; Koppel and Ordan, 2011; Volansky et al., 2015; Avner et al., 2016). Features that contribute strongly to classification are interpreted as ind"
2020.coling-main.532,P18-1072,0,0.0449613,"Missing"
2020.coling-main.532,2020.emnlp-main.257,0,0.0217513,"Missing"
2020.emnlp-main.202,P17-1042,0,0.0282177,"The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trained using word2vec (Mikolov et al., 2013)5 on the complete WP editions are projected into a common multilingual space via vecmap6 (Artetxe et al., 2017) to attain bilingual embeddings between en–{f r,de,es}. These initialise the NMT word embeddings (Cw ). 2 Dumps were downloaded on January 2019 from dumps. wikimedia.org/ 3 github.com/cristinae/WikiTailor 4 github.com/ruitedk6/comparableNMT 5 github.com/tmikolov/word2vec 6 github.com/artetxem/vecmap 2562 WP, L1 L1–L2 en–f r en–de en–es # Sent. WP, L2 # Tokens Sent./Article 117 / 42 2693/1205 117 / 37 2693/987 117 / 35 2693/937 28 29 32 EP, L1 # Sent. # Tokens Sent./Article 38/25 644/710 51/30 1081/742 27/20 691/572 EP, L2 # Sent. # Tokens # Sent. # Tokens 1+6 25+80 1+9 25+180 1+7 24+84 1+3 27+"
2020.emnlp-main.202,D18-1399,0,0.0205255,"further work in Section 5. 2 Related Work Machine translation has experienced major improvements in translation quality due to the introduction of neural architectures (Cho et al., 2014; 2560 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2560–2571, c November 16–20, 2020. 2020 Association for Computational Linguistics Bahdanau et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them"
2020.emnlp-main.202,P19-1019,0,0.0305005,"au et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a few-shot setting. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) is an alternative approach focusing on comparable, rather than parallel data. The internal representations of an emergent NMT system are used to identify useful sentence pairs in comparable documents. Selection depends on the current state of the model, resembling a type of self-paced l"
2020.emnlp-main.202,J82-2005,0,0.540679,"Missing"
2020.emnlp-main.202,P19-1309,0,0.170458,"hang et al. (2018), showing that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or tailored architectures are achieving a new state-of-the-art in parallel sentence extraction and filtering (Espa˜naBonet et al., 2017; Gr´egoire and Langlais, 2018; Artetxe and Schwenk, 2019; Hangya and Fraser, 2019; Chaudhary et al., 2019). Using a highly multilingual sentence encoder, Schwenk et al. (2019) scored Wikipedia sentence pairs across various language combinations (WikiMatrix). Due to its multi2561 lingual aspect and the close similarity with the raw Wikipedia data we use, we also use scored WikiMatrix data for one of the comparisons (Section 3.2). 3 Self-Supervised Neural Machine Translation (SSNMT) SSNMT is a joint data selection and training framework for machine translation, introduced in Ruiter et al. (2019). SSNMT enables learning NMT from comparable rather than"
2020.emnlp-main.202,W15-3402,1,0.896662,"Missing"
2020.emnlp-main.202,W19-5435,0,0.14859,"Missing"
2020.emnlp-main.202,D14-1179,0,0.0209941,"Missing"
2020.emnlp-main.202,D18-1045,0,0.0273502,"report the sizes for both the monolingual/comparable editions; for Europarl (EP), true+false splits (see Section 3.2). SSNMT SotA L1-to-L2 L2-to-L1 L1-to-L2 L2-to-L1 L1–L2 BLEU TER METEOR BLEU TER METEOR BLEU BLEU en–f r en–de en–es 29.5±.6 15.2±.5 28.6±.7 51.9±.6 68.5±.7 52.6±.7 46.4±.6 30.3±.5 47.8±.7 27.7±.6 21.2±.6 28.4±.7 53.4±.7 62.8±.9 54.1±.7 30.3±.4 25.4±.4 30.5±.4 45.6/25.1/37.5 37.9/17.2/28.3 –/–/– –/24.2/34.9 –/21.0/35.2 –/–/– Table 2: Automatic evaluation of SSNMT on NT14 (f r) NT16 (de) NT13 (es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translatio"
2020.emnlp-main.202,C18-1122,0,0.0268214,"Missing"
2020.emnlp-main.202,P19-1118,0,0.0186012,"that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or tailored architectures are achieving a new state-of-the-art in parallel sentence extraction and filtering (Espa˜naBonet et al., 2017; Gr´egoire and Langlais, 2018; Artetxe and Schwenk, 2019; Hangya and Fraser, 2019; Chaudhary et al., 2019). Using a highly multilingual sentence encoder, Schwenk et al. (2019) scored Wikipedia sentence pairs across various language combinations (WikiMatrix). Due to its multi2561 lingual aspect and the close similarity with the raw Wikipedia data we use, we also use scored WikiMatrix data for one of the comparisons (Section 3.2). 3 Self-Supervised Neural Machine Translation (SSNMT) SSNMT is a joint data selection and training framework for machine translation, introduced in Ruiter et al. (2019). SSNMT enables learning NMT from comparable rather than parallel data, where com"
2020.emnlp-main.202,W11-2123,0,0.01649,"ser in the cross-lingual space, and the system is able to exploit this by extracting increasingly similar and accurate pairs. 4.2 Order & Complexity Establishing the complexity of a sentence is a complex task by itself. Complexity can be estimated by the loss of an instance with respect to the gold or target. In our self-supervised approach, there is no target for the sentence extraction task, so we try to infer complexity by other means. First, we study the behaviour of the average perplexity throughout training. Perplexities of the extracted data are estimated using a LM trained with KenLM (Heafield, 2011) on the monolingual WPs for the four languages in our study. We observe the same behaviour in the four cases illustrated by the English curves plotted in Figure 2 (top). Perplexity drops heavily within the first 10 k steps for all languages and models. This indicates that the data extracted in the first epoch includes more outliers, and the distribution of extracted sentences moves closer to the average observed in the monolingual WPs as training advances. The larger number of outliers at the beginning of training can be attributed to the larger number of homographs (bottom Figure 3) and short"
2020.emnlp-main.202,P17-4012,0,0.0320479,"ruecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trained using word2vec (Mikolov et al., 2013)5 on the complete WP editions are projected into a common multilingual space via vecmap6 (Artetxe et al., 2017) to attain bilingual embeddings between en–{f r,de,es}. These initialise the NMT word embeddings (Cw ). 2 Dumps were downloaded on January 2019 from dumps. wikimedia.org/ 3 github.com/cristinae/WikiTailor 4 github.com/ruitedk6/comparableNMT 5 github.com/tmikolov/word2vec 6 github.com/artetxem/vecmap 2562 WP, L1 L1–L2 en–f r"
2020.emnlp-main.202,kocmi-bojar-2017-curriculum,0,0.0658892,"n data, which comes with a high computational cost. To alleviate this cost, Kumar et al. (2019) use reinforcement learning on the pre-scored noisy corpus to jointly learn the denoising curriculum with NMT. In Section 3.2 we show that our model exploits its self-supervised nature to perform denoising by selecting parallel pairs with increasing accuracy, without the need of additional noise metrics. Difficulty-based curricula for NMT that take into account sentence length and vocabulary frequency have been shown to improve translation quality when samples are presented in increasing complexity (Kocmi and Bojar, 2017). Platanios et al. (2019) link the introduction of difficult samples with the NMT models’ competence. Other difficulty-orderings have been explored extensively in Zhang et al. (2018), showing that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or"
2020.emnlp-main.202,W04-3250,0,0.504822,"Missing"
2020.emnlp-main.202,2005.mtsummit-papers.11,0,0.0596494,"15.2±.5 28.6±.7 51.9±.6 68.5±.7 52.6±.7 46.4±.6 30.3±.5 47.8±.7 27.7±.6 21.2±.6 28.4±.7 53.4±.7 62.8±.9 54.1±.7 30.3±.4 25.4±.4 30.5±.4 45.6/25.1/37.5 37.9/17.2/28.3 –/–/– –/24.2/34.9 –/21.0/35.2 –/–/– Table 2: Automatic evaluation of SSNMT on NT14 (f r) NT16 (de) NT13 (es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable"
2020.emnlp-main.202,P07-2045,0,0.00683322,"ium recall approach in Ruiter et al. (2019). Whenever enough pairs have been collected to create a batch, the system trains on it, updating its weights, improving both its translation and extraction ability to fill the next batch. 3.1 Translation Quality Experimental Setup We use Wikipedia (WP) as a comparable corpus and download the English, French, German and Spanish dumps,2 pre-process them and extract comparable articles per language pair using WikiTailor3 (Barr´on-Cede˜no et al., 2015; Espa˜na-Bonet et al., 2020). All articles are normalized, tokenized and truecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using"
2020.emnlp-main.202,N19-1208,0,0.115059,"Missing"
2020.emnlp-main.202,W07-0734,0,0.00925881,"ystems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). Note that unsupervised systems such as Lample et al. (2018b) use"
2020.emnlp-main.202,2020.tacl-1.47,0,0.0248273,"need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a few-shot setting. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) is an alternative approach focusing on comparable, rather than parallel data. The internal representations of an emergent NMT system are used to identify useful sentence pairs in comparable documents. Selection depends on the current state of the model, resembling a type of self-paced learning (Kumar et al., 2010). Data selection in SSNMT is directly related to curriculum learning, the idea of presenting train"
2020.emnlp-main.202,P02-1040,0,0.108728,"(es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). N"
2020.emnlp-main.202,N19-1119,0,0.0929573,"Missing"
2020.emnlp-main.202,P19-1178,1,0.890518,"Missing"
2020.emnlp-main.202,P16-1162,0,0.0614651,"o create a batch, the system trains on it, updating its weights, improving both its translation and extraction ability to fill the next batch. 3.1 Translation Quality Experimental Setup We use Wikipedia (WP) as a comparable corpus and download the English, French, German and Spanish dumps,2 pre-process them and extract comparable articles per language pair using WikiTailor3 (Barr´on-Cede˜no et al., 2015; Espa˜na-Bonet et al., 2020). All articles are normalized, tokenized and truecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trai"
2020.emnlp-main.202,2006.amta-papers.25,0,0.0510763,"the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). Note that unsupervised syste"
2020.emnlp-main.202,P18-1005,0,0.0384186,"5. 2 Related Work Machine translation has experienced major improvements in translation quality due to the introduction of neural architectures (Cho et al., 2014; 2560 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2560–2571, c November 16–20, 2020. 2020 Association for Computational Linguistics Bahdanau et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a f"
2020.emnlp-main.202,N19-1189,0,0.199873,"Missing"
2020.emnlp-main.202,P19-1123,0,0.0327964,"Missing"
2020.emnlp-main.202,W18-6314,0,0.159093,"ory behavior of benefiting from both increasingly difficult (domain-distant) and easy (domain-relevant) samples has been analyzed by Weinshall et al. (2018), showing that the initial phases of training benefit from easy samples with respect to a hypothetical competent model (target hypothesis), while also being boosted (Freund and Schapire, 1996) by samples that are difficult with respect to the current state of the model (Hacohen and Weinshall, 2019). In Wang et al. (2019), both domain-relevance and denoising are combined into a single curriculum. The denoising curriculum for NMT proposed by Wang et al. (2018) is related to our approach in that they also use online data selection to build the curriculum based on the current state of the model. However, the noise scores for the dataset at each training step depend on fine-tuning the model on a small selection of clean data, which comes with a high computational cost. To alleviate this cost, Kumar et al. (2019) use reinforcement learning on the pre-scored noisy corpus to jointly learn the denoising curriculum with NMT. In Section 3.2 we show that our model exploits its self-supervised nature to perform denoising by selecting parallel pairs with incre"
2020.emnlp-main.202,D17-1147,0,0.0475825,"Missing"
2020.emnlp-main.205,P19-4007,0,0.0605276,"Missing"
2020.emnlp-main.205,W14-3348,0,0.0566461,"cludes word-level QE, sentence-level QE, document-level QE and QE as a Metric tasks. The QE as a Metric task requires QE models to score a translation on the sentence level similar to the sentence-level QE task, but these two tasks are different as the goal of the sentence-level QE task (Martins et al., 2017) is to predict the percentage of edits needed to fix the translation for post-editing purposes while the goal of the QE as a Metric task is to estimate the general quality of the translation like machine translation (MT) evaluation metrics, such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014), except without using reference translations. Supervised learning of the QE as a Metric task requires human evaluation of translation quality as training data. Human evaluation of translation quality is generally very costly and can be performed in different ways, such as Direct Assessment (DA: requiring human assessors to assign an absolute score to a translation) (Barrault et al., 2019; Graham et al., 2013, 2014, 2017) or Relative Ranking (RR: requiring human assessors to rank different translations) (Bojar et al., 2015). Since the QE as a Metric task requires QE models to assign an absolut"
2020.emnlp-main.205,N19-1423,0,0.622996,"be straightforwardly used as training data for the QE as a Metric task. In order to also make use of the RR human evaluation data, we propose a multi-task learning QE model that jointly learns two tasks, score a translation and rank two translations. Multi-task learning of these two closely related tasks enables us to use both DA and RR human evaluation data for training the QE model and improve performance compared to learning these two tasks separately. Our model performs translation quality estimation based on cross-lingual sentence embeddings from pretrained multilingual language models (Devlin et al., 2019; Conneau et al., 2019) and does not need reference translations. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task (Ma et al., 2019). A number of previous works also used sentence embeddings for evaluating translation quality (Shimanaka et al., 2018; Guzm´an et al., 2015; Gupta et al., 2015). However, Shimanaka et al. (2018); Gupta et al. (2015)’s models only learn to score a translation and Guzm´an et al. (2015)’s model only learns to rank two translations while our model jointly learns to score a translation and r"
2020.emnlp-main.205,W13-2305,0,0.184057,"of the QE as a Metric task is to estimate the general quality of the translation like machine translation (MT) evaluation metrics, such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014), except without using reference translations. Supervised learning of the QE as a Metric task requires human evaluation of translation quality as training data. Human evaluation of translation quality is generally very costly and can be performed in different ways, such as Direct Assessment (DA: requiring human assessors to assign an absolute score to a translation) (Barrault et al., 2019; Graham et al., 2013, 2014, 2017) or Relative Ranking (RR: requiring human assessors to rank different translations) (Bojar et al., 2015). Since the QE as a Metric task requires QE models to assign an absolute score to a translation, DA human evaluation data can be straightforwardly used as training data for the QE as a Metric task. In order to also make use of the RR human evaluation data, we propose a multi-task learning QE model that jointly learns two tasks, score a translation and rank two translations. Multi-task learning of these two closely related tasks enables us to use both DA and RR human evaluation d"
2020.emnlp-main.205,E14-1047,0,0.0539451,"Missing"
2020.emnlp-main.205,D15-1124,1,0.895745,"Missing"
2020.emnlp-main.205,P15-1078,0,0.0606967,"Missing"
2020.emnlp-main.205,W19-5358,0,0.122656,"slations while our model jointly learns to score a translation and rank two translations in order to make use of different types of human evaluation data for model training. In 2592 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2592–2598, c November 16–20, 2020. 2020 Association for Computational Linguistics addition, Shimanaka et al. (2018); Guzm´an et al. (2015); Gupta et al. (2015)’s models use the reference translation for evaluating translation quality while our QE model does not require reference translations. There are existing QE models (Lo, 2019; Yankovskaya et al., 2019) that do not need the reference translation and perform translation quality estimation based on cross-lingual word/sentence embeddings, but these QE models give relatively poor and unstable results for different language pairs (Ma et al., 2019) while our QE model achieves more robust and better results. In addition, Lo (2019); Yankovskaya et al. (2019)’s QE models only score a translation while our QE model jointly learns to score a translation and rank two translations via multi-task learning. 2 Our Approach We propose a multi-task learning QE model that jointly lea"
2020.emnlp-main.205,W19-5302,0,0.230865,"ranslation and rank two translations. Multi-task learning of these two closely related tasks enables us to use both DA and RR human evaluation data for training the QE model and improve performance compared to learning these two tasks separately. Our model performs translation quality estimation based on cross-lingual sentence embeddings from pretrained multilingual language models (Devlin et al., 2019; Conneau et al., 2019) and does not need reference translations. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task (Ma et al., 2019). A number of previous works also used sentence embeddings for evaluating translation quality (Shimanaka et al., 2018; Guzm´an et al., 2015; Gupta et al., 2015). However, Shimanaka et al. (2018); Gupta et al. (2015)’s models only learn to score a translation and Guzm´an et al. (2015)’s model only learns to rank two translations while our model jointly learns to score a translation and rank two translations in order to make use of different types of human evaluation data for model training. In 2592 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 259"
2020.emnlp-main.205,Q17-1015,0,0.0286017,"T 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task. 1 Introduction The translation quality estimation (QE) task (Fonseca et al., 2019) aims to evaluate the quality of a translation based on the translation and the source sentence without using reference translations. The QE task includes word-level QE, sentence-level QE, document-level QE and QE as a Metric tasks. The QE as a Metric task requires QE models to score a translation on the sentence level similar to the sentence-level QE task, but these two tasks are different as the goal of the sentence-level QE task (Martins et al., 2017) is to predict the percentage of edits needed to fix the translation for post-editing purposes while the goal of the QE as a Metric task is to estimate the general quality of the translation like machine translation (MT) evaluation metrics, such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014), except without using reference translations. Supervised learning of the QE as a Metric task requires human evaluation of translation quality as training data. Human evaluation of translation quality is generally very costly and can be performed in different ways, such as Direct Ass"
2020.emnlp-main.205,P02-1040,0,0.108139,"erence translations. The QE task includes word-level QE, sentence-level QE, document-level QE and QE as a Metric tasks. The QE as a Metric task requires QE models to score a translation on the sentence level similar to the sentence-level QE task, but these two tasks are different as the goal of the sentence-level QE task (Martins et al., 2017) is to predict the percentage of edits needed to fix the translation for post-editing purposes while the goal of the QE as a Metric task is to estimate the general quality of the translation like machine translation (MT) evaluation metrics, such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014), except without using reference translations. Supervised learning of the QE as a Metric task requires human evaluation of translation quality as training data. Human evaluation of translation quality is generally very costly and can be performed in different ways, such as Direct Assessment (DA: requiring human assessors to assign an absolute score to a translation) (Barrault et al., 2019; Graham et al., 2013, 2014, 2017) or Relative Ranking (RR: requiring human assessors to rank different translations) (Bojar et al., 2015). Since the QE as a Metric task"
2020.emnlp-main.205,W12-3116,0,0.0762743,"Missing"
2020.emnlp-main.205,D19-1410,0,0.0157325,"models give relatively poor and unstable results for different language pairs (Ma et al., 2019) while our QE model achieves more robust and better results. In addition, Lo (2019); Yankovskaya et al. (2019)’s QE models only score a translation while our QE model jointly learns to score a translation and rank two translations via multi-task learning. 2 Our Approach We propose a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model is based on cross-lingual sentence embeddings from multilingual BERT (M-BERT) (Devlin et al., 2019; Reimers and Gurevych, 2019). To compute the sentence embedding for a given sentence, we feed this sentence into M-BERT and then perform MEAN pooling over the output of M-BERT to obtain fixedsize sentence embedding. We fine-tune M-BERT for the QE tasks. The scoring task To score a translation t given the source sentence s, we use the cosine similarity between the source sentence embedding ~s and the target sentence embedding ~t as the score of the translation.1 Equation 1 gives the loss function for the scoring task, where Yhuman (0 ≤ Yhuman ≤ 1) is the normalized DA score of the translation assigned by human assessors."
2020.emnlp-main.205,N18-4015,0,0.0168596,"both DA and RR human evaluation data for training the QE model and improve performance compared to learning these two tasks separately. Our model performs translation quality estimation based on cross-lingual sentence embeddings from pretrained multilingual language models (Devlin et al., 2019; Conneau et al., 2019) and does not need reference translations. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task (Ma et al., 2019). A number of previous works also used sentence embeddings for evaluating translation quality (Shimanaka et al., 2018; Guzm´an et al., 2015; Gupta et al., 2015). However, Shimanaka et al. (2018); Gupta et al. (2015)’s models only learn to score a translation and Guzm´an et al. (2015)’s model only learns to rank two translations while our model jointly learns to score a translation and rank two translations in order to make use of different types of human evaluation data for model training. In 2592 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2592–2598, c November 16–20, 2020. 2020 Association for Computational Linguistics addition, Shimanaka et al. (2018); Guz"
2020.emnlp-main.205,W19-5410,0,0.046437,"Missing"
2020.icon-main.7,N03-1017,0,0.0424458,"diting Effort for each sentence. We measure: Manipuri and Mizo are the lingua francas of Manipur and Mizoram, two neighbouring north-eastern states of India. Both Manipuri and Mizo are low resource languages. Limited availability of data in Manipuri and Mizo is one of the main reasons that hamper the development of NLP systems for the language. The training datasets used for training the MT system for the languages are shown in Table 3. On the same EnglishManipuri training dataset, we first examine the performance of MT systems trained with Phrase Based Statistical Machine Translation, PBSMT (Koehn et al., 2003) and the RNNbased NMT with attention mechanism (Bahdanau et al., 2014). The trained MT systems are evaluated on a held-out test dataset of 900 sentences. The result shows a BLEU score of 6.45, (34.7/9.6/3.5/1.5) on the PBSMT system while the NMT system achieved a BLEU score of 0.00, (11.8/0.3/0.0/0.0). For this reason, we use PBSMT systems for both English-Manipuri and English-Mizo MT systems as our parallel NMT results are substantially worse in these low-resource scenarios. To build language models for the target languages, we used the dataset in (Singh and Bandyopadhyay, 2010b) and (Meetei"
2020.icon-main.7,2012.amta-wptp.2,0,0.0297502,"ready annotated, the posteditors could skip the effort of identifying the errors and concentrate only on the highlighted error text segment in the PE process. Focusing on how PE effort changes with the different types of MT errors, the authors reported a weak correlation between PE time and PE effort. The authors also report that no direct dependency was found between the temporal and technical PE effort. Investigating the various types of PE operations for French to English and English to Spanish translation outputs, Popovic et al. (2014) reported lexical edits as the main factor in PE time. Koponen et al. (2012), study the cognitive effort of post-editing MT output based on measuring PE time and HTER (Snover et al., 2006). HTER (Human-targeted Translation Edit Rate), is an automatic metric that computes the minimum number of edits required to change MT output into the post-edited version. The authors reported that the absolute PE time increases with the number of printable keystrokes and sentence length while seconds per word remain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE eff"
2020.icon-main.7,D19-5224,1,0.874746,"f PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected dataset and data partitioning. from the CAT tool to analyze PE effort and the time required to generate a reasonable target te"
2020.icon-main.7,P02-1040,0,0.108324,"emain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample"
2020.icon-main.7,P07-2045,0,0.0165078,"ess3 . The news articles are in English. The complete dataset consists of 3770 news articles from the period July 2011 to October 2019 comprising 64976 sentences. We randomly select 200 sentences (DP E ) for our PE experiment. The statistics of the dataset and data partitioning are shown in Table 2. The dataset is collected using a web-scrapper built in-house. 4.2 Pre-processing Data collected from the web is not free from noise. The pre-processing step includes removal of non-ascii special characters. Each of the news articles in our dataset is split into sentences using the Moses tokenizer (Koehn et al., 2007). Methodology and Experimental Design We use an English language corpus collected from a local daily newspaper as the source text. We normalize the data in a pre-processing step. The normalized text is then machine translated into three different target languages using different MT systems. After post-editing a sample dataset of the machine translated text using a CAT tool, we study the data collected 4.3 Building Machine Translated Target Text We build a machine translated dataset using MT systems resulting in three language pairs, 3 52 https://ifp.co.in/ en-mn monomn en-mz monomz Sentences T"
2020.icon-main.7,W17-5717,0,0.0156915,"and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected dataset and data partitioning. from the CAT tool to analyze PE effort and the time required to generate a"
2020.icon-main.7,W13-0802,1,0.813607,"the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected data"
2020.icon-main.7,W10-3811,1,0.885174,"captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of ou"
2020.icon-main.7,W10-3605,1,0.818486,"Missing"
2020.icon-main.7,2006.amta-papers.25,0,0.0603019,"lighted error text segment in the PE process. Focusing on how PE effort changes with the different types of MT errors, the authors reported a weak correlation between PE time and PE effort. The authors also report that no direct dependency was found between the temporal and technical PE effort. Investigating the various types of PE operations for French to English and English to Spanish translation outputs, Popovic et al. (2014) reported lexical edits as the main factor in PE time. Koponen et al. (2012), study the cognitive effort of post-editing MT output based on measuring PE time and HTER (Snover et al., 2006). HTER (Human-targeted Translation Edit Rate), is an automatic metric that computes the minimum number of edits required to change MT output into the post-edited version. The authors reported that the absolute PE time increases with the number of printable keystrokes and sentence length while seconds per word remain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effo"
2020.icon-main.7,W14-0314,1,0.836424,"n may not always be sufficient to meet the demand. MT output may sometimes be erroneous and needs to be checked and corrected. The use of translation technology such as MT systems, transla1 2 https://ifp.co.in/ http://censusindia.gov.in 50 Proceedings of the 17th International Conference on Natural Language Processing, pages 50–59 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) English SVO Roman Manipuri SOV Bengali Mizo OSV Roman Hindi SOV Devanagari and Choice Network Analysis (CNA). Several studies investigated bi-lingual PE and monolingual PE. In bilingual PE (Zampieri and Vela, 2014) post editors have access to the source text, while in monolingual PE (Nitzke, 2016) MT output is edited without the source text. Zampieri and Vela (2014) studied the use of TMs generated by MT output and their effect on human translation. The authors reported a significant increase in translation speed while using the TM as compared to translating from the scratch. Table 1: Typological Word Order and Script of Languages in the Study. Acronyms: O = Object, S = Subject, V = Verb. target languages Manipuri, Mizo and Hindi resulting in three parallel datasets. The basic word order of the language"
2020.iwslt-1.34,P04-3031,0,0.178791,"om Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE) (Sennrich et al., 2016) with 32 k merge operations trained jointly on en– de data is applied before training neural systems. After shuffling, 1,000 sentences are set aside for tuning/validation. 4.2 Machine Translation Engines We train three different architectures, one statistical and two neural, on the corpora above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to the small dimensions of the Phrase-Based Statistical Machine Translation (SMT). SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) 283 3 https://pypi.org/project/langdetect/ Europarl-UdS German Translation Written German Written English EPIC-UdS lines tokens 137,813 427,779 372,"
2020.iwslt-1.34,W19-5204,0,0.0164475,"al-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train bias-free systems. Aranberri (2020) analyzed translationese characteristics on translations obtained by five state-of-the-art Spanish-to-Basque translation systems, neural and rule-based. The author quantifies translationese by measuring lexical variety, lexical density, length ratio and perplexity with part of spe"
2020.iwslt-1.34,N16-1111,0,0.0608831,"Missing"
2020.iwslt-1.34,P07-2045,0,0.0221128,"ora from OPUS (Tiedemann, 2012), one of them text-oriented (Ct ) and the other speech-oriented (Cs ). The distribution of their sub-corpora is shown in Table 2. Note that we do not include Europarl data here so that there is no overlap between MT training and our analysis data. Note also that our speech data (TED talks and subtitles) is still made up from translations and not simultaneous interpreting. This is important since it prevents MT systems from simply mimicking interpreting’s pronounced translationese. All datasets are normalised, tokenized and truecased using standard Moses scripts (Koehn et al., 2007) and cleaned for low quality pairs. Duplicates are removed and sentences shorter than 4 tokens or with a length ratio greater than 9 are discarded. We also eliminate sentence pairs which are not identified as English/German by langdetect3 and apply basic cleaning procedures. With this, we reduce the corpus size by more than half of the sentences. In order to build balanced corpora we limit the number of sentences we used from ParaCrawl to 5 million and from Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE"
2020.iwslt-1.34,P11-1132,0,0.279913,"in two main categories: (i) source’s interference, or shining-though as put forward by Teich (2003). For example, a translation replicating a syntactic pattern which is typical of the source language, and rare in the target language, displays a typical form of shiningthrough; (ii) aherence or over-adherence to the target language’s standards, that is normalisation. For example, translating a sentence displaying marked order in the source with a sentence displaying standard order in the target it a typical example of overnormalization. Nonetheless, translationese’s main causes remain unclear (Koppel and Ordan, 2011; Schmied and Sch¨affler, 1996). Translationese displays different patterns depending on the translation’s mode and register: a typical example is simultaneous interpreting, which shows translationese patterns distinct from those observed in written translations (Bernardini et al., 2016). We can interpret differences as either (i) an effect of the limitations of the human language apparatus that constrain translations, (ii) an inevitable effect of the structural and semantic differences between languages; or (iii) a combination of the two. To test these hypotheses, it is common to compare huma"
2020.iwslt-1.34,W19-8706,0,0.483084,"Missing"
2020.iwslt-1.34,J13-4007,0,0.0133526,"). Due to the particularly harsh constraints imposed on human interpreters, such particularities can be useful to better understand the nature and causes of translationese in general (Shlesinger and Ordan, 2012). The features of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs close"
2020.iwslt-1.34,J03-1002,0,0.0114141,"ra above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to the small dimensions of the Phrase-Based Statistical Machine Translation (SMT). SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) 283 3 https://pypi.org/project/langdetect/ Europarl-UdS German Translation Written German Written English EPIC-UdS lines tokens 137,813 427,779 372,547 3,100,647 7,869,289 8,693,135 German Interpreting Spoken German Spoken English lines tokens 4,080 3,408 3,623 58,371 57,227 68,712 Table 1: Corpus collections used to train our language models: Europarl-UdS (written) and EPIC-UdS (spoken). German translation and interpreting are both from English. lines de tokens en tokens Ct Cs CommonCrawl MultiUN NewsCommentary academia career limiting moves Rapid ParaCrawl-5M TED OpenSubtitles-10M 2,212,292"
2020.iwslt-1.34,P02-1040,0,0.1096,"4,608 8,316,081 24,563,476 96,262,081 3,833,653 85,773,795 54,140,396 4,924,596 46,222,416 148,360,866 103,287,049 20,141,669 93,287,837 3 3 3 3 3 7 7 3 3 3 3 7 3 3 Total clean Speech Total clean Text 13,379,441 9,121,710 187,551,444 198,340,602 197,175,542 207,434,038 7 3 3 7 Table 2: Text-oriented (Ct ) and speech-oriented (Cs ) corpora used for training the MT systems. and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimization of the feature weights of the model is done with Minimum Error Rate Training (MERT) (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. As features, we use the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and lexicalized reordering. The neural systems are trained using the Marian toolkit (Junczys-Dowmunt et al., 2018) in a bidirectional setting {en,de}↔{de,en}: layer encoder–decoder with 8-head self-attention, a 2048-dim hidden feed-forward, and 512-dim word vectors. Optimization algorithm, dropout, smoothings and learning rate (with warmup till update 16,000) are the same as for RNN. RNN-Based Neural Machine Translation (RNN)."
2020.iwslt-1.34,tiedemann-2012-parallel,0,0.0210451,"n Original spoken English (transcript) Original spoken German (transcript) English to German translations English to German interpreting (transcript) vocabulary (17 PoS), 5 iterations over 3000 sentences suffice to converge. In our experiments, we measure the average perplexity of each model on unseen human data from of each category, and on the translations produced by three MT architectures in two different settings (see Section 4.2). MT Training Data. In order to adapt our machine translation engines to the previous modalities as much as possible, we gather two different corpora from OPUS (Tiedemann, 2012), one of them text-oriented (Ct ) and the other speech-oriented (Cs ). The distribution of their sub-corpora is shown in Table 2. Note that we do not include Europarl data here so that there is no overlap between MT training and our analysis data. Note also that our speech data (TED talks and subtitles) is still made up from translations and not simultaneous interpreting. This is important since it prevents MT systems from simply mimicking interpreting’s pronounced translationese. All datasets are normalised, tokenized and truecased using standard Moses scripts (Koehn et al., 2007) and cleaned"
2020.iwslt-1.34,R11-1091,0,0.222431,"k Translationese seems to affect the semantic as well as the structural level of text, but much of its effects can be seen in syntax and grammar (Santos, 1995; Puurtinen, 2003). An interesting aspect of translationese is that, while it is somewhat difficult to detect for the human eye (Tirkkonen-Condit, 2002), it can be machine learned with high accuracy (Baroni and Bernardini, 2006; Rubino et al., 2016). Many ways to automatically detect translationese have been devised, both with respect to textual translations and simultaneous interpreting (Baroni and Bernardini, 2006; Ilisei et al., 2010; Popescu, 2011). Simultaneous interpreting has shown specific forms of translationese distinct from those of textual translation (He et al., 2016; Shlesinger, 1995), with a tendency of going in the direction of simplification and explicitation (Gumul, 2006). Due to the particularly harsh constraints imposed on human interpreters, such particularities can be useful to better understand the nature and causes of translationese in general (Shlesinger and Ordan, 2012). The features of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translat"
2020.iwslt-1.34,W19-6627,0,0.218571,"ne translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train"
2020.iwslt-1.34,N16-1110,1,0.685734,"Missing"
2020.iwslt-1.34,W19-6622,0,0.143417,"ined by five state-of-the-art Spanish-to-Basque translation systems, neural and rule-based. The author quantifies translationese by measuring lexical variety, lexical density, length ratio and perplexity with part of speech (PoS) language models and finds no clear correlation with automatic translation quality across different test sets. The results are not conclusive but translation quality seems not to correlate with translationese. Similar results are obtained by Kunilovskaya and Lapshinova-Koltunski (2019) when using 45 morpho-syntactic features to analyze English-to-Russian translations. Vanmassenhove et al. (2019) noted that statistical systems reproduce human lexical diversity better than 281 neural systems, for English–Spanish and English– French even if transformer models, i.e. neural, are those with the highest BLEU score. However, their transformer model did not use subword segmentation putting a limit on the plausible lexical diversity it can achieve. In our study, we focus on English (en) and German (de) texts and compare the presence of translationese in human translations and interpreting, and in machine translations obtained by in-house MT engines. Differently to Aranberri (2020), we develop"
2020.iwslt-1.34,W19-5208,0,0.119171,"s of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train"
2020.iwslt-1.34,P16-1162,0,0.0199201,"d cleaned for low quality pairs. Duplicates are removed and sentences shorter than 4 tokens or with a length ratio greater than 9 are discarded. We also eliminate sentence pairs which are not identified as English/German by langdetect3 and apply basic cleaning procedures. With this, we reduce the corpus size by more than half of the sentences. In order to build balanced corpora we limit the number of sentences we used from ParaCrawl to 5 million and from Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE) (Sennrich et al., 2016) with 32 k merge operations trained jointly on en– de data is applied before training neural systems. After shuffling, 1,000 sentences are set aside for tuning/validation. 4.2 Machine Translation Engines We train three different architectures, one statistical and two neural, on the corpora above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.422,L18-1213,1,0.891813,"Missing"
2020.lrec-1.422,L18-1205,1,0.791884,"Missing"
2020.lrec-1.422,hinrichs-krauwer-2014-clarin,0,0.16717,"ering legal guidance to language data providers, as in many cases their data was not foreseen to be delivered to a repository or to be used for training MT systems. As a consequence, information about the type of licenses that can best suit the purpose of re-using the data is needed . While from the outside it can seem that ELRC shares similarities with other initiatives like CLARIN 8, there are several differences. First, CLARIN is a research infrastructure which aims to support the sharing, use and sustainability of language data and tools for research in the humanities and social sciences (Hinrichs & Krauwer, 2014). In this sense, CLARIN’s scope is broader than that of ELRC, which focuses mainly on language resources and technologies for multilinguality. While CLARIN addresses mainly the research community, ELRC targets data owners from the public sector. This difference with regard to the targeted audience, entails different operations for data identification, processing, sharing and re-purposing. Since ELRC is mainly dealing with language data that reside in public organisations, it is heavily engaged in efforts to “unlock” these data through raising awareness of their value and potential for language"
2020.wmt-1.129,P19-1310,0,0.0293016,"Missing"
2020.wmt-1.129,P17-1042,0,0.069759,"Missing"
2020.wmt-1.129,P19-1309,0,0.0607407,"arl (EP monode , (Koehn, 2005)) and News Commentary (NC monode , (Barrault et al., 2019)) datasets for the monolingual German data. Apart from this, we also use Wikipedia Dumps1 for both German and Upper Sorbian. We extract articles using Wikiextractor2 , which are aligned using Wikipedia langlinks3 to create a comparable corpus for SSNMT extraction. Supervised Task Apart from the provided parallel data, we use high-quality E UROPARL (EP, (Koehn, 2005)) and medium-quality JW300 (Agi´c and Vuli´c, 2019; Tiedemann, 2012) corpora for de-cs-hsb. For parallel text mining with LASER (Schwenk, 2018; Artetxe and Schwenk, 2019), we use the combination of all the monolingual corpora of German and Upper Sorbian from the unsupervised section of Table 1, which is discussed in detail later in Section 4.3. Preprocessing Our preprocessing steps include normalization, tokenization, deduplication, and truecasing. We attach feature labels related to the source language (&lt;src&gt;), target language (&lt;tgt&gt;), and the data quality (&lt;quality&gt;), for 1 https://dumps.wikimedia.org/ (March 2020) https://github.com/attardi/ wikiextractor 3 https://www.mediawiki.org/wiki/API: Langlinks 2 1092 Proceedings of the 5th Conference on Machine Tra"
2020.wmt-1.129,W19-6811,1,0.741038,"nrich and Haddow, 2016b; Koehn and Knowles, 2017). This approach can play a significant role in increasing grammatical coherence. Syntactic and semantic information can be useful to generalize neural models trained on parallel corpora. We augment our parallel data to include factors like lemma (using Snowball Stemmer (Porter, 2001)) and PoS tags (using spaCy8 open source library (Honnibal and Montani, 2017)) for German words. The language-agnostic UDPipe trainable pipeline (Straka et al., 2016) has been used for lemmatization and PoS tagging for Sorbian words. We follow an approach similar to Bandyopadhyay (2019, 2020), where we factor the data at wordlevel to include the root word (lemma) and the part 8 https://github.com/explosion/spaCy BPE de (fac.)→hsb hsb (fac.)→de 2k 5k 10k 20k 31.01 41.15 35.67 34.70 37.09 32.17 38.23 37.62 Table 3: Supervised Source Factored NMT systems with B LEU scores on DevTest20. System B LEU de→hsb (fac.) de (fac.)→hsb (fac.) hsb→de (fac.) hsb (fac.)→de (fac.) 5.67 6.03 7.24 7.49 Table 4: Unsupervised Factored NMT systems with B LEU scores for 10k BPE on DevTest20. 4.3 Data Mining with LASER We use LASER (Schwenk, 2018; Artetxe and Schwenk, 2019) to filter and mine para"
2020.wmt-1.129,D19-1632,0,0.0232135,"Missing"
2020.wmt-1.129,W18-2703,0,0.0124699,"rate were avoided due to slower training and lower accuracy scores. We use a batch size of 50. The Phrase-Based Statistical MT systems (PBSMT) are standard Moses (Koehn et al., 2007) systems trained without applying BPE to the data. Iterative Backtranslation For the unsupervised task, we use an SSNMT system as described in Ruiter et al. (2019) to extract parallel sentences from the Wikipedia dumps. SSNMT jointly and iteratively extracts parallel data, and learns the MT task on the extracted parallel data. The resulting trained NMT model is our base model (M0 ). For iterative back-translation (Hoang et al., 2018), we take a model Mi and use it to translate the hsb monolingual data monohsb and EP monode to generate monoide and EP monoihsb respectively. Following Sennrich et al. (2016a), we use the generated data at iteration i on the source side with 5 https://github.com/tmikolov/word2vec https://github.com/artetxem/vecmap 7 We use the Moses multi-bleu script for evaluation. https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl 1093 6 the original data on the target to train a new model Mi+1 . This is done iteratively, in our case until i = 5. As the translation qualit"
2020.wmt-1.129,P17-4012,0,0.0474369,"Missing"
2020.wmt-1.129,2005.mtsummit-papers.11,0,0.0609309,"e and Conneau, 2019) using transformer-base models (Vaswani et al., 2017) for the training of the systems. This paper begins by presenting the data we used for the tasks and the preprocessing pipeline (Section 2). This is followed by an overview of the training setup (Section 3) and the methods we applied ∗ * Equal contribution Data Unsupervised Task For Sorbian, we use the given data from the Sorbian Institute (Inshsb ), from Witaj Sprachzentrum (Witajhsb ), and web-scraped data (Webhsb ). Table 1 gives a summary of the data we use in the unsupervised track. We use the Europarl (EP monode , (Koehn, 2005)) and News Commentary (NC monode , (Barrault et al., 2019)) datasets for the monolingual German data. Apart from this, we also use Wikipedia Dumps1 for both German and Upper Sorbian. We extract articles using Wikiextractor2 , which are aligned using Wikipedia langlinks3 to create a comparable corpus for SSNMT extraction. Supervised Task Apart from the provided parallel data, we use high-quality E UROPARL (EP, (Koehn, 2005)) and medium-quality JW300 (Agi´c and Vuli´c, 2019; Tiedemann, 2012) corpora for de-cs-hsb. For parallel text mining with LASER (Schwenk, 2018; Artetxe and Schwenk, 2019), we"
2020.wmt-1.129,P07-2045,0,0.00694274,"sed for the unsupervised and supervised tasks. 3 Transformer base is the same as in Vaswani et al. (2017) with 6 encoder-decoder layers after having explored other options of Transformer depth. We set the dropout to 0.4 in all experiments. We use adam (Kingma and Ba, 2014) for optimization with β1 = 0.9 and β2 = 0.998. The learning rate is varied from 0 to 2 with a warm update of 4000 and decayed using noam. Lower values of learning rate were avoided due to slower training and lower accuracy scores. We use a batch size of 50. The Phrase-Based Statistical MT systems (PBSMT) are standard Moses (Koehn et al., 2007) systems trained without applying BPE to the data. Iterative Backtranslation For the unsupervised task, we use an SSNMT system as described in Ruiter et al. (2019) to extract parallel sentences from the Wikipedia dumps. SSNMT jointly and iteratively extracts parallel data, and learns the MT task on the extracted parallel data. The resulting trained NMT model is our base model (M0 ). For iterative back-translation (Hoang et al., 2018), we take a model Mi and use it to translate the hsb monolingual data monohsb and EP monode to generate monoide and EP monoihsb respectively. Following Sennrich et"
2020.wmt-1.129,W17-3204,0,0.0122727,"ables 3 (supervised) and 4 (unsupervised). The resulting B LEU scores on Dev20 for each of the iterations is shown in Table 2. The best performance for hsb→de is achieved at i = 4 (11.31 B LEU) and for de→hsb at i = 5 (13.61 B LEU). These constitute two of the models submitted to the unsupervised task. 4.2 Factorization Limited monolingual language analysis tools and few linguistic analysis tools with acceptable performance are available for low-resource (LowRes) languages. In our experiments, we explore factored machine translation (Garc´ıa-Mart´ınez et al., 2016; Sennrich and Haddow, 2016b; Koehn and Knowles, 2017). This approach can play a significant role in increasing grammatical coherence. Syntactic and semantic information can be useful to generalize neural models trained on parallel corpora. We augment our parallel data to include factors like lemma (using Snowball Stemmer (Porter, 2001)) and PoS tags (using spaCy8 open source library (Honnibal and Montani, 2017)) for German words. The language-agnostic UDPipe trainable pipeline (Straka et al., 2016) has been used for lemmatization and PoS tagging for Sorbian words. We follow an approach similar to Bandyopadhyay (2019, 2020), where we factor the d"
2020.wmt-1.129,P02-1040,0,0.11623,"Missing"
2020.wmt-1.129,P19-1178,1,0.882156,"Missing"
2020.wmt-1.129,P18-2037,0,0.0862796,"e use the Europarl (EP monode , (Koehn, 2005)) and News Commentary (NC monode , (Barrault et al., 2019)) datasets for the monolingual German data. Apart from this, we also use Wikipedia Dumps1 for both German and Upper Sorbian. We extract articles using Wikiextractor2 , which are aligned using Wikipedia langlinks3 to create a comparable corpus for SSNMT extraction. Supervised Task Apart from the provided parallel data, we use high-quality E UROPARL (EP, (Koehn, 2005)) and medium-quality JW300 (Agi´c and Vuli´c, 2019; Tiedemann, 2012) corpora for de-cs-hsb. For parallel text mining with LASER (Schwenk, 2018; Artetxe and Schwenk, 2019), we use the combination of all the monolingual corpora of German and Upper Sorbian from the unsupervised section of Table 1, which is discussed in detail later in Section 4.3. Preprocessing Our preprocessing steps include normalization, tokenization, deduplication, and truecasing. We attach feature labels related to the source language (&lt;src&gt;), target language (&lt;tgt&gt;), and the data quality (&lt;quality&gt;), for 1 https://dumps.wikimedia.org/ (March 2020) https://github.com/attardi/ wikiextractor 3 https://www.mediawiki.org/wiki/API: Langlinks 2 1092 Proceedings of the 5"
2020.wmt-1.129,W16-2209,0,0.310605,"20 for German to Upper Sorbian (de→hsb) and Upper Sorbian to German (hsb→de). Our submitted systems are constrained for the very low resource supervised and unconstrained for the unsupervised task, in that we use Wikipedia dumps as additional data. Current machine translation systems that deal with low-resource languages are based on unsupervised neural machine translation, semi-supervised methods and pre-trained models leveraging monolingual data (Guzm´an et al., 2019), and multilingual systems among others. In this work, we explore different systems which include baseline NMT, factored NMT (Sennrich and Haddow, 2016a), iterative backtranslation, self-supervised NMT (SSNMT) (Ruiter et al., 2019) and pretraining with XLM (Lample and Conneau, 2019) using transformer-base models (Vaswani et al., 2017) for the training of the systems. This paper begins by presenting the data we used for the tasks and the preprocessing pipeline (Section 2). This is followed by an overview of the training setup (Section 3) and the methods we applied ∗ * Equal contribution Data Unsupervised Task For Sorbian, we use the given data from the Sorbian Institute (Inshsb ), from Witaj Sprachzentrum (Witajhsb ), and web-scraped data (We"
2020.wmt-1.129,P16-1009,0,0.182848,"WMT), pages 1092–1098 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics every individual sentence. The quality of a sentence depends on the corpus it is from and the quality tags of &lt;low&gt;, &lt;medium&gt;, or &lt;high&gt; are added to all sentences of the corpora according to the quality labels assigned to the data provided for the shared task: e.g. Inshsb is high quality Sorbian. A typical sentence from the corpus after our preprocessing pipeline has the following format: &lt;src&gt; &lt;tgt&gt; &lt;quality&gt; sentence After factoring (4.2), we proceed to apply joint byte-pair encoding (BPE) (Sennrich et al., 2016b) on the corpora to finally get our preprocessed data which we use for training all our NMT models. Unless specified otherwise, we use a default of 5k merge operations. Corpus # Sentences Unsupervised 339k 222k 134k 2,107k 422k 833k 76k Supervised Bitextde 60k Bitexthsb 60k EPde 568k EPcs 568k JW300de 1,179k JW300cs 1,179k Dev & Test Dev20de 2k Dev20hsb 2k DevTest20de 2k DevTest20hsb 2k Inshsb Witajhsb Webhsb EP monode NC monode Wikide Wikihsb # Tokens Evaluation Metric We use B LEU7 (Papineni et al., 2002) scores to evaluate the performance of our trained models. 1,002k 737k 13,098k 11,571k"
2020.wmt-1.129,P16-1162,0,0.325007,"WMT), pages 1092–1098 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics every individual sentence. The quality of a sentence depends on the corpus it is from and the quality tags of &lt;low&gt;, &lt;medium&gt;, or &lt;high&gt; are added to all sentences of the corpora according to the quality labels assigned to the data provided for the shared task: e.g. Inshsb is high quality Sorbian. A typical sentence from the corpus after our preprocessing pipeline has the following format: &lt;src&gt; &lt;tgt&gt; &lt;quality&gt; sentence After factoring (4.2), we proceed to apply joint byte-pair encoding (BPE) (Sennrich et al., 2016b) on the corpora to finally get our preprocessed data which we use for training all our NMT models. Unless specified otherwise, we use a default of 5k merge operations. Corpus # Sentences Unsupervised 339k 222k 134k 2,107k 422k 833k 76k Supervised Bitextde 60k Bitexthsb 60k EPde 568k EPcs 568k JW300de 1,179k JW300cs 1,179k Dev & Test Dev20de 2k Dev20hsb 2k DevTest20de 2k DevTest20hsb 2k Inshsb Witajhsb Webhsb EP monode NC monode Wikide Wikihsb # Tokens Evaluation Metric We use B LEU7 (Papineni et al., 2002) scores to evaluate the performance of our trained models. 1,002k 737k 13,098k 11,571k"
2020.wmt-1.129,L16-1680,0,0.0335276,"Missing"
2020.wmt-1.129,tiedemann-2012-parallel,0,0.0133288,"sb ). Table 1 gives a summary of the data we use in the unsupervised track. We use the Europarl (EP monode , (Koehn, 2005)) and News Commentary (NC monode , (Barrault et al., 2019)) datasets for the monolingual German data. Apart from this, we also use Wikipedia Dumps1 for both German and Upper Sorbian. We extract articles using Wikiextractor2 , which are aligned using Wikipedia langlinks3 to create a comparable corpus for SSNMT extraction. Supervised Task Apart from the provided parallel data, we use high-quality E UROPARL (EP, (Koehn, 2005)) and medium-quality JW300 (Agi´c and Vuli´c, 2019; Tiedemann, 2012) corpora for de-cs-hsb. For parallel text mining with LASER (Schwenk, 2018; Artetxe and Schwenk, 2019), we use the combination of all the monolingual corpora of German and Upper Sorbian from the unsupervised section of Table 1, which is discussed in detail later in Section 4.3. Preprocessing Our preprocessing steps include normalization, tokenization, deduplication, and truecasing. We attach feature labels related to the source language (&lt;src&gt;), target language (&lt;tgt&gt;), and the data quality (&lt;quality&gt;), for 1 https://dumps.wikimedia.org/ (March 2020) https://github.com/attardi/ wikiextractor 3"
2021.acl-long.23,P19-1174,0,0.0485984,"Missing"
2021.acl-long.23,D19-1139,0,0.0159935,"s the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the use of a bag-of-wor"
2021.acl-long.23,P18-1008,0,0.184863,"for each token, i.e. n times if the number of tokens in the sequence is n. However, the complexity of a self-attention network which compares each token with all the other tokens is O(n2 ), while for LSTM (Hochreiter and Schmidhuber, 1997) it is only O(n). In practice, however, LSTM is slower than the self-attention network in training. This is mainly due to the fact that the computation of its current step relies on the computation output of the previous step, which prevents efficient parallelization over the sequence. As for the performance of using recurrent models in machine translation, Chen et al. (2018) shows that an LSTM-based decoder can further improve the performance over the Transformer. Introduction The Transformer translation model (Vaswani et al., 2017) has achieved great success and is used extensively in the NLP community. It achieves outstanding performance compared to previous RNN/CNN In this paper, we investigate how we can efficiently parallelize all linear transformations of an LSTM at the sequence level, i.e. compute its linear transformations only once with a given input sequence. Given that linear transformations are implemented by matrix multiplication, compared to the oth"
2021.acl-long.23,D14-1179,0,0.0450971,"Missing"
2021.acl-long.23,W04-3250,0,0.0464317,"ad is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Parameters were initialized under the Lipschitz constraint (Xu et al., 2020c). We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints for the Transformer Base setting and 20 checkpoints for the Transformer Big setting saved with an interval of 1500 training steps. We also conducted significance tests (Koehn, 2004). 4.2 Main Results We first verify the performance by comparing our approach with the Transformer in both the base setting and the big setting. Results are shown in Table 1. Table 1 shows that using an LSTM-based decoder can bring significant improvements over the self-attention decoder. Specifically, using MHPLSTM improves +0.82 and +0.77 BLEU on the En-De and En-Fr task respectively using the base setting, +1.13 and +0.92 correspondingly using the big setting. The fact that using an LSTM-based decoder can improve the translation quality is consistent with Chen et al. (2018), with MHPLSTM fur"
2021.acl-long.23,W18-6301,0,0.0164371,"LSTM 13 14 15 &gt;15 MHPLSTM Figure 6: Subject-verb agreement analysis. X-axis and y-axis represent subject-verb distance in words and accuracy respectively. rent models. To accelerate RNN models, Zhang et al. (2018b) propose a heavily simplified ATR network to have the smallest number of weight matrices among units of all existing gated RNNs. Peter et al. (2016) investigate exponentially decaying bag-of-words input features for feedforward NMT models. In addition to sequencelevel parallelization, asynchronous optimization (Heigold et al., 2014) and data parallelization with a larger batch size (Ott et al., 2018; Chen et al., 2018; Xu et al., 2020a) can also accelerate training. 6 In this paper, we observe that the sequence-level parallelization issue of LSTM is due to the fact that its computation of gates and hidden states of the current step relies on the computation result of the preceding step, and linear transformations have to be propagated the same number of times as the sequence length. To improve the sequencelevel parallelization of the LSTM, we propose to remove the dependency of the current step LSTM computation on the result of the previous step by computing hidden states and gates with"
2021.acl-long.23,P16-2048,0,0.025915,"aselines in capturing dependencies of various distances with the linguistically-informed verb-subject agreement analysis on the Lingeval97 dataset (Sennrich, 279 0.99 0.98 0.97 0.96 0.95 0.94 0.93 0.92 1 2 3 4 Transformer Base 5 6 7 AAN 8 9 ATR 10 11 12 LN-LSTM 13 14 15 &gt;15 MHPLSTM Figure 6: Subject-verb agreement analysis. X-axis and y-axis represent subject-verb distance in words and accuracy respectively. rent models. To accelerate RNN models, Zhang et al. (2018b) propose a heavily simplified ATR network to have the smallest number of weight matrices among units of all existing gated RNNs. Peter et al. (2016) investigate exponentially decaying bag-of-words input features for feedforward NMT models. In addition to sequencelevel parallelization, asynchronous optimization (Heigold et al., 2014) and data parallelization with a larger batch size (Ott et al., 2018; Chen et al., 2018; Xu et al., 2020a) can also accelerate training. 6 In this paper, we observe that the sequence-level parallelization issue of LSTM is due to the fact that its computation of gates and hidden states of the current step relies on the computation result of the preceding step, and linear transformations have to be propagated the"
2021.acl-long.23,E17-2060,0,0.0592361,"Missing"
2021.acl-long.23,P16-1162,0,0.0435271,"d representation into n folds: i1 |...|in =Ws i + bs (20) (19) Next, the kth input ik is fed into the corresponding HPLSTM network HPLSTMk , and the output ok is obtained: (21) Experiments We replace the self-attention layers of the Transformer decoder with the MHPLSTM in our experiments. 4.1 Settings To compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as test set. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merging operations on all data sets. We only kept sentences with a maximum of 256 subword tokens for training. Training sets were randomly shuffled in each training epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and"
2021.acl-long.23,N18-2074,0,0.0260917,"keep the batch size and training steps the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring info"
2021.acl-long.23,P16-1008,0,0.0238206,"Figure 5: Decoding speed on a single GTX 1080Ti GPU with respect to various input sentence length. Yaxis: number of sentences / second. Beam size: 4. context in one forward pass, while MHPLSTM has to compute 2 forward passes, one for the forward direction, another one for the reverse direction. For each direction, relevant context is processed separately in the recurrent models. 4.6 30 AAN Length Analysis To analyze the effects of MHPLSTM on performance with increasing input length, we conducted a length analysis on the news test set of the WMT 14 En-De task. Following Bahdanau et al. (2015); Tu et al. (2016); Xu et al. (2020b), we grouped sentences of similar lengths together and computed BLEU scores of the MHPLSTM and our baselines for each group. BLEU score results and decoding speed-up of each group are shown in Figure 4 and 5 respectively. Figure 4 shows that MHPLSTM surpasses the other approaches in most length groups, and improvements of using an MHPLSTM based-decoder are more significant for long sentences than short sentences. Figure 5 shows that all recurrent-based approaches are faster than the self-attention decoder in all length groups, and MHPLSTM achieves comparable decoding speed a"
2021.acl-long.23,D19-1145,0,0.0213577,"e and training steps the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the"
2021.acl-long.23,2020.acl-main.323,1,0.559223,"Missing"
2021.acl-long.23,2020.acl-main.37,1,0.856289,"Missing"
2021.acl-long.23,2020.acl-main.38,1,0.645553,"Missing"
2021.acl-long.23,P19-1295,0,0.0127433,"8.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the use of a bag-of-words representation (Equation 9) enables MHPLSTM to connect tokens directly regardless of the distance, thus MHPLSTM is able to leverage both local (Equation 16) and global patterns (Xu et al., 2019). (Please refer to Section 4.7 for empirical verification.) • Compared to the self-attention network, the MHPLSTM computation is more complex. • The computation for the LSTM hidden state (Equation 14) and output gate (Equation 17) in MHPLSTM is enhanced compared to the LN-LSTM. 4.3 Effect of FFN Layers We conducted ablation studies on the WMT 14 En-De task. Since the LSTM hidden state computation may take the role of the position-wise Feed-Forward Network (FFN) sub-layer of decoder layers, we first study removing the FFN sub-layer in decoder layers. Results are shown in Table 3. Table 3 shows"
2021.acl-long.23,P18-1166,1,0.897969,"ing epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and 2048 respectively, the corresponding values for the Transformer Big 276 Speed-Up Train Decode Model BLEU Para. (M) Attention Based Transformer (Vaswani et al., 2017) AAN (Zhang et al., 2018a) 27.55 27.63 62.37 74.97 1.00 1.04 1.00 1.52 Recurrent LN-LSTM (Chen et al., 2018) ATR (Zhang et al., 2018b) 27.96 27.93 68.69 59.23 0.45 0.50 1.47 1.69 Ours MHPLSTM 28.37 62.80 1.16 1.69 Table 2: Comparison on WMT 14 En-De. For recurrent approaches, we replace the self-attention sub-layer of standard Transformer decoder layers with the corresponding module proposed in previous work. setting were 1024 and 4096 respectively. The dimension of each head is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron"
2021.acl-long.23,D18-1459,1,0.916533,"ing epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and 2048 respectively, the corresponding values for the Transformer Big 276 Speed-Up Train Decode Model BLEU Para. (M) Attention Based Transformer (Vaswani et al., 2017) AAN (Zhang et al., 2018a) 27.55 27.63 62.37 74.97 1.00 1.04 1.00 1.52 Recurrent LN-LSTM (Chen et al., 2018) ATR (Zhang et al., 2018b) 27.96 27.93 68.69 59.23 0.45 0.50 1.47 1.69 Ours MHPLSTM 28.37 62.80 1.16 1.69 Table 2: Comparison on WMT 14 En-De. For recurrent approaches, we replace the self-attention sub-layer of standard Transformer decoder layers with the corresponding module proposed in previous work. setting were 1024 and 4096 respectively. The dimension of each head is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron"
2021.acl-long.24,J93-2003,0,0.153768,"Missing"
2021.acl-long.24,W17-4716,0,0.013114,"n models, such as RNN-based (Bahdanau et al., 2014) and Transformer (Vaswani et al., 2017) models, generally have an encoder-decoder structure with a target-to-source attention mechanism. The targetto-source attention in NMT can provide rough word alignments but with a rather low accuracy (Koehn and Knowles, 2017). High-quality word alignment can be used to help NMT in many different ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task"
2021.acl-long.24,2020.emnlp-main.42,0,0.452,"ed (Bahdanau et al., 2014) and Transformer (Vaswani et al., 2017) models, generally have an encoder-decoder structure with a target-to-source attention mechanism. The targetto-source attention in NMT can provide rough word alignments but with a rather low accuracy (Koehn and Knowles, 2017). High-quality word alignment can be used to help NMT in many different ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task through unsupervised"
2021.acl-long.24,N19-1423,0,0.0363117,"Missing"
2021.acl-long.24,N13-1073,0,0.0370627,"alignments. The popular statistical word alignment tool GIZA++ (Och and Ney, 2003) implements the statistical IBM models (Brown et al., 1993). The statistical IBM models are mainly based on lexical translation probabilities. Words that co-occur frequently in parallel sentences generally have higher lexical translation probabilities and are more likely to be aligned. The statistical IBM models are trained using parallel sentence pairs with no wordlevel alignment annotations and therefore learn the word alignment task through unsupervised learning. Based on a reparameterization of IBM Model 2, Dyer et al. (2013) presented another popular statistical word alignment tool fast align which can be trained faster than GIZA++, but GIZA++ generally produces better word alignments than fast align. 284 3.2 Neural Alignment Models With neural networks being successfully applied to many NLP tasks, neural word alignment approaches have received much attention. The first neural word alignment models are based on feedforward neural networks (Yang et al., 2013) and recurrent neural networks (Tamura et al., 2014) which can be trained in an unsupervised manner by noise-contrastive estimation (NCE) (Gutmann and Hyv¨ari"
2021.acl-long.24,D19-1453,0,0.369599,"NMT in many different ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task through unsupervised learning and do not require gold alignments from humans as training data. As deep neural networks have been successfully applied to many natural language processing (NLP) tasks, neural word alignment approaches have developed rapidly and outperformed statistical word aligners (Zenkel et al., 2020; Garg et al., 2019). Neural word alignm"
2021.acl-long.24,W17-3204,0,0.0150621,"erforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Vaswani et al., 2017) achieves stateof-the-art results for various translation tasks (Barrault et al., 2019, 2020). Neural translation models, such as RNN-based (Bahdanau et al., 2014) and Transformer (Vaswani et al., 2017) models, generally have an encoder-decoder structure with a target-to-source attention mechanism. The targetto-source attention in NMT can provide rough word alignments but with a rather low accuracy (Koehn and Knowles, 2017). High-quality word alignment can be used to help NMT in many different ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993)"
2021.acl-long.24,N03-1017,0,0.278442,"Missing"
2021.acl-long.24,D19-1087,0,0.0202387,"2014; Vaswani et al., 2017) achieves stateof-the-art results for various translation tasks (Barrault et al., 2019, 2020). Neural translation models, such as RNN-based (Bahdanau et al., 2014) and Transformer (Vaswani et al., 2017) models, generally have an encoder-decoder structure with a target-to-source attention mechanism. The targetto-source attention in NMT can provide rough word alignments but with a rather low accuracy (Koehn and Knowles, 2017). High-quality word alignment can be used to help NMT in many different ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The po"
2021.acl-long.24,W17-4804,0,0.0566951,"Missing"
2021.acl-long.24,2020.emnlp-main.41,0,0.0162312,"anslation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task through unsupervised learning and do not require gold alignments from humans as training data. As deep neural networks have been successfully applied to many natural language processing (NLP) tasks, neural word alignment approaches have developed rapidly and outperformed statistical word aligners (Zenkel et al., 2020; Garg et al., 2019). Neural word alignment approaches include both supervised and unsupervised approaches: supervised approaches"
2021.acl-long.24,P00-1056,0,0.824148,"Missing"
2021.acl-long.24,J03-1002,0,0.29501,"into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task through unsupervised learning and do not require gold alignments from humans as training data. As deep neural networks have been successfully applied to many natural language processing (NLP) tasks, neural word alignment approaches have developed rapidly and outperformed statistical word aligners (Zenkel et al., 2020; Garg et al., 2019). Neural word alignment approaches include both supervised and unsupervised approaches: supervised approaches (Stengel-Eskin et al., 2019; Nagata et al., 2020) use gold"
2021.acl-long.24,P16-1162,0,0.378021,"words in the sentence. The full context based optimization method can be seen as a fine-tuning of the original BTBA model, i.e. we fine-tune the two parameters WnQ and WnK in the last target-to-source attention layer based on full target context to compute more accurate word alignments. 4.3 et al. (2019)’s work.3 During supervised training, the BTBA model is trained to learn the alignment task instead of masked target word prediction, therefore the target sentence does not need to be masked. La (A) = − 1 |Ga | N −1 X X log (Apqn ) (4) (p,q)∈Ga n=0 Note that we apply byte pair encoding (BPE) (Sennrich et al., 2016) for both source and target sentences before we feed them into the BTBA model. Therefore the alignments inferred from the BTBA model is on BPE-level. We convert4 BPE-level alignments to word-level alignments before we perform alignment symmetrization. After alignment symmetrization, we want to use the symmetrized alignments to further fine-tune each BTBA model through supervised learning and therefore we convert5 the word-level alignments back to BPE-level for supervised training of the BTBA models. Self-Supervised Training The BTBA model learns word alignment through unsupervised learning and"
2021.acl-long.24,D19-1084,0,0.0367439,"Missing"
2021.acl-long.24,P14-1138,0,0.017675,"learn the word alignment task through unsupervised learning. Based on a reparameterization of IBM Model 2, Dyer et al. (2013) presented another popular statistical word alignment tool fast align which can be trained faster than GIZA++, but GIZA++ generally produces better word alignments than fast align. 284 3.2 Neural Alignment Models With neural networks being successfully applied to many NLP tasks, neural word alignment approaches have received much attention. The first neural word alignment models are based on feedforward neural networks (Yang et al., 2013) and recurrent neural networks (Tamura et al., 2014) which can be trained in an unsupervised manner by noise-contrastive estimation (NCE) (Gutmann and Hyv¨arinen, 2010) or in a supervised manner by using alignments from human annotators or existing word aligners as labelled training data. As NMT (Bahdanau et al., 2014; Vaswani et al., 2017) achieves great success, the target-to-source attention in NMT models can be used to infer rough word alignments, but with a rather low accuracy. A number of recent works focus on improving the target-to-source attention in NMT to produce better word alignments (Garg et al., 2019; Zenkel et al., 2019; Chen et"
2021.acl-long.24,P13-1017,0,0.0240526,"h no wordlevel alignment annotations and therefore learn the word alignment task through unsupervised learning. Based on a reparameterization of IBM Model 2, Dyer et al. (2013) presented another popular statistical word alignment tool fast align which can be trained faster than GIZA++, but GIZA++ generally produces better word alignments than fast align. 284 3.2 Neural Alignment Models With neural networks being successfully applied to many NLP tasks, neural word alignment approaches have received much attention. The first neural word alignment models are based on feedforward neural networks (Yang et al., 2013) and recurrent neural networks (Tamura et al., 2014) which can be trained in an unsupervised manner by noise-contrastive estimation (NCE) (Gutmann and Hyv¨arinen, 2010) or in a supervised manner by using alignments from human annotators or existing word aligners as labelled training data. As NMT (Bahdanau et al., 2014; Vaswani et al., 2017) achieves great success, the target-to-source attention in NMT models can be used to infer rough word alignments, but with a rather low accuracy. A number of recent works focus on improving the target-to-source attention in NMT to produce better word alignme"
2021.acl-long.24,2020.acl-main.146,0,0.487313,"ent ways, such as detecting source words that are missing in the translation (Lei et al., 2019), integrating an external lexicon into NMT to improve translation for domain-specific terminology or low-frequency words (Chatterjee et al., 2017; Chen et al., 2020), transferring word-level annotations (e.g. underline and hyperlink) from source to target for document/webpage translation (M¨uller, 2017). A number of approaches have been proposed to learn the word alignment task, including both statistical models (Brown et al., 1993) and recently neural models (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020; StengelEskin et al., 2019; Nagata et al., 2020). The popular word alignment tool GIZA++ (Och and Ney, 2003) is based on statistical IBM models (Brown et al., 1993) which learn the word alignment task through unsupervised learning and do not require gold alignments from humans as training data. As deep neural networks have been successfully applied to many natural language processing (NLP) tasks, neural word alignment approaches have developed rapidly and outperformed statistical word aligners (Zenkel et al., 2020; Garg et al., 2019). Neural word alignment approaches includ"
2021.acl-long.527,E14-2007,0,0.0765734,"Missing"
2021.acl-long.527,2015.tc-1.15,0,0.0458996,"Missing"
2021.acl-long.527,2020.acl-main.155,1,0.833511,"Missing"
2021.acl-long.527,2020.acl-demos.37,1,0.842277,"Missing"
2021.acl-long.527,2020.amta-pemdt.7,1,0.761913,"Missing"
2021.acl-long.527,W12-3123,0,0.0386642,"to “Neutral”. Referents: Referents are described as the effect which is triggered by a gesture (Wobbrock et al., 2009). The referents used in elicitation studies are an essential part, since the results established are limited to this set. In our case, referents are PE operations; we will thus use referents and operations interchangeably. To find good referents, we looked at different PE task classifications discussed in the literature. Popovic et al. (2014) propose 5 PE operations: correcting word form, correcting word order, adding omission, deleting addition, and correcting lexical choice. Koponen (2012) additionally distinguishes between moving single words or groups of words and the distance of the movement. Based on these studies as well as our previous elicitation procedure (Herbig et al., 2019), we propose the referents presented below as PE tasks for which we explore gestural input. • I: Insertion • Ds : Deleting a single item • Dg : Deleting a group of items • RPs : Replacing a single item • RPg : Replacing a group of items • ROs : Reordering a single item • ROg : Reordering a group of items Performing those referents implicitly includes other operations, namely selecting a position, a"
2021.acl-long.527,2014.eamt-1.33,0,0.0555545,"Missing"
2021.acl-long.527,2009.mtsummit-btm.8,0,0.633385,"ars, discussions about reaching human parity are still ongoing (L¨aubli et al., 2020) and limited to a small set of language pairs and domains for which ample training data is available. For most application scenarios, however, MT quality is far from reaching the quality of highly trained professionals. In an attempt to combine the best of both worlds, post-editing (PE) is becoming common practice, where human translators use raw MT output and make the necessary changes to produce an acceptable level of quality (Koponen, 2016). Although translators have approached PE with fear and skepticism (Lagoudaki, 2009), more recent studies found that nowadays translators are more open to it and that much of the original dislike was attributed to outdated perceptions of MT quality (Plitt and Masselot, 2010; Green et al., 2013). Independent of translators’ perceptions, studies found that PE increases productivity and decreases errors compared to translation from scratch (Green et al., 2013). PE changes the translation task from mostly text generation to text editing, which involves an increased usage of navigation and deletion keys (Toral et al., 2018). As a result, translators need better support with text e"
2021.acl-long.527,W19-6702,1,0.896483,"Missing"
2021.acl-short.46,C18-1263,0,0.0167812,"elated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trai"
2021.acl-short.46,P15-1166,0,0.0231329,"yer model of Zhang et al. (2020). Results are shown in Table 1. Table 1 shows that our approach achieves better performance in all evaluations while being 1.31 times as fast. 4.3 Ablation Study We study removing MIMO transformations and task-aware attention. Results are shown in Table 2. Table 2 verifies that both mechanisms contribute to the performance. We also examine different combinations of depth and cardinality. Results are shown in Table 3. Table 3 shows that using 6 layers with 4 transformations in each block leads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wa"
2021.acl-short.46,D16-1026,0,0.0445997,"Missing"
2021.acl-short.46,2020.wmt-1.66,0,0.0226759,"are shown in Table 3. Table 3 shows that using 6 layers with 4 transformations in each block leads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual"
2021.acl-short.46,P19-1120,0,0.0485518,"Missing"
2021.acl-short.46,D19-1167,0,0.0254023,"presentation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism"
2021.acl-short.46,C18-1054,0,0.0160407,"t studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We"
2021.acl-short.46,2020.emnlp-main.476,0,0.0283726,"target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2"
2021.acl-short.46,2020.emnlp-main.187,0,0.011521,"(Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectivel"
2021.acl-short.46,P02-1040,0,0.109171,"Missing"
2021.acl-short.46,D18-1039,0,0.0186647,"o-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve"
2021.acl-short.46,W18-6319,0,0.0200658,"Missing"
2021.acl-short.46,W18-6327,0,0.0163699,"dle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Concl"
2021.acl-short.46,P19-1297,0,0.022488,"specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast. Acknowledgments We thank anonymous reviewers for th"
2021.acl-short.46,2020.acl-main.252,0,0.0198946,"massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We pr"
2021.acl-short.46,2020.acl-main.324,0,0.0224391,"s (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast. Acknowledgments We thank anonymous reviewers for their insightful comm"
2021.acl-short.46,2020.emnlp-main.75,0,0.0340627,"NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO archit"
2021.acl-short.46,D19-1089,0,0.0421137,"Missing"
2021.acl-short.46,2020.acl-main.38,1,0.87325,"Missing"
2021.acl-short.46,tiedemann-2012-parallel,0,0.104965,"Missing"
2021.acl-short.46,2020.acl-main.754,0,0.0194131,"NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO archit"
2021.acl-short.46,D18-1326,0,0.0144444,"NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NM"
2021.acl-short.46,P19-1117,0,0.0175864,"5), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of"
2021.acl-short.46,2020.emnlp-main.77,0,0.0684984,"Missing"
2021.acl-short.46,2020.acl-main.148,0,0.213971,"ng3,4∗ DFKI and Saarland University, Informatics Campus, Saarland, Germany 2 China Mobile Online Services, Henan, China 3 Tianjin University, Tianjin, China 4 Global Tone Communication Technology Co., Ltd. {hfxunlp, liuqhano}@foxmail.com, josef.van genabith@dfki.de, dyxiong@tju.edu.cn 1 Abstract Despite their advantages, multilingual systems tend to underperform their bilingual counterparts as the number of languages increases (Johnson et al., 2017; Aharoni et al., 2019). This is due to the fact that multilingual NMT must distribute its modeling capacity over different translation directions. Zhang et al. (2020) show that the model capacity is crucial for massively multilingual NMT to support language pairs with varying typological characteristics, and propose to increase the modeling capacity by deepening the Transformer. However, compared to going deeper or wider, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective when we increase the model capacity (Xie et al., 2017). In this paper, we efficiently increase the capacity of the multilingual NMT model by increasing the cardinality, i.e. stacking sub-layers that aggregate a set of t"
2021.acl-short.46,2020.acl-main.150,0,0.0400861,"on is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenari"
2021.acl-short.46,N16-1004,0,0.0244862,"ads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020)."
2021.emnlp-demo.4,P16-1162,0,0.0310734,"ocessed, e.g. tokenized. Sentences to be translated need to be preprocessed in the same way. Also, the translation provided by the MT model might require postprocessing, e.g detokenization. With Marian, pre-/postprocessing often resorts to Perl scripts written for the Moses statistical MT system (Koehn et al., 2007). For TransIns, we use a Python reimplementation provided by Sacremoses8 . Transformer MT models often apply subword tokenization in preprocessing. In postprocessing, subword tokenization has to be undone in the translated sentence. For transformer models, Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and SentencePiece (Kudo and Richardson, 2018) are popular subword tokenizers. We use publicly available implementations of both subword tokenizers.9,10 Undoing the subword tokenization in the translated sentence in postprocessing is straightforward by applying simple string replacements. TransIns wraps the steps described above in a web service that provides corresponding endpoints for pre-/postprocessing single sentences or batches. The steps to apply can be configured separately for each translation direction. The Marian connector for Okapi calls this web service to preprocess a sentence be"
2021.emnlp-demo.4,2010.eamt-1.23,0,0.0188257,"nabith@dfki.de Abstract Furthermore, MT has to be able to handle inline sentence markup, i.e. to make sure that markup in the source sentence is correctly transferred to the appropriate parts of the target sentence. It is possible to train markup-aware MT models, e.g. by replacing tags with unique mask tokens in training and translation, as described in (Zhechev and van Genabith, 2010), but in order to use an existing MT model that is unaware of markup, the only option is to remove markup from the source sentence and to reinsert it at proper positions in the target sentence after translation. Du et al. (2010) describe a reinsertion strategy based on the phrase segmentation indicated by the decoder. This is refined by Hudik and Ruopp (2011) who use word alignments instead of phrase segmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows t"
2021.emnlp-demo.4,2020.eamt-1.61,0,0.0265459,"gmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows transformer models to be trained using guided alignment so that the decoder produces translations together with alignments between source and target tokens. The OPUS-MT3 project (Tiedemann and Thottingal, 2020) provides pre-trained Marian models for many language pairs, mostly trained with guided alignment based on eflomal4 word alignments (Östling and Tiedemann, 2016), but unaware of markup. Below, we describe TransIns, a translator for non-plain text documents. We use the Okapi5 framework to process such documents and extend For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans cont"
2021.emnlp-demo.4,2011.eamt-1.9,0,0.0636223,"ource sentence is correctly transferred to the appropriate parts of the target sentence. It is possible to train markup-aware MT models, e.g. by replacing tags with unique mask tokens in training and translation, as described in (Zhechev and van Genabith, 2010), but in order to use an existing MT model that is unaware of markup, the only option is to remove markup from the source sentence and to reinsert it at proper positions in the target sentence after translation. Du et al. (2010) describe a reinsertion strategy based on the phrase segmentation indicated by the decoder. This is refined by Hudik and Ruopp (2011) who use word alignments instead of phrase segmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows transformer models to be trained using guided alignment so that the decoder produces translations together with alignments between sou"
2021.emnlp-demo.4,W10-3806,1,0.716792,"i.de Abstract Furthermore, MT has to be able to handle inline sentence markup, i.e. to make sure that markup in the source sentence is correctly transferred to the appropriate parts of the target sentence. It is possible to train markup-aware MT models, e.g. by replacing tags with unique mask tokens in training and translation, as described in (Zhechev and van Genabith, 2010), but in order to use an existing MT model that is unaware of markup, the only option is to remove markup from the source sentence and to reinsert it at proper positions in the target sentence after translation. Du et al. (2010) describe a reinsertion strategy based on the phrase segmentation indicated by the decoder. This is refined by Hudik and Ruopp (2011) who use word alignments instead of phrase segmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows t"
2021.emnlp-demo.4,2013.mtsummit-wptp.9,0,0.0573531,"sentence. It is possible to train markup-aware MT models, e.g. by replacing tags with unique mask tokens in training and translation, as described in (Zhechev and van Genabith, 2010), but in order to use an existing MT model that is unaware of markup, the only option is to remove markup from the source sentence and to reinsert it at proper positions in the target sentence after translation. Du et al. (2010) describe a reinsertion strategy based on the phrase segmentation indicated by the decoder. This is refined by Hudik and Ruopp (2011) who use word alignments instead of phrase segmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows transformer models to be trained using guided alignment so that the decoder produces translations together with alignments between source and target tokens. The OPUS-MT3 project (Tiedemann and Thottingal, 2020)"
2021.emnlp-demo.4,P18-4020,0,0.0315578,"Missing"
2021.emnlp-demo.4,P07-2045,0,0.0136296,"can be run remotely, supporting distributed setups with multiple Marian servers. In order to use Marian as a translation service from the leveraging step, we provide a Marian connector that implements the Okapi connector interface. Most MT models are trained on parallel data where sentences are preprocessed, e.g. tokenized. Sentences to be translated need to be preprocessed in the same way. Also, the translation provided by the MT model might require postprocessing, e.g detokenization. With Marian, pre-/postprocessing often resorts to Perl scripts written for the Moses statistical MT system (Koehn et al., 2007). For TransIns, we use a Python reimplementation provided by Sacremoses8 . Transformer MT models often apply subword tokenization in preprocessing. In postprocessing, subword tokenization has to be undone in the translated sentence. For transformer models, Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and SentencePiece (Kudo and Richardson, 2018) are popular subword tokenizers. We use publicly available implementations of both subword tokenizers.9,10 Undoing the subword tokenization in the translated sentence in postprocessing is straightforward by applying simple string replacements. Trans"
2021.emnlp-demo.4,D18-2012,0,0.0225068,"ranslated need to be preprocessed in the same way. Also, the translation provided by the MT model might require postprocessing, e.g detokenization. With Marian, pre-/postprocessing often resorts to Perl scripts written for the Moses statistical MT system (Koehn et al., 2007). For TransIns, we use a Python reimplementation provided by Sacremoses8 . Transformer MT models often apply subword tokenization in preprocessing. In postprocessing, subword tokenization has to be undone in the translated sentence. For transformer models, Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and SentencePiece (Kudo and Richardson, 2018) are popular subword tokenizers. We use publicly available implementations of both subword tokenizers.9,10 Undoing the subword tokenization in the translated sentence in postprocessing is straightforward by applying simple string replacements. TransIns wraps the steps described above in a web service that provides corresponding endpoints for pre-/postprocessing single sentences or batches. The steps to apply can be configured separately for each translation direction. The Marian connector for Okapi calls this web service to preprocess a sentence before translation and again afterwards to postp"
2021.emnlp-demo.4,W17-4804,0,0.0962143,"nd translation, as described in (Zhechev and van Genabith, 2010), but in order to use an existing MT model that is unaware of markup, the only option is to remove markup from the source sentence and to reinsert it at proper positions in the target sentence after translation. Du et al. (2010) describe a reinsertion strategy based on the phrase segmentation indicated by the decoder. This is refined by Hudik and Ruopp (2011) who use word alignments instead of phrase segmentation. Joanis et al. (2013) propose a hybrid approach combining phrase segmentation with word alignments. Building on these, Müller (2017) evaluates different markup handling strategies and provides implementations as part of the Zurich NLP mtrain1 framework. In order to utilize state-of-the-art MT technology and obtain alignments at the same time, we use Marian2 NMT (Junczys-Dowmunt et al., 2018). Marian allows transformer models to be trained using guided alignment so that the decoder produces translations together with alignments between source and target tokens. The OPUS-MT3 project (Tiedemann and Thottingal, 2020) provides pre-trained Marian models for many language pairs, mostly trained with guided alignment based on eflom"
2021.emnlp-main.676,2021.motra-1.1,1,0.79007,"Missing"
2021.emnlp-main.676,2020.iwslt-1.34,1,0.834671,"Missing"
2021.emnlp-main.676,J19-2006,0,0.0464818,"Missing"
2021.emnlp-main.676,P11-1132,0,0.0887813,"Missing"
2021.emnlp-main.676,2009.mtsummit-papers.9,0,0.219741,"Missing"
2021.emnlp-main.676,L16-1664,0,0.0285148,"Missing"
2021.emnlp-main.676,R11-1091,0,0.0837069,"Missing"
2021.emnlp-main.676,P17-1049,0,0.0443256,"Missing"
2021.emnlp-main.676,Q15-1030,0,0.0191601,"–ALL 92.4±0.2 82.6±1.1 87.3±0.6 - 76.6±0.7 94.4±0.1 85.3±0.4 - 90.7±0.1 72.9±0.9 81.7±0.5 - 64.7±1.4 91.9±0.4 78.3±0.7 - 92.3±0.2 78.8±1.6 85.9±0.9 78.8±0.9 91.4±0.3 85.0±0.6 90.5±0.3 91.8±0.4 90.9±0.3 - 87.3±0.4 88.6±0.4 87.9±0.4 - 90.6±0.1 89.0±0.2 89.9±0.1 Table 3: BERT translationese classification accuracy of all TRG–SRC and TRG–ALL models on TRG–SRC and TRG–ALL test sets (average and standard deviation over 5 runs). Columns: training set; rows: test set. e.g., trail BERT by ∼20 accuracy points. To make sure our hand-crafted-feature-based SVM results are competitive, we compare them with Rabinovich and Wintner (2015) on our data. Rabinovich and Wintner (2015) show that training a SVM classifier on the top 1000 most frequent POS- or character-trigrams yields SOTA translationese classification results on Europarl data. On our data, POS-trigrams yield around 5 points increase in accuracy for most of the datasets and character-trigrams tend to lower the accuracy by around 4 points (Appendix A.3). For the remainder of the paper we continue to work with our handcrafted features, designed to capture various linguistic aspects of translationese. 5 Multilinguality and Cross-Language Performance Since neural archit"
2021.emnlp-main.676,2020.acl-main.691,0,0.0734577,"Missing"
2021.emnlp-main.676,N16-1110,1,0.899056,"Missing"
2021.emnlp-main.676,R19-1130,0,0.0272166,"Missing"
2021.emnlp-main.676,W17-0230,0,0.0459553,"Missing"
2021.emnlp-main.799,2020.acl-demos.37,1,0.827104,"Missing"
2021.emnlp-main.799,2020.amta-pemdt.7,1,0.842247,"Missing"
2021.emnlp-main.799,2021.acl-long.527,1,0.83096,"Missing"
2021.emnlp-main.799,W16-2378,0,0.0403599,"Missing"
2021.emnlp-main.799,W19-5406,0,0.0244811,"Missing"
2021.emnlp-main.799,P19-3020,0,0.0345172,"Missing"
2021.emnlp-main.799,P17-4012,0,0.0679099,"Missing"
2021.emnlp-main.799,2020.wmt-1.118,0,0.0567329,"Missing"
2021.emnlp-main.799,N19-4009,0,0.0171469,"e post-edited version of the translation to the reference. TER was used because of its popularity as an automatic translation quality metric, and because it counts edit operations and therefore reflects PE effort. Naturally, we also store the QE model and visualization used per segment to compare the different conditions. 4.2 Text Selection In line with previous recent QE research, we used data from the training set of the WMT 2020 QE shared task (Specia et al., 2020), which relies on an up-to-date NMT model based on an attentional encoder-decoder architecture built using the fairseq toolkit (Ott et al., 2019). The training set consists of source, MT, reference, MT quality scores, tags, and source-MT alignment information. We used a common language pair (namely EN-DE) and selected text from a general domain (namely Wikipedia) to ease participant recruitment as it does not require specific domain expertise. In order to understand whether QE helps more or less with different MT quality, we selected sentences with low, medium and high MT quality for different sentence lengths. The selection of the text segments follows the following steps: short sentences, 15 medium length sentences, and 8 long senten"
2021.emnlp-main.799,P02-1040,0,0.111759,"Missing"
2021.emnlp-main.799,W19-6738,0,0.0320876,"Missing"
2021.emnlp-main.799,2009.mtsummit-btm.8,0,0.205929,"Missing"
2021.emnlp-main.799,2006.amta-papers.25,0,0.541093,"and flag potential errors. can save time and reduce errors. Automatic Quality Estimation (QE) focuses on this goal by word-level quality estimation (QE) aims to prepredicting how good MT output is based on factors dict the correctness of words in MT output and holds great promise to aid PE by flagsuch as fluency, comprehensibility, and adequacy ging problematic output. Quality of QE is (Moorkens et al., 2018). In contrast to standard crucial, as incorrect QE might lead to translaMT evaluation methods such as BLEU (Papineni tors missing errors or wasting time on already et al., 2002) and TER (Snover et al., 2006), QE correct MT output. Achieving accurate autodoes not rely on reference translations for comparmatic word-level QE is very hard, and it is curison. Instead, QE can be framed as a supervised rently not known (i) at what quality threshold machine learning task, predicting the quality of QE is actually beginning to be useful for human PE, and (ii), how to best present wordan MT output for a source sentence (Turchi et al., level QE information to translators. In partic2013). QE models can be created at the wordular, should word-level QE visualization indilevel, sentence-level, and document-level"
2021.emnlp-main.799,P14-1067,0,0.0453085,"Missing"
2021.emnlp-main.799,W13-2231,0,0.0750623,"Missing"
2021.emnlp-main.799,P15-2087,0,0.0399518,"Missing"
2021.emnlp-main.799,W18-6465,0,0.0367117,"Missing"
2021.findings-emnlp.67,P19-1285,0,0.0562935,"Missing"
2021.findings-emnlp.67,2020.acl-main.269,0,0.0157973,"iativity property of matrix products. Wang et al. (2020) approximate the self-attention mechanism by a low-rank matrix. Beltagy et al. (2020) introduce an attention mechanism that scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43"
2021.findings-emnlp.67,P19-1290,0,0.017011,"et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]3101, 201807040056). Josef van Genabith and Hon"
2021.findings-emnlp.67,D15-1166,0,0.0624278,"standard residuals. Zhang et al. (2020) propose a dimension-wise attention mechanism to reduce the attention complexity. Katharopoulos et al. (2020) express the selfattention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products. Wang et al. (2020) approximate the self-attention mechanism by a low-rank matrix. Beltagy et al. (2020) introduce an attention mechanism that scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the l"
2021.findings-emnlp.67,P18-2059,0,0.0427911,"Missing"
2021.findings-emnlp.67,N19-1313,0,0.0213063,"e sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]3101, 201807040056). Josef van Genabith and Hongfei Xu are supported"
2021.findings-emnlp.67,P19-1146,0,0.0120926,"scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]31"
2021.findings-emnlp.67,P16-1162,0,0.124024,"Missing"
2021.findings-emnlp.67,2020.acl-main.38,1,0.893889,"Missing"
2021.findings-emnlp.67,2021.acl-long.23,1,0.827296,"Missing"
2021.findings-emnlp.67,2020.acl-main.323,1,0.895535,"Missing"
2021.findings-emnlp.67,D19-1083,0,0.040009,"Missing"
2021.findings-emnlp.67,P18-1166,1,0.83164,", using the same setting of the Transformer Base as on the WMT 14 En-De task. Results are shown in Table 5. Table 5 shows that hard retrieval attention is able to match the performance in all tested language pairs in both translation directions, with training sets ranging from 0.2M to 52.02M sentence pairs. The largest performance loss (−0.26 BLEU) is on the Cs-En task. decoder or both encoder and decoder. Results are shown in Table 3. Table 3 shows that applying the hard retrieval attention mechanism to encoder self-attention net- 5 Related Work works significantly hampers performance. We conZhang et al. (2018) accelerate the decoder selfjecture potential reasons might be: 1) the encoder might be harder to train than the decoder as its gra- attention with the average attention network. Xu et al. (2021) propose to replace the self-attention dients come from cross-attention networks while layer by multi-head highly parallelized LSTM. Kim the decoder receives more direct supervision from the classifier, and the hard attention training ap- et al. (2019) investigate knowledge distillation and proach makes the encoder’s training even harder. quantization for faster NMT decoding. Tay et al. (2021) investig"
2021.motra-1.1,cartoni-meyer-2012-extracting,0,0.144395,"gned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the information about the Members of the European Parliament (MEPs) from the European Parliament’s website in or"
2021.motra-1.1,islam-mehler-2012-customization,0,0.0167292,"rpus from TED talks, annotated for translation direction. They also provide aligned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the information about the Member"
2021.motra-1.1,2005.mtsummit-papers.11,0,0.603632,"lish–French corpus from TED talks, annotated for translation direction. They also provide aligned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the inf"
2021.motra-1.1,P11-1132,0,0.266496,"some examples. It is also useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that"
2021.motra-1.1,D11-1034,0,0.181291,"original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that compare translations coming from different source lang"
2021.motra-1.1,L16-1664,0,0.671741,"ource language patterns in target text (Toury, 1979; Teich, 2003). In order to be successfully used for studying translationese phenomena, corpora need to be equipped with additional meta-information: whether the text is original or translated, the direction of translation, production mode of the source text (spoken/written) to give some examples. It is also useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”br"
2021.motra-1.1,Q15-1030,0,0.54467,"so useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that compare translations coming f"
2021.motra-1.1,N16-1110,1,0.77393,"Missing"
2021.motra-1.1,L16-1561,0,0.0274017,"nised as follows. Section 2 presents previous work done on building corpora for translationese research, and, in particular, corpora based on the proceedings of the European Parliament. Section 3 describes the procedure of creating the corpora. In Section 4, we compare the ”reliable” and ”unreliable” parts of the corpus on the task of translationese classification. Lastly, in Section 5 we present our conclusions and ideas for future work. 2 Related Work 2.1 Data available for translationese research There are only a few multilingual corpora for translationese research. The UN parallel corpus (Ziemski et al., 2016) consists of multilingual parliamentary documents of the United Nations in 6 languages, organized into bilingual parallel corpora. From this corpus Tolochinsky et al. (2018) derived 5 parallel corpora from English into other languages and annotated them for translation direction. The Canadian Hansard corpus1 consists of transcriptions of the Canadian parliament in English and French and their translations, and has metadata indicating the original language. Rabinovich et al. (2015) compile a parallel English–French corpus from TED talks, annotated for translation direction. They also provide al"
2021.mtsummit-research.7,P19-1310,0,0.168016,"Missing"
2021.mtsummit-research.7,P17-1042,0,0.017738,"e monolingual Wikipedias to initialize SSNMT. As the monolingual Wikipedia for Yor`ub´a is especially small (65 k sentences), we use the Yor`ub´a side of JW300 (Agi´c and Vuli´c, 2019) as additional monolingual initialization data. For each monolingual data pair en–{af ,...,yo}, the large English monolingual corpus is downsampled to its low(er)-resource counterpart before using the data (Monolingual in Table 1). For the word-embedding-based initialization, we learn CBOW word embeddings using word2vec (Mikolov et al., 2013), which are then projected into a common multilingual space via vecmap (Artetxe et al., 2017) to attain bilingual embeddings between en–{af ,...,yo}. For the weak-supervision of the bilingual mapping process, we use a list of numbers (en–f r only) which is augmented with 200 Swadesh list4 entries for the low-resource experiments. For DAE initialization, we do not use external, highly-multilingual pre-trained language models, since in practical terms these may not cover the language combination of interest5 . We therefore use the monolingual data to train a bilingual (en+{af ,...yo}) DAE using BART-style 1 Dumps were downloaded on February 2021 from dumps.wikimedia.org/ 2 github.com/at"
2021.mtsummit-research.7,P19-1019,0,0.214691,"g autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili). 1 Introduction Neural machine translation (NMT) achieves high quality translations when large amounts of parallel data are available (Barrault et al., 2020). Unfortunately, for most language combinations, parallel data is non-existent, scarce or low-quality. To overcome this, unsupervised MT (UMT) (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019) focuses on exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et"
2021.mtsummit-research.7,D18-1549,0,0.230264,"al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on smal"
2021.mtsummit-research.7,P19-1309,0,0.372739,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,Q19-1038,0,0.168425,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,P06-4018,0,0.0228013,"5) for word sequence masking. We add one random mask insertion per sequence and perform a sequence permutation. For the multilingual DAE (MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardso"
2021.mtsummit-research.7,W11-2138,0,0.0261784,"., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional"
2021.mtsummit-research.7,W19-5435,0,0.0270045,"Missing"
2021.mtsummit-research.7,2020.acl-main.747,0,0.0744637,"Missing"
2021.mtsummit-research.7,W17-4715,0,0.0387347,"ed on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been us"
2021.mtsummit-research.7,2020.eamt-1.10,0,0.073161,"Missing"
2021.mtsummit-research.7,D18-1045,0,0.0130344,"d translation. Given a rejected sentence sL1 with tokens wL1 ∈ L1, we replace each token with its nearest neighbor wL2 ∈ L2 in the bilingual word embedding layer of the T WT model to obtain sW L2 . We then train on the synthetic pair in the opposite direction sL2 → sL1 . As with BT, this is applied to both language directions. To ensure sufficient volume of synthetic data (Figure 2, right), WT data is trained on without filtering. Noise (N): To increase robustness and variance in the training data, we add noise, i.e. token deletion, substitution and permutation, to copies of source sentences (Edunov et al., 2018) in parallel pairs identified via SPE, back-translations and word-translated sentences and, as with WT, we use these without additional filtering. Initialization: When languages are related and large amounts of training data is available, the initialization of SSNMT is not important. However, similarly to UMT, initialization becomes crucial in the low-resource setting (Edman et al., 2020). We explore four different initialization techniques: i) no initialization (none), i.e. random initialization for all model parameters, ii) initialization of tied source and target side word embedding layers"
2021.mtsummit-research.7,2020.emnlp-main.480,0,0.0278832,"Missing"
2021.mtsummit-research.7,W19-5315,1,0.881943,"Missing"
2021.mtsummit-research.7,D19-1632,0,0.147595,"Missing"
2021.mtsummit-research.7,W18-2703,0,0.0179433,"amachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results ("
2021.mtsummit-research.7,Q17-1024,0,0.229862,"Missing"
2021.mtsummit-research.7,2020.eamt-1.5,0,0.0358621,"Missing"
2021.mtsummit-research.7,W04-3250,0,0.0618963,"Missing"
2021.mtsummit-research.7,P07-2045,0,0.010509,"(MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruite"
2021.mtsummit-research.7,2021.dravidianlangtech-1.7,0,0.0417202,"n be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate from comparable data and works best on 100s of thousands of documents per language, well beyond what is available in true low-resource settings. 3 UMT-Enhanced SSNMT SSNMT jointly learns MT and extracting similar sentences for training from comparable corpora in a loop on-line. Sentence pairs from documents in languages L1 and L2 are fed as input to a bidirectional NMT system {L1, L2} → {L1, L2}, which filters out non-simila"
2021.mtsummit-research.7,D18-2012,0,0.0218435,"NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruiter et al. (2020) in order to minimize experimental differences for later comparison. 4.3 Model Specifications and Evaluation Systems are either not initialized, initialized via bilingual word embeddings, or via pre-training using (M)DAE. Our implementation of SSNMT is a transformer base with default parameters. We use a batch size of 50 sentences and a maximum sequence length of 100 tokens. For evaluation, we use BLEU (Papineni et al., 2002) calculated using Sa"
2021.mtsummit-research.7,P19-1017,0,0.0159691,"rocessing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several la"
2021.mtsummit-research.7,2020.findings-emnlp.371,0,0.0206148,". NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate fr"
2021.mtsummit-research.7,E17-2002,0,0.0448202,"Missing"
2021.mtsummit-research.7,2020.wmt-1.68,0,0.0126971,"exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et al., 2019; Marchisio et al., 2020). To date, UMT data augmentation techniques have not been explored in SSNMT. However, both approaches can benefit from each other, as i) SSNMT has strong internal quality checks on the data it admits for training, which can be Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 76 of use to filter low-quality synthetic data, and ii) UMT data augmentation makes monolingual data available for SSNMT. In this paper we explore and test the effect of combining UMT data augmentation with SSNMT on different data sizes, ranging"
2021.mtsummit-research.7,W18-2710,0,0.018043,"high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). Ho"
2021.mtsummit-research.7,P02-1040,0,0.109861,"Missing"
2021.mtsummit-research.7,W18-6319,0,0.022108,"Missing"
2021.mtsummit-research.7,D17-1039,0,0.161776,"word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and b"
2021.mtsummit-research.7,P19-1178,1,0.848055,"Missing"
2021.mtsummit-research.7,2020.emnlp-main.202,1,0.852975,"Missing"
2021.mtsummit-research.7,2021.eacl-main.115,0,0.354207,"Missing"
2021.mtsummit-research.7,P16-1009,0,0.0936491,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P16-1162,0,0.546366,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P18-1005,0,0.0178722,"zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolin"
2021.mtsummit-research.7,D16-1163,0,0.0451399,"iques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et"
2021.naacl-main.7,2020.cl-1.1,0,0.0203607,"Deep Encoder and Shallow Decoder on other Language Pairs To investigate how a deep encoder with a shallow decoder will perform in other tasks, we conducted experiments on the WMT 14 English-French and WMT 15 Czech-English news translation tasks in addition to the WMT 14 English-German task. Results on newstest 2014 (En-De/Fr) and 2015 (CsEn) respectively are shown in Table 6. Table 6 shows that the 10-2 model consistently achieves higher BLEU scores than the 6-layer model, and the 18-4 model consistently leads to significant improvements in all 3 tasks. 5 Related Work Analysis of NMT Models. Belinkov et al. (2020) analyze the representations learned by NMT models at various levels of granularity and evaluate their quality through relevant extrinsic properties. Li et al. (2019a) analyze the word alignment quality in NMT and the effect of alignment errors on translation errors. They demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) pr"
2021.naacl-main.7,D18-1313,0,0.0209431,"components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al. (2019a) use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers. Early work by Bisazza and Tump (2018) performs a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder. While they are unable to find any correlation between the accuracy of source morphology encoding and translation quality, they discover that morphological features are only captured in context and only to the extent that they are directly transferable to the target words, and suggest encoder layers are “lazy”. Our analysis offers an explanation for their results as the translation already starts at the source embedding layer, and possibly source embeddings al"
2021.naacl-main.7,D19-1275,0,0.0304929,"Missing"
2021.naacl-main.7,P18-1198,0,0.0271826,"on already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (EnDe) at a speed-up of 1.4. 1 To investigate the roles of Transformer layers in translation, in this paper, we adopt probing approaches (Adi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018) and propose to measure the word translation accuracy of output representations of individual Transformer layers by probing how capable they are at translating words. Probing uses linear classifiers, referred to as “probes”, where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training (Alain and Bengio, 2017). In addition to analyzing the role of each encoder/decoder layer, we also analyze the contribution of the source context and the decodin"
2021.naacl-main.7,N19-1423,0,0.0374997,"y demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic role"
2021.naacl-main.7,P19-1356,0,0.0176912,"t. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability"
2021.naacl-main.7,N13-1073,0,0.00959033,"ation quality of the transformer. Motivations targeting efficiency include: Discussion Our probing approach involves crucial information from the decoder (encoder-decoder attention from all decoder layers). However, we argue that probe training requires supervision. For the decoder, we can directly use gold references. On the encoder side, parallel data does not provide word translations for source tokens, and we have to generate this data by aligning target tokens to source tokens. One choice is extracting alignments by taking an argmax of alignment matrices or using toolkits like fastalign (Dyer et al., 2013). In this case, probe training does not involve attention matrices, but this has drawbacks: multiple/no target tokens may align to one source token. We use soft aggregation to preserve more information (other attention possibilities besides the highest are kept) and to alleviate error propagation. We argue that the use of attention matrices is only to bring supervision (word translations) from the target side to the source side, which is inevitable. Decoder representations cannot flow back to the frozen encoder. Our paper also empirically reveals the impact of attention matrices: 1) In Section"
2021.naacl-main.7,P19-1124,0,0.312501,"Ad∗k . We use a d ∗ k dimension weight vector w to combine all attention matrices. The weight vector is normalized by softmax to a probability distribution p: pi = ewi d∗k P 2.2 The analysis of the prediction accuracy of the decoder is simpler than the encoder, as we can directly use the shifted target sequence (teacher forcing) without the requirement to bridge different sequence lengths between the source sentence and the target while analyzing the encoder. We use the output representations of the analyzed layer, and evaluate its prediction accuracy after projection. However, as studied by Li et al. (2019a), the decoder involves two kinds of “translation”. One (performed by the self-attention sub-layer) translates the history token sequence to the next token, another (performed by the cross-attention sub-layer) translates by attending source tokens. We additionally analyze the effects of these two kinds of translation on predicting accuracy by dropping the corresponding sub-layer (either cross- or masked self-attention) of the analyzed decoder layer (i.e., we only compute the other sub-layer and the feedforward layer where only the residual connection is kept as the computation of the skipped"
2021.naacl-main.7,D19-1573,0,0.0635857,"Missing"
2021.naacl-main.7,D18-1336,0,0.0238054,"er decoder to accelerate decoding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models"
2021.naacl-main.7,P19-1493,0,0.0211164,"encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability to generalize cross-lingually is underpinned by a multilingual representation. 81 Accelerating Decoding. Zhang et al. (2018a) propose average attention as an alternative to the self-attention network in the Transformer decoder to accelerate decoding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural"
2021.naacl-main.7,D19-1088,0,0.0206476,"relatively small. 80 Depth Encoder Decoder 6 10 18 2 4 En-De En-Fr Cs-En 27.96 28.47 29.38† 40.13 40.49 40.90† 28.69 28.87 29.75† learning objectives matter more in the downstream tasks such as machine translation. Tsai et al. (2019) regard attention as applying a kernel smoother over the inputs with the kernel scores being the similarities between inputs, and analyze individual components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al. (2019a) use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers. Early work by Bisazza and Tump (2018) performs a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder. While they are unable to find any correlation between the accuracy of sou"
2021.naacl-main.7,P16-1009,0,0.125855,"6 -15.12 -8.70 -0.79 Table 2: Word translation accuracy of Transformer layers on the WMT 15 Cs-En task. BLEU score of 27.96 on the test set. The projection matrix and the weight vector w of 48 elements for alignment were trained on the training set with the frozen Transformer. We monitored the accuracy on the development set, and report results on the test set. straint (Xu et al., 2020) to ensure the convergence of deep encoders. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We only kept sentences with a maximum of 256 sub-word tokens for training. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as the test set. The number of warm-up steps was set to 8k.1 The model was trained for 100k training steps with around 25k target tokens in each batch. We followed all the other settings of Vaswani et al. (2017). We averaged the last 5 checkpoints saved with an interval of 1, 500 training steps. For decoding, we used a beam size of 4, and evaluated tokenized casesensitive BLEU.2 The averaged model"
2021.naacl-main.7,P19-1580,0,0.320373,"lp@foxmail.com, Josef.Van_Genabith@dfki.de, liuqhano@foxmail.com, dyxiong@tju.edu.cn 1 Abstract Recently, a wide range of studies related to the Transformer have been conducted. For example, Bisazza and Tump (2018) perform a fine-grained analysis of how various source-side morphological features are captured at different levels of an NMT encoder. Surprisingly, they do not find any correlation between the accuracy of source morphology encoding and translation quality. Morphological features are only captured in context and only to the extent that they are directly transferable to target words. Voita et al. (2019a) study how information flows across Transformer layers and find that representations differ significantly depending on the objectives (machine translation, standard left-toright language models and masked language modeling). Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. However, to the best of our knowledge, to date there is no study about how the Transformer translation model transforms individual source tokens into corresponding target tokens (i.e., word translations), and specifically, which role each Transformer"
2021.naacl-main.7,P16-1162,0,0.211787,"6 -15.12 -8.70 -0.79 Table 2: Word translation accuracy of Transformer layers on the WMT 15 Cs-En task. BLEU score of 27.96 on the test set. The projection matrix and the weight vector w of 48 elements for alignment were trained on the training set with the frozen Transformer. We monitored the accuracy on the development set, and report results on the test set. straint (Xu et al., 2020) to ensure the convergence of deep encoders. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We only kept sentences with a maximum of 256 sub-word tokens for training. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as the test set. The number of warm-up steps was set to 8k.1 The model was trained for 100k training steps with around 25k target tokens in each batch. We followed all the other settings of Vaswani et al. (2017). We averaged the last 5 checkpoints saved with an interval of 1, 500 training steps. For decoding, we used a beam size of 4, and evaluated tokenized casesensitive BLEU.2 The averaged model"
2021.naacl-main.7,2020.emnlp-main.14,0,0.0316272,"Missing"
2021.naacl-main.7,P19-1288,0,0.0199281,"(2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models. Our work explains why using a deep"
2021.naacl-main.7,D19-1149,0,0.0302126,"Missing"
2021.naacl-main.7,P19-1125,0,0.0181091,"coding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models. Our work explain"
2021.naacl-main.7,P19-1452,0,0.0250068,"pose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability to generalize cross-lingually is underpinned by a multilingual representation. 81 Accelerating Decoding. Zhang et al. (2018a) propose average attention as an alternative to the self-attention n"
2021.naacl-main.7,D19-1443,0,0.0282376,"han those of the 6-layer encoder (1.90 vs. 0.87), indi4 A full grid search over configurations is tedious and expensive. We take inspiration from Table 4 where going from 5 to 4 decoder layers brings about the biggest relative jump in translation quality. We explored a few configurations and find that using more than 18 encoder layers can still bring improvements, but the gains are relatively small. 80 Depth Encoder Decoder 6 10 18 2 4 En-De En-Fr Cs-En 27.96 28.47 29.38† 40.13 40.49 40.90† 28.69 28.87 29.75† learning objectives matter more in the downstream tasks such as machine translation. Tsai et al. (2019) regard attention as applying a kernel smoother over the inputs with the kernel scores being the similarities between inputs, and analyze individual components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al."
2021.naacl-main.7,2020.acl-main.38,1,0.781729,"Missing"
2021.naacl-main.7,P19-1354,0,0.0202659,"elinkov et al. (2020) analyze the representations learned by NMT models at various levels of granularity and evaluate their quality through relevant extrinsic properties. Li et al. (2019a) analyze the word alignment quality in NMT and the effect of alignment errors on translation errors. They demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BE"
2021.naacl-main.7,D19-1448,0,0.212255,"lp@foxmail.com, Josef.Van_Genabith@dfki.de, liuqhano@foxmail.com, dyxiong@tju.edu.cn 1 Abstract Recently, a wide range of studies related to the Transformer have been conducted. For example, Bisazza and Tump (2018) perform a fine-grained analysis of how various source-side morphological features are captured at different levels of an NMT encoder. Surprisingly, they do not find any correlation between the accuracy of source morphology encoding and translation quality. Morphological features are only captured in context and only to the extent that they are directly transferable to target words. Voita et al. (2019a) study how information flows across Transformer layers and find that representations differ significantly depending on the objectives (machine translation, standard left-toright language models and masked language modeling). Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. However, to the best of our knowledge, to date there is no study about how the Transformer translation model transforms individual source tokens into corresponding target tokens (i.e., word translations), and specifically, which role each Transformer"
2021.naacl-main.7,W18-5448,0,0.0619539,"Missing"
2021.naacl-main.7,D18-1460,0,0.15958,"s and Analysis We examine the effects of reducing the number of decoder layers while adding corresponding numbers of encoder layers, and results are shown in Table 4. “Speed up” stands for the decoding acceleration compared to the 6-layer Transformer. Table 4 shows that while the acceleration of trading decoder layers for encoder layers in training is small, in decoding it is significant. Specifically, the 3 Since there is no re-ordering of the target language performed, which makes the merging of translated sub-word units in the source sentence order pointless. 79 Model Depth Encoder Decoder Zhang et al. (2018a) Transformer BLEU Para. (M) Train Time Decode (/s) Speed up 6 6 28.13 74.97 40h09m 29 1.52 6 7 8 9 10 11 6 5 4 3 2 1 27.96 28.07 28.61 28.53 28.47 27.02 62.37 61.32 60.27 59.22 58.17 57.12 33h33m 32h17m 31h26m 30h29m 30h11m 29h27m 44 38 31 25 19 13 1.00 1.16 1.42 1.76 2.32 3.38 18 4 29.38 91.77 52h56m 32 1.38 Table 4: Effects of encoder/decoder depth on the WMT 14 En-De task. The decoding time is for the test set of 3, 003 sentences with a beam size of 4. Encoder Layer 0 1 2 3 4 5 6 7 8 9 10 Acc 40.48 41.29 43.00 44.07 45.86 46.54 47.46 48.92 49.58 50.24 50.35 ∆ 0.81 1.71 1.07 1.79 0.68 0.92"
2021.naacl-main.7,D18-1511,0,0.159935,"s and Analysis We examine the effects of reducing the number of decoder layers while adding corresponding numbers of encoder layers, and results are shown in Table 4. “Speed up” stands for the decoding acceleration compared to the 6-layer Transformer. Table 4 shows that while the acceleration of trading decoder layers for encoder layers in training is small, in decoding it is significant. Specifically, the 3 Since there is no re-ordering of the target language performed, which makes the merging of translated sub-word units in the source sentence order pointless. 79 Model Depth Encoder Decoder Zhang et al. (2018a) Transformer BLEU Para. (M) Train Time Decode (/s) Speed up 6 6 28.13 74.97 40h09m 29 1.52 6 7 8 9 10 11 6 5 4 3 2 1 27.96 28.07 28.61 28.53 28.47 27.02 62.37 61.32 60.27 59.22 58.17 57.12 33h33m 32h17m 31h26m 30h29m 30h11m 29h27m 44 38 31 25 19 13 1.00 1.16 1.42 1.76 2.32 3.38 18 4 29.38 91.77 52h56m 32 1.38 Table 4: Effects of encoder/decoder depth on the WMT 14 En-De task. The decoding time is for the test set of 3, 003 sentences with a beam size of 4. Encoder Layer 0 1 2 3 4 5 6 7 8 9 10 Acc 40.48 41.29 43.00 44.07 45.86 46.54 47.46 48.92 49.58 50.24 50.35 ∆ 0.81 1.71 1.07 1.79 0.68 0.92"
attia-etal-2010-automatically,W98-1002,0,\N,Missing
attia-etal-2010-automatically,D08-1030,0,\N,Missing
attia-etal-2010-automatically,W05-0711,0,\N,Missing
attia-etal-2010-automatically,P05-1071,0,\N,Missing
attia-etal-2010-automatically,2007.jeptalnrecital-poster.13,0,\N,Missing
attia-etal-2010-automatically,elkateb-etal-2006-building,0,\N,Missing
attia-etal-2010-automatically,farber-etal-2008-improving,0,\N,Missing
attia-etal-2012-automatic,schluter-van-genabith-2008-treebank,1,\N,Missing
attia-etal-2012-automatic,W04-1602,0,\N,Missing
attia-etal-2012-automatic,J08-1003,1,\N,Missing
attia-etal-2012-automatic,W09-0806,1,\N,Missing
attia-etal-2012-automatic,J05-3003,1,\N,Missing
attia-etal-2012-automatic,P04-1041,1,\N,Missing
attia-etal-2012-automatic,P06-1055,0,\N,Missing
attia-etal-2012-automatic,P07-1031,0,\N,Missing
attia-etal-2012-automatic,W04-3224,0,\N,Missing
avramidis-etal-2012-richly,steinberger-etal-2006-jrc,0,\N,Missing
avramidis-etal-2012-richly,vandeghinste-etal-2008-evaluation,1,\N,Missing
avramidis-etal-2012-richly,W09-0424,0,\N,Missing
avramidis-etal-2012-richly,P07-2045,0,\N,Missing
avramidis-etal-2012-richly,C04-1072,0,\N,Missing
avramidis-etal-2012-richly,W08-0309,0,\N,Missing
avramidis-etal-2012-richly,2005.mtsummit-papers.11,0,\N,Missing
avramidis-etal-2012-richly,W10-1720,1,\N,Missing
avramidis-etal-2012-richly,P03-1021,0,\N,Missing
C08-1038,C00-1007,0,0.431987,"dels have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribu"
C08-1038,N07-1021,0,0.0277415,"egorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. These grammar rules are either carefully handcrafted, as those used in FUF/SURGE (Elhadad, 1991), LKB (Carroll et al., c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a"
C08-1038,P04-1041,1,0.894373,"Missing"
C08-1038,A00-2023,0,0.0601371,"in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, re"
C08-1038,P06-1130,1,0.92336,"Missing"
C08-1038,W02-2103,0,0.289707,"input to our generator are unordered fstructures automatically derived from the development and test set trees of our treebanks, which do not contain any string position information. But, due to the particulars of the automatic f-structure annotation algorithm, the order of sub-f-structures in set-valued GFs, such as ADJ, COORD, happens to correspond to their surface order. To avoid unfairly inflating evaluation results, we lexically reorder the GFs in each sub-f-structure of the development and test input before the generation process. This resembles the “permute, no dir” type experiment in (Langkilde, 2002). 5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics. As our system guarantees that all input fstructures can generate a complete sentence, special coverage-dependent evaluation (as has been Lexicalisation plays an important role in both English and Chinese, boosting the BLEU score without features from 0.5074 to 0.6741 for English, and from 0.5752 to 0.6639 for Chinese. Atomic-valued features"
C08-1038,W07-2303,0,0.0254705,"ct), OBJ(ect) and ADJ(unct). F-structures are hierarchical attribute2.2 Generation from F-Structures Work on generation in LFG generally assumes that the generation task is to determine the set of strings of the language that corresponds to a specified fstructure, given a particular grammar (Kaplan and Wedekind, 2000). Previous work on generation 2 F-structures can be also interpreted as quasi-logical forms (van Genabith and Crouch, 1996), which more closely resemble inputs used by some other generators. 298 within LFG includes the XLE,3 Cahill and van Genabith (2006), Hogan et al. (2007) and Cahill et al. (2007). The XLE generates sentences from fstructures according to parallel handcrafted grammars for English, French, German, Norwegian, Japanese, and Urdu. Based on the German XLE resources, Cahill et al. (2007) describe a two-stage, log-linear generation model. Cahill and van Genabith (2006) and Hogan et al. (2007) present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks (Cahill et al., 2004). 3 Dependency-Based Generation: the Basic Idea Traditional LFG generation models can be regarded as the reverse process of parsing, and use bi-direction"
C08-1038,J93-2004,0,0.0297399,"Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, su"
C08-1038,W05-1510,0,0.495799,"er carefully handcrafted, as those used in FUF/SURGE (Elhadad, 1991), LKB (Carroll et al., c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realis"
C08-1038,P02-1040,0,0.0784176,"which do not contain any string position information. But, due to the particulars of the automatic f-structure annotation algorithm, the order of sub-f-structures in set-valued GFs, such as ADJ, COORD, happens to correspond to their surface order. To avoid unfairly inflating evaluation results, we lexically reorder the GFs in each sub-f-structure of the development and test input before the generation process. This resembles the “permute, no dir” type experiment in (Langkilde, 2002). 5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics. As our system guarantees that all input fstructures can generate a complete sentence, special coverage-dependent evaluation (as has been Lexicalisation plays an important role in both English and Chinese, boosting the BLEU score without features from 0.5074 to 0.6741 for English, and from 0.5752 to 0.6639 for Chinese. Atomic-valued features play an important role in English, and boost the BLEU score from 0.5074 in the baseline model to 0.6842 when feature names are integra"
C08-1038,A00-2026,0,0.111311,"ls are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. 297 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297–304 Manchester, August 2008  S NP VP PRP VBP PP We believe IN in NP NP PP DT NN IN NP the law of NNS averages  ‘believe’ pres  PRED  TENSE      SUBJ           f1       OBL            PRED  f2 PERS  NUM ‘pro’  1  pl ‘in’  PFORM           f"
C08-1038,C96-1045,1,0.742104,"Missing"
C08-1038,2005.mtsummit-papers.15,0,0.171931,"acterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. 297 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297–304 Manchester, August 2008  S N"
C08-1038,2007.mtsummit-ucnlg.4,0,0.496422,"c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to"
C08-1038,C00-1062,0,0.0175863,"postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)-structure. As illustrated in Figure 1, a c-structure is a conventional phrase structure tree and captures surface grammatical configurations. The f-structure encodes more abstract functional relations like SUBJ(ect), OBJ(ect) and ADJ(unct). F-structures are hierarchical attribute2.2 Generation from F-Structures Work on generation in LFG generally assumes that the generation task is to determine the set of strings of the language that corresponds to a specified fstructure, given a particular grammar (Kaplan and Wedekind, 2000). Previous work on generation 2 F-structures can be also interpreted as quasi-logical forms (van Genabith and Crouch, 1996), which more closely resemble inputs used by some other generators. 298 within LFG includes the XLE,3 Cahill and van Genabith (2006), Hogan et al. (2007) and Cahill et al. (2007). The XLE generates sentences from fstructures according to parallel handcrafted grammars for English, French, German, Norwegian, Japanese, and Urdu. Based on the German XLE resources, Cahill et al. (2007) describe a two-stage, log-linear generation model. Cahill and van Genabith (2006) and Hogan e"
C08-1038,D07-1028,1,\N,Missing
C08-1038,W96-0501,0,\N,Missing
C08-1038,N09-2057,0,\N,Missing
C08-1038,W08-2121,0,\N,Missing
C08-1038,C04-1097,0,\N,Missing
C08-1038,N03-1031,0,\N,Missing
C08-1038,N03-2002,0,\N,Missing
C08-1038,C02-1036,0,\N,Missing
C08-1038,D09-1043,0,\N,Missing
C08-1038,C00-2126,0,\N,Missing
C08-1038,W08-1111,0,\N,Missing
C08-1038,P07-1041,0,\N,Missing
C08-1038,N09-2041,0,\N,Missing
C08-1038,P05-1012,0,\N,Missing
C08-1038,P06-1085,0,\N,Missing
C08-1038,W07-2416,0,\N,Missing
C08-1038,D09-1000,0,\N,Missing
C08-1038,W02-2105,0,\N,Missing
C08-1038,A97-1039,0,\N,Missing
C08-1038,P03-1021,0,\N,Missing
C08-1038,W15-4700,0,\N,Missing
C10-2043,N03-1017,0,0.00602433,"at we are able to create a gold standard by ranking the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list and keep the top-5 distinct outputs. Our data set consists of mainly short sentences, leading to many duplications in the N-b"
C10-2043,2009.eamt-1.5,0,0.0329971,"e formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is to improve the MT confidence measures to better predict post-editing effort in order to obtain a quality estimation that has the potential to replace the fuzzy match score in the TM. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a), which uses regression on both the automatic scores and scores assigned by posteditors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgment of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Another strand is to integrate high confidence MT outputs into the TM, so that the ‘good’ TM entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitabl"
C10-2043,J82-2005,0,0.747793,"Missing"
C10-2043,2009.mtsummit-papers.16,0,0.0374791,"e formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is to improve the MT confidence measures to better predict post-editing effort in order to obtain a quality estimation that has the potential to replace the fuzzy match score in the TM. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a), which uses regression on both the automatic scores and scores assigned by posteditors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgment of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Another strand is to integrate high confidence MT outputs into the TM, so that the ‘good’ TM entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitabl"
C10-2043,W07-0736,0,0.035337,"e source and TM entry, normalized by the length of the source as in Eq. (4), as most of the current implementations are based on edit distance while allowing some additional flexible matching. F uzzyM atch(t) = min e EditDistance(s, e) Len(s) (4) where s is the source side of the TM hit t, and e is the source side of an entry in the TM. 4.2 Problem Formulation Ranking lists is a well-researched problem in the information retrieval community, and Ranking SVMs (Joachims, 2002), which optimizes on the ranking correlation τ have already been applied successfully in machine translation evaluation (Ye et al., 2007). We apply the same method here to rerank a merged list of MT and TM outputs. Formally given an MT-produced N-best list M = {m1 , m2 , ..., mn }, a TM-produced m-best list T = {t1 , t2 , ..., tm } for a input sentence s, we define the gold standard using the TER ∪ metric (Snover et al., 2006): for each d ∈ M T, (di , dj ) ∈ r(s) iff T ER(di ) &lt; T ER(dj ). We train and test a Ranking SVM using cross validation on a data set created according to this criterion. Ideally the gold standard would be created by human annotators. We choose to use TER 376 as large-scale annotation is not yet available"
C10-2043,P02-1038,0,0.0608918,"ning 8K sentence pairs, which is used to run cross validation. Note that the 8K sentence pairs are from the same TM, so that we are able to create a gold standard by ranking the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list an"
C10-2043,P03-1021,0,0.0274724,"g the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list and keep the top-5 distinct outputs. Our data set consists of mainly short sentences, leading to many duplications in the N-best output of the SMT decoder. In such ca"
C10-2043,N04-1023,0,0.0439495,"in a ranking model. The ranking scheme also enables us to show all TM hits to the user, and thus further protects the TM assets. There has also been work to improve SMT using the knowledge from the TM. In (Simard and Isabelle, 2009), the SMT system can produce a better translation when there is an exact or close match in the corresponding TM. They use regression Support Vector Machines to model the quality of the TM segments. This is also related to our work in spirit, but our work is in the opposite direction, i.e. using SMT to enrich TM. Moreover, our ranking model is related to reranking (Shen et al., 2004) in SMT as well. However, our method does not focus on producing better 1-best translation output for an SMT system, but on improving the overall quality of the k-best list that TM systems present to post-editors. Some features in our work are also different in nature to those used in MT reranking. For instance we cannot use N-best posterior scores as they do not make sense for the TM outputs. 3 3.1 The Support Vector Machines The SVM Classifier Classical SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized"
C10-2043,2009.mtsummit-papers.14,0,0.468633,"entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitable for post-editing for a particular sentence. The research presented here continues the line of research in the second strand. The difference is that we do not limit ourselves to the 1-best output but try to produce a k-best output in a ranking model. The ranking scheme also enables us to show all TM hits to the user, and thus further protects the TM assets. There has also been work to improve SMT using the knowledge from the TM. In (Simard and Isabelle, 2009), the SMT system can produce a better translation when there is an exact or close match in the corresponding TM. They use regression Support Vector Machines to model the quality of the TM segments. This is also related to our work in spirit, but our work is in the opposite direction, i.e. using SMT to enrich TM. Moreover, our ranking model is related to reranking (Shen et al., 2004) in SMT as well. However, our method does not focus on producing better 1-best translation output for an SMT system, but on improving the overall quality of the k-best list that TM systems present to post-editors. S"
C10-2043,2006.amta-papers.25,0,0.384507,"erved and the cost estimation is still valid as an upper bound. More specifically, we recast SMT-TM integration as a ranking problem, where we apply the Ranking SVM technique to produce a ranked list of translations combining the k-best lists of both the MT and the TM systems. We use features independent of the MT and TM systems for ranking, so that outputs from MT and TM can have the same set of features. Ideally the translations should be ranked by their associated postediting efforts, but given the very limited amounts of human annotated data, we use an automatic MT evaluation metric, TER (Snover et al., 2006), which is specifically designed to simulate postediting effort to train and test our ranking model. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review Ranking SVMs in Section 3. The formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is"
C10-2043,P07-2045,0,\N,Missing
C12-1006,P08-1083,0,0.0556977,"Missing"
C12-1006,2006.bcs-1.5,1,0.895801,"N ARABIC:  التوسع المعجمي، اإلثراء المعجمي، القاموس العائم، الكلمات الغير مدرجة في القواميس، الكلمات الغير معروفة،اللغة العربية Proceedings of COLING 2012: Technical Papers, pages 83–96, COLING 2012, Mumbai, December 2012. 83 1 Introduction Due to the complexity and semi-algorithmic nature of Arabic morphology (that employs numerous rules and constraints on inflection, derivation and cliticization), it has been a challenge for computational processing and analysis (Kiraz, 2001; Beesley 2003). A lexicon is an indispensable part of a morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001), and the coverage of the lexical database is a key factor in the coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. Moreover, out of vocabulary words (or OOVs) have impact negatively on the performance of parsers (Attia et al., 2010) and MT applications (Huang et al. 2010). This is why an automatic method for updating a lexical database and dealing with unknown words is crucially important. We present the first attempt, to the best of our knowledge, to address the lemmatization (rather t"
C12-1006,W10-1408,1,0.88539,"Missing"
C12-1006,W11-4417,1,0.749922,"Missing"
C12-1006,N04-4038,0,0.0440034,"Missing"
C12-1006,2003.mtsummit-semit.5,0,0.0230558,"xical extension KEYWORDS IN ARABIC:  التوسع المعجمي، اإلثراء المعجمي، القاموس العائم، الكلمات الغير مدرجة في القواميس، الكلمات الغير معروفة،اللغة العربية Proceedings of COLING 2012: Technical Papers, pages 83–96, COLING 2012, Mumbai, December 2012. 83 1 Introduction Due to the complexity and semi-algorithmic nature of Arabic morphology (that employs numerous rules and constraints on inflection, derivation and cliticization), it has been a challenge for computational processing and analysis (Kiraz, 2001; Beesley 2003). A lexicon is an indispensable part of a morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001), and the coverage of the lexical database is a key factor in the coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. Moreover, out of vocabulary words (or OOVs) have impact negatively on the performance of parsers (Attia et al., 2010) and MT applications (Huang et al. 2010). This is why an automatic method for updating a lexical database and dealing with unknown words is crucially important. We present the first attempt, to the best of our knowledge, to address the lemmatizat"
C12-1006,2010.amta-papers.13,0,0.0136507,"flection, derivation and cliticization), it has been a challenge for computational processing and analysis (Kiraz, 2001; Beesley 2003). A lexicon is an indispensable part of a morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001), and the coverage of the lexical database is a key factor in the coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. Moreover, out of vocabulary words (or OOVs) have impact negatively on the performance of parsers (Attia et al., 2010) and MT applications (Huang et al. 2010). This is why an automatic method for updating a lexical database and dealing with unknown words is crucially important. We present the first attempt, to the best of our knowledge, to address the lemmatization (rather than stemming) of Arabic unknown words. The problem with lemmatizing unknown words is that they cannot be matched against a morphological lexicon. Furthermore, the specific problem with lemmatizing Arabic words is the richness and complexity of Arabic morphological derivational and inflectional processes. For the purposes of this paper, unknown words are words not found by the SA"
C12-1006,E09-2008,0,0.0321446,"inine Mark (1) Second (5) person Third person (5) TABLE 2 – Proclitics, enclitics, prefixes and suffixes with Arabic nouns Similarly a noun stem can be attached to up to three clitics as shown in Table 2. Although Table 2 shows four clitics, we note that the definite article and the genitive (or possessive) pronoun are mutually exclusive. Nominal stems can also be suffixed with bound morphemes that mark the morpho-syntactic features of number, gender and case. a typical noun like ‘ معلمmuEal~im’ ‘teacher’, generates 519 valid forms. 88 We develop a finite state (Beesley and Karttunen, 2003; Hulden, 2009) morphological guesser for Arabic that can analyse unknown words with all possible clitics, morphosyntactic affixes and all relevant alteration operations that include insertion, assimilation, and deletion. Beesley and Karttunen (2003) give some advice on how to create a basic guesser. The core idea of a guesser is to assume that a stem is composed of any arbitrary sequence of non-numeric characters, and this stem can be prefixed and/or suffixed with a predefined set of prefixes, suffixes or clitics. The guesser marks clitic boundaries and tries to return the stem to its default unmarked form,"
C12-1006,P08-2030,0,0.0826632,"n (rather than stemming) of Arabic unknown words. The problem with lemmatizing unknown words is that they cannot be matched against a morphological lexicon. Furthermore, the specific problem with lemmatizing Arabic words is the richness and complexity of Arabic morphological derivational and inflectional processes. For the purposes of this paper, unknown words are words not found by the SAMA morphological analyser (Maamouri et al., 2010) but accepted by the Microsoft Spell Checker. We develop a rule-based finite-state morphological guesser and use a machine learning based disambiguator, MADA (Roth et al., 2008), in a pipeline-based approach to lemmatization. We test our method against a manually created gold standard of 1,310 types (unique words) and show a significant improvement over the baseline. Furthermore, we devise a novel algorithm for weighting and prioritizing new words for inclusion in a lexicon depending on three factors: number of form variations of the lemmas, cumulative frequency of the forms, and the type of POS (part of speech) tag. This paper is structured as follows. The remainder of the introduction provides more details on the complexity of the lemmatization process in Arabic, w"
C12-1006,mohamed-kubler-2010-arabic,0,\N,Missing
C12-1010,D11-1033,0,0.214475,"Missing"
C12-1010,2011.mtsummit-papers.32,1,0.769403,"Missing"
C12-1010,2012.eamt-1.41,1,0.717213,"Missing"
C12-1010,2005.eamt-1.9,0,0.0486176,"Missing"
C12-1010,N09-1025,0,0.0684642,"Missing"
C12-1010,P11-2071,0,0.0542134,"Missing"
C12-1010,eck-etal-2004-language,0,0.0767698,"Missing"
C12-1010,D10-1044,0,0.208686,"Missing"
C12-1010,W07-0717,0,0.261809,"Missing"
C12-1010,2011.mtsummit-papers.10,0,0.207403,"Missing"
C12-1010,W11-2123,0,0.0575828,"Missing"
C12-1010,2005.eamt-1.19,0,0.198143,"Missing"
C12-1010,W04-3250,0,0.18146,"Missing"
C12-1010,2005.mtsummit-papers.11,0,0.0133593,"Missing"
C12-1010,P07-2045,0,0.0155806,"Missing"
C12-1010,W07-0733,0,0.288199,"Missing"
C12-1010,N10-1062,0,0.0748053,"Missing"
C12-1010,P03-1021,0,0.151809,"Missing"
C12-1010,J03-1002,0,0.0126689,"Missing"
C12-1010,P02-1040,0,0.0843489,"Missing"
C12-1010,2011.mtsummit-papers.27,1,0.837951,"Missing"
C12-1010,2006.amta-papers.25,0,0.0693185,"Missing"
C12-1014,2011.mtsummit-papers.35,1,0.845632,"Missing"
C12-1014,W08-0309,0,0.027292,"using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output. This n-gram matching scheme makes BLEU very sensitive to small changes in the output, and fails to capture linguistic variations, especially in the case where only one reference translation is being used. (Callison-Burch et al., 2008) show that that BLEU has a lower correlation with human judgement than metrics such as TER, which take into account linguistic resources and better matching strategies. Furthermore, BLEU is designed to evaluate MT output on a document level. For this reason, we have used S-BLEU (Sentence-Level BLEU) and TER to compare individual sentences. TER is an Edit Distance-style evaluation metric that measures the amount of editing that a human post-editor would have to perform to change a system output so it matches the given reference translation. It calculates how many insertions, deletions, substitu"
C12-1014,P05-1033,0,0.0797616,"rom the error analysis should enhance feature selection methods. We will research ways to further refine statistical post-editing techniques for both RBMT and SMT systems. In previous work, we developed a contextualised SPE system that attempts to preserve the original context of the source material with a novel method of context modelling (Béchara et al., 2011). We intend to take this work further and refine our use of context information. We will also experiment with different system combinations: in addition to RBMT and PBSMT systems, we will utilise a hierarchical phrase based SMT system (Chiang, 2005). Acknowledgments This work is supported by Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University. We would like to thank Symantec, in particular Dr. Johann Roturier and Dr. Fred Hollowood, for providing us with the data and with access to the Symantec Systran Machine Translation Production System. We would like to thank Dr. Stephen Doherty, from the School of Applied Language and Intercultural Studies at Dublin City University, for providing us with access to masters in translation students to act as o"
C12-1014,W07-0732,0,0.108776,"l., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the manually corrected first stage MT output as target, and then applied to the output of the first stage MT system on unseen source side input data), while the other approach (Oflazer and El-Khalout, 2007; Béchara et al., 2011; Rubino et al., 2012) simply uses available bi-text training data (such as translation memories (TMs) in industrial applications or more generic SM"
C12-1014,P07-2045,0,0.0334804,"Missing"
C12-1014,N03-1017,0,0.0101645,"o the text type and domain of the Symantec translation memory data, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system therefore produces a monolingual translation based on the output produced by the first st"
C12-1014,P03-1021,0,0.0229625,"a, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system therefore produces a monolingual translation based on the output produced by the first stage RBMT system and the target side of the TM training d"
C12-1014,J03-1002,0,0.00632206,"ion system, specifically customised with the use of 10K+ dictionary entries specific to the text type and domain of the Symantec translation memory data, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system th"
C12-1014,W07-0704,0,0.0639693,"utomatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the"
C12-1014,P02-1040,0,0.0927948,"nglish training set. This approach will ensure that we do not translate already seen data, and that the source side of the training set for the SPE system is as close in quality to the test set source as possible. Figure 2 illustrates the SMT+SPE pipeline. MT System F (Input) Moses SPE System E' (Input) E' (Ouput) Moses E'' (Ouput) Figure 2: The Moses+Moses pipeline, using the output of Moses as the input for the second stage SMT system (Moses) 3.5 Automatic Evaluation Metrics We used two metrics for automatic evaluation; BLEU (Bilingual Evaluation Understudy) and TER (Translation Edit Rate) (Papineni et al., 2002; Snover et al., 2006). Both of these metrics depend on a reference translation to estimate the quality of machine translated output. BLEU matches n-grams between the MT output and the reference translation, using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output"
C12-1014,2012.eamt-1.55,1,0.856232,"quire a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the manually corrected first stage MT output as target, and then app"
C12-1014,W05-0822,0,0.0550267,"Missing"
C12-1014,N07-1064,0,0.376754,"2. 215 1 Introduction Human evaluation is a core component of shared tasks such as WMT, and is often considered the gold standard in evaluation of translation systems. Automatic evaluation metrics, however, are much less costly, much more time efficient and enable automatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one appr"
C12-1014,2006.amta-papers.25,0,0.0998391,"is approach will ensure that we do not translate already seen data, and that the source side of the training set for the SPE system is as close in quality to the test set source as possible. Figure 2 illustrates the SMT+SPE pipeline. MT System F (Input) Moses SPE System E' (Input) E' (Ouput) Moses E'' (Ouput) Figure 2: The Moses+Moses pipeline, using the output of Moses as the input for the second stage SMT system (Moses) 3.5 Automatic Evaluation Metrics We used two metrics for automatic evaluation; BLEU (Bilingual Evaluation Understudy) and TER (Translation Edit Rate) (Papineni et al., 2002; Snover et al., 2006). Both of these metrics depend on a reference translation to estimate the quality of machine translated output. BLEU matches n-grams between the MT output and the reference translation, using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output. This n-gram matching"
C12-1014,2007.mtsummit-wpt.4,0,0.0922062,"Human evaluation is a core component of shared tasks such as WMT, and is often considered the gold standard in evaluation of translation systems. Automatic evaluation metrics, however, are much less costly, much more time efficient and enable automatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et"
C12-1014,vilar-etal-2006-error,0,0.0749684,"Missing"
C12-1135,2010.amta-papers.16,1,0.91367,"Missing"
C12-1135,E06-1032,0,0.101057,"Missing"
C12-1135,P11-2071,0,0.0229814,"Missing"
C12-1135,eck-etal-2004-language,0,0.0341917,"), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain m"
C12-1135,W08-0334,0,0.0173916,"can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their re"
C12-1135,D10-1044,0,0.0271916,"ain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train d"
C12-1135,2010.amta-papers.27,1,0.903095,"Missing"
C12-1135,2005.eamt-1.19,0,0.0282124,"guage models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek"
C12-1135,D11-1125,0,0.0279752,"e model significantly prefers hypotheses with altered word order (which is consistent with the two preceding observations). 4. Language model weights (h8 ) do not change substantially, its importance remains similar on general-domain and specific-domain data. These findings are highly consistent across domains and language pairs. The weight vectors of the systems tuned on specific-domain data are quite similar but differ substantially from the parameters obtained by tuning on general-domain. This observation can be quantified by measuring cosine similarity (see Figure 2, right) as proposed by Hopkins and May (2011). Lower scores, as in the first rows/columns of each table, indicate low similarity of the vectors – specific-domain tuned weights differ a lot from the general-domain tuned ones; and vice versa – specific-domain tuned parameters are quite similar when compared to each other. 5.5 Analysis of phrase-length distribution From the analysis presented above, we conclude that a PB-SMT system tuned on data from the same domain as the training data strongly prefers to construct translations consisting of long phrases. Such phrases are usually of good translation quality (local mistakes of word alignmen"
C12-1135,W04-3250,0,0.190801,"Missing"
C12-1135,2005.mtsummit-papers.11,0,0.0315462,"et al. (2011) exploit automatically web-crawled in-domain resources for parameter optimization and improving language models. Pecina et al. (2012) extend the work by using the web-crawled resources to also improve translation models. 4 Experimental setup Our experimental setup follows and extends the one used in Pecina et al. (2011). In addition to the two evaluation domains (env, lab) used in that work, and in order to corroborate their earlier findings, we also carry out experiments on medical domain data (med). 4.1 Data Our general-domain system is trained on the Europarl parallel corpus (Koehn, 2005, v5) extracted from the proceedings of the European Parliament and for the purposes of this work considered to contain general-domain texts (it covers a very broad range of topics and it is to a considerable extent spoken language). The general-domain development and test data used for parameter optimization and testing, respectively, are adopted from the WPT 20051 machine translation shared task. These sets were extracted from the same source as Europarl and contain 2,000 sentence pairs each. The specific-domain development and test data for the env and lab domains were acquired by domain-fo"
C12-1135,P07-2045,0,0.0376072,"tem to a specific domain. Finally, given the fact that a general-domain system can only use limited length translation units when translating specific-domain data, we explore limited length training and decoding. After a brief overview of the log-linear model including its parameter optimization and an overview of the state-of-the-art in domain adaptation for SMT, we describe our experiments, present the results, the analysis, explore the resulting research questions with additional experiments, and conclude. 2 Phrase-Based Statistical Machine Translation In PB-SMT, implemented e.g. in Moses (Koehn et al., 2007), an input sentence is segmented into sequences of consecutive words, called phrases. Each phrase is then translated into a target language phrase, which may be reordered with other translated phrases to produce the output. 2210 Formally, the model is based on the noisy channel model. The translation e of an input sentence f is searched for by maximizing the translation probability p(e|f) formulated as a log-linear combination of a set of feature functions hi and their weights λi : Qn p(e|f) = i hi (e, f)λi Typically, the components include features of the following models: phrase translation"
C12-1135,N03-1017,0,0.0327235,"en longer phrases improve translation quality is for the systems trained, tuned and tested on the same (general) domain. In all other cases, the results for phrases up to three words long are as good as for longer phrases. If the domain of the test data does not match the domain of training and tuning data, the maximum phrase length set to three is enough in all scenarios. Longer phrases lead to degradation of translation quality and increase time for training and decoding, as well as memory requirements for building and storing the translation models. A similar result was reported already by Koehn et al. (2003). They observed that limiting the maximum length of a phrase to only three words achieved top performance. However, current state-of-the-art SMT systems usually benefit from longer phrases than three (see e.g. the top curve in Figure 5 which refers to a general-domain system applied to a general-domain test set), and our result applies only to scenarios where the training and test domains do not match; in that case setting the maximum phrase length to three is sufficient. 6 Conclusions In this work, we have analysed domain adaptation of PB-SMT by tuning parameters of the underlying log-linear"
C12-1135,W07-0733,0,0.0683277,"se, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting simila"
C12-1135,W02-1405,0,0.0358254,"lore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. E"
C12-1135,J05-4003,0,0.0384329,"odel combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train dev test dev test dev test dev test sentences 1,725,096 2,000 2,000 1,392 2,000 1,411 2,000 1,064 2,000 964,242 2,000 2,000"
C12-1135,W08-0320,0,0.0424553,"Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the tra"
C12-1135,P03-1021,0,0.0119307,"odel, which ensures that the translations are fluent, reordering (distortion) model, which allows to reorder phrases in the input sentences (e.g. distance-based and lexicalized reordering) and word penalty, which prevents the translations from being too long or too short. These models are trained on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm can"
C12-1135,P02-1040,0,0.0883898,"ned on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm cannot explore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identifi"
C12-1135,2012.eamt-1.38,1,0.722345,"Missing"
C12-1135,2011.eamt-1.40,1,0.781939,"alizace parametr˚u. Proceedings of COLING 2012: Technical Papers, pages 2209–2224, COLING 2012, Mumbai, December 2012. 2209 1 Introduction Statistical Machine Translation (SMT) is an instance of a machine learning application and, in general, will work best if the data for training and testing are drawn from the same distribution (i.e. domain, genre, and style). In practice, however, it is often difficult to obtain sufficient amounts of in-domain data (in particular parallel data required for translation and distortion models) to train a well performing system for a specific domain. Recently, Pecina et al. (2011) showed that just using in-domain development data for parameter tuning improves output quality of a Phrase-Based SMT (PB-SMT) system trained on general-domain data but applied to a specific domain. Although further additional improvements can be realized by using in-domain parallel and/or monolingual training data, parameter tuning on in-domain data requires only a relatively small set of parallel sentences, which is often easier to obtain. They report on a series of experiments carried out on the domains of Natural Environment (env) and Labour Legislation (lab) and two language pairs: Englis"
C12-1135,2011.mtsummit-papers.58,0,0.0536528,"Missing"
C12-1135,C08-1125,0,0.0344385,"Missing"
C12-2011,W11-4417,1,0.911901,"Missing"
C12-2011,P00-1037,0,0.145659,"Missing"
C12-2011,J92-4003,0,0.0590624,"now use language models trained on different corpora to finally choose the single best correction. We compare the results against the Microsoft Spell Checker in Office 2010, Ayaspell used in OpenOffice, and Google Docs. 4.1 Correction Procedure For automatic spelling correction (or first order ranking) we use n-gram language models. Language modelling assumes that the production of a human language text is characterized by a set of conditional probabilities, , where is the history and is the prediction, so that the probability of a sequence of k words P(w1, …, wk) is formulated as a product (Brown et al., 1992): We use the SRILM toolkit2 (Stolcke et al., 2011) to train 2-, 3- and 4-gram language models on our data sets. As we have two types of candidates, normal words and split words, we use two SRILM tools: disambig and ngram. We use the disambig tool to choose among the normal candidates. Handling split words is done as a posterior step where we use the ngram tool to score the chosen candidate from the first round and the various split-word options. Then the candidate with the least perplexity score is selected. The perplexity of a language model is the reciprocal of the geometric average of the p"
C12-2011,P05-1071,0,0.165899,"Missing"
C12-2011,E09-2008,0,0.0607402,"r, the next step is to generate possible and plausible corrections for that error. For a spelling error and a dictionary , the purpose of the error model is to generate the correction , or list of corrections where ∊ , and are most likely to have been erroneously typed as . In order to do this, the error model generates a list of candidate corrections that bear the highest similarity to the spelling error . We use a finite-state transducer to propose candidate corrections within edit distance 1 and 2 measured by Levenshtein Distance (Levenshtein, 1966) from the misspelled word (Oflazer, 1996; Hulden, 2009b; Norvig, 2009; Mitton, 1996). The transducer works basically as a character-based generator that replaces each character with all possible characters in the alphabet as well as deleting, inserting, and transposing neighbouring characters. There is also the problem of merged (or run-on) words that need to be split, such as >‘ أوأيw>y’ “or any”. Candidate generation using edit distance is a brute-force process that ends up with a huge list of candidates. Given that there are 35 alphabetic letters in Arabic, for a word of length , there will be deletions, − 1 transpositions, 35 replaces, 35 +"
C12-2011,C90-2036,0,0.361216,"Missing"
C12-2011,W06-1648,0,0.17186,"Missing"
C12-2011,P08-2030,0,0.0557039,"nd a corpus of news articles crawled from the Al-Jazeera web site. The Gigaword corpus is a collection of news articles from nine news sources: Agence France-Presse, Xinhua News Agency, An Nahar, Al-Hayat, Al-Quds Al-Arabi, Al-Ahram, Assabah, Asharq Al-Awsat and Ummah Press. Before we start using our available corpora in training the language model, we analyse the data to measure the amount of noise in each subset of the data. In order to do this, we create a list of the most common spelling errors. This list of spelling errors is created by analysing the data using MADA (Habash et al., 2005; Roth et al., 2008) and checking instances where words have been normalized. In this case the original word is considered to be a suboptimal variation of the spelling of the diacritized form. We collect these suboptimal forms and sort them by frequency. 2 http://www.speech.sri.com/projects/srilm/ 108 Then we take the top 100 misspelt forms and see how frequent they are in the different subsets of data in relation to the word count in each data set. The analysis shows that the data has a varying degree of cleanness, ranging from the very clean to the very noisy. Data in the Agence France-Presse (AFP) is the noisi"
C12-2011,J96-1003,0,\N,Missing
C12-2011,shaalan-etal-2012-arabic,1,\N,Missing
C12-2011,I08-2131,0,\N,Missing
C12-2122,P06-1002,0,0.0193855,"nother alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or re"
C12-2122,H05-1009,0,0.0216,"stem combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (D"
C12-2122,P06-1009,0,0.0532677,"Missing"
C12-2122,J93-2003,0,0.0478199,"Missing"
C12-2122,P05-1033,0,0.441175,"robabilities. Instead of extracting phrase pairs that respect the word alignment, Tu et al. (2011) enumerate all potential phrase pairs and calculate their fractional counts. As they soften the alignment consistency constraint, there exists a massive number of phrase pairs extracted from the training corpus. To maintain a reasonable phrase table size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments"
C12-2122,P05-1066,0,0.166329,"Missing"
C12-2122,P11-1043,0,0.0377862,"he process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weigh"
C12-2122,C10-1036,0,0.01903,"2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset"
C12-2122,P08-1115,0,0.0280219,"5; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minim"
C12-2122,D09-1115,1,0.846211,"). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering mo"
C12-2122,N09-2064,0,0.13425,"Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary nu"
C12-2122,J07-3002,0,0.0988242,"igne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or removed from the set of phr"
C12-2122,P06-1121,0,0.10519,"Missing"
C12-2122,P11-1127,0,0.0185151,"s high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; F"
C12-2122,D08-1011,0,0.0397351,"Missing"
C12-2122,W99-0623,0,0.0194602,"eference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alig"
C12-2122,2006.amta-papers.8,0,0.0607293,"Missing"
C12-2122,N03-1017,0,0.0694144,"nd phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-be"
C12-2122,N04-1022,0,0.0650294,"ell together: alignment refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to"
C12-2122,N06-1014,0,0.0889621,"ce pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 a"
C12-2122,P06-1077,1,0.905513,"Missing"
C12-2122,J10-3002,1,0.857717,"n the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 1254 Alignments GIZA++ Berkeley Vigne Selecti"
C12-2122,D09-1106,1,0.959517,"t al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alig"
C12-2122,D08-1022,0,0.0291746,"en explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment s"
C12-2122,P06-1065,0,0.0455271,"Missing"
C12-2122,P03-1021,0,0.0759237,"size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined al"
C12-2122,J03-1002,0,0.167522,"at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), wor"
C12-2122,J04-4002,0,0.308906,"Missing"
C12-2122,P02-1040,0,0.0852613,"ts 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice,"
C12-2122,N07-1029,0,0.0897098,"s calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, wh"
C12-2122,N06-2033,0,0.0331853,"kel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approa"
C12-2122,P08-1066,0,0.102941,"Missing"
C12-2122,P12-1025,0,0.0250082,"using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into ac"
C12-2122,H05-1010,0,0.073942,"Missing"
C12-2122,D08-1065,0,0.016802,"refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard"
C12-2122,C10-1123,1,0.860759,"ly (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to"
C12-2122,I11-1145,1,0.883984,"ur technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alignment ai using the"
C12-2122,C96-2141,0,0.530719,"Missing"
C12-2122,C10-2154,0,0.0147392,"ave the same value. 2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note"
C12-2122,P06-1066,1,0.854268,"Missing"
C16-1072,W06-0903,0,0.0380558,"e show that these features outperform more traditional features, such as token or character n-grams, while leading to more compact models. We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affect"
C16-1072,W16-2121,1,0.764012,"inguistic levels. In corpus-linguistic work on language change, approaches are typically frequency-based (e.g. Biber and Gray (2011; Biber and Gray (2013; Biber and Gray (2016), Taavitsainen and Pahta (2012), Moskowich and Crespo (2012)) and do not inherently account for context – diachronic change being observed through the lens of unconditioned probabilities. In contrast, information density measures as we apply them here, are based on conditional probabilities and thus inherently take context into account. Based on our previous work on long-term change using information-theoretic features (Degaetano-Ortlieb and Teich, 2016), we have shown how these features help model diachronic change, further motivating their use to classify different time periods. 3 Experimental Setup The experiments presented in this paper focus on the use of sentence-level information density measures — in particular n-gram log-probabilities according to a language model and n-gram distribution according to frequency quartiles — to classify texts from different time periods. In this section, we present the supervised classification setup and the set of features as well as the data used. 3.1 Supervised Classification A linear Support Vector"
C16-1072,N01-1021,0,0.151434,"onvention-driven style with respect to grammar (Biber and Gray, 2011; Biber and Gray, 2016; Banks, 2005). Considering that language variation affects all linguistic levels — from sounds and words to syntactic structure — we investigate a set of features extracted at the lexis, part-of-speech and syntactic levels to test how well they act as predictors of time period-specific language use. Moreover, based on psycholinguistic evidence it has been shown that language users choose those linguistic options that they know to be relatively predictable in a specific context to optimize communication (Hale, 2001; Levy, 2008; Demberg and Keller, 2008). To model communication in this sense, in our research we employ features based on the information-theoretic notion of surprisal or information density. Specifically, we make use of information theory inspired features on the linguistic levels of lexis, part-of-speech and syntax. In addition, these features allow an unlexicalized dense-vector representation, which enormously reduces the amount of features used for classification. Besides achieving high performance in classification, we are particularly interested in insights on long-term diachronic lingu"
C16-1072,D08-1038,0,0.0318531,"ds. So far, these kinds of features have been successfully used in classification of Gospels (see Islam and Dundia (2015) being able to identify the Greek Gospel as the original text and the American and Georgian ones as translations) and classification of human translated texts (see Rubino et al. (2016) distinguishing original from manually translated texts of different levels of expertise). 2.3 Language Change Previous computational work on diachronic change in scientific language mostly discusses short-term change (see e.g. Blei and Lafferty (2006; 2007) on changes in scientific topics and Hall et al. (2008) on the ACL anthology corpus, both using topic models) rather than long-term change and is mostly concerned with change related to lexis (such as topical shifts) rather than change on more abstract linguistic levels. In corpus-linguistic work on language change, approaches are typically frequency-based (e.g. Biber and Gray (2011; Biber and Gray (2013; Biber and Gray (2016), Taavitsainen and Pahta (2012), Moskowich and Crespo (2012)) and do not inherently account for context – diachronic change being observed through the lens of unconditioned probabilities. In contrast, information density meas"
C16-1072,Y15-2012,0,0.0205311,"ng lower surprisal. However, not only changes in lexis will be reflected in changes of surprisal values. From studies on language change, we know that diachronically there has been, for example, a shift from a more verbal towards a more nominal style (cf. notably Biber and Gray (2011)). This will have an impact on surprisal values with respect to grammatical units (such as parts of speech or syntactic units), motivating the use of information theory inspired features to classify between time periods. So far, these kinds of features have been successfully used in classification of Gospels (see Islam and Dundia (2015) being able to identify the Greek Gospel as the original text and the American and Georgian ones as translations) and classification of human translated texts (see Rubino et al. (2016) distinguishing original from manually translated texts of different levels of expertise). 2.3 Language Change Previous computational work on diachronic change in scientific language mostly discusses short-term change (see e.g. Blei and Lafferty (2006; 2007) on changes in scientific topics and Hall et al. (2008) on the ACL anthology corpus, both using topic models) rather than long-term change and is mostly conce"
C16-1072,L16-1305,1,0.601364,"cies. Table 1: Statistics (in thousands) of the corpora used in our experiments. 3.2 Datasets Four corpora are used in our experiments, two for each time period (early 19th century: 1800-1850; late 20th century: 1970-2007). Two corpora compose our training, development and test sets (henceforth: 19cA and 20cA) while two others allow us to train language models and extract n-gram frequencies (henceforth 19cLM and 20cLM). Statistics about these corpora are presented in Table 1a and Table 1b. For the 19th century time period, we use a corpus of research articles from the Royal Society of London (Kermes et al., 2016). Abstracts are taken from this corpus to form the 19cA classification subset. For feature extraction full research articles (19cLM) are taken from the same corpus, filtering out articles with abstracts included in 19cA. For the 20th century time period, abstracts are taken from a corpus of research articles (Degaetano-Ortlieb et al., 2013) covering several disciplines1 as our 20cA classification subset. For feature extraction, we collected abstracts from several fields (20cLM) matching those of 20cA. The main difference between 19cLM and 20cLM is the type of document used to extract them, the"
C16-1072,W14-2517,0,0.0136326,"We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affects their language use. Over a longer time period, it drives the evolution of domain-specific language with respect to lexis (Halliday,"
C16-1072,D08-1025,0,0.133066,"iven style with respect to grammar (Biber and Gray, 2011; Biber and Gray, 2016; Banks, 2005). Considering that language variation affects all linguistic levels — from sounds and words to syntactic structure — we investigate a set of features extracted at the lexis, part-of-speech and syntactic levels to test how well they act as predictors of time period-specific language use. Moreover, based on psycholinguistic evidence it has been shown that language users choose those linguistic options that they know to be relatively predictable in a specific context to optimize communication (Hale, 2001; Levy, 2008; Demberg and Keller, 2008). To model communication in this sense, in our research we employ features based on the information-theoretic notion of surprisal or information density. Specifically, we make use of information theory inspired features on the linguistic levels of lexis, part-of-speech and syntax. In addition, these features allow an unlexicalized dense-vector representation, which enormously reduces the amount of features used for classification. Besides achieving high performance in classification, we are particularly interested in insights on long-term diachronic linguistic change"
C16-1072,P14-5010,0,0.0120413,"hematical expressions, etc. 3.3 Feature Sets We consider three sets of features: shallow base-line features, n-gram frequency features, and information density features. Both n-gram and features specifically referred to as information density features capture aspects of information density and rely on the external resources presented in Table 1b. Shallow features Here we consider popular lexical features such as bags of character and token ngrams as a baseline, as well as bags of part-of-speech (POS) n-grams (n ∈ [1; 3]). For POS tagging and syntactic parsing, we use the Stanford NLP toolkit (Manning et al., 2014).2 For bags of token n-grams, three feature sets are built: one taking into account all n-grams, one considering n-grams appearing at least 200 times in the training corpus and one keeping only n-grams appearing at least 500 times, noted Tokens All, Tokens 200 and Tokens 500 respectively. The two latter sets allow for more compact models and less sparsity in the feature vectors. Additionally, 13 surface features are used, extracted from the surface-level of each sentence, which aim to capture meta representations of sentences’ lexical form including sentence and average word lengths, the numbe"
C16-1072,P12-2051,0,0.077108,"Missing"
C16-1072,N16-1110,1,0.890051,"Missing"
C16-1072,W09-0214,0,0.0354212,"o more compact models. We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affects their language use. Over a longer time period, it drives the evolution of domain-specific language with respec"
C16-1241,2011.mtsummit-papers.35,1,0.926867,"Missing"
C16-1241,P15-2026,0,0.107171,"extracted a grammar for each input sentence and applied it to the model. Rosa et al. (2012) and Mareˇcek et al. (2011) applied a rule-based approach to APE of English–Czech MT outputs on the morphological level. They used 20 hand-written rules based on the most frequent errors encountered in translation. The method efficiently corrects morpho-syntactic categories of a word such as number, case, gender, person as well as dependency labels. The inclusion of source-language information in APE is also useful to improve the APE performance (B´echara et al., 2011). To overcome data sparsity issues, Chatterjee et al. (2015) proposed a pipeline where the best language model and pruned phrase table are selected through task-specific dense features. Recently, a bidirectional recurrent neural network model of APE using Tmt –Tpe was proposed by Pal et al. (2016) which consists of an encoder that encodes the MT output into a fixed-length vector from which a decoder provides a post-edited (PE) translation. They reported statistically significant improvement over a strong first stage MT system baseline. Various automatic or semi-automatic post-processing techniques to implement corrections of repetitive errors have been"
C16-1241,P05-1033,0,0.0631731,"n a different language. The same method was also applied to the monolingual Italian data. Next, the parallel corpus was further cleaned using the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003) on the held-out development set. 5 Evaluation During evaluation we take into consideration the output produced by all the three APE systems: PBSAPE with hybrid word alignment, HPB-SAPE with hybrid word alignment and the system combination system (SC-APE) which also includes the output from the first stage system Google MT. As a baseline APE system we use a PB-SAPE system with GIZA++ alignment. The eval"
C16-1241,P11-1043,0,0.137633,"ns in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. How"
C16-1241,W11-2123,0,0.0161942,"g the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003) on the held-out development set. 5 Evaluation During evaluation we take into consideration the output produced by all the three APE systems: PBSAPE with hybrid word alignment, HPB-SAPE with hybrid word alignment and the system combination system (SC-APE) which also includes the output from the first stage system Google MT. As a baseline APE system we use a PB-SAPE system with GIZA++ alignment. The evaluation was carried out in two ways: (i) automatic evaluation and (ii) human evaluation of the 1,000 testset sentences automatically post-edite"
C16-1241,W10-1745,0,0.0277968,"f repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy"
C16-1241,N03-1017,0,0.189052,"ased on a number of alignment approaches, (ii) PB-SAPE, (iii) HPB-SAPE, and (iv) a system combination module (also including the first stage MT system). The SAPE systems are trained monolingually with Italian Tmt generated by Google Translate (GT) and the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006"
C16-1241,J10-4005,0,0.207854,"ly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistica"
C16-1241,N04-1022,0,0.166533,"red as Sa . • Step 3: Delete all the alignment points aij ∈ Sc such that ∃aik ∈ a4 ∪ a5 where j 6= k. • Step 4: Update Sc as Sc = Sc ∪ a4 ∪ a5 . 3.3 System Combination for APE Our system combination framework selects the best hypothesis translation from multiple hypotheses produced by different systems. In order to apply the system combination framework on the translations produced by our SAPE systems and the baseline MT system (Google Translate) we implemented the Minimum Bayes Risk (MBR) coupled with the Confusion Network (MBRCN) framework as described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects for each sentence the best system output from the three outputs by minimizing BLEU (Papineni et al., 2002) loss. This output is known as the backbone. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using the edit-distance based alignment methods (cf. Section 3.1.2). The features used to score each arc in the confusion network (CN) are word posterior probability, target language model and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights. In our experi"
C16-1241,W07-0734,0,0.071243,"generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistical Word Alignment GIZA++ is a statistical word alignment tool which implements IBM models 1–5, an HMM alignment model, as well as the IBM-6 model for covering many to many alignments. The Berkeley word aligner uses an extension of Cross Expectation Maximization and is jointly"
C16-1241,W10-1747,0,0.0191016,"need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to e"
C16-1241,N06-1014,0,0.0609824,"Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistical Word Alignment GIZA++ is a statistical word align"
C16-1241,D09-1106,0,0.0257389,"the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. ("
C16-1241,E06-1005,0,0.461663,"th International Conference on Computational Linguistics: Technical Papers, pages 2559–2570, Osaka, Japan, December 11-17 2016. without the availability of Sip using only sufficient amounts of parallel “target-side” Tmt –Tpe text within the statistical MT (SMT) framework. Usually APE tasks focus on systematic errors made by MT systems - the most frequent ones being incorrect lexical choices, incorrect word ordering, incorrect insertion or deletion of a word. The system presented in this paper explores the use of system combination in APE. System combination in MT has been studied extensively (Matusov et al., 2006; Du et al., 2009; Pal et al., 2014), except in the context of APE. Here we use system combination architectures on three different levels: (i) sequential combination between first-stage system and APE, (ii) parallel combination of alignment systems at the level of the APE and (iii) parallel combination of APE MT systems (including the first stage MT system). More precisely, our approach makes use of a hybrid implementation of multiple alignment combination within phrase-based SAPE (PB-SAPE) and hierarchical PB-SAPE (HPB-SAPE) and a system combination framework (a multi-engine pipeline) – that"
C16-1241,J03-1002,0,0.00748102,"on quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAP"
C16-1241,P03-1021,0,0.385927,"ent model based on a number of alignment approaches, (ii) PB-SAPE, (iii) HPB-SAPE, and (iv) a system combination module (also including the first stage MT system). The SAPE systems are trained monolingually with Italian Tmt generated by Google Translate (GT) and the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment"
C16-1241,W13-2814,1,0.950207,"Sip information. System combination and hybrid word alignment strategies are commonly used in MT, however to the best of our knowledge the work presented in this paper is the first approach to APE that uses system combination and hybrid word alignment methods within the APE engine. System combination has been found to be a very useful technique in MT where translation hypotheses from multiple MT engines are available. Motivated by the success of system combination in MT, we applied system combination in APE. Similarly, the use of multiple word alignments has been shown to improve MT results (Pal et al., 2013). For our APE, alignments have to be produced on “monolingual” target-side data (Tmt and Tpe ). A particular focus of our paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclus"
C16-1241,W15-3026,1,0.8818,"Missing"
C16-1241,P16-2046,1,0.84354,"Missing"
C16-1241,P02-1040,0,0.0959305,". System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to examine how existing word alignment techniques and a system combination framework can be intelligently used to improve monoli"
C16-1241,W12-3146,0,0.370811,"Missing"
C16-1241,N07-1029,0,0.0327304,"plement corrections of repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motiv"
C16-1241,P07-1040,0,0.0305381,"plement corrections of repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motiv"
C16-1241,N07-1064,0,0.171061,"paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclusions and avenues for further research in Section 6. 2 Related Research APE approaches cover a wide methodological range. Simard et al. (2007a) and Simard et al. (2007b) applied phrase-based SMT (PB-SMT) for post-editing that handles the repetitive nature of errors typically made by rule-based MT (RBMT) systems. The APE system was trained on the output of the rule-based system as the source language and reference human translations as the target language. This APE system was able to correct systematic errors produced by the RBMT system and reduce the post-editing effort. The approach achieved large improvements in performance not only over the baseline rule-based system but also over a similar PB-SMT used in a standalone mode. Denk"
C16-1241,W07-0728,0,0.0686589,"paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclusions and avenues for further research in Section 6. 2 Related Research APE approaches cover a wide methodological range. Simard et al. (2007a) and Simard et al. (2007b) applied phrase-based SMT (PB-SMT) for post-editing that handles the repetitive nature of errors typically made by rule-based MT (RBMT) systems. The APE system was trained on the output of the rule-based system as the source language and reference human translations as the target language. This APE system was able to correct systematic errors produced by the RBMT system and reduce the post-editing effort. The approach achieved large improvements in performance not only over the baseline rule-based system but also over a similar PB-SMT used in a standalone mode. Denk"
C16-1241,2006.amta-papers.25,0,0.54011,"human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to examine how existing word alignment techniques and a system combination framework can be inte"
C16-1241,W14-3323,1,0.846697,"d a language identifier (Shuyo, 2010) on both bilingual English–Italian MT output and MT 2 3 Empirically best preforming aligner among the individual aligners (a1 , a2 or a3 ), is considered as Sa . https://www.matecat.com/ 2563 output–PE (Italian) parallel data. We discarded those sentence pairs from the bilingual training data which are considered as belonging to a different language or contain segment(s) in a different language. The same method was also applied to the monolingual Italian data. Next, the parallel corpus was further cleaned using the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003"
C16-1241,I11-1145,0,0.0155645,"edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining"
C16-1241,C12-2122,1,0.889141,"Missing"
C16-1241,W14-0314,0,0.0161135,"amount of manual effort (TAUS Report, 2010). While MT is often not perfect, post-editing MT can yield productivity gains as post-editing MT output may require less effort compared to translating the same input manually from scratch. MT outputs are often post-edited by professional translators and the use of MT has become an important part of the translation workflow. A number of studies confirm that post-editing MT output can improve translators’ performance in terms of productivity and it may positively impact on translation quality and consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). The wide use of MT in modern translation workflows in the localization industry, in turn, has resulted in substantial quantities of PE data which can be used to develop APE systems. APE (Knight and Chander, 1994) has been proposed as an automatic method for improving raw MT output, before performing actual human post-editing on it. The approach is based on collecting human corrected output of a first stage MT system and using this to train a system to correct errors produced by the MT system, possibly resulting in a productivity increase in the translation process. The advantage of APE relie"
C16-1241,W09-0416,0,\N,Missing
C16-2021,C14-2028,0,0.0270396,"t post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensures data capture for APE development and translation process research. The MT and APE systems integrated in CATaLog Online are based on Pal et al. (2015) and Pal et al. (2016b), respectively. In this paper, we present the key features implemented in CATaLog Online and their importance to translation project managers, translators, and MT and APE developers. Compared to state-ofthe-art CAT tools (e.g., MateCat, Lilt) CATaLog Online offers the following ad"
C16-2021,W15-5206,1,0.763618,"Missing"
C16-2021,W15-3017,1,0.660709,"Missing"
C16-2021,L16-1095,1,0.88264,"Missing"
C16-2021,W16-2379,1,0.862937,"Missing"
C16-2021,W14-0314,1,0.843535,"th current state-of-the-art CAT tools, CATaLog Online provides an enhanced interface, an option to integrate APE and more informative logs to help translation process research. 1 Introduction Machine translation (MT) technology has improved substantially over the past few decades. MT output is no longer used just for gisting but also for post-editing by professional translators as an important part of the translation workflow. Several studies confirm that post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensur"
C96-1045,J90-3001,0,\N,Missing
C96-1045,E93-1047,0,\N,Missing
C96-1045,P92-1005,0,\N,Missing
C98-1054,P92-1005,0,0.0272462,"igned an underspecified truth-conditional interpretation (Genabith and Crouch, 1997). a Appendix B gives a relational formulation of the correspondence between f-structures and UDRSs. The UDRS representations are processed by semantic-based transfer. The resulting system is bi-directional. Consider again the simple head switching case discussed in (1) and (3) above. (4) shows the corresponding UDRSs. The structural nfismatch between the two fstructures has disappeared on the level of UDRS representations and transfer is facilitated. 4 3A similar coresl)oi, dence between f-structures and QLFs (Alshawi and Crouch, 1992) lt~ been shown in (Genabith and Crouch, 1996), 4In the implementation, a Neo-Davidsonian style en343 4.1 Embedded Head-Switching The syntactic transfer rules (2) are supplemented by (5). The complex rule for gcrne in (5) overrides 5 (2d) and the COMP rule in (5). For each additional level of embedding triggered by head switching adjuncts a special rule is needed. (5) { { { { { { vermuten(E) } &lt;-> { suspect(E) }. Ede(X) } &lt;-> { Ede(X) }. COMP(E,X) } &lt;-> { COMP(E,X) }. gerne(X),ADJN(E,X),COMP(EI,E) } # SUBJ(E,Y) } &lt;-> like(X) ,XCOMP(X,E) ,SUBJ(X,Y) ,COMP(EI,X) }. By contrast, on the level of UD"
C98-1054,C96-1045,1,0.874848,"Missing"
C98-1054,P97-1052,1,0.868231,"Missing"
C98-1054,J91-2001,0,0.466264,"ransfer This section presents a simple bidirectional translation between LFG f-structures and term representations which serve as input to and output of a transfer component developed within the Verbmobil project (Dorna and Emele, 1996a). The term representation is inspired by earlier work (Kay et al., 1994; Caspari and Sehmid, 1994) which uses terms as a quasisemantic representation for transfer and generation. The translation between f-structures and terms is based on the correspondence between directed graphs representing f-structures and the functional interpretation of these graphs (cf. (Johnson, 1991)). Given an arc labeled f which connects two nodes nl and n2 in a graph, the same can be expressed by a function f ( n l ) = n2. An f-structure is the set of such feature equations describing the associated graph. Instead of feature equations f ( n t ) = n2 we use the relational notation f ( n l , n2). Using this idea f-structures can be converted into sets of terms and vice versa. 1 F-structure 1For motivation why we prefer term representations 342 P R E D features and their ""semantic form"" values are given special treatment. Instead of introducing PRED terms we build unary relations with the"
C98-1054,E93-1024,0,0.0534882,"d semantic representations, respectively. Both support ambiguity preserving transfer to differing degrees (NP scope, operators, adjuncts). F-structure based syntactic represenlr : (11) l~:[ ~ oiten(l~12I l[~:~ Zm: I 345 tations may come up against structural mismatches in transfer. The original co-description based approach in (Kaplan et al., 1989) faced problems when it came to examples involving embedded head-switching and multiple adjuncts (Sadler and Thompson, 1991), which led to the introduction of a restriction operator, to enable transfer on partial f-structures or semantic structures (Kaplan and Wedekind, 1993). One might suppose that the need to refer to partial structures is an artifact of the correspondencebased approach, which doesn&apos;t allow the mapping from a single node of the source f-structure to distinct nodes in the target f-structure without violation of the functional property of the T correspondence. On closer inspection, though, the rewriting approach to syntactic f-structureterm translations presented above suffers from the very same problems that were met by the correspondence-based approach in (Kaplan et al., 1989). By contrast, transfer on the semantic U D R S representations does n"
C98-1054,E89-1037,0,0.432715,"eed to be sensible representations for both parsing and generation. LFG Lstructures are abstract, ""high-lever&apos; syntactic representations which go some way towards meeting these often irreconcilable requirements. * We would like to thank H. Kamp, M. Schiehlen and the anonymous reviewers for helpful comments on earlier versions of this article. Part of this work was flmded by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the Verbinobil project under grant 01 IV 701 Na. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) led to the contention that transfer is facilitated at the level of semantic representation, where structm&apos;al differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wcdekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage of"
C98-1054,J93-4001,0,0.243101,"Missing"
C98-1054,C96-1054,1,\N,Missing
C98-1054,C96-1024,0,\N,Missing
C98-1054,P98-1060,1,\N,Missing
C98-1054,C98-1058,1,\N,Missing
cahill-van-genabith-2002-tts,J93-2004,0,\N,Missing
cahill-van-genabith-2002-tts,J03-4003,0,\N,Missing
cahill-van-genabith-2002-tts,P96-1025,0,\N,Missing
chrupala-etal-2008-learning,W96-0213,0,\N,Missing
chrupala-etal-2008-learning,A00-2013,0,\N,Missing
chrupala-etal-2008-learning,N06-1042,0,\N,Missing
chrupala-etal-2008-learning,P05-1071,0,\N,Missing
chrupala-etal-2008-learning,P98-1080,0,\N,Missing
chrupala-etal-2008-learning,C98-1077,0,\N,Missing
chrupala-etal-2008-learning,tufis-dragomirescu-2004-tiered,0,\N,Missing
chrupala-etal-2008-learning,erjavec-2004-multext,0,\N,Missing
D07-1012,E87-1007,0,0.628084,". Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a learner corpus. However, the evaluation is preliminary, on only 50 test items. 2.2 N-gram Methods Most shallow approaches to grammar error detection originate from the area of real-word spelling error correction. A real-word spelling error is a spelling or typing error which results in a token which is another valid word of the language in question. The (to our knowledge) oldest work in this area is that of Atwell (1987) who uses a POS tagger to flag POS bigrams that are unlikely according to a reference corpus. While he speculates that the bigram frequency should be compared to how often the same POS bigram is involved in errors in an error corpus, the proposed system uses the raw frequency with an empirically established threshold to decide whether a bigram indicates an error. In the same paper, a completely different approach is presented that uses the same POS tagger to consider spelling variants that have a different POS. In the example sentence I am very hit the POS of the spelling variant hot/JJ is add"
D07-1012,bigert-2004-probabilistic,0,0.214997,"te the nature of the error. Thus, like the creation of a treebank, the creation of a corpus of ungrammatical sentences requires time and linguistic knowledge, and is by no means a trivial task. A corpus of ungrammatical sentences which is large enough to be useful can be created automatically by inserting, deleting or replacing words in grammatical sentences. These transformations should be linguistically realistic and should, therefore, be based on an analysis of naturally produced grammatical errors. Automatically generated error corpora have been used before in natural language processing. Bigert (2004) and Wilcox-O’Hearn et al. (2006), for example, automatically introduce spelling errors into texts. Here, we generate a large error corpus by automatically inserting four different kinds of grammatical errors into BNC sentences. 3.2 Commonly Produced Grammatical Errors Following Foster (2005), we define a sentence to be ungrammatical if all the words in the sentence are well-formed words of the language in question, but the sentence contains one or more error. This error can take the form of a performance slip which can occur due to carelessness or tiredness, or a competence error which occurs"
D07-1012,W06-1620,0,0.0136777,"nstructions can be marked as “dispreferred” and constraints resulting in common ungrammatical constructions can be marked as “ungrammatical”. The use of constraint ordering and marking increases the robustness of the grammar, while maintaining the grammatical / ungrammatical distinction (Frank et al., 1998). The English Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a"
D07-1012,W02-1503,0,0.076349,"generative grammar sense (Chomsky, 1957), to distinguish grammatical sentences from ungrammatical sentences. This is in contrast to treebank-based grammars which tend to massively overgenerate and do not generally aim to discriminate between the two. In order for our approach to work, the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences, and for this reason, we choose the XLE (Maxwell and Kaplan, 1996), an efficient and robust parsing system for Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) and the ParGram English grammar (Butt et al., 2002) for our experiments. This system employs robustness techniques, some borrowed from Optimality Theory (OT) (Prince and Smolensky, 1993), to parse extra-grammatical input (Frank et al., 1998), but crucially still distinguishes between optimal and suboptimal solutions. The evaluation corpus is a subset of an ungrammatical version of the British National Corpus (BNC), a 100 million word balanced corpus of British English (Burnard, 2000). This corpus is obtained by automatically inserting grammatical errors into the original BNC sentences based on an analysis of a manually compiled “real” error co"
D07-1012,A00-2019,0,0.499447,"ee features is effective. Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences. The ungrammatical test set is generated automatically by inserting grammatical errors into well-formed BNC sentences. 1 Introduction This paper is concerned with the task of predicting whether a sentence contains a grammatical error. An accurate method for carrying out automatic ∗ Also affiliated to IBM CAS, Dublin. N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell(1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others. Existing approaches are hard to compare since they are evaluated on different test sets which vary in size and error density. Furthermore, most of these approaches concentrate on one type of grammatical error only, namely, context-sensitive or realword spelling errors. We implement a vanilla ngram-based approach which is tested on a very large test set containing four different types of error. The idea behind the parser-based approach to error detection is to use a broad-coverage hand-crafted precision grammar to detect ungrammatical sen112 Proceedings of the 2007 Joint Confer"
D07-1012,copestake-flickinger-2000-open,0,0.0269068,"eloped over several years with the XLE platform (Butt et al., 2002). The XLE parser uses OT to resolve ambiguities (Prince and Smolensky, 1993). Grammar constraints resulting in rare constructions can be marked as “dispreferred” and constraints resulting in common ungrammatical constructions can be marked as “ungrammatical”. The use of constraint ordering and marking increases the robustness of the grammar, while maintaining the grammatical / ungrammatical distinction (Frank et al., 1998). The English Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader"
D07-1012,izumi-etal-2004-overview,0,0.0554633,"glish Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a learner corpus. However, the evaluation is preliminary, on only 50 test items. 2.2 N-gram Methods Most shallow approaches to grammar error detection originate from the area of real-word spelling error correction. A real-word spelling error is a spelling or typing error which results in a token which is ano"
D07-1012,W95-0104,0,0.0951794,"e same POS tagger to consider spelling variants that have a different POS. In the example sentence I am very hit the POS of the spelling variant hot/JJ is added to the list NN-VB-VBD-VBN of possible POS tags of hit. If the POS tagger chooses hit/JJ, the word is flagged and the correction hot is proposed to the user. Unlike most n-gram-based approaches, Atwell’s work aims to detect grammar errors in general and not just real-word spelling errors. However, a complete evaluation is missing. The idea of disambiguating between the elements of confusion sets is related to word sense disambiguation. Golding (1995) builds a classifier based on a rich set of context features. Mays et al. (1991) apply the noisy channel model to the disambiguation problem. For each candidate correction S ′ of the input S the probability P (S ′ )P (S|S ′ ) is calculated and the most likely correction selected. This method is re-evaluated by Wilcox-O’Hearn et al. (2006) on WSJ data with artificial real-word spelling errors. Bigert and Knutsson (2002) extend upon a basic n-gram approach by attempting to match n-grams of low frequency with similar n-grams in order to reduce overflagging. Furthermore, n-grams crossing clause bo"
D07-1027,P04-1041,1,0.927642,"Missing"
D07-1027,W03-1005,0,0.0180297,"ost-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures c"
D07-1027,P04-1082,0,0.253387,"ced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of ric"
D07-1027,E03-1049,0,0.012822,"tical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and"
D07-1027,P04-1042,0,0.0794336,"Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on"
D07-1027,A00-2018,0,0.0088518,"ociated with another position. These relationships are referred to Non-Local Dependencies (NLDs), where the surface location of the constituent is called antecedent , and the site where the antecedent should be interpreted semantically is called trace . Capturing non-local dependencies is crucial to the accurate and complete determination of semantic interpretation in the form of predicateargument-modifier structures or deep dependencies. / / 0 0 However, with few exceptions (Model 3 of Collins, 1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manni"
D07-1027,P06-1023,0,0.0130432,"control constructions, permit a constituent in one position to bear the grammatical role associated with another position. These relationships are referred to Non-Local Dependencies (NLDs), where the surface location of the constituent is called antecedent , and the site where the antecedent should be interpreted semantically is called trace . Capturing non-local dependencies is crucial to the accurate and complete determination of semantic interpretation in the form of predicateargument-modifier structures or deep dependencies. / / 0 0 However, with few exceptions (Model 3 of Collins, 1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), lin"
D07-1027,N06-1024,0,0.183553,"004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 20"
D07-1027,P05-1013,0,0.014892,"parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures converted from CFG-trees. 257 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 257–266, Prague, June 2007. 2007 Association for Computational Linguistics is that, to date, the research has focused almost entirely on"
D07-1027,P02-1018,0,0.543789,"1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to"
D07-1027,Y04-1016,1,0.885568,"Missing"
D07-1027,C02-1145,0,0.0249609,"Missing"
D07-1027,P03-1055,0,0.291446,"ost-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures c"
D07-1027,P03-2006,0,0.0357181,"Missing"
D07-1027,P04-1040,0,0.0267384,"Missing"
D07-1027,J03-4003,0,\N,Missing
D07-1027,P02-1042,0,\N,Missing
D07-1027,P02-1035,0,\N,Missing
D07-1028,C00-1007,0,0.285233,"describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licen"
D07-1028,N07-1021,0,0.179772,"how the new history-based model improves over the baseline. In Section 5 we describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model us"
D07-1028,H92-1026,0,0.0464316,"story (context) to guide local generation decisions. The new history-based probabilistic generation model is defined as: P (T ree|F-Str) := Y P (X → Y |X, F eats, GF) (3) X → Y in T ree F eats = {ai |∃vj (φ(X))ai = vj } (φ(M(X)))GF = φ(X) Note that the new conditioning feature, the fstructure mother grammatical function, GF, is available from structure previously generated in the cstructure tree. As such, it is part of the history of the tree, i.e. it has already been generated in the topdown derivation of the tree. In this way, the generation model resembles history-based models for parsing (Black et al., 1992; Collins, 1999; Charniak, 2000). Unlike, say, the parent annotation for parsing of (Johnson, 1998) the parent GF feature for a particular node expansion is not merely extracted from the parent node in the c-structure tree, but is sometimes extracted from an ancestor node further up the c-structure tree via intervening ↑=↓ functional annotations. Section 6 provides evaluation results for the new model on section 23 of the Penn treebank. 5 Multi-Word Units In another effort to improve generator accuracy over the baseline model we explored the use of multiword units in generation. We expect that"
D07-1028,P06-1130,1,0.832771,"Missing"
D07-1028,P04-1041,1,0.897415,"Missing"
D07-1028,I05-1015,0,0.124868,"the baseline generation model and in Section 4 we show how the new history-based model improves over the baseline. In Section 5 we describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistic"
D07-1028,A00-2018,0,0.0452756,"Azenbergstrae 12, D-70174 Stuttgart, Germany. aoife.cahill@ims.uni-stuttgart.de and van Genabith (2006), which do not rely on handcrafted grammars and thus can easily be ported to new languages. This paper is concerned with sentence generation from Lexical-Functional Grammar (LFG) fstructures (Kaplan, 1995). We present improvements in previous LFG-based generation models firstly by breaking down PCFG independence assumptions so that more f-structure conditioning context is included when predicting grammar rule expansions. This history-based approach has worked well in parsing (Collins, 1999; Charniak, 2000) and we show that it also improves PCFG-based generation. We also present work on utilising named entities and other multi-word units to improve generation results for both accuracy and coverage. There has been a limited amount of exploration into the use of multi-word units in probabilistic parsing, for example in (Kaplan and King, 2003) (LFG parsing) and (Nivre and Nilsson, 2004) (dependency parsing). We are not aware of any similar work on generation. In the LFG-based generation algorithm presented by Cahill and van Genabith (2006) complex named entities (i.e. those consisting of more than"
D07-1028,W03-0423,0,0.0124289,"k on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. We added an additional constraint which prevents edges from being combined if this would result in the generation of a string which contained a named entity which was either incomplete or where the words in the named entity were generated in the wrong order. 5.2 Types of MWUs used in Experiments We carry out experiments with multi-word units from three different sources. First, we use the output of the maximum entropy-based named entity recognition system of (Chieu and Ng, 2003). This system identifies four types of named entity: person, organisation, location, and miscellaneous. Additionally we use a dictionary of candidate multi-word expressions based on a list from the Stanford Multiword Expression Project4 . Finally, we also carry out experiments with multi-word units extracted from the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005). This supplements the Penn WSJ treebank’s one million words of syntax-annotated Wall Street Journal text with additional annotations of 23 named entity types, including nominal-type named entities such"
D07-1028,W01-0812,0,0.0197203,"shi et al. (2005) describe a treebankextracted HPSG-based chart generator. Importing techniques developed for HPSG parsing, they apply a log linear model to a packed representation of all alternative derivation trees for a given input. They found that a model which included syntactic information outperformed a bigram model as well as a combination of bigram and syntax model. The probability model described in this paper also incorporates syntactic information, however, unlike the discriminative HPSG models just described, it is a generative history- and PCFG-based model. While Belz (2007) and Humphreys et al. (2001) mention the use of contextual features for the rules in their generation models, they do not provide details nor do they provide a formal probability model. To the best of our knowledge this is the first paper providing a probabilistic generative, history-based generation model. 3 Surface Realisation from f-Structures Cahill and van Genabith (2006) present a probabilistic surface generation model for LFG (Kaplan, 1995). LFG is a constraint-based theory of grammar, which analyses strings in terms of c(onstituency)-structure and f(unctional)-structure (Figure 1). C-structure is defined in terms"
D07-1028,J98-4004,0,0.0234125,"el is defined as: P (T ree|F-Str) := Y P (X → Y |X, F eats, GF) (3) X → Y in T ree F eats = {ai |∃vj (φ(X))ai = vj } (φ(M(X)))GF = φ(X) Note that the new conditioning feature, the fstructure mother grammatical function, GF, is available from structure previously generated in the cstructure tree. As such, it is part of the history of the tree, i.e. it has already been generated in the topdown derivation of the tree. In this way, the generation model resembles history-based models for parsing (Black et al., 1992; Collins, 1999; Charniak, 2000). Unlike, say, the parent annotation for parsing of (Johnson, 1998) the parent GF feature for a particular node expansion is not merely extracted from the parent node in the c-structure tree, but is sometimes extracted from an ancestor node further up the c-structure tree via intervening ↑=↓ functional annotations. Section 6 provides evaluation results for the new model on section 23 of the Penn treebank. 5 Multi-Word Units In another effort to improve generator accuracy over the baseline model we explored the use of multiword units in generation. We expect that the identification of MWUs may be useful in imposing wordorder constraints and reducing the comple"
D07-1028,P96-1027,0,0.139347,"ses the generation of sequences of words which violate the internal word order of named entities. The input is marked-up in such a way that, although named entities are no longer chunked together to form single words, the algorithm can read which items are part of named entities. See the rightmost f-structure in Figure 5 for an example of an f-structure markedup in this way. The tag NE1 1, for example, indicates that the sub-f-structure is part of a named identity with id number 1 and that the item corresponds to the first word of the named entity. The baseline generation algorithm, following Kay (1996)’s work on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. We added an additional constraint which prevents edges from being combined if this would result in the generation of a string which contained a named entity which was either incomplete or where the words in the named entity were generated in the wrong order. 5.2 Types of MWUs used in Experiments We carry out experiments with multi-word units from three different sources. First, we use the output of the maximum entropy-based named entity recognition syste"
D07-1028,P98-1116,0,0.0680725,"ned and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entro"
D07-1028,W02-2103,0,0.555453,"al) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langk"
D07-1028,A00-2023,0,0.284489,"2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model. This unpacking algorithm is used in (Velldal and Oepen, 2005) to rank realisations with features defined over HPSG"
D07-1028,J93-2004,0,0.0281111,"Missing"
D07-1028,W05-1510,0,0.73623,"surface realisation, is the task of generating meaningful, grammatically correct and fluent text from some abstract semantic or syntactic representation of the sentence. It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al., 2003). While recent work on generation in restricted domains, such as (Belz, 2007), has shown promising results there remains much room for improvement particularly for broad coverage and robust generators, like those of Nakanishi et al. (2005) and Cahill ∗ Now at the Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart, Azenbergstrae 12, D-70174 Stuttgart, Germany. aoife.cahill@ims.uni-stuttgart.de and van Genabith (2006), which do not rely on handcrafted grammars and thus can easily be ported to new languages. This paper is concerned with sentence generation from Lexical-Functional Grammar (LFG) fstructures (Kaplan, 1995). We present improvements in previous LFG-based generation models firstly by breaking down PCFG independence assumptions so that more f-structure conditioning context is included when predicting gr"
D07-1028,A00-2026,0,0.030549,"such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated di"
D07-1028,N06-1032,0,0.05047,"Missing"
D07-1028,N03-1026,0,0.0475794,"Missing"
D07-1028,2005.mtsummit-papers.15,0,0.204767,"ed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model. This unpacking algorithm is used in (Velldal and Oepen, 2005) to rank realisations with features defined over HPSG derivation trees. They achieved the best results when combining the tree-based model with an n-gram language model. Nakanishi et al. (2005) describe a treebankextracted HPSG-based chart generator. Importing techniques developed for HPSG parsing, they apply a log linear model to a packed representation of all alternative derivation trees for a given input. They found that a model which included syntactic information outperformed a bigram model as well as a combination of bigram and syntax model. The probability model described in this paper"
D07-1028,J03-4003,0,\N,Missing
D07-1028,C98-1112,0,\N,Missing
D07-1028,P93-1005,0,\N,Missing
D07-1066,H91-1060,0,0.715134,"Missing"
D07-1066,brants-hansen-2002-developments,0,0.0464714,"Missing"
D07-1066,P03-1013,0,0.154719,"metric as well as a dependency-based evaluation, and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output. We also provide extensive past-parsing crosstreebank conversion. The results of the experiments show that, contrary to K¨ubler et al. (2006), the question whether or not German is harder to parse than English remains undecided. 1 Introduction A long-standing and unresolved issue in the parsing literature is whether parsing less-configurational languages is harder than e.g. parsing English. German is a case in point. Results from Dubey and Keller (2003) suggest that state-of-the-art parsing scores for German are generally lower than those obtained for English, while recent results from K¨ubler et al. (2006) raise the possibility that this might Despite being the standard metric for measuring PCFG parser performance, PARSEVAL has been criticised for not representing ’real’ parser quality (Carroll et al., 1998; Brisco et al., 2002; Sampson and Babarbczy, 2003). PARSEVAL checks label and wordspan identity in parser output compared to the original treebank trees. It neither weights results, differentiating between linguistically more or less sev"
D07-1066,W06-1614,0,0.531017,"Missing"
D07-1066,P06-3004,0,0.13696,"TIGER Test Set Table 6 shows that all error types inserted into Sentence 9 in our test set result in the same evaluation score for the PARSEVAL metric, while the LA metric provides a more discriminative treatment of PP attachment errors, label errors and span errors for the same sentence (Table 6). However, the differences in the LA results are only indirectly caused by the different error types. They actually reflect the number of terminal nodes affected by the error insertion. For Label I and II the LA results vary considerably, because the substitution of the PP for 634 K¨ubler (2005) and Maier (2006) assess the impact of the different treebank annotation schemes on PCFG parsing by conducting a number of modifications converting the T¨uBa-D/Z into a format more similar to the NEGRA (and hence TIGER) treebank. After each modification they extract a PCFG from the modified treebank and measure the effect of the changes on parsing results. They show that with each modification transforming the T¨uBa-D/Z into a more NEGRA-like format the parsing results also become more similar to the results of the NEGRA treebank, i.e. the results get worse. Maier takes this as evidence that the T¨uBa-D/Z is m"
D07-1066,H94-1020,0,0.0325769,"e substantial worse than T¨uBa-D/Zbased parser output trees. We have shown that different treebank annotation schemes have a strong impact on parsing results for similar input data with similar (simulated) parser errors. Therefore the question whether a particular language is harder to parse than another language or not, can not be answered by comparing parsing results for parsers trained on treebanks with different annotation schemes. Comparing PARSEVALbased parsing results for a parser trained on the T¨uBa-D/Z or TIGER to results achieved by a parser trained on the English Penn-II treebank (Marcus et al., 1994) does not provide conclusive evidence about the parsability of a particular language, because the results show a bias introduced by the combined effect of annotation scheme and evaluation metric. This means that the question whether German is harder to parse than English, is still undecided. A possible way forward is perhaps a dependency-based evaluation of TIGER/T¨uBa-D/Z with Penn-II trained grammars for ’similar’ test and training sets and cross-treebank and -language controlled error insertion experiments. Even this is not entirely straightforward as it is not completely clear what constit"
D07-1066,C04-1024,0,0.035224,"lds or unary nodes on PCFG parsing, we convert the trees in the parser output of a parser trained on the original unconverted treebank resources. This allows us to preserve the basic syntactic structure and also the errors present in the output trees resulting from a potential bias in the original treebank training resources. The results for the original parser output evaluated against the unmodified gold trees should not be crucially different from the results for the modified parser output evaluated against the modified gold trees. 5.1 Experimental Setup For Experiment II we trained BitPar (Schmid, 2004), a parser for highly ambiguous PCFG grammars, on the two treebanks. The T¨uBa-D/Z training data consists of the 21067 treebank trees not included in the T¨uBa-D/Z test set. Because of the different size of the two treebanks we selected 21067 sentences from the TIGER treebank, starting from sentence 10000 (and excluding the sentences in the TIGER test set). Before extracting the grammars we resolved the crossing branches in the TIGER treebank as described in Section 3. After this preprocessing step we extracted an unlexicalised PCFG from each of our training sets. Our TIGER grammar has a total"
D07-1066,A97-1014,0,\N,Missing
D15-1124,P11-1103,0,0.0278442,"on Empirical Methods in Natural Language Processing, pages 1066–1072, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. Recently, Guzm´an et al. (2015) presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score. In this paper we propose a compact MT evaluation metric. We hypothesize that our model learns different notions of similarity (which other metrics tend to capture using different metrics) using input, output and forget gates of an LSTM architecture."
D15-1124,de-marneffe-etal-2006-generating,0,0.0176828,"Missing"
D15-1124,W14-3348,0,0.0474244,"ase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system D ISKOTK-PARTY-T UNED (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from"
D15-1124,P13-1139,0,0.0275527,"333 ± .011 .336 ± .011 .329 ± .010 .269 ± .011 .263 ± .011 Average .256 ± .013 .289 ± .014 .290 ± .013 .298 ± .013 .300 ± .013 .304 ± .013 .298 ± .013 .298 ± .013 .386 ± .013 .362 ± .013 .356 ± .013 .354 ± .013 .294 ± .013 .285 ± .013 Avg wmt12 .254 ± .013 .287 ± .014 .287 ± .013 .295 ± .014 .297 ± .013 .301 ± .013 .295 ± .013 .295 ± .013 .386 ± .013 .358 ± .013 .346 ± .013 .341 ± .013 .267 ± .013 .258 ± .014 Table 4: Results: Segment-Level Correlations on WMT-14 lated based on rankings and does not consider the amount of difference between scores. Here is an example similar to that given in (Hopkins and May, 2013). Suppose four systems produce the translations T0, T1, T2 and T3. Suppose we have two metrics M1 and M2 and they produce scores and rankings as follows. GS represents the correct ranking and scores; Scores are in a scale [0, 1] with a higher score indicating a better translation: problems should not occur with Pearson correlation at the system level because system-level scores are calculated using more sophisticated approaches (Koehn, 2012; Hopkins and May, 2013; Sakaguchi et al., 2014). For example, Hopkins and May (2013) model the differences among annotators by adding random Gaussian noise"
D15-1124,W14-3352,0,0.0257093,"Missing"
D15-1124,2012.iwslt-papers.5,0,0.0122805,"ns on WMT-14 lated based on rankings and does not consider the amount of difference between scores. Here is an example similar to that given in (Hopkins and May, 2013). Suppose four systems produce the translations T0, T1, T2 and T3. Suppose we have two metrics M1 and M2 and they produce scores and rankings as follows. GS represents the correct ranking and scores; Scores are in a scale [0, 1] with a higher score indicating a better translation: problems should not occur with Pearson correlation at the system level because system-level scores are calculated using more sophisticated approaches (Koehn, 2012; Hopkins and May, 2013; Sakaguchi et al., 2014). For example, Hopkins and May (2013) model the differences among annotators by adding random Gaussian noise. M1: T0 (0.10), T3 (0.71), T1 (0.72), T2 (0.73) We conclude that our dense-vector-space-based ReVal metric is simple, elegant and effective with state-of-the-art results. ReVal is fully competitive with the best of the current complex alternative approaches that involve system combination, extensive external resources, feature engineering and tuning. M2: T1 (0.71), T0 (0.72), T2 (0.73), T3 (0.74) GS: T0 (0.10), T1 (0.71), T2 (0.72), T3 (0."
D15-1124,W14-3336,0,0.128377,"Missing"
D15-1124,marelli-etal-2014-sick,0,0.0221378,"ments. We select a set for which the correlation coefficient is greater than 0.78.4 The correlation is computed using the annotations for which scores are available in the corpus (prs). In other words, the corpus acts as a scoring function for the available reference translation pairs, which gives a similarity score between a reference and a translation. We selected pairs below the variance values obtained for n = 4, 5, 6, 7 and ≥ 8. Finally, all the pairs are merged to obtain a set (L). Apart from this set, we created three other sets for our experiments. The last two also use the SICK data (Marelli et al., 2014) which was developed for evaluating semantic similarity. All four sets are described below: L: contains the set generated by selecting the pairs ranked four or more times and filtering the segments based on the variance LNF: contains the set generated by selecting the pairs ranked four or more times without any filtering depending on the variance 2 The adapted code for MT evaluation scenarios is available at https://github.com/rohitguptacs/ReVal. 3 http://torch.ch 3 0.65 4 The score was decided so that we obtain around 10K pairs which are annotated at least four times. 1068 L+Sick: Added 4500"
D15-1124,W14-3350,0,0.0303616,"Gim´enez and M`arquez, 2010). The metric computes the number of common sub-trees between a reference and a translation using a convolution 1066 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1066–1072, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. Recently, Guzm´an et al. (2015) presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score. In this paper we propose a compact MT evaluation metric. We hypothesize that o"
D15-1124,P02-1040,0,0.104277,"naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system D ISKOTK-PARTY-T UNED"
D15-1124,D14-1162,0,0.0993371,"ns p and pˆθ using regularised KullbackLeibler (KL) divergence. n ( ) λ 1∑ (i) J(θ) = KL p(i) pˆθ + ||θ||22 (3) n 2 i=1 In Equation 3, i represents the index of each training pair, n is the number of training pairs and p is the sparse target distribution such that y = rT p is defined as follows:   j = ⌊y⌋ + 1 y − ⌊y⌋, pj = ⌊y⌋ − y + 1, j = ⌊y⌋   0 otherwise 1067 for 1 ≤ j ≤ K, where, y ∈ [1, K] is the similarity score of a training pair. For example, for y = 2.7, pT = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. For our work, we use glove word vectors (Pennington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by Tai et al. (2015). 2 The system uses the scientific computing framework Torch3 . Training is performed on the data computed in Section 5. The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The compositional parameters for our Tree-LSTM systems with memory dimensions 150 and 300 are 203,400 and 541,800, respectively. The training is performed for 10 epochs. System-level scores are computed by aggregating and normalising segment-level scores. 5 Computing Similarit"
D15-1124,P15-1078,0,0.111911,"Missing"
D15-1124,W14-3301,0,0.0237144,"Missing"
D15-1124,P13-1045,0,0.0142661,"native approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 2 Related Work Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not requ"
D15-1124,D13-1170,0,0.00954455,"Missing"
D15-1124,P15-1150,0,0.0874352,"Missing"
D15-1124,W13-2201,0,\N,Missing
D19-6501,W05-0909,0,0.376585,"Missing"
D19-6501,N18-1118,0,0.0401054,"so related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 201"
D19-6501,W09-2404,0,0.0383165,", 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different am"
D19-6501,D16-1245,0,0.0724909,"Missing"
D19-6501,W19-5321,0,0.0470238,"Missing"
D19-6501,W19-5315,1,0.857744,"Missing"
D19-6501,P18-4020,0,0.0460975,"Missing"
D19-6501,W17-4809,0,0.0532025,"Missing"
D19-6501,W15-3403,0,0.157926,"only on a lexical form, but also on other linguistic means, e.g. articles or modifying pronouns (Kibrik, 2011). The use of these is influenced by various factors which can be language-dependent (range of linguistic means available in grammar) and also contextindependent (pragmatic situation, genre). Thus, the means of expressing reference differ across languages and genres. This has been shown by some studies in the area of contrastive linguistics (Kunz et al., 2017; Kunz and LapshinovaKoltunski, 2015; Kunz and Steiner, 2012). Analyses in cross-lingual coreference resolution (Grishina, 2017; Grishina and Stede, 2015; Nov´ak and ˇ Zabokrtsk´ y, 2014; Green et al., 2011) show that there are still unsolved problems that should be addressed. them in the three translation variants. We also evaluate them from the point of view of coreference chain translation. The goal of this paper is two-fold. On the one hand, we are interested in various properties of coreference chains in these translations. They include total number of chains, average chain length, the size of the longest chain and the total number of annotated mentions. These features are compared to those of the underlying source texts and also the corr"
D19-6501,W18-6435,1,0.923417,"cted to individual phenomena within coreference. For instance, Zinsmeister et al. (2012) analyse abstract anaphors in English-German translations. To our knowledge, they do not consider chains. Lapshinova-Koltunski and Hardmeier (2017b) in their contrastive analysis of potential coreference chain members in English-German translations, describe transformation patterns that contain different types of referring expressions. However, the authors rely on automatic tagging and parsing procedures and do not include chains into their analysis. The data used by Nov´ak and Nedoluzhko (2015) and Nov´ak (2018) contain manual chain annotations. The authors focus on different categories of anaphoric pronouns in English-Czech translations, though not paying attention to chain features (e.g. their number or size). Chain features are considered in a contrastive analysis by Kunz et al. (2017). Their study concerns different phenomena in a variety of genres in English and German comparable texts. Using contrastive interpretations, they suggest preferred translation strategies from English into German, i.e. translators should use demonstrative proBackground and Related Work Coreference Coreference is relat"
D19-6501,W17-4810,1,0.848958,"slation studies Differences between languages and genres in the linguistic means expressing reference are important for translation, as the choice of an appropriate referring expression in the target language poses challenges for both human and machine translation. In translation studies, there is a number of corpus-based works analysing these differences in translation. However, most of them are restricted to individual phenomena within coreference. For instance, Zinsmeister et al. (2012) analyse abstract anaphors in English-German translations. To our knowledge, they do not consider chains. Lapshinova-Koltunski and Hardmeier (2017b) in their contrastive analysis of potential coreference chain members in English-German translations, describe transformation patterns that contain different types of referring expressions. However, the authors rely on automatic tagging and parsing procedures and do not include chains into their analysis. The data used by Nov´ak and Nedoluzhko (2015) and Nov´ak (2018) contain manual chain annotations. The authors focus on different categories of anaphoric pronouns in English-Czech translations, though not paying attention to chain features (e.g. their number or size). Chain features are cons"
D19-6501,W17-1505,0,0.0189275,"ence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embedd"
D19-6501,L18-1065,1,0.922243,"ons in a chain. For every mention, CoreNLP extracts its gender (male, female, neutral, unknown), number (singular, plural, unknown), and animacy (animate, inanimate, unknown). This information is not added directly but used to enrich the single sentence-based MT training data by applying a set of heuristics implemented in DocTrans4 : 3.2 Test data under analysis As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an EnglishGerman corpus annotated with full coreference chains (Lapshinova-Koltunski et al., 2018).7 The corpus contains ca. 160.7 thousand tokens manually annotated with about 14.9 thousand mentions and 4.7 thousand coreference chains. For our analysis, we select a portion of English news texts and TED talks from ParCorFull and translate them with the three NMT systems described in 3.1 above. As texts considerably differ in their length, we select 17 news texts (494 sentences) and four TED talks (518 sentences). The size (in tokens) of the total data set under analysis – source (src) and human translations (ref) from ParCorFull and the automatic translations produced within this study (S1"
D19-6501,W19-2805,1,0.863219,"Missing"
D19-6501,C14-1003,0,0.0917501,"Missing"
D19-6501,P02-1040,0,0.10349,"Missing"
D19-6501,P14-5010,0,0.00262815,"described in the following section are preprocessed in the same way. S2 uses the same data as S1 with the addition of a filtered portion of Paracrawl. This corpus is known to be noisy, so we use it to create a larger training corpus but it is diluted by a factor 4 to give more importance to high quality translations. S3 S3 uses the same data as S1, but this time enriched with the cross- and intra-sentential coreference chain markup as described below.2 The information is included as follows. Source documents are annotated with coreference chains using the neural annotator of Stanford CoreNLP (Manning et al., 2014)3 . The tool detects pronouns, nominal phrases and proper names as mentions in a chain. For every mention, CoreNLP extracts its gender (male, female, neutral, unknown), number (singular, plural, unknown), and animacy (animate, inanimate, unknown). This information is not added directly but used to enrich the single sentence-based MT training data by applying a set of heuristics implemented in DocTrans4 : 3.2 Test data under analysis As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParC"
D19-6501,W18-6306,0,0.0218498,"ystems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a"
D19-6501,W19-6614,0,0.0116979,"d columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more"
D19-6501,P18-1118,0,0.0199575,"apid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test set"
D19-6501,W17-4811,0,0.0182399,"to find shining through and explicitation effects in automatic translations. 2.3 # lines Common Crawl 2,394,878 Europarl 1,775,445 News Commentary 328,059 Rapid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance,"
D19-6501,Q18-1029,0,0.0192923,"1,775,445 News Commentary 328,059 Rapid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the cr"
D19-6501,D18-1325,0,0.0153457,"x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et"
D19-6501,N12-1046,0,0.0163913,"en et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sen"
D19-6501,W12-2503,0,0.0332319,"where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translati"
D19-6501,P18-1117,0,0.0821721,"ic coreference translation have shown that dedicated systems can lead to improvements in pronoun translation (Guillou et al., 2016; Lo´aiciga et al., 2017). However, standard NMT systems work at sentence level, so improvements in NMT translate into improvements on pronouns with intra-sentential antecedents, but the phenomenon of coreference is not limited to anaphoric pronouns, and even less to a subset of them. Document-level machine translation (MT) systems are needed to deal with coreference as a whole. Although some attempts to include extrasentential information exist (Wang et al., 2017; Voita et al., 2018; Jean and Cho, 2019; JunczysDowmunt, 2019), the problem is far from being solved. Besides that, some further problems of NMT that do not seem to be related to coreference at first glance (such as translation of unknown words and proper names or the hallucination of additional words) cause coreference-related errors. In our work, we focus on the analysis of complete coreference chains, manually annotating We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric info"
D19-6501,D17-1301,0,0.0231774,"studies in automatic coreference translation have shown that dedicated systems can lead to improvements in pronoun translation (Guillou et al., 2016; Lo´aiciga et al., 2017). However, standard NMT systems work at sentence level, so improvements in NMT translate into improvements on pronouns with intra-sentential antecedents, but the phenomenon of coreference is not limited to anaphoric pronouns, and even less to a subset of them. Document-level machine translation (MT) systems are needed to deal with coreference as a whole. Although some attempts to include extrasentential information exist (Wang et al., 2017; Voita et al., 2018; Jean and Cho, 2019; JunczysDowmunt, 2019), the problem is far from being solved. Besides that, some further problems of NMT that do not seem to be related to coreference at first glance (such as translation of unknown words and proper names or the hallucination of additional words) cause coreference-related errors. In our work, we focus on the analysis of complete coreference chains, manually annotating We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sente"
D19-6501,2011.mtsummit-papers.13,0,0.0448914,"et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different amounts of data (6M v"
E14-4036,N10-1062,0,0.159017,"Missing"
E14-4036,W13-2235,0,0.0830041,"entation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower than that of technical documentation, which impacts on the results obtained with incremental retraining. method, where s and t stand for source and target, re"
E14-4036,D11-1033,0,0.261389,"al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower th"
E14-4036,W13-2237,0,0.194557,"Missing"
E14-4036,2013.mtsummit-papers.5,0,0.0193829,"Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2"
E14-4036,N04-1043,0,0.053098,"AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, w"
E14-4036,P10-1088,0,0.0275825,"esented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domai"
E14-4036,P10-2041,0,0.24501,"al approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news i"
E14-4036,J93-2003,0,0.0267032,"mpared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et"
E14-4036,E12-1025,0,0.178427,"Missing"
E14-4036,2013.mtsummit-papers.24,0,0.240964,"Missing"
E14-4036,N09-1047,0,0.217755,"translation job) is presented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation"
E14-4036,2006.amta-papers.25,0,0.175582,"erences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, we aim at reducing the overall PE time in PE-based incremental MT update applications in the localisation industry. In our experiments reported in Section 3 below we want to explore a space consisting of a considerable number of selection strategies and incremental retraining batch sizes. In order to be able to do this, we use the target side of our industry translation memory data to approximate human PE output and automatic TER (Snover et al., 2006) scores as a proxy for human PE times (O’Brien, 2011). 2 • N-gram Overlap. An SMT system will encounter problems translating sentences containing n-grams not seen in the training data. Thus, PEs of sentences with high number of unseen n-grams are considered to be more informative for updating the current MT system. However, for the MT system to translate unseen n-grams accurately, they need to be seen a minimum number V times.2 We use an n-gram overlap function similar to the one described in (Gonz´alez-Rubio et al., 2012) given in Equation 1 where N (T (i) ) and N (S (i) ) return i-grams in t"
E14-4036,P07-2045,0,0.00362596,"t results. 3 Dir EN→FR FR→EN EN→DE DE→EN Random 29.64 27.08 24.00 19.36 Seq. 29.81 27.04 24.08 19.34 Ngram 28.97 26.15 22.34 17.70 CED 29.25 26.63 22.60 17.97 CEDN 29.05 26.39 22.32 17.48 Table 2: TER average scores for Setting 1 Experiments and Results Dir EN→FR FR→EN EN→DE DE→EN We use technical documentation data taken from Symantec translation memories for the English– French (EN–FR) and English–German (EN–DE) language pairs (both directions) for our experiments. The statistics of the data (training and incremental splits) are shown in Table 1. All the systems are trained using the Moses (Koehn et al., 2007) phrase-based statistical MT system, with IRSTLM (Federico et al., 2008) for language modelling (n-grams up to order five) and with the alignment heuristic grow-diag-final-and. Random 36.23 33.26 32.23 27.24 Seq. 36.26 33.34 32.19 27.29 Ngram 35.20 32.26 30.58 26.10 CED 35.48 32.69 31.96 26.73 CEDN 35.17 32.17 29.98 24.94 Table 3: TER average scores for Setting 2 For Setting 1 (Table 2), the best result is obtained by the CEDN criterion for two out of the four directions. For EN→FR, n-gram overlap 4 As this study simulates the post-editing, we use the references of the translated segments inst"
E14-4036,P02-1016,0,0.0611916,"E MT applications. • Our experiments show that AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aim"
E17-1048,D15-1041,0,0.217212,"15; Gillick et al., 2016; Plank et al., 2016). The results are competitive but do not systematically outperform the state of the art. Only Plank et al. (2016) report consistent gains by using shallow neural network architectures in combination with multitask learning, multilingual Introduction Character-based approaches have been studied for many applications in natural language processing, including part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016; Ma and Hovy, 2016), morphological tagging (Labeau et al., 2015), parsing (Ballesteros et al., 2015), named entity recognition (Gillick et al., 2016), language modeling (Ling et al., 2015; Kim et al., 2016), and neural machine translation (Costajuss`a and Fonollosa, 2016). Character-based representations have the advantage of gracefully handling rare or unseen words and tend to produce more compact models as the number of atomic units, i.e., characters, is smaller compared to the number of words in word-level approaches. The issue of rare or unseen words is particularly pro505 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume"
E17-1048,P16-2058,0,0.0797891,"Missing"
E17-1048,N16-1155,0,0.17856,"ict a POSMORPH tag as a complete unit, rather than as the individual component parts. This approach allows us to share large parts of the model but can only produce POS-MORPH analyses attested in the training data (cf. Table 2). This is still the standard approach to morphological tagging and disambiguation as, given sufficient amounts of training data, the number of POS-MORPH descriptions that cannot be produced usually is small. Character-based POS tagging (rather than full POS-MORPH tagging) has been extensively evaluated in the literature (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016). The results are competitive but do not systematically outperform the state of the art. Only Plank et al. (2016) report consistent gains by using shallow neural network architectures in combination with multitask learning, multilingual Introduction Character-based approaches have been studied for many applications in natural language processing, including part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016; Ma and Hovy, 2016), morphological tagging (Labeau et al., 2015), parsing (Ballesteros et al., 2015"
E17-1048,A94-1024,0,0.842018,"Missing"
E17-1048,P98-1080,0,0.743595,"Genabith DFKI DFKI & Saarland University Saarbr¨ucken, Germany Saarbr¨ucken, Germany neumann@dfki.de josef.van genabith@dfki.de Abstract nounced when working on morphologically-rich languages, small amounts of training data or noisy user input. Morphological tagging is the task of assigning a morphological analysis to a token in context. The morphological analysis for a word consists of a sequence of feature:value pairs describing, for example, case, gender, person and tense. A particular concatenation of such feature:value pairs is referred to as a single tag (Oflazer and ˙Ilker Kuroz, 1994; Hajic and Hladka, 1998; Mueller et al., 2013). Following (M¨uller and Schuetze, 2015), we also add the part-of-speech to this morphological tag and refer to it as POS-MORPH: This paper investigates neural characterbased morphological tagging for languages with complex morphology and large tag sets. Character-based approaches are attractive as they can handle rarelyand unseen words gracefully. We evaluate on 14 languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except for English and French, where we match the state-of-the-art. We compare two architectures for co"
E17-1048,P16-2067,0,0.487606,"a complete unit, rather than as the individual component parts. This approach allows us to share large parts of the model but can only produce POS-MORPH analyses attested in the training data (cf. Table 2). This is still the standard approach to morphological tagging and disambiguation as, given sufficient amounts of training data, the number of POS-MORPH descriptions that cannot be produced usually is small. Character-based POS tagging (rather than full POS-MORPH tagging) has been extensively evaluated in the literature (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016). The results are competitive but do not systematically outperform the state of the art. Only Plank et al. (2016) report consistent gains by using shallow neural network architectures in combination with multitask learning, multilingual Introduction Character-based approaches have been studied for many applications in natural language processing, including part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016; Ma and Hovy, 2016), morphological tagging (Labeau et al., 2015), parsing (Ballesteros et al., 2015), named entity recog"
E17-1048,D15-1025,0,0.07497,"Missing"
E17-1048,D15-1176,0,0.0749176,"in context, we predict a POSMORPH tag as a complete unit, rather than as the individual component parts. This approach allows us to share large parts of the model but can only produce POS-MORPH analyses attested in the training data (cf. Table 2). This is still the standard approach to morphological tagging and disambiguation as, given sufficient amounts of training data, the number of POS-MORPH descriptions that cannot be produced usually is small. Character-based POS tagging (rather than full POS-MORPH tagging) has been extensively evaluated in the literature (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016). The results are competitive but do not systematically outperform the state of the art. Only Plank et al. (2016) report consistent gains by using shallow neural network architectures in combination with multitask learning, multilingual Introduction Character-based approaches have been studied for many applications in natural language processing, including part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016; Ma and Hovy, 2016), morphological tagging (Labeau et al., 2015), parsing (Ba"
E17-1048,P16-1101,0,0.250452,"valuated in the literature (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016). The results are competitive but do not systematically outperform the state of the art. Only Plank et al. (2016) report consistent gains by using shallow neural network architectures in combination with multitask learning, multilingual Introduction Character-based approaches have been studied for many applications in natural language processing, including part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015; Gillick et al., 2016; Plank et al., 2016; Ma and Hovy, 2016), morphological tagging (Labeau et al., 2015), parsing (Ballesteros et al., 2015), named entity recognition (Gillick et al., 2016), language modeling (Ling et al., 2015; Kim et al., 2016), and neural machine translation (Costajuss`a and Fonollosa, 2016). Character-based representations have the advantage of gracefully handling rare or unseen words and tend to produce more compact models as the number of atomic units, i.e., characters, is smaller compared to the number of words in word-level approaches. The issue of rare or unseen words is particularly pro505 Proceedings of the 15th Conference"
E17-1048,D13-1032,0,0.110426,"Missing"
E17-1048,N15-1055,0,0.211727,"Missing"
E17-1048,C98-1077,0,\N,Missing
E17-2056,P15-2026,0,0.249424,"human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym ) is an extended version of an attention based NMT"
E17-2056,N16-1102,0,0.204776,"rate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior"
E17-2056,D15-1200,0,0.0129567,"unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to b"
E17-2056,W06-1607,0,0.0220804,"rd pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is the context vecP x tor computed as ct = Ti=1 αti hi . Here, αti is the weight of each hi and can be computed as in Equation 1 exp(eti ) αti = Pm (1) j=1 exp(etj ) where eti = a(ηt−1 , hi ) is a word alignment model. Based on the input (mt) and output (pe) sequence lengths, Tx and Ty , the alignment model is computed Tx × Ty"
E17-2056,N06-1014,0,0.513033,"corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is gener"
E17-2056,D08-1089,0,0.0468216,"ata. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is th"
E17-2056,W11-2123,0,0.0229802,"mt–pe symmetric model (AP ESym ) against the best performing system (W M TBest ) in the WMT 2016 APE task and the standard log-linear mt–pe PB-SMT model with hybrid prior alignment as described in Section 2.1 (AP EB2 ). AP EB2 and AP ESym models are trained on 4.55M (4.5M + 12K + pre-aligned word pairs) parallel mt–pe data. The pre-aligned word pairs are obtained from the hybrid prior word alignments (Section 2.1) of the 12K WMT APE training data. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high pro"
E17-2056,J03-1002,0,0.0268,"edding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the othe"
E17-2056,W16-2378,0,0.447981,"alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the other systems. The main contributions of our research are (i) an application of bilingual symmetry of the bidirectional RNN for APE, (ii) using a hybrid multialignment based approach for the prior alignments, (iii) a smart way of embedding word alignment information in neural APE, and (iv) applying reranking for the APE task. The remainder of the paper i"
E17-2056,P03-1021,0,0.127663,"= σ(W r Ex is the word embedding matrix of the MT output, W r ∈ Rm×n and U r ∈ Rn×n are weight matrices, m is the word embedding dimensionality and n represents the number of hidden units. Symmetric Neural Automatic Post Editing Using Prior Alignment Below we describe bilingual symmetry of bidirectional RNN with embedded prior word alignment for APE. 2.1 Symmetric Neural APE Hybrid Prior Alignment The monolingual mt–pe parallel corpus is first word aligned using a hybrid word alignment method based on the alignment combination of three different statistical word alignment methods: (i) GIZA++ (Och, 2003) word alignment with 350 data described in Bojar et al. (2016) and for some experiments we also use the 4.5M artificially developed APE data described in Junczys-Dowmunt and Grundkiewicz (2016). The training data consists of English–German triplets containing source English text (src) from the IT domain, corresponding German translations (mt) from a firststage MT system and the corresponding human post-edited version (pe). Development and test data contain 1,000 and 2,000 triplets respectively. We considered two baselines: (i) the raw MT output provided by the first-stage MT system serves as B"
E17-2056,W13-2814,1,0.838103,"dings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translatio"
E17-2056,W15-3026,1,0.847079,"Missing"
E17-2056,W04-3250,0,0.0339845,"Missing"
E17-2056,J10-4005,0,0.0193819,"ions produced by MT systems often need to be corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification"
E17-2056,C16-1241,1,0.878149,"Missing"
E17-2056,W07-0734,0,0.0440759,"network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-"
E17-2056,P16-2046,1,0.906567,"Missing"
E17-2056,C16-2021,1,0.88815,"Missing"
E17-2056,W16-2379,1,0.894217,"Missing"
E17-2056,P02-1040,0,0.0983167,"Missing"
E17-2056,W12-3146,0,0.137315,"Missing"
E17-2056,N07-1064,0,0.44034,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,W07-0728,0,0.0707865,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,2006.amta-papers.25,0,0.0951164,"In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by lookin"
E17-2056,C96-2141,0,0.645894,"α are alignment sumj i αi,j (attention) matrices of Tx × Ty dimensions. The advantage of symmetrical alignment cells is that they are normalized using softmax (values in between 0 and 1), therefore, the trace term is bounded above by min(Tx , Ty ), representing perfect one-to-one alignments in both directions. To train each directional attention model (mt → pe and pe → mt), we follow the work described in Cohn et al. (2016), where absolute positional bias between the MT and PE translation (as in IBM Model 2), fertility relative position bias (as in IBM Models 3, 4, 5) and HMM-based Alignment (Vogel et al., 1996) are incorporated with an attention based soft alignment model. 3 Experiments and Results We carried out our experiments on the 12K English–German WMT 2016 APE task training 1 351 http://www.statmt.org/moses/ For setting up our neural network, previous to training the AP ESym model, we performed a number of preprocessing steps on the mt–pe parallel training data. First, we prepare a LUT containing mt–pe hybrid prior word alignment above (Section 2.1) a certain lexical translation probability threshold (0.3). To ensure efficient use of the hybrid prior alignment we replaced each mt word by a un"
E17-3002,W15-0514,0,0.030732,"r, they do not offer effective functionalities for 1) easy access to the argumentative structure of debate content, and 2) quick overviews of the various semantic facets, the polarity and the relevance of the arguments. Some platforms1 allow users to label posts as pro or con arguments, to cite external sources, to assess debate content or to create structured debates across the web, but do not offer any deeper automatic language technologybased analyses. Argumentation mining research, which could help in automatically structuring debates, has only recently been applied to web debate corpora (Boltuzic and Snajder, 2015; Petasis and Karkaletsis, 2016; Egan et al., 2016). Our goal is to address these issues by developing a debate platform that: • Supports debate participants in making substantial and clear contributions • Facilitates an overview of debate contents • Associates relevant information available on the web with debate topics • Connects regional discussions to global deliberations • Supports advanced participation in deliberations, without sacrificing transparency and usability. In this paper, we present the Common Round platform, which implements various functions towards these goals, with followi"
E17-3002,W16-2816,0,0.0376372,"Missing"
E17-3002,P09-1113,0,0.0119288,"odels to recognize standard entity types, such as persons, organizations, locations and date/time expressions. For non-standard concept types, we use SProUT (Drozdzynski et al., 2004), which implements a regular expression-like rule formalism and gazetteers for detecting domain-specific concepts in text. Relation extraction is performed by matching dependency parse trees of sentences to a set of automatically learned dependency patterns (Krause et al., 2012). Relation patterns are learned from corpora manually annotated with event type, argument types, and roles, or using distant supervision (Mintz et al., 2009). For sentiment analysis, we apply a lexicon-based approach that additionally makes use of syntactic information in order to handle negation. Text Analytics and Association with External Material The Common Round platform enriches the contents of debates and posts by extracting information about topics, sentiment, entities and relations. Topic detection helps to find semantically related debates. Sentiment analysis allows determining the emotion level in discussions. By identifying, for example, instances of the relation MayTreatDisorder in a discussion about the legalization of cannabis, we c"
E17-3002,W16-2811,0,\N,Missing
E93-1003,J88-1004,0,0.0532927,"Missing"
federmann-etal-2012-ml4hmt,vandeghinste-etal-2008-evaluation,1,\N,Missing
federmann-etal-2012-ml4hmt,federmann-2010-appraise,1,\N,Missing
federmann-etal-2012-ml4hmt,E06-1005,0,\N,Missing
federmann-etal-2012-ml4hmt,W09-0424,0,\N,Missing
federmann-etal-2012-ml4hmt,P02-1040,0,\N,Missing
federmann-etal-2012-ml4hmt,W02-1019,0,\N,Missing
federmann-etal-2012-ml4hmt,W05-0909,0,\N,Missing
federmann-etal-2012-ml4hmt,W08-0309,0,\N,Missing
federmann-etal-2012-ml4hmt,W10-1720,1,\N,Missing
federmann-etal-2012-ml4hmt,W11-2101,0,\N,Missing
foster-van-genabith-2008-parser,A00-2018,0,\N,Missing
foster-van-genabith-2008-parser,W03-3023,0,\N,Missing
foster-van-genabith-2008-parser,W07-2204,1,\N,Missing
foster-van-genabith-2008-parser,H94-1020,0,\N,Missing
foster-van-genabith-2008-parser,D07-1066,1,\N,Missing
foster-van-genabith-2008-parser,P06-1043,0,\N,Missing
foster-van-genabith-2008-parser,P06-1055,0,\N,Missing
foster-van-genabith-2008-parser,W07-2416,0,\N,Missing
foster-van-genabith-2008-parser,D07-1096,0,\N,Missing
foster-van-genabith-2008-parser,N06-1020,0,\N,Missing
I11-1100,J04-4004,0,0.077163,"Missing"
I11-1100,cer-etal-2010-parsing,0,0.032005,"Missing"
I11-1100,C08-1071,0,0.0212923,"Missing"
I11-1100,N10-1004,0,0.0312252,"iques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular document. We intend to experiment with this idea in the context of Web 2.0 parsing. More Parser Evaluation The cross-parser evaluation we have presented in the first half of the paper is by no means exhaustive. For example, to measure the positive effect of discriminative reranking, the first-stage Brown parser should also be included in the evaluation. Other statistical parsers could be evaluated, and it would be interesting to examine the performance of systems which employ hand-crafted grammars and tre"
I11-1100,de-marneffe-etal-2006-generating,0,0.00984564,"Missing"
I11-1100,P05-1012,0,0.0540053,"ers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser p"
I11-1100,N10-1060,1,0.883765,", Joachim Wagner1 , Joseph Le Roux2 Joakim Nivre3 , Deirdre Hogan1 and Josef van Genabith1 1,3 NCLT/CNGL, Dublin City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parser"
I11-1100,gimenez-marquez-2004-svmtool,0,0.0101406,"Missing"
I11-1100,P11-2008,0,0.0596175,"Missing"
I11-1100,D09-1087,0,0.0469586,"Missing"
I11-1100,D10-1002,0,0.00568548,"d user-generated content can be used to improve parser accuracy. The reasons for the improvements yielded by the three types of retraining need to be determined.6 The underperformance of the TwitterTrain material in comparison to the FootballTrain material suggests that sample selection involving language and topic identification needs to be applied before parser retraining. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example,"
I11-1100,P06-1063,1,0.829525,"Missing"
I11-1100,H94-1020,0,0.0475196,"Missing"
I11-1100,P06-1043,0,0.281537,"he discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get by exploiting unlabelled target domain data. We employ three types of parser retraining, namely, 1) the McClosky et al. (2006) self-training protocol, 2) uptraining of Malt using dependency trees produced by a slightly more accurate phrase structure parser (Petrov et al., 2010), and 3) PCFG-LA self-training (Huang We investigate the problem of parsing the noisy language of social media. We evaluate four Wall-Street-Journal-trained statistical parsers (Berkeley, Brown, Malt and MST) on a new dataset containing 1,000 phrase structure trees for sentences from microblogs (tweets) and discussion forum posts. We compare the four parsers on their ability to produce Stanford dependencies for these Web 2.0 sentences. We find"
I11-1100,P08-1108,1,0.789006,"ng. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a p"
I11-1100,nivre-etal-2006-maltparser,1,0.187118,"rances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a t"
I11-1100,W09-3811,1,0.21318,"ntaining the supplied POS tag for a given word may be removed from the chart during coarse-to-fine pruning.2 3 Baseline Evaluation We first evaluate four widely used WSJ-trained statistical parsers on our new Web 2.0 datasets: Berkeley (Petrov et al., 2006) We train a PCFG-LA using 6 iterations and we run the parser in accurate mode. 3.1 Results Brown (Charniak and Johnson, 2005) We employ this parser in its out-of-the-box settings. Table 2 shows the Parseval f-score and part-ofspeech (POS) tagging accuracy for the Berkeley Malt (Nivre et al., 2006) We use the stacklazy algorithm described in Nivre et al. (2009). We train a linear classifier where the feature interactions are modelled explicitly. 2 In the interest of replicability, detailed information on experimental settings is available at http: //nclt.computing.dcu.ie/publications/ foster_ijcnlp11.html. 895 Parser Berk O Berk P Brown Malt P MST P Berk G Malt G MST G LAS UAS WSJ22 90.5 93.2 89.9 92.5 91.5 94.2 88.0 90.6 88.8 91.3 91.6 93.4 90.0 91.6 90.7 92.3 LAS UAS FootballDev 79.8 84.8 80.1 84.9 82.0 86.3 76.1 81.5 76.4 81.1 83.1 86.4 80.4 83.7 80.8 83.4 LAS UAS TwitterDev 68.9 75.1 68.2 74.2 71.4 77.3 67.3 73.6 68.1 73.8 76.8 80.8 78.3 81.6 78"
I11-1100,C10-1094,1,0.884961,"Missing"
I11-1100,P06-1055,0,0.0152372,"in City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms"
I11-1100,D10-1069,0,0.0996262,"t of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get"
I11-1100,N10-1003,0,0.00470176,"the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular docum"
I11-1100,W10-2105,0,0.0674734,"Missing"
I11-1100,W09-2205,0,0.0221942,"Missing"
I11-1100,P07-1078,0,0.0355222,"Missing"
I11-1100,W10-2606,0,0.0124848,"Missing"
I11-1100,E03-1008,0,0.0496129,"Missing"
I11-1100,W99-0623,0,\N,Missing
J05-3003,P87-1027,0,0.109086,"and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar"
J05-3003,J93-2002,0,0.800674,"ter detail in Section 6, in which we compare our results with those reported elsewhere in the literature. We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based"
J05-3003,A97-1052,0,0.911163,"and the constituents with which they co-occur. He assumes 19 different subcategorization 334 O’Donovan et al. Large-Scale Induction and Evaluation of Lexical Resources frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb cla"
J05-3003,Y04-1016,1,0.884078,"Missing"
J05-3003,P04-1041,1,0.777592,"Missing"
J05-3003,2000.iwpt-1.9,0,0.104136,"e (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004; Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically"
J05-3003,P97-1003,0,0.14526,"n of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head,"
J05-3003,C94-1042,0,0.276371,"Missing"
J05-3003,J93-1005,0,0.22634,"Missing"
J05-3003,W03-2401,0,0.0288161,"Missing"
J05-3003,kingsbury-palmer-2002-treebank,0,0.025675,"Missing"
J05-3003,kinyon-prolo-2002-identifying,0,0.479907,"Missing"
J05-3003,P98-1115,0,0.0235922,"Missing"
J05-3003,H94-1003,0,0.0569859,"Missing"
J05-3003,P95-1037,0,0.00938586,"has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the app"
J05-3003,P93-1032,0,0.746649,"is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hoc"
J05-3003,H94-1020,0,0.0194078,"-structure information. F-structures are attribute–value structures which represent abstract syntactic information, approximating to basic predicate–argument–modifier structures. Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. However, more recent work (Cahill et al. 2002; Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences. We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter, for which an amended version of Magerman (1994) is used. The head is annotated with the LFG equati"
J05-3003,P04-1047,1,0.624486,"Missing"
J05-3003,P98-2184,0,0.0423901,"in higher recall scores than those achieved when we (effectively) reversed the mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a conflation of our more fine-grained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains. We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt,"
J05-3003,C00-2100,0,0.138208,"ts of verb occurrences in the Penn-II Treebank. This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 335 Computational Linguistics Volume 31, Number 3 1998). Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesi"
J05-3003,schulte-im-walde-2002-subcategorisation,0,0.146073,"Missing"
J05-3003,C02-1145,0,0.0658598,"Missing"
J05-3003,W98-1505,0,\N,Missing
J05-3003,C98-2179,0,\N,Missing
J05-3003,C98-1111,0,\N,Missing
J08-1003,J97-4005,0,0.0398419,"Missing"
J08-1003,baldwin-etal-2004-road,0,0.0160361,"btain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive effo"
J08-1003,H91-1060,0,0.0647814,"Missing"
J08-1003,E03-1005,0,0.0139868,"tensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage"
J08-1003,J93-1002,0,0.305911,"0], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 howev"
J08-1003,P06-2006,0,0.0594671,"Missing"
J08-1003,W02-1503,0,0.0261788,"uages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf"
J08-1003,P04-1041,1,0.839632,"Missing"
J08-1003,C02-1013,0,0.117113,"al locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been success"
J08-1003,A00-2018,0,0.01612,"Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically"
J08-1003,P04-1014,0,0.0567308,"Missing"
J08-1003,P07-1032,0,0.0390822,"Missing"
J08-1003,P97-1003,0,0.151407,"arsers). The article also reports on a task-based evaluation experiment to rank the parsers using the grammatical relations as input to an anaphora resolution system. Preiss concluded that parser ranking using grammatical relations reﬂected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep uniﬁcation parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at th"
J08-1003,W03-1005,0,0.0353103,"Missing"
J08-1003,C86-1129,0,0.453477,"ion, as well as traces and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II CFG trees with LFG f-structure information. Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse the tree and deterministically add f-structure equations to the phrasal and leaf nodes of the tree, resulting in an f-structure annotated version of the tree. The annotations are then collected and passed on to a constraint solver which generates an f-structure (if the constraints are satisﬁable). We use a simple graph-uniﬁcation-based constraint solver ¨ (Eisele and Dorre 1986), extended to handle path, set-valued, disjunctive, and existential constraints. Given parser output without Penn-II style annotations and traces, the same algorithm is used to assign annotations to each node in the tree, whereas a separate module is applied at the level of f-structure to resolve any long-distance dependencies (see Section 2.3). 5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms (QLFs), and Underspeciﬁed Discourse Representation Structures (UDRSs). 86 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Tabl"
J08-1003,W03-1008,0,0.0100689,"resent research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser compa"
J08-1003,P03-1046,0,0.0243758,"results show that machine-learning-based resources can outperform deep, state-of-the-art hand-crafted resources with respect to the quality of dependencies generated. Treebank-based, deep and wide-coverage constraint-based grammar acquisition has become an important research topic: Starting with the seminal TAG-based work of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao, Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automati"
J08-1003,P02-1043,0,0.347852,"and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can be classiﬁed as “conversion”- or “annotation”-based. TAG-based approaches convert 1 Our use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations"
J08-1003,P02-1018,0,0.00917585,"s Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) ele"
J08-1003,N04-1013,1,0.397922,"is (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate strings to informatio"
J08-1003,W03-2401,1,0.923661,"ular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep gram"
J08-1003,P03-1054,0,0.0412189,"ime-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treeba"
J08-1003,P04-1042,0,0.0169367,"pendencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al"
J08-1003,H94-1020,0,0.0187269,"of (related) drawbacks: 1. Bracketed trees do not always provide NLP applications with enough information to carry out the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate–argument structures, or simple logical forms. 2. A number of alternative, but equally valid tree representations can potentially be given for the same input. To give just a few examples: In English, VPs containing modals and auxiliaries can be analyzed using (predominantly) binary branching rules (Penn-II [Marcus et al. 1994]), or employ ﬂatter analyses where modals and auxiliaries are sisters of the main verb (AP treebank [Leech and Garside 1991]), or indeed do without a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank bracketing guidelines can use “traditional” CFG categories such as S, NP, and so on (Penn-II) or a maximal projection-inspired analysis with IPs and DPs (Chinese Penn Treebank [Xue et al. 2004]). 3. Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), e"
J08-1003,E06-1011,0,0.0298537,"use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume"
J08-1003,C04-1204,0,0.0126897,"I treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser comparison is a non-trivial and t"
J08-1003,P04-1047,1,0.924795,"Missing"
J08-1003,E03-1025,0,0.233427,"uistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate s"
J08-1003,P02-1035,1,0.159909,"RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are sha"
J08-1003,C96-1045,1,0.742182,"Missing"
J08-1003,P97-1052,1,0.798137,"Missing"
J08-1003,C00-2137,0,0.0191341,"able tuples between the two systems. Approximate randomization produces shufﬂes by random assignments instead of evaluating all 2S possible assignments. Signiﬁcance levels are computed as the percentage of trials where the pseudo statistic, that is the test statistic computed on the shufﬂed data, is greater than or equal to the actual statistic, that is the test statistic computed on the test data. A sketch of an algorithm for approximate randomization testing is given in Figure 12. 21 Applications of this test to natural language processing problems can be found in Chinchor et al. (1993) and Yeh (2000). 104 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Table 10 Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for approximate randomization test for 10,000,000 randomizations. PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 .0003 <.0001 <.0001 - Table 10 gives the p-values (the smallest ﬁxed level at which the null hypothesis can be rejected) for comparing each parser against all of the other"
J08-1003,J98-4004,0,\N,Missing
J08-1003,W96-0213,0,\N,Missing
J08-1003,J03-4003,0,\N,Missing
J08-1003,J93-3001,0,\N,Missing
L16-1095,2013.mtsummit-wptp.13,0,0.0502028,"of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFa"
L16-1095,2012.amta-papers.22,0,0.0455595,"lation process and translator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a u"
L16-1095,C14-2028,0,0.467888,"rection, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFast Anywhere2 , MateCat3 (Federico et al., 2014), Wordbee4 , and many others. In our opinion, this is a trend in the translation industry and it motivated us to release CATaLog online. Online CAT tools have a number of advantages over desktop tools, most notably: they do not require local installation; they can be used from any computer; projects can be easily shared with multiple translators; project managers can track the progress of projects on the fly. This paper presents CATaLog online and summarizes the 1 The tool is available online. For more information, consult the following URL: http://ttg.uni-saarland.de/software/catalog 2 https:"
L16-1095,2014.eamt-1.2,0,0.038703,"Missing"
L16-1095,W15-4905,1,0.897711,"Missing"
L16-1095,P10-1064,1,0.894586,"Missing"
L16-1095,2010.jec-1.3,0,0.0197391,"f translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predi"
L16-1095,2010.jec-1.4,0,0.0200997,"t al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: • It provides a novel and user-friendly online CAT environment to post-editors and translators to reduce postediting time and effort, as displayed in Figure 1. • It collects post-editing logs which are a fundamental source of information for the translation process re"
L16-1095,P07-2045,0,0.00699121,"Missing"
L16-1095,J10-4005,0,0.0159378,"to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. When the translator selects the colour-coded TM alternative (c.f., Fi"
L16-1095,2008.amta-srw.4,0,0.0371658,"ion 2 presents related studies focusing on the integration between TM matches and MT output to improve CAT tools; Section 3 describes in detail the main functions of CATaLog online; Section 4 presents the language pairs and data that are currently included in CATaLog online; Section 5 discusses the main functions of CATaLog online and their importance for translators, researchers and project managements; finally, Section 6 concludes this paper and presents avenues for future research. 2. Related Work CAT tools are regarded to increase translator’s productivity and improve translation quality (Lagoudaki, 2008). The core component of most commercial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge"
L16-1095,W15-5206,1,0.787357,"Missing"
L16-1095,J03-1002,0,0.0059723,"e background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. Whe"
L16-1095,W15-3017,1,0.667715,"Missing"
L16-1095,W15-3026,1,0.840277,"Missing"
L16-1095,W15-4916,1,0.889319,"Missing"
L16-1095,2009.mtsummit-papers.14,0,0.0528677,"egments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: •"
L16-1095,2006.amta-papers.25,0,0.0723154,"y to compare various translation engines taking human evaluation into account. A more detailed description of these functionalities is given in the following sections. 3.1. A Novel CAT Environment In CATaLog online, users can choose between MT output and TM segments. The tool allows the user to choose either the background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single"
L16-1095,W14-3323,1,0.897115,"Missing"
L16-1095,2011.mtsummit-papers.37,0,0.027697,"rcial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools suc"
L16-1095,2014.tc-1.11,0,0.0129636,"anslations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work ha"
L16-1095,W14-0314,1,0.846882,"slator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project man"
L16-1095,2015.eamt-1.17,1,\N,Missing
L16-1095,2015.eamt-1.6,1,\N,Missing
L16-1251,piperidis-etal-2014-meta,1,0.621298,"itectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next steps. 2. Recent Developm"
L16-1251,piperidis-2012-meta,0,0.0809183,"ructures and Architectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next"
L16-1251,W15-4941,1,0.744968,"lishment of a Memorandum of Understanding among the 12 organisations is foreseen with the goal of forming a “Coalition for a Multilingual Europe”. 5 http://rigasummit2015.eu The Riga Summit 2015 was jointly organised by METANET (through the project CRACKER), LT-Innovate (through the project LT_Observatory), Tilde and the European Commission. 7 BDVA, CITIA, CLARIN, EFNIL, ELEN, ELRA, GALA, LTInnovate, META-NET, NPLD, TAUS, W3C. 6 2.5. The Strategic Agenda for the Multilingual Digital Single Market Building upon past activities, in particular the METANET SRA (2013), the two EU-projects CRACKER (Rehm, 2015) and LT_Observatory prepared the Strategic Agenda for the Multilingual Digital Single Market (SRIA, 2015). Here, we can only provide a brief summary: the setup of the large and ambitious strategic programme towards the MDSM consists of three layers. On the top layer we have a set of focused Technology Solutions for Businesses and Public Services. These innovative application scenarios and solutions are supported, enabled, and driven by the middle layer which consists of a small group of Services, Infrastructures and Platforms that provide, through standardised interfaces, data exchange formats"
L18-1213,L18-1599,1,0.756254,"/Licence Ouverte, for France). LRs provided can be classified and viewed depending on the licence available: public domain, open under PSI, open licenses, standard licenses, and non-standard licenses. Validation of Language Resources within ELRC Validation Guidelines Implementation Validation can be understood as the quality control of a LR against a list of relevant criteria (Schneller et al., 2017). Due to the high number of LRs required within the project, the ELRC consortium decided to complement the donated LRs with additional LRs produced from scratch through a website crawling process (Papavassiliou et al, 2018). Web crawling was conducted using ILSP-FC, a comprehensive end-to-end solution for the acquisition of domain-specific monolingual and bilingual corpora from the web. Data Processing Each LR is analysed and processed by ELRC experts to ensure compliance with the Language Resources Data 12 https://www.maxprograms.com/products/tmxvalidator.html https://www.microsoft.com/en-us/download/ details.aspx?id=52608 14 https://github.com/aboSamoor/pycld2 13 10 11 http://lr-coordination.eu/helpdesk http://helpdesk.lr-coordination.eu/overview 1341 The ELRC partners (the National Anchor Points) initially id"
L18-1213,W12-0102,0,0.040596,"Missing"
L18-1213,L18-1391,1,0.797927,"eHealth Business Registers Interconnection System Safer Internet Cybersecurity Public Open Data Europeana Domain Consumers’ rights Social security, insurance Public procurement, contractual agreements Justice, Law Health, Medicine Business, market ICT ICT Multiple domains Culture Table 1: CEF Digital Service Infrastructures (DSIs) and their domains Even though the PSI Directive is an important instrument to open up public sector data, there are many challenges in collecting LRs from public services, such as lack of awareness, lack of technical or legal competence, poor data management, etc. (Vasiļjevs et al., 2018). 1.3 Setting up a European Language Resource Coordination (ELRC) European, national and regional public administrations deal with a huge amount of multilingual textual information in original and translated form. By sharing this linguistic data and turning it into language resources (LRs), they can improve the quality, coverage and performance of CEF eTranslation that needs multilingual LRs to train MT systems. In April 2015, the ELRC Consortium was set up through EC’s Connecting Europe Facility SMART 2014/1074 programme to initiate a number of actions with the aim to support the collection o"
lynn-etal-2012-irish,W98-1507,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2010-partial,1,\N,Missing
lynn-etal-2012-irish,gupta-etal-2010-partial,0,\N,Missing
lynn-etal-2012-irish,nivre-etal-2006-maltparser,0,\N,Missing
lynn-etal-2012-irish,W10-2910,0,\N,Missing
lynn-etal-2012-irish,W02-1503,0,\N,Missing
lynn-etal-2012-irish,W01-1203,0,\N,Missing
lynn-etal-2012-irish,W04-0210,0,\N,Missing
lynn-etal-2012-irish,W08-1301,0,\N,Missing
lynn-etal-2012-irish,P10-1075,0,\N,Missing
lynn-etal-2012-irish,J08-4003,0,\N,Missing
lynn-etal-2012-irish,W10-1401,1,\N,Missing
lynn-etal-2012-irish,P05-1012,0,\N,Missing
lynn-etal-2012-irish,J08-4010,0,\N,Missing
lynn-etal-2012-irish,J08-3003,0,\N,Missing
lynn-etal-2012-irish,W11-4649,0,\N,Missing
lynn-etal-2012-irish,P06-1063,1,\N,Missing
lynn-etal-2012-irish,J08-4004,0,\N,Missing
lynn-etal-2012-irish,P05-1034,0,\N,Missing
lynn-etal-2012-irish,W11-2917,0,\N,Missing
lynn-etal-2012-irish,dzeroski-etal-2006-towards,0,\N,Missing
lynn-etal-2012-irish,mieskes-strube-2006-part,0,\N,Missing
lynn-etal-2012-irish,W11-2929,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2006-part,0,\N,Missing
N13-3003,2009.mtsummit-btm.7,0,0.028567,"ins and also incur significant computational effort. 10 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10–13, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender a fully extendable platform that will allow any number of MT systems (or indeed TM systems) to be plugged into the recommender with little effort. 4 Demo Description Using the Amazon EC25 deployment as a back-end, we have developed a front-end GUI for the system (Figure 2). The interface allows the user to select which of the available translation systems (whether they be MT or TM) they wish"
N13-3003,2010.amta-papers.27,1,0.777221,"Missing"
N13-3003,P10-1064,1,0.919878,"Missing"
N13-3003,C10-2043,1,0.910044,"Missing"
N13-3003,W11-2107,0,0.0457309,"Missing"
N13-3003,2009.mtsummit-papers.8,0,0.0311603,"et al., 2009) note that using glass-box features when available, in addition to black-box features, offer only small gains and also incur significant computational effort. 10 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10–13, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender a fully extendable platform that will allow any number of MT systems (or indeed TM systems) to be plugged into the recommender with little effort. 4 Demo Description Using the Amazon EC25 deployment as a back-end, we have developed a front-end GUI for the system (Figure 2). The int"
N13-3003,2009.mtsummit-papers.14,0,0.321889,"Missing"
N13-3003,2006.amta-papers.25,0,0.198443,"Missing"
N13-3003,2011.eamt-1.12,0,0.0463811,"ed time as suggested translations are discarded in favour of providing a translation from scratch. We present a commercially-relevant software platform providing a translation confidence estimation metric and, based on this, a mechanism for effectively integrating MT with TMs in localisation workflows. The confidence metric ensures that only ∗ Author did this work during his post doctoral research at CNGL. Related Work MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research. (Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators. This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors’ judgements of the translation quality between good and bad, or among three levels of post-editing effort. Our work is an extension of (He et al., 2010a,b,c), and uses outputs and features relevant to the TM and MT systems. We focus on using system external features. This is important for cases where the internals of the"
N13-3003,specia-etal-2010-dataset,0,0.0321652,"Missing"
N13-3003,2009.mtsummit-papers.16,0,0.0251668,"d translations are discarded in favour of providing a translation from scratch. We present a commercially-relevant software platform providing a translation confidence estimation metric and, based on this, a mechanism for effectively integrating MT with TMs in localisation workflows. The confidence metric ensures that only ∗ Author did this work during his post doctoral research at CNGL. Related Work MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research. (Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators. This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors’ judgements of the translation quality between good and bad, or among three levels of post-editing effort. Our work is an extension of (He et al., 2010a,b,c), and uses outputs and features relevant to the TM and MT systems. We focus on using system external features. This is important for cases where the internals of the MT system are not availab"
N13-3003,2009.mtsummit-posters.20,0,0.0294043,"tration Session, pages 10–13, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender a fully extendable platform that will allow any number of MT systems (or indeed TM systems) to be plugged into the recommender with little effort. 4 Demo Description Using the Amazon EC25 deployment as a back-end, we have developed a front-end GUI for the system (Figure 2). The interface allows the user to select which of the available translation systems (whether they be MT or TM) they wish to use within the recommender system. The user can input their own pre-established estimated"
N16-1110,P14-2048,0,0.0398854,"stimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese classifier. 2.4 Translator Experience J¨aa¨ skel¨ainen (1997) describes translational behaviour of professionals and non-professionals who perform translation from English into Finnish. Carl and Buch-Kromann (2010) appl"
N16-1110,C04-1046,0,0.0291151,"s distributed differently in English and German (Doherty, 2006; Fabricius-Hansen, 1996). These contrasts may impact translation, and in case of source language shining through1 , we would expect to observe differences between translations and comparable originals in terms of information density. Additionally, translations are often more specialised and more conventionalised than originals (excluding translation of fictional texts). In this paper we investigate whether and to what extent information density based features are useful in human translation classification. Quality estimation (QE) (Blatz et al., 2004; Ueffing and Ney, 2005) is the attempt to learn models that predict machine translation quality without access to a reference translation at prediction time. Translation, manual or automatic, is always a process of transforming a source into a target text. This process is prone to error. In this paper we explore whether and to what extent the extensive research on QE can be brought to bear on the problem of human translation vs. originals classification, and in particular the discrimination between novice and professional translation output. Below we explore the ability of our features to dis"
N16-1110,2010.eamt-1.14,0,0.238837,"ated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese classifier. 2.4 Translator Experience J¨aa¨ skel¨ainen (1997) describes translational behaviour of professionals and non-professionals who perform translation from English into Finnish. Carl and Buch-Kromann (2010) apply psycholinguistic methods in their analysis. They present a study of translation phases and processes for student and professional translators, relating translators’ eye movements and keystrokes to the quality of the translations produced. They show that the translation behaviour of novice and professional translators differs with respect to how they use the translation phases. Englund Dimitrova (2005) develops a combined process and product analysis and compares translators with different levels of translation experience, but concentrates only on cohesive explicitness. Most of these wor"
N16-1110,J13-4008,0,0.0253462,"on density measured on translated texts compared to comparable originally authored ones in the same language. Source language interference should result in peaks of measured surprisal values in translated texts, while the information density may remain uniform in originals. According to Hale (2001), a surprisal model allows the estimation of the probability of a parse tree given a sentence prefix. Levy (2008) showed that a lexical-based surprisal measure can be obtained by computing the negative log probability of a word given its preceding context: S = − log P (wk+1 |w1 . . . wk ). Following Demberg et al. (2013), we estimate surprisal in three ways, at the word, part-of-speech and syntax levels, based on ngram language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learn"
N16-1110,W03-0413,0,0.0672265,"language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorr"
N16-1110,N01-1021,0,0.00949344,"tion. This is often referred to as the uniform information density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal, a measure of how predictable an expression is given its context. Simplification and explicitation may impact the average information density measured on translated texts compared to comparable originally authored ones in the same language. Source language interference should result in peaks of measured surprisal values in translated texts, while the information density may remain uniform in originals. According to Hale (2001), a surprisal model allows the estimation of the probability of a parse tree given a sentence prefix. Levy (2008) showed that a lexical-based surprisal measure can be obtained by computing the negative log probability of a word given its preceding context: S = − log P (wk+1 |w1 . . . wk ). Following Demberg et al. (2013), we estimate surprisal in three ways, at the word, part-of-speech and syntax levels, based on ngram language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lex"
N16-1110,P11-1132,0,0.0949733,"t our results compared to previous work is given in Section 5. Finally, conclusion and future work are provided in Section 6. 1 If translations demonstrate features more typical for the source language, see e.g. Teich (2003). 961 2 Related Work We briefly review previous work on translationese, information density, machine translation quality estimation and studies on human translation expertise. 2.1 Translationese A number of corpus-based studies on translation have shown that it is possible to automatically predict whether a text is an original or a translation (Baroni and Bernardini, 2006; Koppel and Ordan, 2011). These approaches are based on the concept of translationese – a term coined to capture the specific language of translations by Gellerstam (1986). The idea is that translations exhibit properties which distinguish them from original texts, both the source texts of the translation and comparable texts originally authored in the target language. Baker (1993; 1995) claimed these properties to be universal, i.e. (source) language-independent, emphasising general effects of the process of translation. However, translationese includes features involving both source and target language. Most lingui"
N16-1110,2009.mtsummit-papers.9,0,0.563286,"method they were produced with, are different from their source texts and from originally authored comparable texts in the target language. This has been confirmed by many linguistic studies on translation properties commonly called translationese (Gellerstam, 1986). These studies show that translations tend to share a set of lexical, syntactic and/or textual features distinguishing them from non-translated texts. As most of these features can be measured quantitatively, we are able to automatically distinguish translations from originals (Baroni and Bernardini, 2006; Ozdowska and Way, 2009; Kurokawa et al., 2009). This is useful for Statistical Machine Translation (SMT), as language and translation models can be improved if Languages provide speakers with a large number of possibilities of how they may encode messages. These include the choice of phonemes, words, syntactic structures, as well as arranging sentences in discourse. Speakers’ decisions regarding these choices are influenced by diverse factors: cognitive processing limitations can impact variation in linguistic encoding across all linguistic levels. Text production conditions, including monolingual vs. multilingual settings, can influence"
N16-1110,W13-2510,1,0.895954,"datasets used in our experiments are separated into two subsets: corpora used to extract features and corpora used to train, tune and test our classifiers. The former are taken from the publicly available bilingual English-German parallel corpora consisting of parliamentary proceedings, literary works and political commentary, compiled by (Rabinovich et al., 2015). These corpora are used individually to train language models and compute n-gram frequency distributions. Basic corpus statistics are presented in Table 1. The latter ones are composed of German texts, taken from the VARTRA corpora (Lapshinova-Koltunski, 2013), which were either originally written in German (originals) or translated from English (translations). Originals and translations belong to the same genres and registers and can be considered comparable. They include a mixture of literary, tourism and popular-scientific texts, instruction manuals, commercial letters and political essays and speeches. The VARTRA translations are split in two sets: one produced by professional translators, and one produced by translator trainees. Details are presented in Table 2. We extract balanced subsets of training, tuning and testing data containing three,"
N16-1110,J12-4004,0,0.027507,"uage and translation models. Automatic classification of original vs. translated texts has applications in machine translation, especially in studies showing the impact of the nature (original vs. translation) of the text in translation and language models used in SMT. Kurokawa et al. (2009) show that taking directionality into account when training an English-to-French phrasebased SMT system leads to improved translation performance. Ozdowska & Way (2009) analyse the same language pair and demonstrate that the nature of the original source language has an impact on the quality of SMT output. Lembersky et al. (2012) show that BLEU scores can be improved by language models compiled from translated texts and not from comparable originally authored ones. 2.2 Information Density In a natural communication situation, speakers tend to exploit variations in their linguistic encoding – modulating the order, density and specificity of their expressions to avoid informational peaks and troughs that may result in inefficient communication. This is often referred to as the uniform information density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal, a"
N16-1110,P14-5010,0,0.00390023,"lexical, part-of-speech and syntactic structures observed between originals and translations, as well as between different levels of translation experience. These features are extracted the same way as the suprisal features, but based on language models trained on sentence-level reversed text. The backward language model features are popular in translation quality estimation studies and show interesting results (Duchateau et al., 2002; Rubino et al., 2013b). 3.4 Preprocessing and Tools All data used in our experiments are sentence-split, lower-cased and tokenised using the C ORE NLP toolkit (Manning et al., 2014). The part-of-speech tags and syntactic trees required to extract some features are obtained with the same set of tools. For parsing, we use the probabilistic context-free grammar model trained on the Negra corpus (Brants et al., 2003) and described in (Rafferty and Manning, 2008), before flattening the trees as illustrated in Figure 1. Both part-of-speech and flattened syntac3 Originally authored texts and translations are used separately in order to model their characteristics. (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (AR"
N16-1110,2009.eamt-1.14,0,0.0861729,"tions, regardless of the method they were produced with, are different from their source texts and from originally authored comparable texts in the target language. This has been confirmed by many linguistic studies on translation properties commonly called translationese (Gellerstam, 1986). These studies show that translations tend to share a set of lexical, syntactic and/or textual features distinguishing them from non-translated texts. As most of these features can be measured quantitatively, we are able to automatically distinguish translations from originals (Baroni and Bernardini, 2006; Ozdowska and Way, 2009; Kurokawa et al., 2009). This is useful for Statistical Machine Translation (SMT), as language and translation models can be improved if Languages provide speakers with a large number of possibilities of how they may encode messages. These include the choice of phonemes, words, syntactic structures, as well as arranging sentences in discourse. Speakers’ decisions regarding these choices are influenced by diverse factors: cognitive processing limitations can impact variation in linguistic encoding across all linguistic levels. Text production conditions, including monolingual vs. multilingual"
N16-1110,quirk-2004-training,0,0.0435977,"962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation pro"
N16-1110,Q15-1030,0,0.36774,"Missing"
N16-1110,W08-1006,0,0.0203033,"level reversed text. The backward language model features are popular in translation quality estimation studies and show interesting results (Duchateau et al., 2002; Rubino et al., 2013b). 3.4 Preprocessing and Tools All data used in our experiments are sentence-split, lower-cased and tokenised using the C ORE NLP toolkit (Manning et al., 2014). The part-of-speech tags and syntactic trees required to extract some features are obtained with the same set of tools. For parsing, we use the probabilistic context-free grammar model trained on the Negra corpus (Brants et al., 2003) and described in (Rafferty and Manning, 2008), before flattening the trees as illustrated in Figure 1. Both part-of-speech and flattened syntac3 Originally authored texts and translations are used separately in order to model their characteristics. (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (ART des) (NN Geburtenrückgangs)) (VVPP konfrontiert)) Flatten ($. .)) (TOP (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (ART des) (NN Geburtenrückgangs)) (VVPP konfrontiert)) ($. .))) Delexicalise (TOP (S (ADV ) (VAF"
N16-1110,2013.mtsummit-posters.13,1,0.884681,"Missing"
N16-1110,I13-1166,1,0.921127,"ss of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese clas"
N16-1110,2014.eamt-1.21,0,0.0181558,"speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations."
N16-1110,2010.amta-papers.3,0,0.0187959,"ranslation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of"
N16-1110,H05-1096,0,0.0163349,"ently in English and German (Doherty, 2006; Fabricius-Hansen, 1996). These contrasts may impact translation, and in case of source language shining through1 , we would expect to observe differences between translations and comparable originals in terms of information density. Additionally, translations are often more specialised and more conventionalised than originals (excluding translation of fictional texts). In this paper we investigate whether and to what extent information density based features are useful in human translation classification. Quality estimation (QE) (Blatz et al., 2004; Ueffing and Ney, 2005) is the attempt to learn models that predict machine translation quality without access to a reference translation at prediction time. Translation, manual or automatic, is always a process of transforming a source into a target text. This process is prone to error. In this paper we explore whether and to what extent the extensive research on QE can be brought to bear on the problem of human translation vs. originals classification, and in particular the discrimination between novice and professional translation output. Below we explore the ability of our features to distinguish between 1) non-"
N16-1110,2003.mtsummit-papers.52,0,0.0354724,"e models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, a"
N16-1110,D11-1034,0,\N,Missing
N16-1139,J93-2003,0,0.0718383,"he slower two-sided exchange algorithm, and better perplexities than the slower Brown clustering algorithm. Our B IRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a gener"
N16-1139,W10-1409,0,0.204869,"Missing"
N16-1139,D13-1174,0,0.0197747,"s Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O(|V |× |C |× I), where |V |is the number of word types, |C |is the number of classes, and I is the number of training iterations, typically &lt; 20 . This omits the spec"
N16-1139,N13-1003,0,0.0338456,"implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O(|V |× |C |× I), where |V |is the number of word types, |C |is the number of classes, and"
N16-1139,E03-1009,0,0.0720298,"tes to 1 − λ every a iterations (i): ( 1 − λ0 if i mod a = 0 λi := (2) λ0 otherwise Figure 1 illustrates the benefit of this λ-inversion to help escape local minima, with lower training set perplexity by inverting λ every four iterations: ● 800 Perplexity One of the oldest and still most popular exchange algorithm implementations is mkcls (Och, 1995)1 , which adds various metaheuristics to escape local optima. Botros et al. (2015) introduce their implementation of three exchange-based algorithms. Martin et al. (1998) and M¨uller and Sch¨utze (2015)2 use trigrams within the exchange algorithm. Clark (2003) adds an orthotactic bias.3 Clusterer ● 750 ● ● ● ● ● ● ● ● ● ● ● ● Predictive Exchange ● 700 +Rev 650 5 10 Iteration Figure 1: Training set perplexity using lambda inversion (+Rev), using 100M tokens of the Russian News Crawl (cf. §4.1). Here a = 4, λ0 = 1, and |C |= 800 . The time complexity is O(2×(B+|V |)×|C|×I) . The original predictive exchange algorithm can be obtained by setting λ = 1 and a = 0 .5 Another innovation, both in terms of cluster quality and speed, is cluster refinement. The vocabulary is initially clustered into |G |sets, where |G | |C|, typically 2–10 . After a few itera"
N16-1139,C14-1041,0,0.099517,"Missing"
N16-1139,W14-3360,0,0.0521391,"alternating (B IRA) predictive exchange algorithm. The goal of B IRA is to produce better clusters by using multiple, changing models to escape local optima. This uses both forward and reversed bigram class models to improve cluster quality by evaluating log-likelihood on two different models. Unlike using trigrams, bidirectional bigram models only linearly increase time and memory requirements, and in fact some data structures can be shared. The two directions are interpolated to allow softer inte1 https://github.com/moses-smt/mgiza http://cistern.cis.lmu.de/marlin 3 http://bit.ly/1VJwZ7n 4 Green et al. (2014) provide a Free implementation of the original predictive exchange algorithm within the Phrasal MT system, at http://nlp.stanford.edu/phrasal . Another implementation is in the Cicada semiring MT system. 2 1170 gration of these two models: P (wi |wi−1 , wi+1 ) , P (wi |ci ) (1) · (λP (ci |wi−1 ) + (1 − λ)P (ci |wi+1 )) The interpolation weight λ for the forward direction alternates to 1 − λ every a iterations (i): ( 1 − λ0 if i mod a = 0 λi := (2) λ0 otherwise Figure 1 illustrates the benefit of this λ-inversion to help escape local minima, with lower training set perplexity by inverting λ eve"
N16-1139,D07-1091,0,0.0968759,"ies than the slower Brown clustering algorithm. Our B IRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O(|V |× |C |× I), where |V |is the number"
N16-1139,W04-3250,0,0.111007,"Missing"
N16-1139,D14-1108,0,0.195525,"Missing"
N16-1139,P08-1068,0,0.456328,"Missing"
N16-1139,N04-1043,0,0.139051,"Missing"
N16-1139,N15-1055,0,0.0287227,"Missing"
N16-1139,C00-2163,0,0.186659,"exchange algorithm, and better perplexities than the slower Brown clustering algorithm. Our B IRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity o"
N16-1139,W09-1119,0,0.0406105,"Missing"
N16-1139,D11-1141,0,0.23429,"Missing"
N16-1139,E14-1068,0,0.028423,"Missing"
N16-1139,W12-0704,0,0.156434,"tering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O(|V |× |C |× I), where |V |is the number of word types, |C |is the number of classes, and I is the number of training"
N16-1139,N12-1052,0,0.0563109,"Missing"
N16-1139,P10-1040,0,0.265905,"Missing"
N16-1139,P08-1086,0,0.171634,"an unlexicalized (two-sided) language model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) , where the class ci of the predicted word wi is conditioned on the class ci−1 of the previous word wi−1 . Goodman (2001b) altered this model so that ci is conditioned directly upon wi−1 , hence: P (wi |wi−1 ) = P (wi |ci ) P (ci |wi−1 ) . This new model fractionates the history more, but it allows for a large speedup in hypothesizing an exchange since the history doesn’t change. The resulting partially lexicalized (one-sided) class model gives the accompanying predictive exchange algorithm (Goodman, 2001b; Uszkoreit and Brants, 2008) a time complexity of O((B + |V |) × |C |× I) where B is the number of unique bigrams in the training set.4 We introduce a set of improvements to this algorithm to enable high-quality large-scale word clusters. 3 BIRA Predictive Exchange We developed a bidirectional, interpolated, refining, and alternating (B IRA) predictive exchange algorithm. The goal of B IRA is to produce better clusters by using multiple, changing models to escape local optima. This uses both forward and reversed bigram class models to improve cluster quality by evaluating log-likelihood on two different models. Unlike us"
N16-1139,D13-1138,0,0.0654697,"wn clustering algorithm. Our B IRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 1 2 Introduction Words can be grouped together into equivalence classes to help reduce data sparsity and better generalize data. Word clusters are useful in many NLP applications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), targetside inflection (Chahuneau et al., 2013), SAMT Word Clustering Word clustering partitions a vocabulary V, grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters. The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O(|V |× |C |× I), where |V |is the number of word types, |C |is"
N16-1139,P11-1001,0,0.287751,"Missing"
N16-1139,N09-4002,0,\N,Missing
N16-3009,J92-4003,0,0.647533,"Missing"
N16-3009,W10-1409,0,0.0233482,"ality measured in B LEU scores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of the predicted word wi is"
N16-3009,N13-1003,0,0.0191151,"a 2.5 billion token English News Crawl corpus in 3 hours. We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in B LEU scores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Bas"
N16-3009,N16-1139,1,0.430181,"Missing"
N16-3009,C14-1041,0,0.042743,"Missing"
N16-3009,W14-3360,0,0.016832,"on training time, twosided class-based language model (LM) perplexity 1 δ is the change in log-likelihood, and N (w, c) is the count of a given word followed by a given class. 2 This was independently discovered in Botros et al. (2015). (cf. Brown et al., 1992; Uszkoreit and Brants, 2008), and B LEU scores in phrase-based MT. 4.1 Intrinsic Evaluation For the two-sided class-based LM task we used 800 and 1200 classes for English, and 800 classes for Russian. The clusterers (cf. Sec. 2) are Browncluster (Liang, 2005), ClusterCat (introduced in Section 3), mkcls (Och, 1995), Phrasal’s clusterer (Green et al., 2014), and word2vec’s clustering feature (Mikolov et al., 2013). The data comes from the 2011–2013 News Crawl monolingual data of the WMT task.3 For these experiments the data was deduplicated, shuffled, tokenized, digit-conflated, and lowercased. In order to have a large test set, one line per 100 of the resulting corpus was separated into the test set.4 For English this gave 1B training tokens, 2M training types, and 12M test tokens. For Russian, 550M training tokens, 2.7M training types, and 6M test tokens. All clusterers had a minimum count threshold of 3 occurrences in the training set. All us"
N16-3009,D07-1091,0,0.102322,"Missing"
N16-3009,W04-3250,0,0.0609389,"Missing"
N16-3009,D14-1108,0,0.0127395,"ores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of the predicted word wi is conditioned on the c"
N16-3009,P08-1068,0,0.0277234,"ame translation quality measured in B LEU scores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of"
N16-3009,N04-1043,0,0.0658283,"n Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of the predicted word wi is conditioned on the class ci−1 of the previous word wi−1 . Goodman (2001a) altered thi"
N16-3009,C00-2163,0,0.274139,"Missing"
N16-3009,D11-1141,0,0.052653,"Missing"
N16-3009,W12-0704,0,0.0207915,"News Crawl corpus in 3 hours. We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in B LEU scores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange a"
N16-3009,P10-1040,0,0.0263468,"nd freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of the predicted word wi is conditioned on the class ci−1 of the previous word w"
N16-3009,P08-1086,0,0.0247764,"Clustering The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: P (wi |wi−1 ) = P (wi |ci ) P (ci |ci−1 ) where the class ci of the predicted word wi is conditioned on the class ci−1 of the previous word wi−1 . Goodman (2001a) altered this model so that ci is conditioned directly upon wi−1 : P (wi |wi−1 ) = P (wi |ci ) P (ci |wi−1 ) . This fractionates the history more, but it greatly speeds up hypothesizing an exchange since the history doesn’t change. The resulting partially lexicalized (onesided) model gives the accompanying predictive exchange algorithm (Uszkoreit and Brants, 2008) a time complexity of O((B + |V |) × |C |× I) where B is the number of unique bigrams, |C |is the number of classes, and I is the number of training iterations, usually &lt; 20 . 3 ClusterCat ClusterCat is word clustering software designed to be fast and scalable, while also improving upon the predictive exchange algorithm. We describe in this section improvements in the model, the algorithm, as well as in the implementation. Proceedings of NAACL-HLT 2016 (Demonstrations), pages 42–46, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Model and Algorithm We"
N16-3009,D13-1138,0,0.0432565,"Missing"
N16-3009,P11-1001,0,0.0158201,"3 hours. We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in B LEU scores. ClusterCat is portable and freely available. 1 Introduction Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clusters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014). Word clusterings have also found utility in parsing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), 42 and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others. Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization p time from O(|V |) (the vocabulary size) to ≈ O( |V |) . 2 Exchange-Based Clustering The exchange algorithm (Kneser and Ney, 1993) u"
N16-3009,J93-2003,0,\N,Missing
P04-1041,J97-4005,0,0.150696,"Missing"
P04-1041,P03-1046,0,0.0771796,"the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely loc"
P04-1041,P02-1018,0,0.541974,"lative clauses and interrogative sentences, however, there is an important difference between the location of the (surface) realisation of linguistic material and the location where this material should be interpreted semantically. Resolution of such long-distance dependencies (LDDs) is therefore crucial in the determination of accurate predicate-argument struc1 Manually constructed f-structures for 105 randomly selected trees from Section 23 of the WSJ section of the Penn-II Treebank ture, deep dependency relations and the construction of proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is"
P04-1041,N04-1013,0,0.0502222,"105 (Cahill et al., 2002) 2. The full 2,416 f-structures automatically generated by the f-structure annotation algorithm for the original Penn-II trees, in a CCG-style (Hockenmaier, 2003) evaluation experiment # Parses Lab. F-Score Unlab. F-Score All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) Subset of GFs following (Kaplan et al., 2004) Pipeline Integrated PCFG P-PCFG A-PCFG PA-PCFG 2416 Section 23 trees 2416 2416 2416 2414 75.83 80.80 79.17 81.32 78.28 82.70 81.49 83.28 DCU 105 F-Strs 79.82 79.24 81.12 81.20 83.79 84.59 86.30 87.04 70.00 71.57 73.45 74.61 73.78 77.43 78.76 80.97 2416 F-Strs 81.98 81.49 83.32 82.78 84.16 84.37 86.45 86.00 72.00 73.23 75.22 75.10 74.07 76.12 78.36 78.40 PARC 700 Dependency Bank 77.86 80.24 77.68 78.60 Table 7: Parser Evaluation 3. A subset of 560 dependency structures of the PARC 700 Dependency Bank following (Kaplan et al., 2004) The results are given in Table 7. The parenttransformed gramma"
P04-1041,P03-1054,0,0.116149,"f parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely locally where it occurs in th"
P04-1041,H94-1003,0,0.210445,"Missing"
P04-1041,H94-1020,0,0.0697514,"as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources"
P04-1041,P04-1047,1,0.744247,"Missing"
P04-1041,P02-1035,0,0.0980593,"proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, sta"
P04-1041,W98-0141,0,0.0579954,"Missing"
P04-1041,C96-1045,1,0.860707,"Missing"
P04-1041,A00-2018,0,\N,Missing
P04-1041,J98-4004,0,\N,Missing
P04-1041,J03-4003,0,\N,Missing
P04-1047,J93-2002,0,0.193808,"Missing"
P04-1047,P04-1041,1,0.781736,"Missing"
P04-1047,W98-1505,0,0.100785,"Missing"
P04-1047,2000.iwpt-1.9,0,0.252147,"Missing"
P04-1047,P97-1003,0,0.0933422,"Missing"
P04-1047,kinyon-prolo-2002-identifying,0,0.119169,"Missing"
P04-1047,P98-1115,0,0.0908158,"Missing"
P04-1047,H94-1003,0,0.925761,"ticle verbs. Our method does not predefine the frames to be extracted. In contrast to many other approaches, it discriminates between active and passive frames, properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate. Section 2 reviews related work in the area of automatic subcategorisation frame extraction. Our methodology and its implementation are presented in Section 3. Section 4 presents the results of our lexical extraction. In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource (MacLeod et al., 1994). To our knowledge, this is the largest evaluation of subcategorisation frames for English. In Section 6, we conclude and give suggestions for future work. 2 Related Work Creating a (subcategorisation) lexicon by hand is time-consuming, error-prone, requires considerable linguistic expertise and is rarely, if ever, complete. In addition, a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains. Accordingly, many researchers have attempted to construct lexicons automatically, especially for English. (Brent, 1993) relies on local morphosyntactic cues (su"
P04-1047,P93-1032,0,0.217958,"Missing"
P04-1047,W93-0109,0,0.205245,"Missing"
P04-1047,W00-1307,0,0.0866381,"Missing"
P04-1047,A97-1052,0,\N,Missing
P04-1047,C98-1111,0,\N,Missing
P06-1063,P04-1041,1,0.548322,"Missing"
P06-1063,W04-3215,0,0.0453483,"Missing"
P06-1063,H90-1021,0,0.0594825,"Missing"
P06-1063,P02-1018,0,0.0130775,"the range 82.36 to 82.46, which is comparable to the baseline score. Figure 7 graphs the results for the third ablation experiment. In this case the training set is a fixed amount of the question training set described above (all 3600 questions) and a reducing amount of data from Sections 02-21 of the Penn Treebank. WHNP-1 NP killed Harvey Oswald (a) SQ WP Who AUX did NP VP Harvey Oswald VB (b) kill NP *T*-1 Figure 8: LDD resolved treebank style trees With few exceptions5 the trees produced by current treebank-based probabilistic parsers do not represent long distance dependencies (Figure 9). Johnson (2002) presents a tree-based method for reconstructing LDD dependencies in PennII trained parser output trees. Cahill et al. (2004) present a method for resolving LDDs 5 Collins’ Model 3 computes a limited number of whdependencies in relative clause constructions. 502 SBARQ SBARQ WHNP SQ WP VP Who VBD SBARQ AUX did WHNP =↓ SQ ↑=↓ WP ↑=↓ VP ↑=↓ Who Harvey Oswald (a) VBD ↑=↓ ↑ NP =↓ OBJ killed SQ  WP Who FOCUS NP killed WHNP ↑ NP   VP Harvey Oswald VB (b) FOCUS PRED OBJ SUBJ kill Harvey Oswald (a)  PRED who 1 ’killh  SUBJ OBJi’ PRED ’Harvey Oswald’   ’who’ (b) PRED  1     Figure 10: Annota"
P06-1063,C02-1150,0,0.0122122,"d the question treebank. These 2000 questions consist of the test questions for the first three years of the TREC QA track (1893 questions) and 107 questions from the 2003 TREC test set. 3.2 CCG Group Questions The CCG provide a number of resources for developing QA systems. One of these resources is a set of 5500 questions and their answer types for use in training question classifiers. The 5500 questions were stripped of answer type annotation, duplicated TREC questions were removed and 2000 questions were used for the question treebank. The CCG 5500 questions come from a number of sources (Li and Roth, 2002) and some of these questions contain minor grammatical mistakes so that, in essence, this corpus is more representative of genuine questions that would be put to a working QA system. A number of changes in tokenisation were corrected (eg. separating contractions), but the minor grammatical errors were left unchanged because we believe that it is necessary for a parser for question analysis to be able to cope with this sort of data if it is to be used in a working QA system. 4 Creating the Treebank 4.1 Bootstrapping a Question Treebank The algorithm used to generate the question treebank is an"
P06-1063,J93-2004,0,0.030621,"Missing"
P06-1063,W01-0521,0,\N,Missing
P06-1063,J03-4003,0,\N,Missing
P06-1130,J97-4005,0,0.255096,"parsing architectures: the pipeline and the integrated parsing architecture. In the pipeline architecture, a PCFG (or a history-based lexicalised generative parser) is extracted from the treebank and used to parse unseen text into trees, the resulting trees are annotated with f-structure equations by the f-structure annotation algorithm and a constraint solver produces an f-structure. In the in1 The resources are approximations in that (i) they do not enforce LFG completeness and coherence constraints and (ii) PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). tegrated architecture, first the treebank trees are automatically annotated with f-structure information, f-structure annotated PCFGs with rules of the form NP(↑OBJ=↓)→DT(↑=↓) NN(↑=↓) are extracted, syntactic categories followed by equations are treated as monadic CFG categories during grammar extraction and parsing, unseen text is parsed into trees with f-structure annotations, the annotations are collected and a constraint solver produces an f-structure. The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al. (2004). The generatio"
P06-1130,C00-1007,0,0.596076,"Missing"
P06-1130,W01-0520,0,0.0293862,"Missing"
P06-1130,W05-1601,0,0.0900512,"Missing"
P06-1130,Y04-1016,1,0.880023,"Missing"
P06-1130,P04-1041,1,0.721752,"Missing"
P06-1130,I05-1015,0,0.346325,"rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditioning f-structure features and its grammatical function label. At each level, grammatical function indexed charts are initially unordered. Charts are linearised by generation grammar rules once the charts themselves have produced the most probable"
P06-1130,C00-1062,0,0.121854,"icient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in Kaplan and Wedekind’s proofs. We presented a full set of generati"
P06-1130,P96-1027,0,0.4731,"resented as arrows), we automatically extract the rule S(↑=↓) → NP(↑SUBJ=↓) VP(↑=↓) conditioned on the feature set {PRED,SUBJ,COMP,TENSE}. The probability of the rule is then calculated by counting the number of occurrences of that rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditio"
P06-1130,W02-2103,0,0.388037,"Missing"
P06-1130,A00-2023,0,0.473928,"Missing"
P06-1130,H94-1020,0,0.028187,"Missing"
P06-1130,W05-1510,0,0.733302,"aluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial output. Partial output is a robustness feature for cases where a sub-fstructure component fails to generate a string and the system outputs a concatenation of the strings generated by the remaining components, rather than fail completely. 1037 Sentence length of Training Data ≤ 20 ≤ 25 ≤ 30 ≤ 40 all Evaluation Metric BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage ≤ 20 0.681"
P06-1130,P02-1040,0,0.100966,"nal grammar used in parsing, our generation system was not able to distinguish nominative from accusative contexts. The solution we implemented was to carry out a grammar transformation in a pre-processing step, to automatically annotate personal pronouns with their case information. This resulted in phrasal and lexical rules such as NP(↑SUBJ) → PRPˆnom(↑=↓) and PRPˆnom(↑=↓) → I and greatly improved the accuracy of the pronouns generated. 4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial ou"
P06-1130,A00-2026,0,0.184462,"Missing"
P06-1130,J96-2001,0,\N,Missing
P06-2018,P04-1041,1,0.846859,"Missing"
P06-2018,H05-1100,0,0.0655456,"Missing"
P06-2018,J96-1002,0,0.0126634,"Missing"
P06-2018,A00-2031,0,0.662248,"Missing"
P06-2018,P04-1040,0,0.282875,"Missing"
P06-2018,Y04-1016,1,0.905065,"Missing"
P08-2056,E95-1031,0,0.0397993,"ible. These sentences and their associated trees are then used as training material for a statistical parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while s"
P08-2056,P07-1010,0,0.0603409,"to an ungrammatical treebank. This transformation process has two parts: 1. the yield of each tree is transformed into an ungrammatical sentence by introducing a syntax error; 2. each tree is minimally transformed, but left intact as much as possible to reflect the syntactic structure of the original “intended” sen221 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tence prior to error insertion. Artificial ungrammaticalities have been used in various NLP tasks (Smith and Eisner, 2005; Okanohara and Tsujii, 2007) The idea of an automatically generated ungrammatical treebank was proposed by Foster (2007). Foster generates an ungrammatical version of the WSJ treebank and uses this to train two statistical parsers. The performance of both parsers significantly improves on the artificially created ungrammatical test data, but significantly degrades on the original grammatical test data. We show that it is possible to obtain significantly improved performance on ungrammatical data without a concomitant performance decline on grammatical data. 3 Generating Noisy Treebanks ill-formed sentences that are produ"
P08-2056,W97-0303,0,0.023535,"Missing"
P08-2056,P98-2196,0,0.0451401,"parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while statistical, data-driven parsing has solved the robustness problem, it is not clear that it is has solved the ac"
P08-2056,P05-1044,0,0.0310775,"matically transformed into an ungrammatical treebank. This transformation process has two parts: 1. the yield of each tree is transformed into an ungrammatical sentence by introducing a syntax error; 2. each tree is minimally transformed, but left intact as much as possible to reflect the syntactic structure of the original “intended” sen221 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tence prior to error insertion. Artificial ungrammaticalities have been used in various NLP tasks (Smith and Eisner, 2005; Okanohara and Tsujii, 2007) The idea of an automatically generated ungrammatical treebank was proposed by Foster (2007). Foster generates an ungrammatical version of the WSJ treebank and uses this to train two statistical parsers. The performance of both parsers significantly improves on the artificially created ungrammatical test data, but significantly degrades on the original grammatical test data. We show that it is possible to obtain significantly improved performance on ungrammatical data without a concomitant performance decline on grammatical data. 3 Generating Noisy Treebanks ill-fo"
P08-2056,D07-1012,1,0.883737,"Missing"
P08-2056,P05-1022,0,\N,Missing
P08-2056,C98-2191,0,\N,Missing
P10-1064,quirk-2004-training,0,0.104681,"ifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less rele"
P10-1064,C04-1046,0,0.243124,"thout a TM in the background. 3 Support Vector Machines for Translation Quality Estimation Related Work SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there"
P10-1064,2009.mtsummit-papers.14,0,0.553395,"rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. A third strand of research tries to incorporate confidence measures into a post-editing environ∑ 1 T w w+C ξi 2 i=1 l min w,b,ξ s. t. yi (wT ϕ(xi ) + b) &gt; 1 − ξi ξi &gt; 0 (1) where (xi , yi ) ∈ Rn × {+1, −1} are l training instances that are mapped by the function ϕ to a higher dimensional space. w is the weight vector, ξ is the relaxation variable and C &gt; 0 is the penalty parameter. Solving SVMs is viable using the ‘kernel trick’: finding a kernel function K in (1) with K(xi , xj ) ="
P10-1064,J93-2003,0,0.0180988,"Missing"
P10-1064,D09-1030,0,0.0312648,"d the one recommended to edit, we can measure the true accuracy of our recommendation, as well as the post-editing time we save for the post-editors; Finally, we analyze the characteristics of the integrated outputs. We present results to show that, if measured by number, type and content of edits in TER, the recommended sentences produced by the classification model would bring about less post-editing effort than the TM outputs. • Apply the presented method on open domain data and evaluate it using crowdsourcing. It has been shown that crowdsourcing tools, such as the Amazon Mechanical Turk (Callison-Burch, 2009), can help developers to obtain good human judgements on MT output quality both cheaply and quickly. Given that our problem is related to MT quality estimation in nature, it can potentially benefit from such tools as well. 7 This work can be extended in the following ways. Most importantly, it is useful to test the model in user studies, as proposed in Section 6.3. A user study can serve two purposes: 1) it can validate the effectiveness of the method by measuring the amount of edit effort it saves; and 2) the byproduct of the user study – post-edited sentences – can be used to generate HTER s"
P10-1064,2006.amta-papers.25,0,0.293373,"mization, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TM- and system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review the classification SVMs in Section 3. We formulate the classification model in Section 4 and present experiments in Section 5. In Section 6, we analyze the post-editing effort approximated by the TER metric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 ment. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a). Instead of modeling on translation quality (often measured by automatic evaluation scores), this research uses regression on both the automatic scores and scores assigned by post-editors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgement of the translation quality between ‘good’ and ‘bad’, or among three levels of post-edi"
P10-1064,2009.eamt-1.5,0,0.270491,"everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an"
P10-1064,2009.mtsummit-papers.16,0,0.417056,"everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an"
P10-1064,N03-1017,0,0.0127258,"Missing"
P10-1064,P07-2045,0,0.00728275,"Missing"
P10-1064,2005.eamt-1.35,0,0.019082,"fy an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this pap"
P10-1064,2003.mtsummit-papers.52,0,0.0136704,"cause of this, the precision and recall scores reported in this paper are not directly comparable to those in (Specia et al., 2009b) as the latter are computed on a pure SMT system without a TM in the background. 3 Support Vector Machines for Translation Quality Estimation Related Work SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive t"
P10-1064,P02-1038,0,0.265413,"Missing"
P10-1064,P03-1021,0,\N,Missing
P10-1111,W97-0307,0,0.140606,"Missing"
P10-1111,W02-1503,0,0.0974823,"Missing"
P10-1111,P04-1041,1,0.830825,"Missing"
P10-1111,J08-1003,1,0.872782,"Missing"
P10-1111,P06-2018,1,0.937482,"Missing"
P10-1111,W04-1905,0,0.0139116,"). Due to the constraints imposed on the classification, the function labeller can no longer assign two subjects to the same S node. Faced with two nodes whose most probable label is SB, it has to decide on one of them taking the next best label for the other. This way, it outputs the optimal solution with respect to the set of constraints. Note that this requires the feature model not only to rank the correct label highest but also to provide a reasonable ranking of the other labels as well. 4 Evaluation We conducted a number of experiments using 1,866 sentences of the TiGer Dependency Bank (Forst et al., 2004) as our test set. The TiGerDB is a part of the TiGer Treebank semi-automatically converted into a dependency representation. We use the manually labelled TiGer trees corresponding to the sentences in the TiGerDB for assessing the labelling quality in the intrinsic evaluation, and precision f-score 83.60 83.20 recall tagging acc. 82.81 97.97 Table 1: evalb unlabelled parsing scores on test set for Berkeley Parser trained on 48,000 sentences (sentence length ≤ 40) 4.1 Intrinsic Evaluation In the intrinsic evaluation, we measured the quality of the labelling itself. We used the node span evaluati"
P10-1111,W07-1203,0,0.18101,"Missing"
P10-1111,P03-1054,0,0.0158506,"e functions constitute the core meaning of a sentence (as in: who did what to whom), it is important to get them right. We present a system that adds grammatical function labels to constituent parser output for German in a postprocessing step. We combine a statistical classifier with an integer linear program (ILP) to model non-violable global linguistic constraints, restricting the solution space of the classifier to those labellings that comply with our set of global constraints. There are, of course, many other ways of including functional information into the output of a syntactic parser. Klein and Manning (2003) show that merging some linguistically motivated function labels with specific syntactic categories can improve the performance of a PCFG model on Penn-II En1 Coordinate subjects/objects form a constituent that functions as a joint subject/object. 1087 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics glish data.2 Tsarfaty and Sim’aan (2008) present a statistical model (Relational-Realizational Parsing) that alternates between functional and configurational"
P10-1111,P07-2051,0,0.192396,"be labelled OG or AG9 . 7 Note that some of these constraints are language specific in that they represent linguistic facts about German and do not necessarily hold for other languages. Furthermore, the constraints are treebank specific to a certain degree in that they use a TiGer-specific set of labels and are conditioned on TiGer-specific configurations and categories. 8 SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9 AG = genitive adjunct 1090 Unlike Klenner (2007), we do not use predefined subcategorization frames, instead letting the statistical model choose arguments. In TiGer, sentences whose main verbs are formed from auxiliary-participle combinations, are annotated by embedding the participle under an extra VP node and non-subject arguments are sisters to the participle. Therefore we add an extension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punct"
P10-1111,H05-1064,0,0.0761706,"Missing"
P10-1111,P95-1037,0,0.0370794,"number of daughters the number of terminals covered the lemma of the left/right corner terminal the category of the left/right corner terminal the category of the mother node the category of the mother’s head node the lemma of the mother’s head node the category of the grandmother node the category of the grandmother’s head node the lemma of the grandmother’s head node the case features for noun phrases the category for PP objects the lemma for PP objects (if terminal node) These features are also computed for the head of the phrase, determined using a set of headfinding rules in the style of Magerman (1995) adapted to TiGer. For lemmatisation, we use TreeTagger (Schmid, 1994) and case features of noun 1089 phrases are obtained from a full German morphological analyser based on (Schiller, 1994). If a noun phrase consists of a single word (e. g. pronouns, but also bare common nouns and proper nouns), all case values output by the analyser are used to reflect the case syncretism. For multi-word noun phrases, the case feature is computed by taking the intersection of all case-bearing words inside the noun phrase, i. e. determiners, pronouns, adjectives, common nouns and proper nouns. If, for some re"
P10-1111,P09-1039,0,0.0685521,"Missing"
P10-1111,E06-1011,0,0.201487,"Missing"
P10-1111,P06-1055,0,0.069168,"tension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punctuation does not get a label in TiGer. We set the label for those nodes to −− (no label). Other examples are: the dependencies from TiGerDB for assessing the quality and coverage of the automatically acquired LFG resources in the extrinsic evaluation. In order to test on real parser output, the test set was parsed with the Berkeley Parser (Petrov et al., 2006) trained on 48k sentences of the TiGer corpus (Table 1), excluding the test set. Since the Berkeley Parser assumes projective structures, the training data and test data were made projective by raising non-projective nodes in the tree (K¨ubler, 2005). • If a node’s category is PTKVZ (separated verb particle), it is labeled SVP (separable verb particle). The maximum entropy classifier of the function labeller was trained on 46,473 sentences of the TiGer Treebank (excluding the test set) which yields about 1.2 million nodes as training samples. For training the Maximum Entropy Model, we used the"
P10-1111,C04-1197,0,0.0154566,"of German and does not allow for unary branching. The annotations use non-projective trees modelling long distance dependencies directly by crossing branches. Words are lemmatised and part-of-speech tagged with the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999) and contain morphological annotations (Release 2). TiGer uses 25 syntactic categories and a set of 42 function labels to annotate the grammatical function of a phrase. The function labeller consists of two main components, a maximum entropy classifier and an integer linear program. This basic architecture was introduced by Punyakanok et al. (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes. In our case, its input is a bare tree 4 Although the classifier may, of course, still identify the wrong phrase as subject or object. structure (as obtained by a standard phrase structure parser) and it outputs a tree structure where every node is labelled with the grammatical relation it bears to its mother node. For each possible label and for each node, the classifier assigns a probability that this node is labelled by this label. This results in a complete probabilit"
P10-1111,W04-2401,0,0.0408733,"Missing"
P10-1111,C08-1112,0,0.0700124,"Missing"
P10-1111,A00-2031,0,\N,Missing
P10-1111,J08-2005,0,\N,Missing
P10-1111,J96-1002,0,\N,Missing
P10-5001,J08-1003,1,0.889847,"Missing"
P10-5001,J08-1002,1,0.800767,"Missing"
P10-5001,J07-3004,1,0.850258,"Missing"
P11-1124,W05-0909,0,0.0230239,"of the TM itself; a selfcontained coherent TM would facilitate consistent translations. In the future, we plan to investigate the impact of TM quality on translation consistency when using our approach. Furthermore, we will explore methods to promote translation consistency at document level. Moreover, we also plan to experiment with phrase-by-phrase classification instead of sentenceby-sentence classification presented in this paper, in order to obtain more stable classification results. We also plan to label the training examples using other sentence-level evaluation metrics such as Meteor (Banerjee and Lavie, 2005), and to incorporate features that can measure syntactic similarities in training the classifier, in the spirit of (Owczarzak et al., 2007). Currently, only a standard phrase-based SMT system is used, so we plan to test our method on a hierarchical system (Chiang, 2005) to facilitate direct comparison with (Koehn and Senellart, 2010). We will also carry out experiments on other data sets and for more language pairs. Acknowledgments This work is supported by Science Foundation Ireland (Grant No 07/CE/I1142) and part funded under FP7 of the EC within the EuroMatrix+ project (grant No 231720). Th"
P11-1124,P05-1033,0,0.397873,"Missing"
P11-1124,P10-1064,1,0.848606,"Missing"
P11-1124,2010.jec-1.4,0,0.713736,"f translation, as the translation of new input sentences is closely informed and guided (or constrained) by previously translated sentences. There are several different ways of using the translation information derived from fuzzy matches, with the following two being the most widely adopted: 1) to add these translations into a phrase table as in (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009), or 2) to mark up the input sentence using the relevant chunk translations in the fuzzy match, and to use an MT system to translate the parts that are not marked up, as in (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010). It is worth mentioning that translation consistency was not explicitly regarded as their primary motivation in this previous work. Our research follows the direction of the second strand given that consistency can no longer be guaranteed by constructing another phrase table. However, to categorically reuse the translations of matched chunks without any differentiation could generate inferior translations given the fact that the context of these matched chunks in the input sentence could be completely different from the source side of the fuzzy match. To addre"
P11-1124,N03-1017,0,0.080917,"Missing"
P11-1124,P07-2045,0,0.0210111,"Missing"
P11-1124,C08-1064,0,0.0128297,"urces of improvements by marking up the input sentence, we performed some manual analysis of the output. We observe that the improvements can broadly be attributed to two reasons: 1) the use of long phrase pairs which are missing in the phrase table, and 2) deterministically using highly reliable phrase pairs. Phrase-based SMT systems normally impose a limit on the length of phrase pairs for storage and speed considerations. Our method can overcome 1246 this limitation by retrieving and reusing long phrase pairs on the fly. A similar idea, albeit from a different perspective, was explored by (Lopez, 2008), where he proposed to construct a phrase table on the fly for each sentence to be translated. Differently from his approach, our method directly translates part of the input sentence using fuzzy matches retrieved on the fly, with the rest of the sentence translated by the pre-trained MT system. We offer some more insights into the advantages of our method by means of a few examples. Example 1 shows translation improvements by using long phrase pairs. Compared to the reference translation, we can see that for the underlined phrase, the translation without markup contains (i) word ordering erro"
P11-1124,J03-1002,0,0.0258011,"ations of the matched phrases in the input sentence. The remaining words without specified translations will be translated by an MT system. For example, given an input sentence e1 e2 · · · ei ei+1 · · · eI , and a phrase pair < e¯, f¯′ >, e¯ = ′ ei ei+1 , f¯′ = fj′ fj+1 derived from the fuzzy match, we can mark up the input sentence as: ′ ”> e e e1 e2 · · · <tm=“fj′ fj+1 i i+1 < /tm> · · · eI . Our method to constrain the translations using TM fuzzy matches is similar to (Koehn and Senellart, 2010), except that the word alignment between e′ and f ′ is the intersection of bidirectional GIZA++ (Och and Ney, 2003) posterior alignments. We use the intersected word alignment to minimise the noise introduced by word alignment of only one direction in marking up the input sentence. 3.2 Discriminative Learning Whether the translation information from the fuzzy matches should be used or not (i.e. whether the input sentence should be marked up) is determined using a discriminative learning procedure. The translation information refers to the “phrase pairs” derived using the method described in Section 3.1. We cast this problem as a binary classification problem. 3.2.1 SVMs (Cortes and Vapnik, 1995) are binary"
P11-1124,P03-1021,0,0.0304164,"Missing"
P11-1124,W07-0714,1,0.707741,"Missing"
P11-1124,P02-1040,0,0.0964413,"Missing"
P11-1124,2009.mtsummit-papers.14,0,0.673198,"Missing"
P11-1124,2006.amta-papers.25,0,0.0931137,"input sentence should be marked up using relevant phrase pairs derived from the fuzzy match before sending it to the SMT system for translation. The classifier uses features such as the fuzzy match score, the phrase and lexical translation probabilities of these relevant phrase pairs, and additional syntactic dependency features. Ideally the classifier will decide to mark up the input sentence if the translations of the marked phrases are accurate when taken contextual information into account. As large-scale manually annotated data is not available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for training data annotation. We label the training examples as in (3): y= Support Vector Machines l X 1 T ξi w w+C 2 i=1 yi (wT φ(xi ) + b) > 1 − ξi ξi > 0 ( +1 if T ER(w. markup) < T ER(w/o markup) −1 if T ER(w/o markup) ≥ T ER(w. markup) (3) Each instance is associated with a set of features which are discussed in more detail in Section 4. 3.2.2 Classification Confidence Estimation We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to convert classification margin to posterior probability, so that we can easily threshold our classifier (cf. Se"
P11-1124,2009.mtsummit-papers.16,0,0.0118041,"e design in Section 4. We report the experimental results in Section 5 and conclude the paper and point out avenues for future research in Section 6. 2 Related Research Despite the fact that TM and MT integration has long existed as a major challenge in the localisation industry, it has only recently received attention in main-stream MT research. One can loosely combine TM and MT at sentence (called segments in TMs) level by choosing one of them (or both) to recommend to the translators using automatic classifiers (He et al., 2010), or simply using fuzzy match score or MT confidence measures (Specia et al., 2009). One can also tightly integrate TM with MT at the sub-sentence level. The basic idea is as follows: given a source sentence to translate, we firstly use a TM system to retrieve the most similar ‘example’ source sentences together with their translations. If matched chunks between input sentence and fuzzy matches can be detected, we can directly re-use the corresponding parts of the translation in the fuzzy matches, and use an MT system to translate the remaining chunks. As a matter of fact, implementing this idea is pretty straightforward: a TM system can easily detect the word alignment betw"
P11-1124,W10-3806,1,0.891749,"Missing"
P12-2007,2011.eamt-1.38,0,0.0724794,"of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides fl"
P12-2007,P07-1005,0,0.31714,"ianming 支持/VV 美国/NR 立场/NN zhichi meiguo lichang Eight European countries jointly support America’s stand Figure 1: An example word alignment for a ChineseEnglish sentence pair with the dependency parse tree for the Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin. ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang’s HPB as well as a SAMT-style refined version of HPB. 2 Head-Driven HPB Translation Model Like Chiang (2005) and Chiang (2007), our HDHPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence f ij from position i to position j, we first find heads and then concatenate the POS tags of these heads as f ij ’s non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence f ij , word fk (i ≤"
P12-2007,A00-2018,0,0.186505,"ion is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2"
P12-2007,P05-1033,0,0.82208,"iak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a"
P12-2007,J07-2003,0,0.732834,"lins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way w"
P12-2007,J03-4003,0,0.0227783,"how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 200"
P12-2007,D11-1079,0,0.0366988,"Missing"
P12-2007,D10-1054,0,0.020809,"inals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八国/NN 联名/AD baguo"
P12-2007,D10-1014,0,0.0889748,"Missing"
P12-2007,N03-1017,0,0.0465152,"respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule t"
P12-2007,W04-3250,0,0.126206,"nn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-terminal symbol X in Chiang’s model into head-informed ones in our model. It is also unsurprising, that the test set-filtered rule table size of our model is only ˜0.7 times that of Chiang’s: this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training d"
P12-2007,P08-1114,0,0.0400471,"which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics"
P12-2007,P11-1065,0,0.0933141,"Missing"
P12-2007,P00-1056,0,0.331027,"Missing"
P12-2007,J04-4002,0,0.151762,"t in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in 34 translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 d"
P12-2007,P03-1021,0,0.0579183,"and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-te"
P12-2007,N07-1051,0,0.0482787,"rt cell contains at most b derivations). For Moses HPB, we use “grow-diag-final-and” to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters. We train our model on a dataset with ˜1.5M sentence pairs from the LDC dataset.2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gig"
P12-2007,D09-1008,0,0.0548675,"minals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八"
P12-2007,W06-3119,0,0.0463139,"lled Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important"
P12-2007,P11-1001,0,0.0533361,"syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads us"
P12-2066,W11-0705,0,0.149937,"which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syn"
P12-2066,H05-1091,0,0.0191743,") represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context contai"
P12-2066,W08-1301,0,0.0692146,"Missing"
P12-2066,W10-2910,0,0.0159615,"gly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and co"
P12-2066,P09-2079,0,0.0921626,"-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types"
P12-2066,P03-1054,0,0.00724357,"directly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1 , respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degre"
P12-2066,D09-1017,0,0.350748,"ion Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in docum"
P12-2066,P08-2029,0,0.0249684,"rom dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively."
P12-2066,J08-2003,0,0.102698,"e docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe"
P12-2066,P06-2079,0,0.451789,"Missing"
P12-2066,D09-1143,0,0.0168412,"frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et"
P12-2066,P04-1035,0,0.860926,"xicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus. 1 Introduction An important subtask in sentiment analysis is sentiment classification. Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment clas"
P12-2066,H05-1044,0,0.861528,"tics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009). Our research is inspired by these observations. Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is a"
P12-2066,W03-1017,0,0.174347,"categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Ha"
P12-2066,P06-1104,0,0.0670835,"istic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective parag"
P12-2066,N10-1121,0,\N,Missing
P16-2046,W11-2107,0,0.0154085,"nt between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0.540 T3 0.424 0.232 0.248 T4 0.398 0.540 0.248 - Union’s Framework"
P16-2046,W15-3026,1,0.893196,"Missing"
P16-2046,W11-2123,0,0.0755541,"Missing"
P16-2046,P02-1040,0,0.109086,"ncertain’ option. We measured pairwise inter-annotator agreement between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0."
P16-2046,W12-3146,0,0.306803,"Missing"
P16-2046,D13-1176,0,0.320157,"be modelled as an MT system between SLIP T LM T and T LP E . However, if we do not have access to SLIP , but have sufficiently large amounts of parallel T LM T T LP E data, we can still build an APE model between T LM T and T LP E . Translations provided by state-of-the-art MT systems suffer from a number of errors including incorrect lexical choice, word ordering, word insertion, word deletion, etc. The APE work presented in this paper is an effort to improve the MT output by rectifying some of these errors. For this purpose we use a deep neural network (DNN) based approach. Neural MT (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Cho et al., 2014b) is a newly emerging approach to MT. On the one hand DNNs represent language in a continuous vector space which eases the modelling of semantic similarities (or distance) between phrases or sentences, and on the other hand it can also consider contextual information, e.g., We present a neural network based automatic post-editing (APE) system to improve raw machine translation (MT) output. Our neural model of APE (NNAPE) is based on a bidirectional recurrent neural network (RNN) model and consists of an encoder that encodes an MT output into a fixed-length"
P16-2046,N07-1064,0,0.752153,"On the other hand, given sufficient amounts of training data, LSTMs may lead to better results. Since our task is monolingual and we have more than 200K sentence pairs for training, we use a full LSTM (as the hidden units) to model our NNAPE system. The model takes T LM T as input and provides T LP E as output. To the best of our knowledge the work presented in this paper is the first approach to APE using neural networks. utilizing all available history information in deciding the next target word, which is not an easy task to model with standard APE systems. Unlike phrase-based APE systems (Simard et al., 2007a; Simard et al., 2007b; Pal, 2015; Pal et al., 2015), our NNAPE system builds and trains a single, large neural network that accepts a ‘draft’ translation (T LM T ) and outputs an improved translation (T LP E ). The remainder of the paper is organized as follows. Section 2 gives an overview of relevant related work. The proposed NNAPE system is described in detail in Section 3. We present the experimental setup in Section 4. Section 5 presents the results of automatic and human evaluation together with some analysis. Section 6 concludes the paper and provides avenues for future work. 2 Relate"
P16-2046,W07-0728,0,0.384103,"On the other hand, given sufficient amounts of training data, LSTMs may lead to better results. Since our task is monolingual and we have more than 200K sentence pairs for training, we use a full LSTM (as the hidden units) to model our NNAPE system. The model takes T LM T as input and provides T LP E as output. To the best of our knowledge the work presented in this paper is the first approach to APE using neural networks. utilizing all available history information in deciding the next target word, which is not an easy task to model with standard APE systems. Unlike phrase-based APE systems (Simard et al., 2007a; Simard et al., 2007b; Pal, 2015; Pal et al., 2015), our NNAPE system builds and trains a single, large neural network that accepts a ‘draft’ translation (T LM T ) and outputs an improved translation (T LP E ). The remainder of the paper is organized as follows. Section 2 gives an overview of relevant related work. The proposed NNAPE system is described in detail in Section 3. We present the experimental setup in Section 4. Section 5 presents the results of automatic and human evaluation together with some analysis. Section 6 concludes the paper and provides avenues for future work. 2 Relate"
P16-2046,2006.amta-papers.25,0,0.127359,"pairwise inter-annotator agreement between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0.540 T3 0.424 0.232 0.248 T4"
P16-2046,N09-2055,0,0.646587,"Missing"
P16-2046,W14-0314,1,0.854431,"ms. Lagarda et al. (2009) used statistical information from the trained SMT models for post-editing of rule-based MT output. Rosa et al. (2012) and Mareˇcek et al. (2011) applied a rule-based approach to APE on the morphological level. Denkowski (2015) developed a method for real time integration of post-edited MT output into the translation model by extracting a grammar for each input sentence. Recent studies have even shown that the quality of MT plus PE can exceed the quality of human translation (Fiederer and OBrien, 2009; Koehn, 2009; DePalma and Kelly, 2009) as well as the productivity (Zampieri and Vela, 2014) in some cases. Recently, a number of papers have presented the application of neural networks in MT (Kalchbrenner and Blunsom, 2013; ?; Cho et al., 2014b; Bahdanau et al., 2014). These approaches typically consist of two components: an encoder encodes a source sentence and a decoder decodes into a target sentence. In this paper we present a neural network based approach to automatic PE (NNAPE). Our NNAPE model is inspired by the MT work of Bahdanau et al. (2014) which is based on bidirectional recurrent neural networks (RNN). Unlike Bah3 Neural Network based APE The NNAPE system is based on a"
P16-2046,P03-1021,0,0.320083,"Missing"
P16-2046,P07-2045,0,\N,Missing
P19-1178,P17-1042,0,0.0543905,"der– decoder with 8-head self-attention, 512-dim word embeddings and a 2048-dim hidden feed-forward. Adam optimisation with λ=2 and beta2=0.998; noam λ decay with 8000 warm-up steps. Labels are smoothed (=0.1) and a dropout mask (p=0.1) is applied. The five models described in the LSTM category have transformer counterparts which follow the same transformer base architecture. All systems are trained on a single GPU GTX TITAN using a batch size of 64 (LSTM) or 50 (transformer) sentences. 4 Results and Discussion In order to train the 10 NMT systems, we initialise the word embeddings following Artetxe et al. (2017) using a seed dictionary of 2.591 numerals automatically extracted from our Wikipedia editions, and feed the system directly with comparable articles. This avoids the n × m explosion of possible combinations of sentences, where n is the number of sentences Pin L1 and m in L2. In our approach, we input article ni × mj sentence pairs, that is, only all possible source–target sentence combinations within two articles linked by Wikipedia’s langlinks. Hence we miss the parallel sentences in non-linked articles but we win in speed. Articles are input in lots4 . For them, the appropriate representati"
P19-1178,D18-1399,0,0.149473,"-Bonet and Barr´on-Cede˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in combination with NMT (Lample et al., 2018b; Artetxe et al., 2018a). In our approach, we propose a new and simpler method without a priori parallel corpora. Our premise is that NMT systems —either sequence to sequence models with RNNs, transformers, or any architecture based on encoder–decoder mod"
P19-1178,J82-2005,0,0.736915,"Missing"
P19-1178,W15-3402,1,0.823597,"Missing"
P19-1178,D14-1179,0,0.103132,"Missing"
P19-1178,S17-2019,1,0.902774,"Missing"
P19-1178,Q17-1024,0,0.105086,"Missing"
P19-1178,P17-4012,0,0.0977833,"Missing"
P19-1178,P07-2045,0,0.00898842,"ments). We still use both representations and extend the 1829 number of candidates considered only for S=Ch , which is the most restrictive factor at the beginning of training. (iv) Low precision, high recall. Generalisation of the previous strategy where we make the method symmetric in source–target and Ch –Ce . 3 Experimental Setting Data. We use Wikipedia (WP) dumps1 in English (en) and French (f r), and pre-process the articles and split the text into sentences using the Wikitailor toolkit2 (Barr´on-Cede˜no et al., 2015). We further tokenise and truecase them using standard Moses scripts (Koehn et al., 2007) and apply a byte-pair encoding (Sennrich et al., 2016) of 100 k merge operations trained on the concatenation of English and French data. We also remove duplicates and discard sentences with more than 50 tokens for training the MT systems. We fix these settings as a comparison point for all the experiments even though smaller vocabularies and longer sentences might imply the extraction of more parallel sentences (see Section 4). We use newstest2012 for validation and newstest2014 for testing. WP dumps are used for two different purposes in our systems: (i) to calculate initial word embeddings"
P19-1178,P18-2037,0,0.370256,"cus on similarities estimated from NMT representations. The strength of NMT embeddings as semantic representations was first shown qualitatively in Sutskever et al. (2014); Ha et al. (2016) and Johnson et al. (2017), and used for estimating semantic similarities at sentence level in Espa˜na-Bonet and Barr´on-Cede˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in"
P19-1178,P16-1162,0,0.0994997,"the 1829 number of candidates considered only for S=Ch , which is the most restrictive factor at the beginning of training. (iv) Low precision, high recall. Generalisation of the previous strategy where we make the method symmetric in source–target and Ch –Ce . 3 Experimental Setting Data. We use Wikipedia (WP) dumps1 in English (en) and French (f r), and pre-process the articles and split the text into sentences using the Wikitailor toolkit2 (Barr´on-Cede˜no et al., 2015). We further tokenise and truecase them using standard Moses scripts (Koehn et al., 2007) and apply a byte-pair encoding (Sennrich et al., 2016) of 100 k merge operations trained on the concatenation of English and French data. We also remove duplicates and discard sentences with more than 50 tokens for training the MT systems. We fix these settings as a comparison point for all the experiments even though smaller vocabularies and longer sentences might imply the extraction of more parallel sentences (see Section 4). We use newstest2012 for validation and newstest2014 for testing. WP dumps are used for two different purposes in our systems: (i) to calculate initial word embeddings and (ii) as training corpus. In the first case, we use"
P19-1178,P18-1005,0,0.0923085,"˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in combination with NMT (Lample et al., 2018b; Artetxe et al., 2018a). In our approach, we propose a new and simpler method without a priori parallel corpora. Our premise is that NMT systems —either sequence to sequence models with RNNs, transformers, or any architecture based on encoder–decoder models— already learn st"
P97-1052,E95-1001,0,0.356494,"ial component which operates directly on underspecified representations for fstructures through the translation images of f-structures as UDRSs. 1 ['PRED ~COACH~] NUM S G /SPEC EVERY PRED 'pick (T SUBJ, T OBJ)' [PRED 'PLAYER'] L°B: LSPE¢ iN'M s/ J and QLF representations ?Scope : pick (t erm(+r, &lt;hUm=sg, spec=every>, coach, ?Q, ?X), term (+g, &lt;num=sg, spec=a>, player, ?P, ?R) ) both of which are fiat representations which allow underspecification of e.g. the scope of quantificational NPs. In this companion paper we show that f-structures are just as easily interpretable as UDRSs (Reyle, 1993; Reyle, 1995): Introduction Lexical Functional Grammar (LFG) f-structures (Kaplan and Bresnan, 1982; Dalrymple et al., 1995a) are attribute-value matrices representing high level syntactic information abstracting away from the particulars of surface realization such as word order or inflection while capturing underlying generalizations. Although f-structures are first and foremost syntactic representations they do encode some semantic information, namely basic predicate argument structure in the semantic form value of the PRED attribute. Previous approaches to providing semantic components for LFGs concent"
P97-1052,E93-1047,0,0.0236412,"ealization such as word order or inflection while capturing underlying generalizations. Although f-structures are first and foremost syntactic representations they do encode some semantic information, namely basic predicate argument structure in the semantic form value of the PRED attribute. Previous approaches to providing semantic components for LFGs concentrated on providing schemas for relating (or translating) fstructures (in)to sets of disambiguated semantic representations which are then interpreted model theoretically (Halvorsen, 1983; Halvorsen and Kaplan, 1988; Fenstad et al., 1987; Wedekind and Kaplan, 1993; Dalrymple et al., 1996). More recently, (Genabith and Crouch, 1996) presented a method for providing a direct and underspecified interpretation of f-structures by interpreting them as quasi-logical forms (QLFs) (Alshawi and Crouch, 1992). The approach was prompted by striking structural similarities between f-structure coach(x) layer(y) Ipick(x,y)I We do this in terms of a translation function r from f-structures to UDRSs. The recursive part of the definition states that the translation of an f-structure is simply the union of the translation of its component parts: 'F1 71 ... T( PRED I-[(~r"
P97-1052,C96-1045,1,\N,Missing
P97-1052,P92-1005,1,\N,Missing
P98-1056,P92-1005,0,0.0295005,"assigned an underspecified truth-conditional interpretation (Genabith and Crouch, 1997). 3 Appendix B gives a relational formulation of the correspondence between f-structures and UDRSs. The UDRS representations are processed by semantic-based transfer. The resulting system is bi-directional. Consider again the simple head switching case discussed in (1) and (3) above. (4) shows the corresponding UDRSs. The structural mismatch between the two fstructures has disappeared on the level of UDRS representations and transfer is facilitated. 4 3A similar corespondence between f-structures and QLFs (Alshawi and Crouch, 1992) has been shown in (Genabith and Crouch, 1996). 4In the implementation, a Neo-Davidsonian style en343 4.1 Embedded Head-Switching The syntactic transfer rules (2) are supplemented by (5). The complex rule for gerne in (5) overrides 5 (2d) and the COMP rule in (5). For each additional level of embedding triggered by head switching adjuncts a special rule is needed. (5) { vermuten(E) } &lt;-> { suspect(E) }. Ede(X) } &lt;-> (Ede(X) }. •[ COMP(E,X) } &lt;-> { COMP(E,X) }. { gerne(X),ADJN(E,X),COMP(E1,E) } # (SUBJ(E,Y) } &lt;-> { like(X),XCOMP(X,E),SUBJ(X,Y),COMP(EI,X) }. By contrast, on the level of UDRSs he"
P98-1056,C96-1045,1,0.879182,"Missing"
P98-1056,P97-1052,1,0.871464,"Missing"
P98-1056,J91-2001,0,0.427559,"ransfer This section presents a simple bidirectional translation between LFG f-structures and term representations which serve as input to and output of a transfer component developed within the Verbmobil project (Dorna and Emele, 1996a). The term representation is inspired by earlier work (Kay et al., 1994; Caspari and Schmid, 1994) which uses terms as a quasisemantic representation for transfer and generation. The translation between f-structures and terms is based on the correspondence between directed graphs representing f-structures and the functional interpretation of these graphs (cf. (Johnson, 1991)). Given an arc labeled f which connects two nodes nl and n2 in a graph, the same can be expressed by a function f ( n l ) = n2. An f-structure is the set of such feature equations describing the associated graph. Instead of feature equations f ( n l ) -- n2 we use the relational notation f ( n l , n2). Using this idea f-structures can be converted into sets of terms and vice versa} F-structure 1For motivation why we prefer term representations 342 PRED features and their ""semantic form"" values are given special treatment. Instead of introducing PRED terms we build unary relations with the sem"
P98-1056,E93-1024,0,0.0510858,"h and Technology (BMBF) in the framework of the Verbmobil project under grant 01 IV 701 N3. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) - led to the contention that transfer is facilitated at the level of semantic representation, where structural differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wedekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage of defining transfer on semantic representations could well be counterbalanced by the overhead generated by multiple disambiguated structures as input to transfer. This and the observation that many semantic (and syntactic) ambiguities can be preserved when translating into a target language that is ambiguous in similar ways, sheds light on the issue of the properties of representations for the task of defining transfer. In principle, the"
P98-1056,E89-1037,0,0.177318,"eed to be sensible representations for both parsing and generation. LFG f-structures are abstract, ""high-level"" syntactic representations which go some way towards meeting these often irreconcilable requirements. "" We would like to thank H. Kamp, M. Schiehlen and the anonymous reviewers for helpful comments on earlier versions of this article. Part of this work was funded by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the Verbmobil project under grant 01 IV 701 N3. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) - led to the contention that transfer is facilitated at the level of semantic representation, where structural differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wedekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage"
P98-1056,J93-4001,0,0.268167,"Missing"
P98-1056,C96-1054,1,\N,Missing
P98-1056,C96-1024,0,\N,Missing
P98-1056,P98-1060,1,\N,Missing
P98-1056,C98-1058,1,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S13-1034,S12-1051,0,0.0491719,"ines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1 , S2 ) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the STS task from a different perspective using the 289 features extracted dependent on the interpretants using MTPP. We extract the features both on 2 A LIX= B + C 100 , where A is the number of words, C is A words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). R+P+L+S TL R+P+L+S L+P+S TL L+P+S R+P+S R+P+L L+S TL L+S L+P R+S R+L R+P S"
S13-1034,P98-1013,0,0.0600613,"0}: Measures the fluency of the sentences according to language models (LM). We use both forward ({30}) and backward ({15}) LM based features for S and T. • Entropy {4}: Calculates the distributional similarity of test sentences to the training set. • Retrieval Closeness {24}: Measures the degree to which sentences close to the test set are found in the training set. 236 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1 , S2 ) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build"
S13-1034,S12-1059,0,0.140581,"Missing"
S13-1034,S13-2098,1,0.412068,"Missing"
S13-1034,W11-2131,1,0.8865,"Missing"
S13-1034,W11-2137,1,0.747012,"Missing"
S13-1034,J93-2003,0,0.0318842,"arity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants. 2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models. SMT system performance is affected by the amount of training data used as well • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T."
S13-1034,W12-3102,0,0.163049,"Missing"
S13-1034,N12-1021,0,0.0864323,"between S1 and S2 . 2 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves very good performance in judging the semantic similarity of sentences and we can also use RTM to automatically assess the correctness of student answers to obtain better results (Bic¸ici and van Genabith, 2013) than the state-of-the-art (Dzikovska et al., 2012). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically judging the semantic similarity between texts. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context."
S13-1034,P02-1040,0,0.0865181,"al MT (SMT) models. SMT system performance is affected by the amount of training data used as well • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 2 as the closeness of the test set to the training set. MTPP (Bic¸ici et al., 2013) is a top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without"
S13-1034,N03-1033,0,0.0331495,"2 .8434 .8432 .844 .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588 Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM models in directions S1 → S2 , S2 → S1 , and the bi-directional models S1  S2 are displayed. the training set and the test set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Europarl (Callison-Burch et al., 2012) 3 . In-domain corpora are likely to improve the performance. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the perspectives P and L. We use the training corpus to build a 5-gram target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value. The parameters that govern the behavior of RR and SVR are the regularization λ for RR and the C, , and γ parameters for SVR. At testing time, the predictions are bounded to obtain scores in the range [0, 5]. We perform tuning on a subset of the training set separately for each RTM model a"
S13-1034,C98-1013,0,\N,Missing
S13-2098,S12-1059,0,0.224176,"Missing"
S13-2098,S13-1034,1,0.411782,"Missing"
S13-2098,W11-2131,1,0.792247,"Missing"
S13-2098,W11-2137,1,0.667755,"Missing"
S13-2098,J93-2003,0,0.0285367,"sentations. • Perplexity {90}: Measures the fluency of the sentences according to language models (LM). We use both forward ({30}) and backward ({15}) LM based features for S and T. • Entropy {4}: Calculates the distributional similarity of test sentences to the training set. • Retrieval Closeness {24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models. SMT system performance is affected by the amount of training d"
S13-2098,W12-3102,0,0.156996,"Missing"
S13-2098,N12-1021,0,0.315338,"nd the test 586 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves very good performance in judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to obtain better results than the baselines proposed by (Dzikovska et al., 2012), which achieve the best performance on some tasks (Dzikovska et al., 2013). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically grading student answers. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point"
S13-2098,S13-2045,0,0.0759513,"swer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge. 1 Automatically Grading Student Answers We introduce a fully automated student answer grader that performs well in the student response analysis (SRA) task (Dzikovska et al., 2013) and especially well in tasks with unseen answers. Automatic grading can be used for assessing the level of competency for students and estimating the required tutoring effort in e-learning platforms. It can also be used to adapt questions according to the average student performance. Low scored topics can be discussed further in classrooms, enhancing the overall coverage of the course material. The quality estimation task (QET) (CallisonBurch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to the reference. Bicici et al. (2"
S13-2098,P02-1040,0,0.0856965,"{24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models. SMT system performance is affected by the amount of training data used as well as the closeness of the test set to the training set. MTPP (Bic¸ici et al., 2013) is a top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a trans"
S15-2015,S13-1004,0,0.0316499,"submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 201"
S15-2015,W05-0909,0,0.143784,"STS English Task: • ModelX: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regres"
S15-2015,S13-1020,0,0.218511,"Missing"
S15-2015,S13-1034,1,0.905276,"Missing"
S15-2015,S14-2085,0,0.112676,"Missing"
S15-2015,N10-1031,0,0.0555248,"X: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regressor is a Bayesian ridge regr"
S15-2015,W14-3351,0,0.173018,"Missing"
S15-2015,C12-2044,0,0.0694697,": 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2009). Much of the previous work on using MT evaluation metrics is based on improving the regressors through algorithm choice, feature selection and parameters tuning. We introduce a novel architecture of hybrid supervised machine learning, Deep Regression, which attempts to combine different regressors and automating feature selection by means of dimensionality reduction. 1 Refers to the token cosine baseline (baseline-tokencos) from the task organizers. 85 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89, c Denver, Colorado, Jun"
S15-2015,S14-2102,0,0.348269,"ce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2"
S15-2015,P09-1034,0,0.064364,"Missing"
S15-2015,P02-1040,0,0.0962895,"ch metric comprises several features that compute the translation quality by comparing every translation against one or several reference translations. We consider three sets of features: n-gram overlaps, Shallow Parsing metrics and METEOR. These metrics correspond to the lexical, syntactic and semantic levels respectively. 4.1 N -gram Overlaps Gonz`alez et al. (2014) reintroduces the notion of language independent metrics relying on n-gram overlaps. This is similar to the BLEU metric that calculates the geometric mean of n-gram precision by comparing the translation against its reference(s) (Papineni et al., 2002) without the brevity penalty. Different from BLEU, the n-gram overlaps are computed as similarity coefficients instead of taking the crude proportion of overlap n-gram. n -gramoverlap = sim n -gramtrans ∩ n -gramref Figure 1: Deep Regression Architecture. Figure 1 presents the Deep Regression architecture where the inputs are fed into the different hidden regressors and unlike traditional neural network, each regressor produces a discrete output with a different cost function unlike the consistent activation function in neural nets. Different from ensemble learning, the voting/selection determ"
S15-2015,S12-1100,1,0.739167,"University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous"
S15-2015,W00-0726,0,0.036069,"Missing"
S15-2015,1992.tmi-1.7,0,0.226955,"input is reduced to the number of hidden regressors and the input for the last layer regressors is a latent layer in the higher dimensional space. Within a standard neural net, every node in the latent layer is influenced by all the perceptrons in the previous layer. In contrast, each latent dimen86  We use 16 features of n-gram overlap by considering both the cosine similarity and Jaccard Index in calculating the n-gram overlaps for character and token n-gram from the order of bigrams to 5-grams. In addition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing The Shallow Parsing (SP) metric measures the syntactic similarities by computing the overlaps between the translation and the reference translation at the Parts-Of-Speech (POS), word lemmas and base phrase chunks level. The purpose of the SP metric is to capture the proportion of lexical items correctly translated according to their shallow syntactic realization. The base phrase chunks are tagged using the BIOS toolkit (Surdeanu et al., 2005) and POS tagging and lemmatization are achieved using SVMTool (Gim´enez and M`arquez, 2"
S15-2015,S14-2010,0,\N,Missing
S15-2015,S12-1051,0,\N,Missing
S15-2155,P99-1008,0,0.314373,"onomy6 for the equipment domain, the Google product taxonomy7 for the food domain and the Taxonomy of Fields and their Different Sub-fields8 for the science domain. In addition, all four domains are also evaluated against the sub-hierarchies from the WordNet ontology that subsumes the Suggested Upper Merged Ontology (Pease et al., 2002). 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1"
S15-2155,S15-2151,0,0.389165,", June 4-5, 2015. 2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifying a previously complex process of learning a hypernymhyponym transition matrix. The implementation of our ontological induction approach is open-sourced and available on our GitHub repository.5 1.1 Task Definition Similar to Fountain and Lapata (2012), the SemEval-2015 Taxonomy Extraction Evaluation (TaxEval) task addresses taxonomy learning without the term discovery step, i.e. the terms for which to create the taxonomy are given (Bordea et al., 2015). The focus is on creating the hypernym-hyponym relations. In the TaxEval task, taxonomies are evaluated through comparison with gold standard taxonomies. There is no training corpus provided by the organisers of the task and the participating systems are to generate hyper-hyponyms pairs using a list of terms from four different domains, viz. chemicals, equipment, food and science. The gold standards used in evaluation are the ChEBI ontology for the chemical domain (Degtyarenko et al., 2008), the Material Handling Equipment taxonomy6 for the equipment domain, the Google product taxonomy7 for t"
S15-2155,W14-2211,1,0.733293,"Missing"
S15-2155,N12-1051,0,0.387063,"are not noun (entities/arguments), verbs (predicates), adjectives or adverbs (adjuncts). 932 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 932–937, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifying a previously complex process of learning a hypernymhyponym transition matrix. The implementation of our ontological induction approach is open-sourced and available on our GitHub repository.5 1.1 Task Definition Similar to Fountain and Lapata (2012), the SemEval-2015 Taxonomy Extraction Evaluation (TaxEval) task addresses taxonomy learning without the term discovery step, i.e. the terms for which to create the taxonomy are given (Bordea et al., 2015). The focus is on creating the hypernym-hyponym relations. In the TaxEval task, taxonomies are evaluated through comparison with gold standard taxonomies. There is no training corpus provided by the organisers of the task and the participating systems are to generate hyper-hyponyms pairs using a list of terms from four different domains, viz. chemicals, equipment, food and science. The gold s"
S15-2155,P14-1113,0,0.411438,"onally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1 For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. However, their approach requires an existing hypernym-hyponym pairs for training before discovering new pairs. Our system submitted to the SemEval-2015 taxonomy building task is most similar to the approach by Fu et al. (2014) in using word embeddings projections to identify"
S15-2155,W03-1210,0,0.154222,"and the Taxonomy of Fields and their Different Sub-fields8 for the science domain. In addition, all four domains are also evaluated against the sub-hierarchies from the WordNet ontology that subsumes the Suggested Upper Merged Ontology (Pease et al., 2002). 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach,"
S15-2155,C92-2082,0,0.233427,"rms from four different domains, viz. chemicals, equipment, food and science. The gold standards used in evaluation are the ChEBI ontology for the chemical domain (Degtyarenko et al., 2008), the Material Handling Equipment taxonomy6 for the equipment domain, the Google product taxonomy7 for the food domain and the Taxonomy of Fields and their Different Sub-fields8 for the science domain. In addition, all four domains are also evaluated against the sub-hierarchies from the WordNet ontology that subsumes the Suggested Upper Merged Ontology (Pease et al., 2002). 2.1 Pattern/Rule Based Approaches Hearst (1992) first introduced ontology learning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For i"
S15-2155,D10-1108,0,0.48241,"f nodes connected by a directed edges. In this regard, There are a variety of methods used in taxonomy a single root node is not guaranteed and to produce induction. They can be broadly categorized as (i) a tree-like structure. pattern/rule based, (ii) clustering based, (iii) graph Disregarding the overall hierarchical structure, based and (iv) vector space approaches. the crux of graph induction focuses on the different techniques of edge weighting between individ5 https://github.com/alvations/USAAR-SemEvalual node pairs and graph pruning or edge collaps2015/tree/master/task17-USAAR-WLV ing (Kozareva and Hovy, 2010; Navigli et al., 2011; 6 http://www.ise.ncsu.edu/kay/mhetax/index.htm 7 http://www.google.com/basepages/producttype/taxonomy.en- Fountain and Lapata, 2012; Tuan et al., 2014). US.txt 8 http://sites.nationalacademies.org/PGA/Resdoc/PGA 044522 933 9 Commonly known as Lin information content measure. 2.4 Vector Space Approaches Semantic knowledge can be thought of as a twodimensional vector space where each word is represented as a point and semantic association is indicated by word proximity. The vector space representation for each word is constructed from the distribution of words across cont"
S15-2155,P08-1119,0,0.246695,"Missing"
S15-2155,P98-2127,0,0.330599,"arning by exploiting lexico-syntactic patterns that explicitly links a hypernym to its hyponym, e.g. “X and other Ys” and “Ys such as X”. These patterns could be manually constructed (Berland and Charniak, 1999; Kozareva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vert"
S15-2155,N13-1090,0,0.103099,"ce the recent advancement in neural nets and word embeddings that vector space models are gaining ground for ontology induction and relation extraction (Saxe et al., 2013; Khashabi, 2013). 3 Methodology This section provides a brief overview of our system’s approach to taxonomy induction. The full system is released as open-source and contains documentation with additional implementation details.10 3.1 Projecting a Hyponym to its Hypernym with Transition Matrix Fu et al. (2014) discovered that hypernymhyponyms pairs have similar semantic properties as the linguistics regularities discussed in Mikolov et al. (2013b). For instance: v(shrimp)-v(prawn) ≈ v(fish)-v(goldfish). Intuitively, the assumption is that all words can be projected to their hypernyms based on a transition matrix. That is, given a word x and its hypernym y, a transition matrix Φ exists such that y = Φx, e.g. v(goldfish) = Φ×v(fish). Fu et al. proposed two projection approaches to identify hypernym-hyponym pairs, (i) uniform linear projection where Φ is the same for all words and Φ is learnt by minimizing the mean squared error of kΦx-yk across all word-pairs (i.e. a domain independent Φ) and (ii) piecewise linear projection that learn"
S15-2155,N04-1041,0,0.117459,"reva et al., 2008) or automatically bootstrapped (Girju, 2003). These methods rely on surface-level patterns and incorrect items are frequently extracted because of parsing errors, polysemy, idiomatic expressions, etc. 2.2 Clustering Approaches Clustering based approaches are mostly used to discover hypernym (is-a) and synonym (is-like) relations. For instance, to induce synonyms, Lin (1998) clustered words based on the amount of information needed to state the commonality between two words.9 Contrary to most bottom-up clustering approaches for taxonomy induction (Caraballo, 2001; Lin, 1998), Pantel and Ravichandran (2004) introduced a top-down approach, assigning the hypernyms to clusters using co-occurrence statistics and then pruning the cluster by recalculating the pairwise similarity between every hyponym pair within the cluster. 2.3 Graph-based Approaches In graph theory (Biggs et al., 1976), similar ideas are conceived with a different jargon. In graph notation, nodes/vertices form the atom units of the graph and nodes are connected by directed edges. A graph, unlike an ontology, regards the hierarchical structure of 2 Related Work a taxonomy as a by-product of the individual pairs of nodes connected by"
S15-2155,R09-1071,0,0.0184746,"ratus) is a freshwater fish”.3 Intuitively, if we single-tokenize the ‘is a’ phrase prior to training a vector space, we can make use of the vector that represents the phrase in capturing a hypernym-hyponym pair as such the multiplication of v(goldfish) and v(is-a) will be similar to the cross product v(fish) (v(goldfish)×v(is-a) ≈ v(fish)). There is little or no previous work that manipulates non-content word vectors in vector space models studies for natural language processing. Often, non-content words4 were implicitly incorporated into the vector space models by means of syntactic frames (Sarmento et al., 2009) or syntactic parses (Thater et al., 2010). Our main contribution for ontological induction 2 In this case, the projection matrix is a vector space feature function. 3 From http://en.wikipedia.org/wiki/Goldfish. 4 Words that are not noun (entities/arguments), verbs (predicates), adjectives or adverbs (adjuncts). 932 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 932–937, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics using vector space models are primarily (i) the use of non-content word vectors and (ii) simplifyin"
S15-2155,P06-1101,0,0.282173,"any hyponym to its hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1 For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. Ho"
S15-2155,P10-1097,0,0.0604558,"Missing"
S15-2155,D14-1088,0,0.64825,"Missing"
S15-2155,J13-3007,0,0.209515,"hypernym. This is done by making use of function words, which are usually overlooked in vector space approaches to NLP. Our system performs best in the chemical domain and has achieved competitive results in the overall evaluations. 1 Introduction Traditionally, broad-coverage semantic taxonomies such as CYC (Lenat, 1995) and WordNet ontology (Miller, 1995) have been manually created with much effort and yet they suffer from coverage sparsity. This motivated the move towards unsupervised approaches to extract structured relational knowledge from texts (Lin and Pantel, 2001; Snow et al., 2006; Velardi et al., 2013).1 Previous work in taxonomy extraction focused on rule-based, clustering and graph-based approaches. Although vector space approaches are popular in current NLP researches, ontology induction studies have yet to catch on the frenzy. Fu et al. (2014) proposed a vector space approach to hypernymhyponym identification using word embeddings that 1 For the rest of the paper, taxonomy and ontology will be used interchangeably to refer to a hierarchically structure that organizes a list of concepts. trains a projection matrix2 that converts a hyponym vector to its hypernym. However, their approach r"
S15-2155,P99-1016,0,\N,Missing
S15-2155,C98-2122,0,\N,Missing
S16-1095,S13-1004,0,0.0692141,"cutting open a box.”, an STS system predicts a real number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Su"
S16-1095,1983.tc-1.13,0,0.706143,"Missing"
S16-1095,N10-1031,0,0.113645,"ls that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit sema"
S16-1095,W14-3351,0,0.058462,"Missing"
S16-1095,D15-1124,1,0.873884,"Missing"
S16-1095,S14-2102,0,0.554526,"Missing"
S16-1095,S14-2003,0,0.013839,"LEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT evalu1 Refers to the token cosine (baseline-tokencos) in STS-2012. 628 Proceedings of SemEval-2016, pages 628–633, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 Eng"
S16-1095,P04-1077,0,0.0772826,"2) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al.,"
S16-1095,W05-0904,0,0.0464141,"ineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT ev"
S16-1095,W12-3129,0,0.0199825,"ompute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT ev"
S16-1095,P02-1040,0,0.114057,"task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 200"
S16-1095,S12-1100,1,0.813789,"ation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Li"
S16-1095,S12-1060,0,0.060836,"Missing"
S16-1095,2006.amta-papers.25,0,0.0633173,"xical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scor"
S16-1095,W14-3354,0,0.0234987,"feature set, we use 52 shallow parsing features described in (Tan et al., 2015); they measure the similarity coefficients from the n-gram overlaps of the lexicalized shallow parsing (aka chunking) annotations. As for semantics, we use 44 similarity coefficients from Named Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Bo"
S16-1095,S15-2027,0,0.0913526,"e tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014; Bicici, 2015). 3 Approach Following the success of systems that use MT evaluation metrics, we train three regression models using an array of MT metrics based on lexical, syntactic and semantic features. 3.1 Feature Matrix Machine translation evaluation metrics utilize various degrees of lexical, syntactic and semantic information. Each metric consi"
S16-1095,P15-1150,0,0.057508,"erently. We use all four variants of METEOR: exact, stem, synonym and paraphrase. gressor (XGBoost) (Chen and He, 2015; Chen and Guestrin, 2015). They were trained using all features described in Section 3. We have released the MT metrics annotations of the STS data and implementation of systems on https://github.com/alvations/stasis /blob/master/notebooks/ARMOR.ipynb 3.1.4 ReVal Features 4 ReVal (Gupta et al., 2015) is a deep neural net based metric which uses the cosine similarity score between the Tree-based Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Tai et al., 2015) dense vector space representations of two sentences. 3.2 Models We annotated the STS 2012 to 2015 datasets with the features as described in Section 3.1 and submitted three models to the SemEval-2016 English STS Task using (i) a linear regressor (Linear), (ii) boosted tree regressor (Boosted) (Friedman, 2001) and (iii) eXtreme Gradient Boosted tree re630 Results Table 1 presents the official results for our submissions to the English STS task. The bottom part of the table presents the median and the best correlation results across all participating teams for the respective domains. Our baseli"
S16-1095,S15-2015,1,0.847418,"Missing"
S16-1095,U06-1019,0,0.0223859,"California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention o"
S16-1095,W07-0404,0,0.0167353,"d Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Boosted XGBoost Median Best answer-answer 0.31539 0.37717 0.47716 0.48018 0.69235 headlines 0.76551 0.77183 0.78848 0.76439 0.82749 plagiarism 0.82063 0.81529 0.83212 0.78949 0.84138 postediting 0.83329 0.84528 0.84960 0.81241 0.86690 question-question 0.73987 0.66825"
S16-1095,S15-2010,0,\N,Missing
S16-1095,S14-2085,0,\N,Missing
S16-1095,W05-0909,0,\N,Missing
S16-1095,S12-1051,0,\N,Missing
S16-1096,S13-1004,0,0.0131576,"ion Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the words (i, j) between sentences S (1) and S (2) , given that the aligned units belong to a set of content words, C. Since the proportion is monotonic, the equation above only provides the proportion of semantic unit alignments for S (1) . The Al alignments pairs are automatic"
S16-1096,P14-1023,0,0.315427,"or changing but the denominator combined the proportions using harmonic mean: changes with it. The difference between v(S (1) ) and (1) (2) (2) 2 ∗ propAl ∗ propAl sim(S (1) , S (2) ) = (3) v(S ) contributes to distributional semantic dis(1) (2) tance. propAl + propAl To calculate a real value similarity score between Instead of simply using the alignment propor- the sentence vectors, we take the dot product betions, Sultan et al. (2015) extended their hypothe- tween the vectors to compute the cosine similarity sis by leveraging pre-trained neural net embeddings between the sentence vectors: (Baroni et al., 2014). They posited that the semantics of the sentence can be captured by the centroid v(S (1) ) · v(S (2) ) sim(S (1) , S (2) ) = (5) of its content words1 computed by the element-wise |v(S (1) ) ||v(S (2) )| sum of the content word embeddings normalized by the number of content words in the sentence. ToThere was no clear indication of which vector gether with the similarity scores from Equation 3 space Sultan et al. (2015) have chosen to compute and the cosine similarity between two sentence em- the similarity score from Equation 5. Thus we combeddings, they trained a Bayesian ridge regressor to"
S16-1096,S13-1020,0,0.03134,"Missing"
S16-1096,W05-1203,0,0.0616982,"ions. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the w"
S16-1096,N13-1092,0,0.100661,"Missing"
S16-1096,S14-2139,1,0.890069,"Missing"
S16-1096,D15-1124,1,0.89722,"Missing"
S16-1096,S14-2102,0,0.0241025,"ared to Sultan et al. (2015) results of 0.8015 we are 0.04 points short of their results which should technically rank our system at 20th out of 70+ submissions to the STS 2015 task4 . Machine Translation (MT) evaluation metrics have shown competitive performance in previous 4 Our replication attempt obtained better results compared to our STS-2015 submission (MiniExperts) that used a Support Vector Machine regressor trained on a number of linguistically motivated features (Gupta et al., 2014); it achieved 0.7216 mean score (B´echara et al., 2015). 637 STS tasks (Barr´on-Cede˜no et al., 2013; Huang and Chang, 2014; Bertero and Fung, 2015; Tan et al., 2015). Tan et al. (2016) annotated the STS datasets with MT metrics scores for every pair of sentence in the training and evaluation data. We extend our XGBoost model with these MT metric annotations and achieved a higher score for every domain leading to an overall Pearson correlation score of 0.73050 (+Saarsheff in Table 1). 6 Conclusion In this paper, we have presented our findings on replicating the top system in the STS 2014 and 2015 task and evaluated our replica of the system in the English STS task of SemEval-2016. We have introduced variants and e"
S16-1096,W14-1618,0,0.0369043,"d context window, 10 negative samples, subsampling, 400 dimensions • 2-word context window, PMI weighting, no compression, 300K dimensions In this case, we extracted two similarity features for every sentence pair. With the harmonic proportion feature from Equation 3 and the similarity scores from Equation 5, we trained a boosted tree ensemble on the 3 features using the STS 2012 to 2015 datasets and submitted the outputs from this model as our baseline submission in the English STS Task in SemEval 2016. 3.1 Replacing COMPOSES with GloVe Pennington et al. (2014) handles semantic regularities (Levy et al., 2014) explicitly by using a global log-bilinear regression model which combines the global matrix factorization and the local context vectors when training word embeddings. Instead of using the COMPOSES vector space, we experimented with replacing the v(wi ) component in Equation 4 with the GloVe vectors,2 vglove (wi ) such that: simglove (S (1) , S (2) ) = distributions p and pˆθ using regularised KullbackLeibler (KL) divergence. vglove (S (1) ) · vglove (S (2) ) |vglove (S (1) ) ||vglove (S (2) )| (6) The novelty lies in the usage of the global matrix to capture corpus wide phenomena that might n"
S16-1096,N03-1020,0,0.0860567,"sentence representations. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that"
S16-1096,P11-1124,1,0.889794,"Missing"
S16-1096,D14-1162,0,0.0785412,"(personal communication with Arafat Sultan). 635 • 5-word context window, 10 negative samples, subsampling, 400 dimensions • 2-word context window, PMI weighting, no compression, 300K dimensions In this case, we extracted two similarity features for every sentence pair. With the harmonic proportion feature from Equation 3 and the similarity scores from Equation 5, we trained a boosted tree ensemble on the 3 features using the STS 2012 to 2015 datasets and submitted the outputs from this model as our baseline submission in the English STS Task in SemEval 2016. 3.1 Replacing COMPOSES with GloVe Pennington et al. (2014) handles semantic regularities (Levy et al., 2014) explicitly by using a global log-bilinear regression model which combines the global matrix factorization and the local context vectors when training word embeddings. Instead of using the COMPOSES vector space, we experimented with replacing the v(wi ) component in Equation 4 with the GloVe vectors,2 vglove (wi ) such that: simglove (S (1) , S (2) ) = distributions p and pˆθ using regularised KullbackLeibler (KL) divergence. vglove (S (1) ) · vglove (S (2) ) |vglove (S (1) ) ||vglove (S (2) )| (6) The novelty lies in the usage of the global ma"
S16-1096,Q14-1018,0,0.396864,"1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemE"
S16-1096,S14-2039,0,0.166692,"1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemE"
S16-1096,S15-2027,0,0.275822,"cess of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemEval2016. We replicated"
S16-1096,P15-1150,0,0.0335968,"2 Similarity Using Tree LSTM Recurrent Neural Nets (RNNs) allow arbitrarily sized sentence lengths (Elman, 1990) but early work on RNNs suffered from the vanishing/exploding gradients problem (Bengio et al., 1994). Hochreiter and Schmidhuber (1997) introduced multiplicative input and output gate units to solve the vanishing gradients problem. While RNN and LSTM process sentences in a sequential manner, Tree-LSTM extends the LSTM architecture by processing the input sentence through a syntactic structure of the sentence. We use the ReVal metric (Gupta et al., 2015) implementation of Tree-LSTM (Tai et al., 2015) to generate the similarity score. ReVal represents both sentences (h1 , h2 ) using Tree-LSTMs and predicts a similarity score yˆ based on a neural network which considers both distance and angle between h1 and h2 : n   λ 1X (i) KL p(i) pˆθ + ||θ||22 n 2 for 1 ≤ j ≤ K, where, y ∈ [1, K] is the similarity score of a training pair. This gives us a similarity score between [1, K] which is mapped between [0, 1].3 Please refer to Gupta et al. (2015) for training details. 4 Submission We submitted three models based on the original replication of the Sultan et al. (2014b) and Sultan et al. (2015)"
S16-1096,S15-2015,1,0.890832,"Missing"
S16-1096,S16-1095,1,0.857884,"Missing"
S16-1096,W15-3051,1,0.839378,"he median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the words (i, j) between sentences S (1) and S (2) , given that the aligned units belong to a set of content words, C. Since the proportion is monotonic, the equation above only provides the proportion of sem"
S16-1096,S14-2010,0,\N,Missing
S16-1096,S15-2017,1,\N,Missing
S16-1096,S15-2004,0,\N,Missing
S16-1096,S12-1051,0,\N,Missing
S16-1155,C12-1023,0,0.0225042,"reas text simplification comprises also the modification of syntactic structures to improve readability. Most text simplification systems also contain a lexical simplification module or component which often relies on the accurate identification of complex words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are"
S16-1155,W13-1703,0,0.0202682,"rom suitable text corpora (Zipf, 1949). Another aspect to consider is the length of the words. Words that are more frequent tend to be shorter as noted by Zipf: ‘the magnitude of words tends, on the whole, to stand in an inverse (not necessarily proportionate) relationship to the number of occurrences’ (Zipf, 1935). That said, our approach takes both frequency and word length into account to determine whether a word is complex or not. Finally, another aspect that we take into account is the difficulty in vocabulary acquisition that is related to the spelling of complex words (Xu et al., 2011; Dahlmeier et al., 2013). Educational applications that are tailored towards non-native speakers use character-level n-grams to identify possible spelling errors that language learner make. Thus making character combinations another interesting aspect to be consider in this task. 2 Related Work CWI is a sub-task included in many lexical and text simplification systems. Lexical simplification, as the name suggests, focuses only on the substitution of complex words for simpler words in texts whereas text simplification comprises also the modification of syntactic structures to improve readability. Most text simplificat"
S16-1155,W13-1728,1,0.861926,"ation of the n-grams since we can assume that longer words are more complex. But we have the word length feature to account for the length of words, so the normalization of the n-grams probabilities would account for density of the n-gram probabilities independent of the length of the word. Additionally, we computed (iii) sentence length and (iv) sum probability of the character trigrams of the sentence to account for contextual orthographic complexity with respect to the word-level spelling complexity. These sentence-level features are similar to those used in Native Language Identification (Gebre et al., 2013; Malmasi and Dras, 2015; Malmasi et al., 2015b). As a meta-feature that captures both word and sentential level spelling complexity, we use the proportion of word to sentence orthographic difficulty by taking the ratio of the aforementioned features (ii) and (iv). 3.2.3 Classifiers We trained 3 different classifiers using the features described in Table 1: a (i) Random Forest Classifier (RFC), (ii) Nearest Neighbor Classifier6 (NNC) and (iii) Support Vector Machine7 (SVM). Nearest neighbor classifiers usually work well when the distribution between the training set data points are dense and s"
S16-1155,D14-1144,0,0.0326451,"Missing"
S16-1155,W15-0620,0,0.032725,"t longer words are more complex. But we have the word length feature to account for the length of words, so the normalization of the n-grams probabilities would account for density of the n-gram probabilities independent of the length of the word. Additionally, we computed (iii) sentence length and (iv) sum probability of the character trigrams of the sentence to account for contextual orthographic complexity with respect to the word-level spelling complexity. These sentence-level features are similar to those used in Native Language Identification (Gebre et al., 2013; Malmasi and Dras, 2015; Malmasi et al., 2015b). As a meta-feature that captures both word and sentential level spelling complexity, we use the proportion of word to sentence orthographic difficulty by taking the ratio of the aforementioned features (ii) and (iv). 3.2.3 Classifiers We trained 3 different classifiers using the features described in Table 1: a (i) Random Forest Classifier (RFC), (ii) Nearest Neighbor Classifier6 (NNC) and (iii) Support Vector Machine7 (SVM). Nearest neighbor classifiers usually work well when the distribution between the training set data points are dense and similar to (or representative) of test set. Sin"
S16-1155,P15-4015,0,0.0387075,"Missing"
S16-1155,P13-3015,0,0.529415,"x words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are no borderline cases or gradation, all words are either complex of simple. A tokenized data set containing English sentences annotated with the complex or simple label for each word was provided. The training set contained 2,237 sentences, and the test"
S16-1155,S12-1046,0,0.0171217,"res to improve readability. Most text simplification systems also contain a lexical simplification module or component which often relies on the accurate identification of complex words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are no borderline cases or gradation, all words are either complex of simple. A to"
S16-1155,W13-1706,0,0.0283488,"the complex or simple label for each word was provided. The training set contained 2,237 sentences, and the test set contained 88,221 sentences. The shared task website2 states that: ‘the data was collected through a survey, in which 400 annotators were presented with several sentences and asked to select which words they did not understand their meaning’. There was no information of whether annotators were English native speakers. The proportion of training vs. test instances makes the task more challenging than other similar shared tasks which provide much more training than test instances (Tetreault et al., 2013; Zampieri et al., 2015), a common practice in text classification tasks.3 3.2 Approach Given the motivation described in Section 1.1, we approach the CWI task using word frequency and character-level n-gram features.4 To emulate a language learner exposure to English, we use newspa2 http://alt.qcri.org/semeval2016/task11/ The Chinese grammatical error diagnosis (CGED) shared task (Yu et al., 2014) is an exception. See the discussion in Zampieri and Tan (2014). 4 Our implementation is open source and it can be found on: https://github.com/alvations/MacSaar-CWI 3 pers text from the English subs"
S16-1155,D11-1119,0,0.0211827,"Missing"
S16-1155,W15-5401,1,0.888572,"Missing"
S16-1203,S15-2151,0,0.196441,"ge taxonomies such as CYC (Lenat, 1995), SUMO (Pease et al., 2002; Miller, 1995), YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) have been manually created or curated with much effort. With the rapid technological evolution, it is more feasible to construct a domain-specific taxonomy that caters to domain or company specific terminology (Lefever, 2015). This motivated the move towards unsupervised approaches to taxonomy extraction (Berland and Charniak, 1999; Lin and Pantel, 2001; Snow et al., 2006) and specifically focused towards particular domains (Velardi et al., 2013; Bordea et al., 2015). The aim of the Taxonomy Extraction Evaluation (TExEval) task is to automatically find lexical relations between pairs of terms within several specified domains. Previously, we have developed a hypernym extraction system using word embeddings by exploiting the frequent occurrence of the ‘X is a Y’ pattern in encyclopedic text (Tan et al., 2015). We have achieved competitive results in SemEval-2015 and as a follow up to our study, we would like to explore the endocentric nature of hyponyms that contributed substantially to the system performance in the previous TExEval task. Below, we will bri"
S16-1203,S16-1168,0,0.320784,"Missing"
S16-1203,S15-2156,0,0.0179609,"evious TExEval task. Below, we will briefly (i) describe related work on different approaches to taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der P"
S16-1203,S16-1205,0,0.239954,"ned low precision (Pocostales, 2016). Similar to our endocentric-based approach, the TAXI team extended the substring-based approach by filtering the hypernym candidates based on corpora statistics of lexico-syntactic patterns. Additionally, they applied pruning methods to improve the ontological structure which resulted in high Fowlkes and Mallows (F&M) Measure (Panchenko and Biemann, 2016). QASSIT used lexical patterns to extract hypernym candidates and applied the pretopological space graph optimization technique that is based on genetic algorithm to achieve the desired taxonomy structure (Cleuziou and Moreno, 2016). TAXI and QASSIT ranked first and second in the taxonomy construction criterion of the TExEval task. Both teams used graph pruning techniques to improve the taxonomy structure and implicitly improve the F&M scores5 of their taxonomy. Although our endocentricity based hypo-hypernym extraction system ranked first in hypernym identification of TExEval task, we ranked third in taxonomy construction with an overall F&M score of 0.0013. 7 Conclusion In this paper, we have described our submission to the Taxonomy Extraction Evaluation (TExEval2) Task for SemEval-2016. We have empirically shown that"
S16-1203,S15-2159,0,0.221084,"Missing"
S16-1203,S15-2158,0,0.0143016,"ification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a"
S16-1203,N12-1051,0,0.0156503,"oposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other"
S16-1203,P14-1113,0,0.0342695,"e where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other within the vector space (Mitchell and Lapata, 2010). With the present advancement in neural nets and word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2014; Shazeer et al., 2016), neural space models are gaining popularity in taxonomy induction and relation extraction tasks (Saxe et al., 2013; Fu et al., 2014; Tan et al., 2015). for information retrieval, Jones (1979) observed that compound nouns often follow the head-modifier principle where the meaning of the term can be conveyed by part(s) of the compound. Approaching endocentricity from a different angle, Nichols et al. (2005) identified the semantic head(s) of a term as its hypernym using the lowest scoping element of the Robust Minimal Recursion Semantics (RMRS) (Copestake et al., 2005) structure of the dictionary definition of the term. In the first TExEval task in SemEval-2015, both Lefever (2015) and Tan et al. (2015)1 independently devel"
S16-1203,W03-1210,0,0.0757481,"to the system performance in the previous TExEval task. Below, we will briefly (i) describe related work on different approaches to taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector"
S16-1203,S15-2152,0,0.146206,"in the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural net"
S16-1203,C92-2082,0,0.669419,"substantially to the system performance in the previous TExEval task. Below, we will briefly (i) describe related work on different approaches to taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurge"
S16-1203,D10-1108,0,0.0669032,"etween terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that simil"
S16-1203,P08-1119,0,0.0707082,"Missing"
S16-1203,S15-2157,0,0.678751,"on of novel relations not covered by the gold standard taxonomies. 1 Introduction Semantic taxonomies provide structured world knowledge to Artificial Intelligence (AI) and Natural Language Processing (NLP) systems. Traditional broad-coverage taxonomies such as CYC (Lenat, 1995), SUMO (Pease et al., 2002; Miller, 1995), YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) have been manually created or curated with much effort. With the rapid technological evolution, it is more feasible to construct a domain-specific taxonomy that caters to domain or company specific terminology (Lefever, 2015). This motivated the move towards unsupervised approaches to taxonomy extraction (Berland and Charniak, 1999; Lin and Pantel, 2001; Snow et al., 2006) and specifically focused towards particular domains (Velardi et al., 2013; Bordea et al., 2015). The aim of the Taxonomy Extraction Evaluation (TExEval) task is to automatically find lexical relations between pairs of terms within several specified domains. Previously, we have developed a hypernym extraction system using word embeddings by exploiting the frequent occurrence of the ‘X is a Y’ pattern in encyclopedic text (Tan et al., 2015). We ha"
S16-1203,S12-1012,0,0.0187638,"ing and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other within the vector space (Mitchell and Lapata, 2010). With the present advancement in neural nets and word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy e"
S16-1203,W14-1618,0,0.0495305,", 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other within the vector space (Mitchell and Lapata, 2010). With the present advancement in neural nets and word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2014; Shazeer et al., 2016), neural space models are gaining popularity in taxonomy induction and relation extraction tasks (Saxe et al., 2013; Fu et al., 2014; Tan et al., 2015). for information retrieval, Jones (1979) observed that compound nouns often follow the head-modifier principle where the meaning of the term can be conveyed by part(s) of the compound. Approaching endocentricity from a different angle, Nichols et al. (2005) identified the semantic head(s) of a term as its hypernym using the lowest scoping element of the Robust Minimal Recursion Semantics (RMRS) (Copestake et al., 2005) st"
S16-1203,P98-2127,0,0.504088,"k on different approaches to taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014)"
S16-1203,N04-1041,0,0.0592528,"taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016. 2 Related Work The hierarchical structure of domain concepts is made of hypo-hypernymy relations between terms. Different approaches have been proposed to induce these relations automatically ranging from pattern/rule-based approaches (Hearst, 1992; Girju, 2003; Kozareva et al., 2008; Ceesay and Hou, 2015) to clustering and frequency based approaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep le"
S16-1203,D14-1162,0,0.0919512,", 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other within the vector space (Mitchell and Lapata, 2010). With the present advancement in neural nets and word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2014; Shazeer et al., 2016), neural space models are gaining popularity in taxonomy induction and relation extraction tasks (Saxe et al., 2013; Fu et al., 2014; Tan et al., 2015). for information retrieval, Jones (1979) observed that compound nouns often follow the head-modifier principle where the meaning of the term can be conveyed by part(s) of the compound. Approaching endocentricity from a different angle, Nichols et al. (2005) identified the semantic head(s) of a term as its hypernym using the lowest scoping element of the Robust Minimal Recursion Semantics (RMRS) (Copesta"
S16-1203,S16-1202,0,0.22248,"s presented in Bordea et al. (2016). JUNLP relied on substrings and relations extracted from BabelNet (Navigli and Ponzetto, 2012) to identify hyper-hyponym pairs. Although it is sensible to approach the task using an existing ontology, their system achieved relatively low precision on the manual evaluation of novel hyper-hyponym pairs. The NUIG-UNLP team extended previous work on vector space approaches to taxonomy induction by comparing the similarity between the dense word embeddings of the hyponyms and their candidate hypernyms. They system achieved high recall but attained low precision (Pocostales, 2016). Similar to our endocentric-based approach, the TAXI team extended the substring-based approach by filtering the hypernym candidates based on corpora statistics of lexico-syntactic patterns. Additionally, they applied pruning methods to improve the ontological structure which resulted in high Fowlkes and Mallows (F&M) Measure (Panchenko and Biemann, 2016). QASSIT used lexical patterns to extract hypernym candidates and applied the pretopological space graph optimization technique that is based on genetic algorithm to achieve the desired taxonomy structure (Cleuziou and Moreno, 2016). TAXI and"
S16-1203,E14-4008,0,0.0340838,"pproaches (Lin, 1998; Caraballo, 2001; Pantel and Ravichandran, 2004; Grefenstette, 2015), classification approaches (Snow et al., 2004; Ritter et al., 2009; Espinosa Anke et al., 2015) and graph-based ap1303 Proceedings of SemEval-2016, pages 1303–1309, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics proaches (Kozareva and Hovy, 2010; Navigli et al., 2011; Fountain and Lapata, 2012; Tuan et al., 2014; Cleuziou et al., 2015). More recently, there is a resurgence of vector space or distributional approaches (Van Der Plas, 2005; Lenci and Benotto, 2012; Santus et al., 2014) primarily because of the renaissance of deep learning and neural networks. Semantic knowledge can be thought of as a vector space where each word is presented by a point and the proximity between words in this space quantifies their semantic association. The vector space is usually constructed from the distribution of words across contexts such that similar meanings tend to be found close to each other within the vector space (Mitchell and Lapata, 2010). With the present advancement in neural nets and word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2014; Shazeer e"
S16-1203,P06-1101,0,0.113825,"cial Intelligence (AI) and Natural Language Processing (NLP) systems. Traditional broad-coverage taxonomies such as CYC (Lenat, 1995), SUMO (Pease et al., 2002; Miller, 1995), YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) have been manually created or curated with much effort. With the rapid technological evolution, it is more feasible to construct a domain-specific taxonomy that caters to domain or company specific terminology (Lefever, 2015). This motivated the move towards unsupervised approaches to taxonomy extraction (Berland and Charniak, 1999; Lin and Pantel, 2001; Snow et al., 2006) and specifically focused towards particular domains (Velardi et al., 2013; Bordea et al., 2015). The aim of the Taxonomy Extraction Evaluation (TExEval) task is to automatically find lexical relations between pairs of terms within several specified domains. Previously, we have developed a hypernym extraction system using word embeddings by exploiting the frequent occurrence of the ‘X is a Y’ pattern in encyclopedic text (Tan et al., 2015). We have achieved competitive results in SemEval-2015 and as a follow up to our study, we would like to explore the endocentric nature of hyponyms that cont"
S16-1203,S15-2143,1,0.840747,"k17-USAAR-WLV 2 https://en.wikipedia.org/wiki/List of lists of lists 3 Our open-source implementation can be found at https://github.com/alvations/Endrocentricity captured by this swap rule are (elixir of life, elixir), (sociology of education, sociology). To improve the precision of the identifier, we set a threshold of a minimum character length of three when identifying a term as a hypernym. 5 Extending a Taxonomy with Wikipedia List of Lists of Lists The Wikipedia List of Lists of Lists (LOLOL) is a crowdsourced list of lists of terms. We adapted the customized crawler4 (Tan et al., 2014; Tan and Ordan, 2015) to crawl for tables or bullet points in the Wikipedia subpages of the LOLOL for the food domain. We started the crawl from these seed pages under the bullet point of https://en.wikipedia.org/wiki/ List of lists of lists#Food and drink. When the crawler lands on each List of Lists (LOL) page, it will treat the URL suffix as the hypernym and find words in the bullet points or tables that contain endocentric hyponyms. If an endocentric hyponym exists, it will extract either (i) all the terms in bold font if the LOL page is bulleted or (ii) all terms in the first column if the LOL page is in tabl"
S16-1203,S14-2094,1,0.827597,"15/tree/master/task17-USAAR-WLV 2 https://en.wikipedia.org/wiki/List of lists of lists 3 Our open-source implementation can be found at https://github.com/alvations/Endrocentricity captured by this swap rule are (elixir of life, elixir), (sociology of education, sociology). To improve the precision of the identifier, we set a threshold of a minimum character length of three when identifying a term as a hypernym. 5 Extending a Taxonomy with Wikipedia List of Lists of Lists The Wikipedia List of Lists of Lists (LOLOL) is a crowdsourced list of lists of terms. We adapted the customized crawler4 (Tan et al., 2014; Tan and Ordan, 2015) to crawl for tables or bullet points in the Wikipedia subpages of the LOLOL for the food domain. We started the crawl from these seed pages under the bullet point of https://en.wikipedia.org/wiki/ List of lists of lists#Food and drink. When the crawler lands on each List of Lists (LOL) page, it will treat the URL suffix as the hypernym and find words in the bullet points or tables that contain endocentric hyponyms. If an endocentric hyponym exists, it will extract either (i) all the terms in bold font if the LOL page is bulleted or (ii) all terms in the first column if t"
S16-1203,S15-2155,1,0.874214,"raction tasks (Saxe et al., 2013; Fu et al., 2014; Tan et al., 2015). for information retrieval, Jones (1979) observed that compound nouns often follow the head-modifier principle where the meaning of the term can be conveyed by part(s) of the compound. Approaching endocentricity from a different angle, Nichols et al. (2005) identified the semantic head(s) of a term as its hypernym using the lowest scoping element of the Robust Minimal Recursion Semantics (RMRS) (Copestake et al., 2005) structure of the dictionary definition of the term. In the first TExEval task in SemEval-2015, both Lefever (2015) and Tan et al. (2015)1 independently developed string-based systems that exploit the endocentric nature of hyponyms. In our submission to the TExEval-2 task (Bordea et al., 2016), we seek to answer the question of exactly “how many hyponyms within a taxonomy are endocentric?”. Additionally, we exploit the endocentric nature of the hyponyms to extend the taxonomy by crawling and cleaning Wikipedia’s List of Lists of Lists.2 Often these lists of terms are found in Wikipedia marked up as tables or in bullet forms. 3 4 Endocentricity Early research in theoretical linguistics discussed the idea of"
S16-1203,D14-1088,0,0.0323931,"Missing"
S16-1203,I05-7011,0,0.0892162,"Missing"
S16-1203,J13-3007,0,0.136173,"aditional broad-coverage taxonomies such as CYC (Lenat, 1995), SUMO (Pease et al., 2002; Miller, 1995), YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) have been manually created or curated with much effort. With the rapid technological evolution, it is more feasible to construct a domain-specific taxonomy that caters to domain or company specific terminology (Lefever, 2015). This motivated the move towards unsupervised approaches to taxonomy extraction (Berland and Charniak, 1999; Lin and Pantel, 2001; Snow et al., 2006) and specifically focused towards particular domains (Velardi et al., 2013; Bordea et al., 2015). The aim of the Taxonomy Extraction Evaluation (TExEval) task is to automatically find lexical relations between pairs of terms within several specified domains. Previously, we have developed a hypernym extraction system using word embeddings by exploiting the frequent occurrence of the ‘X is a Y’ pattern in encyclopedic text (Tan et al., 2015). We have achieved competitive results in SemEval-2015 and as a follow up to our study, we would like to explore the endocentric nature of hyponyms that contributed substantially to the system performance in the previous TExEval ta"
S16-1203,P99-1016,0,\N,Missing
S16-1203,P99-1008,0,\N,Missing
S16-1203,C98-2122,0,\N,Missing
schluter-van-genabith-2008-treebank,C04-1024,0,\N,Missing
schluter-van-genabith-2008-treebank,hockenmaier-steedman-2002-acquiring,0,\N,Missing
schluter-van-genabith-2008-treebank,J05-3003,1,\N,Missing
schluter-van-genabith-2008-treebank,P04-1041,1,\N,Missing
schluter-van-genabith-2008-treebank,P06-1064,0,\N,Missing
schluter-van-genabith-2008-treebank,P05-2024,0,\N,Missing
schluter-van-genabith-2008-treebank,P06-2018,1,\N,Missing
shaalan-etal-2012-arabic,J96-1003,0,\N,Missing
shaalan-etal-2012-arabic,C90-2036,0,\N,Missing
shaalan-etal-2012-arabic,P00-1037,0,\N,Missing
shaalan-etal-2012-arabic,W11-4417,1,\N,Missing
shaalan-etal-2012-arabic,I08-2131,0,\N,Missing
shaalan-etal-2012-arabic,W06-1648,0,\N,Missing
sulea-etal-2017-predicting,J13-4005,0,\N,Missing
sulea-etal-2017-predicting,E14-4004,1,\N,Missing
sulea-etal-2017-predicting,W12-0411,0,\N,Missing
sulea-etal-2017-predicting,W16-0314,1,\N,Missing
sulea-etal-2017-predicting,W04-1006,0,\N,Missing
sulea-etal-2017-predicting,W12-0515,0,\N,Missing
tounsi-van-genabith-2010-arabic,N07-1051,0,\N,Missing
tounsi-van-genabith-2010-arabic,J98-4004,0,\N,Missing
tounsi-van-genabith-2010-arabic,D07-1116,0,\N,Missing
tounsi-van-genabith-2010-arabic,J04-4004,0,\N,Missing
tounsi-van-genabith-2010-arabic,W04-1602,0,\N,Missing
tounsi-van-genabith-2010-arabic,J08-1003,1,\N,Missing
tounsi-van-genabith-2010-arabic,W09-0806,1,\N,Missing
tounsi-van-genabith-2010-arabic,P03-1054,0,\N,Missing
tounsi-van-genabith-2010-arabic,P06-1130,1,\N,Missing
tounsi-van-genabith-2010-arabic,P04-1041,1,\N,Missing
tounsi-van-genabith-2010-arabic,E03-1014,0,\N,Missing
ui-dhonnchadha-van-genabith-2010-partial,ui-dhonnchadha-van-genabith-2006-part,0,\N,Missing
ui-dhonnchadha-van-genabith-2010-partial,A97-1011,0,\N,Missing
W04-1711,ui-dhonnchadha-2002-two,1,\N,Missing
W06-3112,W05-0909,0,0.0735864,"data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb class databases, and extensive text preparation: stemming, tagging, etc. The advantage of our method is that it produc"
W06-3112,P05-1074,0,0.125146,"Missing"
W06-3112,P02-1033,0,0.0803152,"Missing"
W06-3112,N03-1017,0,0.0209544,"Missing"
W06-3112,koen-2004-pharaoh,0,0.0138884,"s the resources available. Therefore, it would be desirable to find a way to automatically generate legitimate translation alternatives not present in the reference(s) already available. 86 Proceedings of the Workshop on Statistical Machine Translation, pages 86–93, c New York City, June 2006. 2006 Association for Computational Linguistics In this paper, we present a novel method that automatically derives paraphrases using only the source and reference texts involved in for the evaluation of French-to-English Europarl translations produced by two MT systems: statistical phrase-based Pharaoh (Koehn, 2004) and rulebased Logomedia.1 In using what is in fact a miniature bilingual corpus our approach differs from the mainstream paraphrase generation based on monolingual resources. We show that paraphrases produced in this way are more relevant to the task of evaluating machine translation than the use of external lexical knowledge resources like thesauri or WordNet2, in that our paraphrases contain both lexical equivalents and low-level syntactic variants, and in that, as a side-effect, evaluation bitextderived paraphrasing naturally yields domainspecific paraphrases. The paraphrases generated fro"
W06-3112,2005.mtsummit-papers.11,0,0.0709031,"the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. On the other hand, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections of parallel corpora like Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, often exceeds the resources available. Therefore, it would be desirable to find a way to automatically generate legitimate translation alternatives not present in the reference(s) already available. 86 Proceedings of the Workshop on Statistical Machine Translation, pages 86–93, c New York City, June 2006. 2006 Association for Computational Linguistics In this paper, we present a novel method that automatically derives pa"
W06-3112,E06-1031,0,0.0320273,"tion is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb clas"
W06-3112,N03-1024,0,0.0729959,"admire the reply mrs parly gave this morning however we have turned a blind eye to that Paraphrase 5: i admire the answer mrs parly gave this morning however we have turned a blind eye to it Paraphrase 6: i admire the reply mrs parly gave this morning but we have turned a blind eye to it This can potentially prevent higher n-grams being successfully matched if two or more equivalent expressions find themselves within the range of ngrams being tested by BLEU and NIST. To avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using a lattice, much like in (Pang et al., 2003). 4 Results As expected, the use of multiple references produced by our method raises both the BLEU and NIST scores for translations produced by Pharaoh (test set PH) and Logomedia (test set LM). The results are presented in Table 1. PH single ref PH multi ref LM single ref LM multi ref BLEU 0.2131 0.2407 0.1782 0.2043 NIST 6.1625 7.0068 5.5406 6.3834 The hypothesis that the multiple-reference scores reflect better human judgment is also confirmed. For 100-sentence subsets (Subset PH and Subset LM) randomly extracted from our test sets PH and LM, we calculated Pearson’s correlation between the"
W06-3112,P02-1040,0,0.113715,"during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software. Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation. The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system. 1 Introduction Since their appearance, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating the quality of machine translation. They both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided. Such automatic measures are indispensable in the development of machine translation systems, because they allow the developers to conduct frequent, cost-effective, and fast evaluations of their evolving models. These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translat"
W06-3112,C04-1031,0,0.038569,"source language phrase fpi are often paraphrases of each other. For example, in our experiment, for the French word question the most probable automatically aligned English translations are question, matter, and issue, which in English are practically synonyms. Section 3.2 presents more examples of such equivalent expressions. 3.1 We used the GIZA++ word alignment software3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file, and the refined word alignment strategy of (Och and Ney, 2003; Koehn et al., 2003; Tiedemann, 2004) to obtain improved word and phrase alignments. For each source word or phrase fi that is aligned with more than one target words or phrases, its possible translations ei1, ..., ein were placed in a list as equivalent expressions (i.e. synonyms, near-synonyms, or paraphrases of each other). A few examples are given in (1). (1) agreement - accordance adopted - implemented matter - lot - case funds - money arms - weapons area - aspect question – issue – matter we would expect - we certainly expect bear on - are centred around Experimental design For our experiment, we used two test sets, each co"
W06-3112,2003.mtsummit-papers.51,0,0.109197,"noted by (Och et al., 2003) and (Russo-Lassner et al., 2005). A side effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests t"
W06-3112,2004.tmi-1.9,0,0.101216,"ny divergence from them. In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical choices and syntactic structure it contains, even though perfectly legitimate, are not present in at least one of the references. Necessarily, this score would not reflect a much more favourable human judgment that such a translation would receive. The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in the BLEU- or NIST-based evaluation in the first place. While (Zhang and Vogel, 2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. On the other hand, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections"
W06-3112,E06-1032,0,\N,Missing
W06-3112,J03-1002,0,\N,Missing
W07-0411,N06-1058,0,0.264045,"zation and provide a “normalized” representation of (some) syntactic variants of a given sentence. The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al., 2004), which produces a set of dependency triples for each input. The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organ"
W07-0411,W05-0909,0,0.10468,"ion, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortium’s (LDC) Multiple Translation project; Section 5 discusses ongoing work; Section 6 conclu"
W07-0411,P04-1041,1,0.683222,"Missing"
W07-0411,koen-2004-pharaoh,0,0.0155325,"Missing"
W07-0411,2005.mtsummit-papers.11,0,0.0926597,"ces for a candidate translation in BLEU- or NIST-based evaluations. While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. In addition, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive. Therefore, it would be desirable to find an evaluation method that accepts legitimate syntactic and lexical differences 80 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics between the translation and the reference, thus better mirroring human assessmen"
W07-0411,2004.tmi-1.8,0,0.241948,"Missing"
W07-0411,E06-1031,0,0.0544376,"lopment of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2005), which computes the number of substitutions, inserts, deletions, and shifts necessary to transform the translation text to match the reference. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet5 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model dev"
W07-0411,J03-1002,0,0.00159553,"U scored Pharaoh 0.0349 points higher, NIST scored Pharaoh 0.6219 points higher, but human judges scored Logomedia output 0.19 points higher (on a 5-point scale). 4.1.1 Experimental design In order to check for the existence of a bias in the dependency-based metric, we created a set of 4,000 sentences drawn randomly from the SpanishEnglish subset of Europarl (Koehn, 2005), and we produced two translations: one by a rule-based system Logomedia, and the other by the standard phrase-based statistical decoder Pharaoh, using alignments produced by GIZA++8 and the refined word alignment strategy of Och and Ney (2003). The translations were scored with a range of metrics: BLEU, NIST, GTM, TER, METEOR, and the dependency-based method. 4.1.2 Adding synonyms Besides the ability to allow syntactic variants as valid translations, a good metric should also be able to accept legitimate lexical variation. We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al. (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). Bitext-derived paraphrases Owczar"
W07-0411,P02-1040,0,0.117147,"the translation. In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation. 1 Introduction Since their appearance, string-based evaluation metrics such as BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating MT quality. Both score a candidate translation on the basis of the number of n-grams shared with one or more reference translations. Automatic measures are indispensable in the development of MT systems, because they allow MT developers to conduct frequent, costeffective, and fast evaluations of their evolving models. These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any dive"
W07-0411,2006.amta-papers.25,0,0.0752774,"es is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortium’s (LDC) Multiple Translation project; Section 5"
W07-0411,2003.mtsummit-papers.51,0,0.639646,"mpared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consort"
W07-0411,2004.tmi-1.9,0,0.058185,"nce strings, and will penalize any divergence from them. In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references. Necessarily, this score would differ from a much more favourable human judgement that such a translation would receive. The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in BLEU- or NIST-based evaluations. While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. In addition, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creatin"
W07-0411,E06-1032,0,\N,Missing
W07-0411,W06-3112,1,\N,Missing
W07-0714,W05-0909,0,0.110351,"nd the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. terms of functional a"
W07-0714,P04-1041,1,0.860716,"Missing"
W07-0714,N06-1058,0,0.342387,"a treebank-based, probabilistic LFG parser (Cahill et al., 2004), which produces a set of dependency triples for each input. The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111, c Prague, June 2007. 2007 Association for Computational Linguistics precision, recall, and f-score for each particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining de"
W07-0714,koen-2004-pharaoh,0,0.0345475,"Missing"
W07-0714,2005.mtsummit-papers.11,0,0.017666,"Missing"
W07-0714,2004.tmi-1.8,0,0.0233568,"Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation’s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 3 A demo of the parser can be found at http"
W07-0714,E06-1031,0,0.0428548,"ment of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model dev"
W07-0714,W05-0904,0,0.794276,"nces between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference. Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a “normalized” representation of (some) syntactic variants of a given sentence. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency stru"
W07-0714,J03-1002,0,0.00266914,"Missing"
W07-0714,P02-1040,0,0.111228,"e present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 1 Introduction Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention. Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006). As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the tra"
W07-0714,2006.amta-papers.25,0,0.245971,"fferences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Se"
W07-0714,2003.mtsummit-papers.51,0,0.380941,"on, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Tr"
W07-0714,2004.tmi-1.9,0,0.0676059,"Missing"
W07-0714,E06-1032,0,\N,Missing
W07-0714,J03-4003,0,\N,Missing
W07-0714,W06-3112,1,\N,Missing
W07-2204,P05-1022,0,0.224574,"Missing"
W07-2204,A00-2018,0,0.444658,"Missing"
W07-2204,J03-4003,0,0.0705179,"Missing"
W07-2204,W01-0521,0,0.468993,"Missing"
W07-2204,J93-2004,0,0.0282093,"Missing"
W07-2204,N06-1020,0,0.203344,"Missing"
W07-2204,P06-1043,0,0.355074,"Missing"
W07-2204,E03-1008,0,0.14938,"Missing"
W07-2460,brants-hansen-2002-developments,0,0.0749195,"Missing"
W07-2460,W06-1614,0,0.083109,"Missing"
W07-2460,P06-3004,0,0.039055,"Missing"
W07-2460,C04-1024,0,0.0423635,"al., 2004) than for a parser trained on the NEGRA treebank (Skut et al., 1997). Maier (2006) takes that as evidence that the NEGRA annotation scheme is less adequate for PCFG parsing, while a parser trained on the T¨uBa-D/Z yields PARSEVAL results in the same range as a parser trained on the English Penn-II treebank (K¨ubler et al., 2006). These results are based on the assumption that PARSEVAL is an appropriate measure for comparing parser performance of a PCFG parser trained on treebanks with different annotation schemes. This paper presents parsing experiments with the PCFG parser BitPar (Schmid, 2004) trained on two German treebanks. The treebanks contain text from the same domain, namely two German daily newspapers, but differ considerably with regard to their annotation schemes. We score parsing results using three different evaluation measures and show that the PARSEVAL results do not correlate with the results of the other metrics. An analysis of specific error types shows the differences between the three measures. Our results indicate that dependencybased evaluation is most appropriate to compare parser output for parsers trained on different treebank annotation schemes. Section 2 de"
W07-2460,A97-1014,0,0.754871,"Missing"
W08-1112,A00-2018,0,0.226682,"(ni ), and (↑) refers to the Lexical Functional Grammar Lexical Functional Grammar (Kaplan and Bresnan, 1982) is a constraint-based grammar formalism which postulates (minimally) two levels of rep87 Model PCFG HB-PCFG LEX-PCFG Grammar Rule VP[↑=↓] → VV[↑=↓] NP[↑OBJ=↓] VP[↑=↓] → VV[↑=↓] NP[↑OBJ=↓] VP( )[↑=↓] → VV( )[↑=↓] NP( ¬ ¬ on)[↑OBJ=↓] Conditions VP[↑=↓], {PRED, SUBJ, OBJ} VP[↑=↓], {PRED, SUBJ, OBJ}, TOP VP( )[↑=↓], {PRED, SUBJ, OBJ} ¬ Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. f-structure associated with the mother (M ) node of ni , i.e. φ(M (ni )). 2.2 Generation from f-Structures The generation task in LFG is to determine which sentences correspond to a specified f-structure, given a particular grammar, such as (1). Kaplan and Wedekind (2000) proved that the set of strings generated by an LFG grammar"
W08-1112,P97-1003,0,0.0679979,"ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function φ that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995). As illustrated in Figure 1, given a c-structure node ni , the corresponding f-structure component fj is φ(ni ). Admissible c-structures are specified by a context-free grammar. The corresponding f-structures are derived from functional annotations attached to the CFG rewriting rules. of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based on wide-coverage LFG grammars auto"
W08-1112,D07-1028,1,0.867227,"Missing"
W08-1112,J98-4004,0,0.25336,"(GFs) such as SUBJ(ect), OBJ(ect), ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function φ that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995). As illustrated in Figure 1, given a c-structure node ni , the corresponding f-structure component fj is φ(ni ). Admissible c-structures are specified by a context-free grammar. The corresponding f-structures are derived from functional annotations attached to the CFG rewriting rules. of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based"
W08-1112,C00-1062,0,0.0343699,"models phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. f-structure associated with the mother (M ) node of ni , i.e. φ(M (ni )). 2.2 Generation from f-Structures The generation task in LFG is to determine which sentences correspond to a specified f-structure, given a particular grammar, such as (1). Kaplan and Wedekind (2000) proved that the set of strings generated by an LFG grammar from fully specified f-structures is a context-free language. Based on this theoretical cornerstone, Cahill and van Genabith (2006) presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank. The LFG-based statistical generation model defines the conditional probability P (T |F ), for each candidate functionally annotated c-structure tree T (which fully specifies a surface realisation) given an fstructure F . The generation model searches for the Tbest that maximises"
W08-1112,P96-1027,0,0.0616902,"wn in the last line (LEX-PCFG) of Table 1. As in CKY chart parsing, generation grammars are binarised in our chart generator. Thus all grammar rules are either unary of the form X → H or binary X → Y H (or X → HY ), where H is the head constituent and Y is the modifier. To handle the problem of sparse data while estimating rule probabilities, a back-off to baseline model is employed. As, from a linguistic perspective, it is the modifier 4 Chart Generation and Smoothing Algorithms 4.1 Chart Generation Algorithm The PCFG-based generation algorithms are implemented in terms of a chart generator (Kay, 1996). In the generation algorithm, each (sub-)f-structure indexes a (sub-)chart. Each local chart generates the most probable trees for the local f-structure in a bottom-up manner: • generating lexical edges from the the local GF PRED and some atomic features representing function words, mood or aspect etc. 2 3 We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the treebank data. Except for prepositional phrases, localiser and some verbal phrases. 89 m NP( ) NN NR NN NN [↓∈↑ADJUNCT] [↓∈↑ADJUNCT] [↓∈↑ADJUNCT] þ° ¥ Shanghai  tennis m m masters cup ("
W08-1112,A00-2023,0,0.0909096,"Missing"
W08-1112,W02-2103,0,0.0254319,"such as [↑SUBJ =↓], [↑=↓] etc. As a heuristic based on linguistic experience, we define the order of importance of these elements as follows: Type PCFG HB-PCFG LEX-PCFG X(c) &gt; H(c) &gt; Y (a) &gt; Y (c) &gt; X(a) &gt; H(a) (4) IP[↑COMP=↓] → NP[↑SUBJ=↓] VP[↑=↓] For the above example rule (4), the importance of the elements is: with features 22,372 28,487 325,094 without features 8,548 11,969 286,468 Table 3: Number of rules in the training set IP &gt; VP &gt; [↑SUBJ=↓] &gt; NP &gt; [↑COMP=↓] &gt; [↑=↓] The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. The elements can be deleted from the rules in an importance order from low to high.5 The partial rules adopted in our system ignore the least important 3 elements, viz. the functional annotation of the head node H(a), the functional annotation on LHS X(a) and constituent category of the modifier node Y"
W08-1112,C00-1007,0,0.0965467,"Missing"
W08-1112,P06-1130,1,0.842207,"Missing"
W08-1112,W07-2303,0,0.0271024,"Missing"
W08-1112,W05-1510,0,0.0346457,"Missing"
W08-1112,P02-1040,0,0.0762368,"fine the order of importance of these elements as follows: Type PCFG HB-PCFG LEX-PCFG X(c) &gt; H(c) &gt; Y (a) &gt; Y (c) &gt; X(a) &gt; H(a) (4) IP[↑COMP=↓] → NP[↑SUBJ=↓] VP[↑=↓] For the above example rule (4), the importance of the elements is: with features 22,372 28,487 325,094 without features 8,548 11,969 286,468 Table 3: Number of rules in the training set IP &gt; VP &gt; [↑SUBJ=↓] &gt; NP &gt; [↑COMP=↓] &gt; [↑=↓] The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. The elements can be deleted from the rules in an importance order from low to high.5 The partial rules adopted in our system ignore the least important 3 elements, viz. the functional annotation of the head node H(a), the functional annotation on LHS X(a) and constituent category of the modifier node Y (c). Examples of the two types of smoothed rules are shown in Table 2. Table 4 reports th"
W08-1112,2005.mtsummit-papers.15,0,0.0421581,"Missing"
W08-1112,2007.mtsummit-ucnlg.4,0,0.0346131,"Missing"
W08-1112,J96-2001,0,\N,Missing
W08-1122,P06-1130,1,0.892695,"Missing"
W08-1122,P04-1041,1,0.894093,"Missing"
W08-1122,W07-2204,1,0.869992,"Missing"
W08-1122,W01-0521,0,0.0306942,"ecific training data is automatically produced using state-of-the-art parser output. The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data. 1 Introduction Grammars extracted from the Wall Street Journal (WSJ) section of the Penn Treebank have been successfully applied to natural language parsing, and more recently, to natural language generation. It is clear that high-quality grammars can be extracted for the WSJ domain but it is not so clear how these grammars scale to other text genres. Gildea (2001), for example, has shown that WSJ-trained parsers suffer a drop in performance when applied to the more varied sentences of the Brown Corpus. We investigate the effect of domain variation in treebank-grammar-based generation by applying a WSJ-trained generator to sentences from the British National Corpus (BNC). As with probabilistic parsing, probabilistic generation aims to produce the most likely output(s) given In an initial evaluation, we apply our probabilistic WSJ-trained generator to BNC material, and show that the generator suffers a substantial performance degradation, with a drop in"
W08-1122,D07-1028,1,0.874936,"Missing"
W08-1122,A00-2023,0,0.0896334,"Missing"
W08-1122,N06-1020,0,0.0158375,"n our BNC test set. The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing. Some success has been achieved by training a parser, not on gold standard hand-corrected trees, but on parser output trees. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario (McClosky et al., 2006). We tackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees. This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61. The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced). 2 Background and a BLEU score of 0.7733. White et al. (2007) describe a CCG-based realisation system which has been trained on logical forms derived from CCGBank (H"
W08-1122,W05-1510,0,0.0468979,"Missing"
W08-1122,E03-1008,0,0.0275757,"ention to the problem of adapting the generator so that it can more accurately generate the 1,000 sentences in our BNC test set. The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing. Some success has been achieved by training a parser, not on gold standard hand-corrected trees, but on parser output trees. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario (McClosky et al., 2006). We tackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees. This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61. The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced). 2 Background and a BLEU score of 0.7733. White et al. ("
W08-1122,2005.mtsummit-papers.15,0,0.0313248,"Language Technology School of Computing Dublin City University Ireland {dhogan, jfoster, jwagner, josef}@computing.dcu.ie Abstract the input. We can distinguish three types of probabilistic generators, based on the type of probability model used to select the most likely sentence. The first type uses an n-gram language model, e.g. (Langkilde, 2000), the second type uses a probability model defined over trees or feature-structureannotated trees, e.g. (Cahill and van Genabith, 2006), and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g. (Velldal and Oepen, 2005). The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations (Cahill and van Genabith, 2006; Hogan et al., 2007). While the effect of domain variation on Penntreebank-trained probabilistic parsers has been investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator. We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLE"
W08-1122,2007.mtsummit-ucnlg.4,0,0.046008,"Missing"
W08-1122,P05-1022,0,\N,Missing
W09-0806,Y04-1016,1,0.855487,"Missing"
W09-0806,P04-1041,1,0.939098,"Missing"
W09-0806,H94-1020,0,0.182035,"Missing"
W09-0806,P06-1130,1,0.923805,"Missing"
W09-0806,J08-1003,1,0.88981,"Missing"
W09-0806,P02-1035,0,0.0371649,"Missing"
W09-0806,schluter-van-genabith-2008-treebank,1,0.839624,"Missing"
W09-0806,N04-1013,0,\N,Missing
W09-0806,2006.bcs-1.10,0,\N,Missing
W09-0806,W04-1602,0,\N,Missing
W09-0806,P04-1047,1,\N,Missing
W09-3823,W02-1503,0,0.0308938,"Missing"
W09-3823,W02-1506,0,0.0407156,"Missing"
W09-3823,2005.mtsummit-papers.11,0,0.0122338,"Missing"
W09-3823,P02-1040,0,0.0798233,"Missing"
W09-3823,P02-1035,0,\N,Missing
W09-4624,P98-1106,0,0.235539,"dependencies in phrase structures, which should, by definition, be impossible. This is because the only types of non-projective dependencies theoretically represented in LFG are actually pseudo-projectivities. 3.2.2 Non-Projectivity and Pseudo-Projectivity Dependency trees also model non-projective structures that have no correspondence with any constituent trees—that is, they may be non-projective. This added “increase” in power for dependency grammars is shown to be useful for syntactic representations of certain languages (for example, the cross-serial dependencies of Dutch). However, as (Kahane et al., 1998) explain, pseudo-projective dependency trees may be parsed as projective trees with the aid of a simple transformation. Consider two non-projective labeled dependency trees, T1 = (V, E1 , L1 ) and T2 = (V, E2 , L2 ). T2 is called a lift if one of the following conditions hold, for some e = (a, l, b), e0 = (b, l0 , c) ∈ E1 .6 1. E2 = (E1 − {e, e0 }) ∪ {(a, l : l0 , c)}, L2 ⊆ L1 ∪ {l : l0 }, or 2. T3 is a lift of T1 and T2 is a lift of T3 . 6 This definition is equivalent to the one given in (Kahane et al., 1998), where a lift was defined as in terms of governance for unlabeled dependency trees."
W09-4624,P05-1012,0,0.0133333,"ead. 5 Dependency Parsing Results The parsing architecture works as follows. The annotation algorithm is applied to MFT trees, creating f-structure annotated trees that are then transformed into the projective depenendency representation described in Section 4, using the c-structure with the (only) f-structure equations. A dependency parser is then trained on this data, and the test set parsed. The parser output is then transformed back to f-structure equations, which are evaluated against the f-structure gold standard. Two different dependency parsers were used for this research: MST parser (McDonald et al., 2005) and MALT parser (Nivre et al., 2006). Experiments were done with the simplified architecture (in which long-distance dependencies are given as complex path equations in training), and in the established architecture (with a separated longdistance dependency resolution task).9 The results are given in Tables 2 and 3. Parser MST MALT coord dist no yes no yes precision 87.46 87.45 86.23 86.17 recall 54.67 54.66 52.17 51.95 f-score 67.28 67.27 65.01 64.82 obj  coord adjunct elem coord elem coord ` a court et Table 2: Simplified Architecture Parsing Results Parser moyen terme Figure 5: Dependenc"
W09-4624,P06-1033,0,0.0247622,"rt our LFG representation of coordination (Section 4.1). Finally, f-structures may be specified in terms of annotated c-structures with the local metavariables ↑ and ↓, and grammatical function regular paths. This restricts the structure of dependencies actually occurring in LFG f-structure syntax, as we will show in Section 3.2. 3.2 The Breadth of Functional Equations in LFG LFG’s f-structures often have re-entrancies (or shared sub-f-structures)—two functional equations resolve to take the same (f-structure) value—making them dags, rather than simple de5 Grouping may be indicated on labels (Nilsson et al., 2006). pendency trees. In LFG, the term functional uncertainty describes the uncertainty in the resolution given a simple grammatical function, in the definition of the grammar. The set of options for resolution may be finite and given by a disjunction, in which case resolution is down a chain of f-structure nodes of bounded length, or (theoretically) infinite in which case they are given by a regular expression (including the Kleene star operator) and resolution is down a chain of f-structure nodes of unbounded length. We note, however, that in statistical parsing of f-structures, the functional u"
W09-4624,nivre-etal-2006-maltparser,0,0.0725774,"rsing architecture works as follows. The annotation algorithm is applied to MFT trees, creating f-structure annotated trees that are then transformed into the projective depenendency representation described in Section 4, using the c-structure with the (only) f-structure equations. A dependency parser is then trained on this data, and the test set parsed. The parser output is then transformed back to f-structure equations, which are evaluated against the f-structure gold standard. Two different dependency parsers were used for this research: MST parser (McDonald et al., 2005) and MALT parser (Nivre et al., 2006). Experiments were done with the simplified architecture (in which long-distance dependencies are given as complex path equations in training), and in the established architecture (with a separated longdistance dependency resolution task).9 The results are given in Tables 2 and 3. Parser MST MALT coord dist no yes no yes precision 87.46 87.45 86.23 86.17 recall 54.67 54.66 52.17 51.95 f-score 67.28 67.27 65.01 64.82 obj  coord adjunct elem coord elem coord ` a court et Table 2: Simplified Architecture Parsing Results Parser moyen terme Figure 5: Dependency graph for a court et moyen terme (‘"
W09-4624,W07-2416,0,0.111031,"tional Grammar for which there is a one-to-one correspondence between predicates and original word-forms, these extended definitions may easily be applied. However, LFG’s treatment of long-distance dependency resolution and of subject/object raising is non-projective, illustrate non-projective dags. For French, for example, an interesting nonprojective structure is found in en pronouns and NP extraction. Projectivity in dependency trees or dags is obviously a result of the definition of the generating dependency grammar. This is true also of cases that are not LFG re-entrancies. For example, (Johansson and Nugues, 2007) propose a conversion of the Penn Treebank into dependency trees that introduces more projective edges than the conversion proposed by (Yamada and Matsumoto, 2003; Nivre, 2006). In addition to long-distance dependencies, for example, their representation of gapping always introduces non-projective branches (Johansson and Nugues, 2007). LFG is capable of locally representing nonprojective dependencies in phrase structures, which should, by definition, be impossible. This is because the only types of non-projective dependencies theoretically represented in LFG are actually pseudo-projectivities."
W09-4624,schluter-van-genabith-2008-treebank,1,0.686477,"Missing"
W09-4624,W04-2706,0,0.0741245,"Missing"
W09-4624,W03-3023,0,0.0195635,", LFG’s treatment of long-distance dependency resolution and of subject/object raising is non-projective, illustrate non-projective dags. For French, for example, an interesting nonprojective structure is found in en pronouns and NP extraction. Projectivity in dependency trees or dags is obviously a result of the definition of the generating dependency grammar. This is true also of cases that are not LFG re-entrancies. For example, (Johansson and Nugues, 2007) propose a conversion of the Penn Treebank into dependency trees that introduces more projective edges than the conversion proposed by (Yamada and Matsumoto, 2003; Nivre, 2006). In addition to long-distance dependencies, for example, their representation of gapping always introduces non-projective branches (Johansson and Nugues, 2007). LFG is capable of locally representing nonprojective dependencies in phrase structures, which should, by definition, be impossible. This is because the only types of non-projective dependencies theoretically represented in LFG are actually pseudo-projectivities. 3.2.2 Non-Projectivity and Pseudo-Projectivity Dependency trees also model non-projective structures that have no correspondence with any constituent trees—that"
W09-4624,C98-1102,0,\N,Missing
W10-1408,P05-1038,0,0.0366439,"the only edge which is added to the chart at this position is the one corresponding to the rule V BD → UNK-ed. For our English experiments we use the unknown word classes (or signatures) which are used in the Berkeley parser. A signature indicates whether a words contains a digit or a hyphen, if a word starts with a capital letter or ends with one of the following English suffixes (both derivational and inflectional): -s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al. For our French experiments we employ the same signature list as Crabb´e and Candito (2008), which itself was adapted from Arun and Keller (2005). This list consists of (a) conjugation suffixes of reguIn order to use morphological clues for Arabic we go further than just looking at suffixes. We exploit all the richness of the morphology of this language which can be expressed through morphotactics. these indicators are prefixes, suffixes and word templates. A template (Beesley and Karttunen, 2003) is a kind of vocalization mould in which a word fits. In derivational morphology Arabic words are formed through the amalgamation of two tiers, namely, root and template. A root is a sequence of three (rarely two or four) consonants which are"
W10-1408,W98-1007,0,0.0593928,"signature information for French and English is shown in Table 3. Beside each f-score the absolute improvement over the UNKNOWN baseline (Table 2) is given. For both languages there is an improvement at all unknown thresholds. The improvement for English is statistically significant at unknown thresholds 1 and 10.4 The improvement is more marked for French and is statistically significant at all levels. In the next section, we experiment with signature lists for Arabic.5 6 Arabic Signatures Handling Arabic Morphotactics Morphotactics refers to the way morphemes combine together to form words (Beesley, 1998; Beesley and Karttunen, 2003). Generally speaking, morphotactics can be concatenative, with morphemes either prefixed or suffixed to stems, or non-concatenative, with stems undergoing internal alternations to convey morphosyntactic information. Arabic is considered a typical example of a language that employs non-concatenative morphotactics. Arabic words are traditionally classified into three types: verbs, nouns and particles. Adjectives take almost all the morphological forms of, and share the same templatic structures with, nouns. Adjectives, for example, can be definite, and are inflected"
W10-1408,W09-3821,0,0.223368,"Missing"
W10-1408,W09-1008,0,0.0505203,"Missing"
W10-1408,A00-2018,0,0.244617,"61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from English morphology, we can do just that, resulting in the"
W10-1408,2008.jeptalnrecital-long.17,0,0.145906,"Missing"
W10-1408,E09-1038,0,0.160992,"Missing"
W10-1408,P08-2015,0,0.0114925,"Missing"
W10-1408,D09-1087,0,0.13706,"Missing"
W10-1408,J98-4004,0,0.381711,"hnique is extended to include morphological information and present parsing results for English and French. In Section 6, we describe the Arabic morphological system and explain how we used heuristic rules to cluster words into word-classes or signatures. We present parsing results for the version of the parser which uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations a"
W10-1408,W04-1602,0,0.0128084,"Missing"
W10-1408,H94-1020,0,0.166217,"Missing"
W10-1408,P05-1010,0,0.1217,"uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. E"
W10-1408,W05-0711,0,0.0499235,"Missing"
W10-1408,N07-1051,0,0.0697355,"each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the above procedure and parses using the max1 Estimation of the parameters is performed by running Expectation/Maximisation on the training corpus. 68 rule parsing algorithm (Petrov et al., 2006; Petrov and Klein, 2007). PCFG-LA parsing is relatively language-independent but has been shown to be very effective on several languages (Petrov, 2009). For our experiments, we set the number of iterations to be 5 and we test on sentences less than or equal to 40 words in length. All our experiments, apart from the final one, are carried out on the development sets of our three languages. 3 The Datasets Arabic We use the the Penn Arabic Treebank (ATB) (Bies and Maamouri, 2003; Maamouri and Bies., 2004). The ATB describes written Modern Standard Arabic newswire and follows the style and guidelines of the English Penn"
W10-1408,P06-1055,0,0.487124,"tent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the"
W10-1408,I05-3005,0,0.0234663,"URE UNTagging Accuracy 94.03 91.16 89.06 95.60 94.66 93.61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from Engli"
W10-1408,P03-1054,0,\N,Missing
W10-1410,P04-1041,1,0.868231,"Missing"
W10-1410,W09-3821,1,0.89873,"Missing"
W10-1410,W09-1008,1,0.89151,"Missing"
W10-1410,A00-2018,0,0.735055,"rrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original F TB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (B KY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the F TB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the B KY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the B KY parser trained on the F TB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to perform better. This was shown in (Candito and Crabbé, 2009) where distributional word clusters were acquired from a 125 mill"
W10-1410,chrupala-etal-2008-learning,1,0.815209,"Missing"
W10-1410,Y09-1013,0,0.14934,"Missing"
W10-1410,C94-2149,0,0.129478,"Missing"
W10-1410,E09-1038,0,0.139265,"Missing"
W10-1410,P06-1055,0,0.345326,"Missing"
W10-1410,D07-1066,1,0.886037,"Missing"
W10-1410,sagot-etal-2006-lefff,0,0.0720109,"Missing"
W10-1410,W09-3820,0,0.130258,"Missing"
W10-1410,J93-2004,0,\N,Missing
W10-1410,J03-4003,0,\N,Missing
W10-1753,E06-1032,0,0.12591,"ncy-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as B LEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorpor"
W10-1753,W05-0904,0,0.135213,"ation Localisation School of Computing Dublin City University Dublin 9, Ireland {yhe,jdu,away,josef}@computing.dcu.ie Abstract (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:(Kaplan and Bresnan, 1982)) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, Owczarzak et al. (2007)’s method considerably outperforms Liu and Gildea’s (2005) w.r.t. corre"
W10-1753,nivre-etal-2006-maltparser,0,0.159655,"nslation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of M ETEOR to penalize discontinuous matches. We sort the matches according to the match level and the dependency type, and weight the matches to maximize correlation with human judgement. The remainder of the paper is organized as follows. Section 2 reviews the dependency-based metric. Sections 3, 4, 5 and 6 introduce our improvements on this me"
W10-1753,W07-0714,1,0.869034,"Missing"
W10-1753,P09-1034,0,0.275084,"abels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and (Owczarzak et al., 2007). E DPM also incorporates more information sources: e.g. the parser confidence, the Porter stemmer, WordNet synonyms and paraphrases. Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (Pado et al., 2009). In this paper we extend the work of (Owczarzak et al., 2007) in a different manner: we use an We describe DCU’s LFG dependencybased metric submitted to the shared evaluation task of WMT-MetricsMATR 2010. The metric is built on the LFG F-structurebased approach presented in (Owczarzak et al., 2007). We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penal"
W10-1753,P02-1040,0,0.0808056,"the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of M ETEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement. Experiments show that these enhancements improve the dependency-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as B LEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and lingui"
W10-1753,W05-0909,0,0.365379,"anslation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of M ETEOR to penalize discontinuous matches. We sort the matc"
W10-1753,P04-1041,1,0.877558,"Missing"
W10-3704,attia-etal-2010-automatically,1,0.604545,"Missing"
W10-3704,deksne-etal-2008-dictionary,0,0.29076,"een applied to bigrams and trigrams, and it becomes more problematic to extract MWEs of more than three words. As a consequence, each approach requires specific resources and is suitable for dealing with only one side of a multifaceted problem. Pecina (2010) evaluates 82 lexical association measures for the ranking of collocation candidates and concludes that it is not possible to select a single best universal measure, and that different measures give different results for different tasks depending on data, language, and the types of MWE that the task is focused on. Similarly, Ramisch et al. (2008) investigate the hypothesis that MWEs can be detected solely by looking at the distinct statistical properties of their individual words and conclude that the association measures can only detect trends and preferences in the co-occurrences of words. A lot of effort has concentrated on the task of 2 Data Resources In this project we use three data resources for extracting MWEs. These resources differ widely in nature, size, structure and the main purpose they are used for. In this section we give a brief introduction to each of these data resources. Wikipedia (WK) is a freely-available multili"
W10-3704,elkateb-etal-2006-building,0,0.0380663,"Missing"
W10-3704,W09-2905,0,0.0428556,"Missing"
W10-3704,W04-0411,0,0.0213009,"ne Translation (Deksne, 2008). There are two basic criteria for identifying MWEs: first, component words exhibit statistically significant co-occurrence, and second, they show a certain level of semantic opaqueness or non-compositionality. Statistically significant cooccurrence can give a good indication of how likely a sequence of words is to form an MWE. This is particularly interesting for statistical techniques which utilize the fact that a large number of MWEs are composed of words that co-occur together more often than can be expected by chance. The compositionality, or decomposability (Villavicencio et al. 2004), of MWEs is also a core issue that presents a challenge for NLP applications because the meaning of the expression is not directly predicted from the meaning of the component words. In this respect, compositionalily varies between phrases that are highly comIntroduction A lexicon of multiword expressions (MWEs) has a significant importance as a linguistic resource because MWEs cannot usually be analyzed literally, or word-for-word. In this paper we apply three approaches to the extraction of Arabic MWEs from multilingual, bilingual, and monolingual data sources. We rely on linguistic informat"
W10-3704,W97-0311,0,0.144125,"two well-known among them are “non-substitutability”, when a word in the expression cannot be substituted by a semantically equivalent word, and “single-word paraphrasability”, when the expression can be paraphrased or translated by a single word. These two indications have been exploited differently by different researchers. Van de Cruys and Moiro´ n (2006) develop an unsupervised method for detecting MWEs using clusters of semantically related words and taking the ratio of the word preference over the cluster preference as an indication of how likely a particular expression is to be an MWE. Melamed (1997) investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take noncompositional compounds into account are more accurate. Moir´on and Tiedemann (2006) use word alignment of parallel corpora to locate the translation of an MWE in a target language and decide whether the original expression is idiomatic or literal. The technique used here is inspired by that of Zarrieß and Kuhn (2009) who rely on the linguistic intuition that if a group of words in one language is translated as a single word in another lang"
W10-3704,vintar-fiser-2008-harvesting,0,0.0593775,"Missing"
W10-3704,W06-2405,0,0.0453337,"expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs. Given the heterogeneity of MWEs, different approaches were devised. Broadly speaking, work on the extraction of MWEs revolves around four approaches: (a) statistical metho"
W10-3704,W09-2904,0,0.321046,"ase”, and those  that show a degree of idiomaticity, such as,          madiynatu ’l-mal¯ ahiy, “amusement park”, lit. “city of amusements”. In extreme cases the meaning of the expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs"
W10-3704,W03-1812,0,\N,Missing
W10-3704,boulaknadel-etal-2008-multi,0,\N,Missing
W10-3806,W05-0909,0,0.213822,"Missing"
W10-3806,1996.eamt-1.12,0,0.149229,"as to the amount of effort that might be needed to postedit the suggested translation (Simard and Isabelle, 2009). Not having such automatic quality metrics also has the side effect of it being impossible for a Translation-Services Provider (TSP) company to reliably determine in advance the increase in translator productivity due to the use of MT and to adjust their resources-allocation and cost models correspondingly. The second major problem for users is that SMTgenerated translations are as a rule only obtained for cases where the TM system could not produce a good-enough translation (cf. Heyn, 1996). Given that the SMT system used is usually trained only on the data available in the TM, expectedly it also has few examples from which to construct the translation, thus producing low quality output. With the steadily increasing demand for high-quality translation, the localisation industry is constantly searching for technologies that would increase translator throughput, with the current focus on the use of high-quality Statistical Machine Translation (SMT) as a supplement to the established Translation Memory (TM) technology. In this paper we present a novel modular approach that utilises"
W10-3806,P02-1040,0,0.0805717,"Missing"
W10-3806,2009.mtsummit-papers.14,0,0.38498,"egration of Machine Translation (MT) techniques is seen as the only feasible development that has the potential to significantly reduce the amount of manual translation required. At the same time, the use of SMT is frowned upon by the users of CAT tools as they still do not trust the quality of the SMT output. There are two main reasons for that. First, currently there is no reliable way to automatically ascertain the quality of SMT-generated translations, so that the user could at a glance make a judgement as to the amount of effort that might be needed to postedit the suggested translation (Simard and Isabelle, 2009). Not having such automatic quality metrics also has the side effect of it being impossible for a Translation-Services Provider (TSP) company to reliably determine in advance the increase in translator productivity due to the use of MT and to adjust their resources-allocation and cost models correspondingly. The second major problem for users is that SMTgenerated translations are as a rule only obtained for cases where the TM system could not produce a good-enough translation (cf. Heyn, 1996). Given that the SMT system used is usually trained only on the data available in the TM, expectedly it"
W10-3806,2006.amta-papers.25,0,0.157646,"Missing"
W10-3806,C08-1139,1,\N,Missing
W10-3806,P07-2045,0,\N,Missing
W10-3806,J03-1002,0,\N,Missing
W10-3815,W08-0319,0,0.037095,"Missing"
W10-3815,D07-1091,0,0.0605128,"Missing"
W10-3815,2005.mtsummit-papers.11,0,0.017517,"Missing"
W10-3815,N03-1017,0,0.006063,"t in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems. 1 Introduction In Phrase-Based Models of Machine Translation all phrases consistent with the word alignment are extracted (Koehn et al., 2003), with shorter phrases needed for high coverage of unseen data and longer phrases providing improved fluency in Leaving aside heuristic language modeling for a moment, the difficulty of integrating a traditional string-based language model into the decoding process in a hierarchical system, highlights a slight incongruity between the translation model and language model in Hierarchical Models. According to the translation model, the best way to build a fluent TL translation is via discontiguous phrases, while the language model can only provide information about the fluency of contiguous seque"
W10-3815,J07-2003,0,0.181076,"iang, 2005) build on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques (Chiang, 2007; Chiang, 2005). Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local"
W10-3815,N06-1032,0,0.0172942,"ency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to their direct bigram model, but is not restricted to bigrams. Riezler and Maxwell (2006) use a trigram deep syntax language model in German-English dependency tree-to-tree transfer to re-rank decoder output. The language model of Riezler and Maxwell (2006) is similar to the model in our evaluation, but differs in that it is restricted to a trigram model trained on LFG f-structures. In addition, as language modeling is not the main focus of their work, they provide little detail on the language model they use, except to say that it is based on “log-probability of strings of predicates from root to frontier of target f-structure, estimated from predicate trigrams in English f-struc"
W10-3815,P05-1033,0,0.0582532,"ild on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques (Chiang, 2007; Chiang, 2005). Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the"
W10-3815,P02-1035,0,0.0171643,"the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Sent. 19 16 Vocab 345K 28"
W10-3815,N04-1013,0,0.0160487,"st be in-coverage of the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Se"
W10-3815,W02-1506,0,0.0132038,"y selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Sent. 19 16 Vocab 345K 280K Table 1: Language model statistics for string-based and deep syntax language models, statistics are for string tokens and LFG lemmas for the same set of 7.29M English sentences al., 2004; Riezler et al., 2002)."
W10-3815,P08-1066,0,0.0287187,"nt and the deeper context of a word, if available, may provide more meaningful information and result in better lexical choice. Integrating such a model into a Hierarchical SMT system is not straightforward, however, and we believe before embarking on this its worthwhile to evaluate the model independently of any MT system. We therefore provide an experimental evaluation of the model and in order to provide an interesting comparison, we evaluate a traditional string-based language model on the same data. 2 Related Work The idea of using a language model based on deep syntax is not new to SMT. Shen et al. (2008) use a dependency-based language model in a string to dependency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to t"
W10-3815,P07-2045,0,\N,Missing
W10-3815,P98-2230,0,\N,Missing
W10-3815,C98-2225,0,\N,Missing
W10-4237,P03-2030,0,0.0265808,"Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and use"
W10-4237,P06-1130,1,0.915952,"Missing"
W10-4237,I05-1015,0,0.0150032,"underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use"
W10-4237,W96-0501,0,0.0418685,"cognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings wi"
W10-4237,H94-1010,0,0.0443218,"Missing"
W10-4237,J02-3001,0,0.0132479,"ogy, and coreference. The current goal is to annotate over a million words each of English and Chinese, and half a million words of Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work"
W10-4237,P98-1116,0,0.0286606,"more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to pr"
W10-4237,W02-2103,0,0.0463994,"ical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as m"
W10-4237,A97-1039,0,0.0697101,"many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/"
W10-4237,W04-0413,0,0.0130197,"ank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annot"
W10-4237,P01-1052,0,0.0217028,"ame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is is the CoNLL shared task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predica"
W10-4237,W05-1510,0,0.232262,"surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While public"
W10-4237,P05-1008,0,0.0183244,"-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Tre"
W10-4237,J05-1004,0,0.0427927,"these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBan"
W10-4237,W08-2121,0,0.0516008,"Missing"
W10-4237,N01-1001,0,0.0158715,"added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009)"
W10-4237,P09-1110,0,0.022058,"Missing"
W10-4237,D09-1043,0,\N,Missing
W10-4237,C98-1112,0,\N,Missing
W11-0804,W09-3821,0,0.0891431,"Missing"
W11-0804,W10-1410,1,0.846662,"is the integration of named 1 It is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce a category for say names of people if examples of such words occurred in similar syntactic patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with"
W11-0804,A00-2018,0,0.35169,"ers and a latent variable constituency parser is trained and tested on the transformed corpus. We explore two different methods for mapping words to entities, and look at the effect of mapping various subsets of named entity types. Thus far, results show no improvement in parsing accuracy over the best baseline score; we identify possible problems and outline suggestions for future directions. 1 Introduction Techniques for handling lexical data sparsity in parsers have been important ever since the lexicalisation of parsers led to significant improvements in parser performance (Collins, 1999; Charniak, 2000). The original treebank set of non-terminal labels is too general to give good parsing results. To overcome this problem, in lexicalised constituency parsers, non-terminals are enriched with lexical information. Lexicalisation of the grammar vastly increases the number of parameters in the model, spreading the data over more specific events. Statistics based on low frequency events are not as reliable as statistics on phenomena which occur regularly in the data; frequency counts involving words are typically sparse. Word statistics are also important in more recent unlexicalised approaches to"
W11-0804,N09-1037,0,0.0822447,"Missing"
W11-0804,P03-1054,0,0.119556,". Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with associated probabilities. When faced with a word in the test set that the parser has not seen in its training set - the unknown word is mapped to the special UNKNOWN token. In syntactic parsing, rather than map all low frequency words to one generic UNKNOWN type, it is"
W11-0804,N10-1089,0,0.12767,"rt introduction of the named entity resource used in our experiments and a description of the types of basic entity mappings we examine. In §3.1 and §3.2 we describe the two different types of mapping technique. Results are presented in Section 4, followed by a brief discussion in Section 5 indicating possible problems and avenues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-"
W11-0804,P05-1010,0,0.0539307,"Missing"
W11-0804,P06-1055,0,0.24327,"patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with associated probabilities. When faced with a word in the test set that the parser has not seen in its training set - the unknown word is mapped to the special UNKNOWN token. In syntactic parsing, rather than map all low frequency words to one g"
W11-0804,N09-2041,0,0.021122,"es, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce a category for say names of people if examples of such words occurred in similar syntactic patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences"
W11-0804,I05-3005,0,0.0171282,"s to one generic UNKNOWN type, it is useful to have several different clusters of unknown words, grouped according to morphological and other ‘surfacey’ clues in the original word. For example, certain suffixes in English are strong predictors for the part-of-speech tag of the word (e.g. ‘ly’) and so all low frequency words ending in ‘ly’ are mapped to ‘UNKNOWN-ly’. As well as suffix information, UNKNOWN words are commonly grouped based on information on capitalisation and hyphenation. Similar techniques for handling unknown words have been used for POS tagging (e.g. (Weischedel et al., 1993; Tseng et al., 2005)) and are used in the Charniak (Charniak, 2000), Berkeley (Petrov et al., 2006) and Stanford (Klein and Manning, 2003) parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the Berkeley parser. 3 Experiments The BBN Entity Type Corpus (Weischedel and Brunstein, 2005) consists of sentences from the Penn WSJ corpus, manually annotated with named entities. The Entity Type corpus includes annotatype PERSON PER DESC FAC FAC DESC ORGANIZATION ORG DESC GPE GPE DESC LOCATION NORP PRODUCT PRODUCT DESC EVENT WORK OF ART LAW LANGUAGE CONTACT INFO PLANT AN"
W11-0804,D07-1110,0,0.0533878,"Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different parsing methodologies, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variable parsers automatically induc"
W11-0804,W10-3705,0,0.133234,"scribe the two different types of mapping technique. Results are presented in Section 4, followed by a brief discussion in Section 5 indicating possible problems and avenues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different p"
W11-0804,J93-2006,0,0.273917,"ap all low frequency words to one generic UNKNOWN type, it is useful to have several different clusters of unknown words, grouped according to morphological and other ‘surfacey’ clues in the original word. For example, certain suffixes in English are strong predictors for the part-of-speech tag of the word (e.g. ‘ly’) and so all low frequency words ending in ‘ly’ are mapped to ‘UNKNOWN-ly’. As well as suffix information, UNKNOWN words are commonly grouped based on information on capitalisation and hyphenation. Similar techniques for handling unknown words have been used for POS tagging (e.g. (Weischedel et al., 1993; Tseng et al., 2005)) and are used in the Charniak (Charniak, 2000), Berkeley (Petrov et al., 2006) and Stanford (Klein and Manning, 2003) parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the Berkeley parser. 3 Experiments The BBN Entity Type Corpus (Weischedel and Brunstein, 2005) consists of sentences from the Penn WSJ corpus, manually annotated with named entities. The Entity Type corpus includes annotatype PERSON PER DESC FAC FAC DESC ORGANIZATION ORG DESC GPE GPE DESC LOCATION NORP PRODUCT PRODUCT DESC EVENT WORK OF ART LAW LANGUAGE"
W11-0804,W06-1206,0,0.0680142,"ues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different parsing methodologies, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variabl"
W11-0804,W06-1200,0,\N,Missing
W11-0804,J03-4003,0,\N,Missing
W11-2833,P06-1130,1,0.900123,"Missing"
W11-2833,D07-1028,1,0.905589,"Missing"
W11-2833,W02-2103,0,0.0610867,"que based on dependency-based n-gram models, described in some detail in (Guo et al., 2010). The generation method captures the mapping between the surface form sentences and the unordered syntactic representations of the shallow representation by linearising a set of dependencies directly, rather than via the application of grammar rules as in more traditional chart-style or unification-based generators (White, 2004; Nakanishi et al., 2005; Cahill and van Genabith, 2006; Hogan et al., 2007; White and Rajkumar, 2009). In contrast to conventional n-gram language models over surface word forms (Langkilde-Geary, 2002), we exploit structural information and various linguistic features inherent in the dependency representations to con∗ Throughout this document DCU stands for the joint team of Dublin City University and Toshiba (China) Research and Development Center participating in the SR Task 2011. Figure 1: Unordered dependency tree for the input of the sentence: the young athlete ran fast 2 Dependency-based N-gram Models The shallow input representation takes the form of an unordered dependency tree. The basic approach of the surface realisation method is to traverse the input tree ordering the nodes at"
W11-2833,W05-1510,0,0.019322,"1 Introduction nmod L M pred:young pred:the DCU submitted outputs for SR-Shallow, the shallow sub-task of the surface realisation shared task, using a surface realisation technique based on dependency-based n-gram models, described in some detail in (Guo et al., 2010). The generation method captures the mapping between the surface form sentences and the unordered syntactic representations of the shallow representation by linearising a set of dependencies directly, rather than via the application of grammar rules as in more traditional chart-style or unification-based generators (White, 2004; Nakanishi et al., 2005; Cahill and van Genabith, 2006; Hogan et al., 2007; White and Rajkumar, 2009). In contrast to conventional n-gram language models over surface word forms (Langkilde-Geary, 2002), we exploit structural information and various linguistic features inherent in the dependency representations to con∗ Throughout this document DCU stands for the joint team of Dublin City University and Toshiba (China) Research and Development Center participating in the SR Task 2011. Figure 1: Unordered dependency tree for the input of the sentence: the young athlete ran fast 2 Dependency-based N-gram Models The shal"
W11-2833,D09-1043,0,0.0758883,"hallow, the shallow sub-task of the surface realisation shared task, using a surface realisation technique based on dependency-based n-gram models, described in some detail in (Guo et al., 2010). The generation method captures the mapping between the surface form sentences and the unordered syntactic representations of the shallow representation by linearising a set of dependencies directly, rather than via the application of grammar rules as in more traditional chart-style or unification-based generators (White, 2004; Nakanishi et al., 2005; Cahill and van Genabith, 2006; Hogan et al., 2007; White and Rajkumar, 2009). In contrast to conventional n-gram language models over surface word forms (Langkilde-Geary, 2002), we exploit structural information and various linguistic features inherent in the dependency representations to con∗ Throughout this document DCU stands for the joint team of Dublin City University and Toshiba (China) Research and Development Center participating in the SR Task 2011. Figure 1: Unordered dependency tree for the input of the sentence: the young athlete ran fast 2 Dependency-based N-gram Models The shallow input representation takes the form of an unordered dependency tree. The b"
W11-2833,C08-1038,1,\N,Missing
W11-2925,J04-4004,0,0.0407885,"reranker. Because we have approximately five times the Our dataset, summarised in Table 1, consists of a small treebank of hand-corrected phrase structure parse trees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the test set 223. The experiments in this paper are carried out on the development set (which we refer to as FootballDev). Discussion Forum Corpus The same discussion forum used to create the treebank was scraped during the final quarter of 2010. The content was stripped of HTML markup and passed through an i"
W11-2925,P07-1078,0,0.0505586,"t the experiment with Web 2.0 data, believing that the two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use"
W11-2925,A00-2018,0,0.282004,"Missing"
W11-2925,I11-1100,1,0.880246,"Missing"
W11-2925,N10-1060,1,0.857306,"retrain the first-stage parser using combinations of trees produced by the reranking parser for sentences from Sections 2 to 21 of the WSJ section of the Penn Treebank and from FootballTrainEdited|Discussion. We then parse the sentences in FootballDev using the retrained first-stage parser and the original reranker. Because we have approximately five times the Our dataset, summarised in Table 1, consists of a small treebank of hand-corrected phrase structure parse trees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the"
W11-2925,D09-1087,0,0.0157088,"data, believing that the two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use of edited, professional"
W11-2925,H94-1020,0,0.0413534,"ees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the test set 223. The experiments in this paper are carried out on the development set (which we refer to as FootballDev). Discussion Forum Corpus The same discussion forum used to create the treebank was scraped during the final quarter of 2010. The content was stripped of HTML markup and passed through an in-house sentence splitter and tokeniser, resulting in a corpus of 1,009,646 sentences. We call this the FootballTrainDiscussion corpus. Figure 1: Comparing the performance"
W11-2925,P08-2026,0,0.0401093,"Missing"
W11-2925,N06-1020,0,0.645374,"pus” (McClosky et al., 2006b). We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We find that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 2 Related Work McClosky et al. (2006b) demonstrate that a WSJtrained parser can be adapted to the fiction domains of the Brown corpus by performing a type of self-training that involves the use of the twostage Brown reranking parser (Charniak and Johnson, 2005). Their training protocol is as follows: sentences from the LA Times are parsed using the first-stage parser (Charniak, 2000) and reranked in the second stage. These parse trees are added to the original WSJ training set and the first-stage parser is retrained. The sentences from the target domain, in this case, Brown corpus sentences are then parsed using the newly traine"
W11-2925,P06-1043,0,0.444935,"pus” (McClosky et al., 2006b). We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We find that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 2 Related Work McClosky et al. (2006b) demonstrate that a WSJtrained parser can be adapted to the fiction domains of the Brown corpus by performing a type of self-training that involves the use of the twostage Brown reranking parser (Charniak and Johnson, 2005). Their training protocol is as follows: sentences from the LA Times are parsed using the first-stage parser (Charniak, 2000) and reranked in the second stage. These parse trees are added to the original WSJ training set and the first-stage parser is retrained. The sentences from the target domain, in this case, Brown corpus sentences are then parsed using the newly traine"
W11-2925,N10-1004,0,0.0141352,"+7.1) 4 (+14.0) (-7.4) (+2.9) 107 30 13 150 94 32 24 150 5 (+11.9) (-4.2) (+8.1) 5 (+9.5) (-1.6) (+5.7) 78 10 32 120 81 0 39 120 &gt;=6 (+17.2) (-5.7) (+9.7) 2308 4210 1222 7740 &gt;=6 (+10.4) (—) (-11.2) (+3.4) 1975 4807 958 7740 TOTAL (+13.6) (-9.4) (+2.6) TOTAL (+12.1) (-8.5) (+2.0) score for FootballDev increases from 83.2 to 85.6. When we include the baseline grammar (f-score: 79.7), this increases to 86.4. This suggests that the next step in our research is to build such a classifier including as features the sentential properties we examined in Section 5, as well as the features described in McClosky et al. (2010) and Ravi et al. (2008). • Self-training with both FootballTrainDiscussion and FootballTrainEdited data tends to benefit sentences containing several unknown words, with the discussion grammars being superior. Acknowledgements This research has been supported by Enterprise Ireland (CFTD/2007/229) and by Science Foundation Ireland (Grant 07/CE/ I1142) as part of the CNGL (www.cngl.ie) at the School of Computing, DCU. 6 Conclusion We compare the use of edited versus unedited text in the task of adapting a WSJ-trained parser to the noisy language of an online discussion forum. Given the small siz"
W11-2925,D10-1069,0,0.0122123,"two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use of edited, professionally written sentences o"
W11-2925,D08-1093,0,0.0199424,"9) 107 30 13 150 94 32 24 150 5 (+11.9) (-4.2) (+8.1) 5 (+9.5) (-1.6) (+5.7) 78 10 32 120 81 0 39 120 &gt;=6 (+17.2) (-5.7) (+9.7) 2308 4210 1222 7740 &gt;=6 (+10.4) (—) (-11.2) (+3.4) 1975 4807 958 7740 TOTAL (+13.6) (-9.4) (+2.6) TOTAL (+12.1) (-8.5) (+2.0) score for FootballDev increases from 83.2 to 85.6. When we include the baseline grammar (f-score: 79.7), this increases to 86.4. This suggests that the next step in our research is to build such a classifier including as features the sentential properties we examined in Section 5, as well as the features described in McClosky et al. (2010) and Ravi et al. (2008). • Self-training with both FootballTrainDiscussion and FootballTrainEdited data tends to benefit sentences containing several unknown words, with the discussion grammars being superior. Acknowledgements This research has been supported by Enterprise Ireland (CFTD/2007/229) and by Science Foundation Ireland (Grant 07/CE/ I1142) as part of the CNGL (www.cngl.ie) at the School of Computing, DCU. 6 Conclusion We compare the use of edited versus unedited text in the task of adapting a WSJ-trained parser to the noisy language of an online discussion forum. Given the small size of our development se"
W11-2925,P05-1022,0,\N,Missing
W11-3802,W10-1408,1,0.915979,"Missing"
W11-3802,W10-1404,0,0.0543112,"representing syntactic information for morphologically rich and free-word order languages. However, the results of CoNLL shared tasks on 18 different languages, including Arabic (Nivre et al., 2007a) using either the MaltParser (Nivre et al., 2007b) or the MSTParser (McDonald and Crammer, 2005) suggests that Arabic is nonetheless quite a difficult language to parse1 , leaving open the question as to the effectiveness of dependency parsing for Arabic. One reason for this ineffectiveness is that many parsers do not make much, if any, use of morphological information (Tsarfaty and Sima’an, 2008; Bengoetxea and Gojenola, 2010; Marton et al., 2010). In fact, many established parsing models do not capture visible morphological information provided by wordforms and thus fail to make important distributional distinctions. In this paper, using a re-implementation of the Berkeley latent-variable PCFG parser we study how morphological features, as encoded in POS 1 The quality and size of the treebanks certainly are important issues. 12 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 12–21, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational L"
W11-3802,P99-1065,0,0.215693,"Missing"
W11-3802,H05-1100,0,0.0201931,"sed POS tagset (Maamouri et al., 2009) and does not employ morphological information to handle unknown words. This paper is organized as follows: In Section 2, we review the PCFG-LA parsing model and the dataset for our experiments. Section 3 addresses the different techniques we have applied to explore the morphological features space over the possible combinations, including a comparison with the 2 By comparison, the case feature improved parsing for Czech (Collins et al., 1999) and the combination of the number feature for adjectives and mode feature for verbs improved results for Spanish (Cowan and Collins, 2005). 13 best morphological features of previous works. In Section 4, we describe the results of applying the parser. 2 General Background 2.1 Parsing Models Johnson (1998) enriched treebank categories with context information to improve the performance of PCFG parsing, then Klein and Manning (2003b) explored manual and automatic extensions of syntactic categories into a richer category set to enhance PCFG parsing results. Later, Matsuzaki et al. (2005) used unsupervised techniques, known as PCFG-Latent Annotation (PCFG-LA), to learn more fine-grained categories from the treebank. This method invo"
W11-3802,W07-0812,0,0.0239671,"Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 12–21, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics tags, can be learned automatically by modifying the distributional restriction of initial grammar symbols, and how they impact Arabic constituency parsing. We have selected PCFG-LA parsing models because they have been shown to be relatively language-independent with state-ofthe-art performance for several languages (Petrov, 2009). Kulick et al. (2006) reported that extending POS tags with definiteness information helps Arabic PCFG parsing2 , Diab (2007) enriched the POS tagset with gender, number and definiteness to improve Arabic base phrase chunking, and Marton et al. (2010) reported that definiteness, person, number and gender were most helpful for Arabic dependency parsing on predicted tag input. Our method is comparable to this work in terms of the investigation of the morphological features. However, the results are not comparable, as we use a different parsing paradigm, a different form of the treebank, and most importantly, we extend the investigation to use several automated feature selection methods. Increasing the tagset size can"
W11-3802,P05-1071,0,0.053793,"ne plurals Noun can be definite even if it does not start with al-, e.g. proper nouns are assumed to be inherently definite Presence of the future prefix sa- for verbs Table 1: Morphological features in Arabic 2.3 Part-Of-Speech Tagset The POS tagset in the ATB covers nouns, pronouns, adjectives, verbs, adverbs, prepositions, interjections, particles, conjunctions, and subordinating conjunctions. This tagset uses 78 atomic components, such as: ABBREV for abbreviation, ACC for accusative case and ADJ for adjective. While there are theoretically hundreds of thousands of full ATB-style POS tags (Habash and Rambow, 2005), only 498 full tags occur in the above-mentioned version of the hours vs. 8 hours, with F -scores being almost the same. 4 Specifically for every 10 sentences, the first 8 go into the training set, the 9th sentence go into the test set, and the 10th sentence into the dev set. 14 We investigate two directions to find optimal tagsets, as follows: Direction 1: starts from ATB top tagset (|498|) and reduces the size of the tagset by excluding the features that hurt parsing performance. In our initial set of experiments, we trained and parsed 15 different configurations, each configuration startin"
W11-3802,P09-2056,0,0.0620186,"n the powerset, and moved stochastically. The differences in the results indicate that many of the high scores lie relatively near the bottom, whereas there is much greater uncertainty in the middle of the feature powerset. For comparative purposes, Table 6 presents the results obtained by using the most helpful features proposed in two previous works.8 The Determiner feature in the first row was added by Kulick et al. (2006) to the Bikel-Bies POS tagset, using the Bikel parser. The features in the last two rows were determined by using the MaltParser with the Columbia Arabic Treebank (CATiB: Habash and Roth, 2009) and discussed in Marton et al. (2010). 8 Using the same setup as in the other experiments in this paper (eg. PCFG-LA parser and ATB3v3.2). 18 While the case feature (9) helped their gold-tag parsing, it was not helpful for either vocalized or unvocalized parsing in our experiments. Case markings in Arabic exhibit ambiguity with certain noun forms—there are particular instances where both the genitive and accusative endings are the same, such as the duals and masculine plurals. Related to this is the imperfect alignment in Arabic between true grammatical function and morphological case marking"
W11-3802,J98-4004,0,0.176312,"Missing"
W11-3802,N03-1016,0,0.157187,"art results for Arabic constituency parsing. 1 Introduction Parsing Arabic is challenging due to its morphological richness and syntactic complexity. In particular, the number of distinct word-forms, the relative freedom with respect to word order, and the information expressed at the level of words make parsing Arabic a difficult task. Previous research established that adapting constituency parsing models developed from English to Arabic (and josef@computing.dcu.ie other languages) is a non-trivial task. Significant effort has been deployed to parse Chinese using the unlexicalized parser of Klein and Manning (2003a,b) with modest performance gains over previous approaches. Due to the specification of head rules, lexicalized parsing models also turned out to be difficult to generalize to other languages: Kulick et al. (2006) describe Arabic parsing results far below English or even Chinese using the Collins parsing model as implemented in the Bikel parser (Bikel, 2004). In order to side-step the surface representations involved in constituency parsing, several studies have focused on Arabic dependency parsing. The general assumption is that dependency structures are better suited for representing syntac"
W11-3802,P03-1054,0,0.141006,"art results for Arabic constituency parsing. 1 Introduction Parsing Arabic is challenging due to its morphological richness and syntactic complexity. In particular, the number of distinct word-forms, the relative freedom with respect to word order, and the information expressed at the level of words make parsing Arabic a difficult task. Previous research established that adapting constituency parsing models developed from English to Arabic (and josef@computing.dcu.ie other languages) is a non-trivial task. Significant effort has been deployed to parse Chinese using the unlexicalized parser of Klein and Manning (2003a,b) with modest performance gains over previous approaches. Due to the specification of head rules, lexicalized parsing models also turned out to be difficult to generalize to other languages: Kulick et al. (2006) describe Arabic parsing results far below English or even Chinese using the Collins parsing model as implemented in the Bikel parser (Bikel, 2004). In order to side-step the surface representations involved in constituency parsing, several studies have focused on Arabic dependency parsing. The general assumption is that dependency structures are better suited for representing syntac"
W11-3802,H94-1020,0,0.372549,"Missing"
W11-3802,W10-1402,0,0.487468,"ion for morphologically rich and free-word order languages. However, the results of CoNLL shared tasks on 18 different languages, including Arabic (Nivre et al., 2007a) using either the MaltParser (Nivre et al., 2007b) or the MSTParser (McDonald and Crammer, 2005) suggests that Arabic is nonetheless quite a difficult language to parse1 , leaving open the question as to the effectiveness of dependency parsing for Arabic. One reason for this ineffectiveness is that many parsers do not make much, if any, use of morphological information (Tsarfaty and Sima’an, 2008; Bengoetxea and Gojenola, 2010; Marton et al., 2010). In fact, many established parsing models do not capture visible morphological information provided by wordforms and thus fail to make important distributional distinctions. In this paper, using a re-implementation of the Berkeley latent-variable PCFG parser we study how morphological features, as encoded in POS 1 The quality and size of the treebanks certainly are important issues. 12 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 12–21, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics tags, can b"
W11-3802,P11-1159,0,0.0795221,"Missing"
W11-3802,P05-1010,0,0.125606,"Missing"
W11-3802,P05-1012,0,0.0758508,"r below English or even Chinese using the Collins parsing model as implemented in the Bikel parser (Bikel, 2004). In order to side-step the surface representations involved in constituency parsing, several studies have focused on Arabic dependency parsing. The general assumption is that dependency structures are better suited for representing syntactic information for morphologically rich and free-word order languages. However, the results of CoNLL shared tasks on 18 different languages, including Arabic (Nivre et al., 2007a) using either the MaltParser (Nivre et al., 2007b) or the MSTParser (McDonald and Crammer, 2005) suggests that Arabic is nonetheless quite a difficult language to parse1 , leaving open the question as to the effectiveness of dependency parsing for Arabic. One reason for this ineffectiveness is that many parsers do not make much, if any, use of morphological information (Tsarfaty and Sima’an, 2008; Bengoetxea and Gojenola, 2010; Marton et al., 2010). In fact, many established parsing models do not capture visible morphological information provided by wordforms and thus fail to make important distributional distinctions. In this paper, using a re-implementation of the Berkeley latent-varia"
W11-3802,P06-1055,0,0.079353,"ion coefficient of 0.34 between the accuracies of the development set of the Peen Treebank and the test set, and therefore suggested less of a reliance on any particular random seed to yield better grammars. This led him to propose combining grammars from several seeds to produce a higher-quality product model. In this work, we use a re-implementation of the Berkeley parser, which trains a PCFG-LA using the split-merge-smooth procedure and parses using the max-rule algorithm (Petrov et al., 2006; Petrov and Klein, 2007). For our experiments, we apply the split-merge-smooth cycle five times3 3 Petrov et al. (2006) reports that the best grammars for English using the Penn Treebank are obtained by repeating this cycle 5 or 6 times, depending on the test set. We opted for five cycles due to the vast difference in training times: 2 and we parse on sentences of less than or equal to 40 words in length. For English, the Berkeley parser explores n-gram suffixes to distinguish between unknown words ending with -ing, -ed, s, etc. to assign POS tags. The Berkeley parser does not provide the same methodology to Arabic. For Arabic, we apply the technique used by Attia et al. (2010) for the purpose of classificatio"
W11-3802,N07-1051,0,0.0666847,"Missing"
W11-3802,C08-1112,0,0.237193,"Missing"
W11-3802,N10-1003,0,\N,Missing
W11-3802,D07-1096,0,\N,Missing
W11-4417,2006.bcs-1.5,1,0.693113,"ttested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a historical vers"
W11-4417,2003.mtsummit-semit.5,0,0.72348,"exical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a hi"
W11-4417,E09-2008,0,0.126089,"technology that makes it especially attractive in dealing with human language morphologies; among these are the ability to handle concatenative and nonconcatenative morphotactics, and the high speed and efficiency in handling large automata of lexicons with their derivations and inflections that can run into millions of paths. The Xerox XFST System (Beesley and Karttunen, 2003) is a well-known finite state compiler, but the disadvantage of this tool is that it is a proprietary software, which limits its use in the larger research community. Fortunately, there is an alternative, namely Foma, (Hulden, 2009), which is an opensource finite-state toolkit that implements the Xerox lexc and xfst utilities. We have developed an opensource morphological analyser for Arabic using the Foma compiler allowing us to share our morphology with third parties. The lexical database, which is being edited and validated, is used to automatically extend and update the morphological analyser, allowing for greater coverage and better capabilities. Arabic words are formed through the amalgamation of two tiers, namely root and pattern. A root is a sequence of three consonants and the pattern is a template of vowels (or"
W11-4417,P08-2030,0,0.204709,"Missing"
W12-0106,J10-4005,0,0.0486746,"m of a hybrid SMTEBMT approach. It is often the case that an EBMT system produces a good translation where SMT systems fail and vice versa (Dandapat et al., 2011). Introduction An EBMT system relies on past translations to derive the target output for a given input. Runtime EBMT approaches generally do not include any training stage, which has the advantage of not having to depend on time-consuming preprocessing. On the other hand, their runtime complexity can be considerable. This is due to the time-consuming matching stage at runtime that finds the example State-of-the-art phrase-based SMT (Koehn, 2010a) is the most successful MT approach in many large scale evaluations, such as WMT,1 IWSLT2 etc. At the same time, work continues in the area of EBMT. Some recent EBMT systems include Cunei (Phillips, 1 2 http://www.statmt.org/wmt11/ http://www.iwslt2011.org/ 48 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–58, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics (or set of examples) which most closely matches the source-language sentence to be translated. This matching step often uses some"
W12-0106,P07-2045,0,0.0153836,"Missing"
W12-0106,W04-3250,0,0.354876,"Missing"
W12-0106,J03-1002,0,0.00542541,"anually or automatically (e.g. using the marker-hypothesis (Groves et al., 2006)). Usually, TUs are linguistically motivated translation units. In this paper however, we explore a different route, as manual construction of high-quality TMs is time consuming and expensive. Furthermore, only considering linguistically motivated TUs may limit the matching potential of a TM. Because of this, we used SMT technology to automatically create the subsentential part of our TM at the phrase (i.e. no longer necessarily linguistically motivated) and word level. Based on Moses word alignment (using GIZA++ (Och and Ney, 2003)) and phrase table construction, we construct the additional TM for further use within an EBMT approach. Firstly, we add entries to the TM based on the aligned phrase pairs from the Moses phrase table using the following two scores: 1. Direct phrase translation probabilities: φ(t|s) 2. Direct lexical weight: lex(t|s) Table 1 shows an example of phrase pairs with the associated probabilities learned by Moses. We keep all target equivalents in a sorted order based on the 3.2 EBMT Engine The overview of the three stages of the EBMT engine is given below: Matching: In this stage, we find a sentenc"
W12-0106,P02-1040,0,0.0827422,"Missing"
W12-0106,2006.tc-1.9,0,0.0376592,"Missing"
W12-0106,2011.eamt-1.28,1,0.879031,"lore two methods to make the system scalable at runtime. First, we use an heuristic-based approach. Secondly, we use an IR-based indexing technique to speed up the time-consuming matching procedure of the EBMT system. The index-based matching procedure substantially improves run-time speed without affecting translation quality. 1 Keeping these in mind, our objective is to develop a good quality MT system choosing the best approach for each input in the form of a hybrid SMTEBMT approach. It is often the case that an EBMT system produces a good translation where SMT systems fail and vice versa (Dandapat et al., 2011). Introduction An EBMT system relies on past translations to derive the target output for a given input. Runtime EBMT approaches generally do not include any training stage, which has the advantage of not having to depend on time-consuming preprocessing. On the other hand, their runtime complexity can be considerable. This is due to the time-consuming matching stage at runtime that finds the example State-of-the-art phrase-based SMT (Koehn, 2010a) is the most successful MT approach in many large scale evaluations, such as WMT,1 IWSLT2 etc. At the same time, work continues in the area of EBMT."
W12-0106,2006.eamt-1.15,1,0.838534,"Missing"
W12-0106,2010.eamt-1.21,0,0.0234162,"for Quality and Scale Sandipan Dandapat1 , Sara Morrissey1 , Andy Way2 , Joseph van Genabith1 1 CNGL, School of Computing Dublin City University, Glasnevin, Dublin 9, Ireland {sdandapat,smorri,josef}@computing.dcu.ie 2 Applied Language Solutions, Delph, UK andy.way@appliedlanguage.com Abstract 2011), CMU-EBMT (Brown, 2011) and OpenMaTrEx (Dandapat et al., 2010). The success of an SMT system often depends on the amount of parallel training corpora available for the particular language pair. However, low translation accuracy has been observed for language pairs with limited training resources (Islam et al., 2010; Khalilov et al., 2010). SMT systems effectively discard the actual training data once the models (translation model and language model) have been estimated. This can lead to their inability to guarantee good quality translation for sentences closely matching those in the training corpora. By contrast, EBMT systems usually maintain a linked relationship between the full sentence pairs in source and target texts. Because of this EBMT systems can often capture long range dependencies and rich morphology at runtime. In contrast to SMT, however, most EBMT models lack a wellformed probability mode"
W12-0106,2009.mtsummit-papers.14,0,0.0557551,"nguage translation pairs for effective reuse of previous translations originally created by human translators. TMs are often used to store examples for EBMT systems. After retrieving a set of examples with associated translations, EBMT systems automatically extract translations of suitable fragments and combine them to produce a grammatical target output. Phrase-based SMT systems (Koehn, 2010a), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to be used in a CAT environment (Simard, 2003; Bic¸ici and Dymetman, 2008; Bourdaillet et al., 2009; Simard and Isabelle, 2009). Koehn and Senellart (2010b) use SMT to produce the translation of the non-matched fragments after obtaining the TMbased match. EBMT phrases have also been used to populate the knowledge database of an SMT system (Groves et al., 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no sufficiently well matching translation unit (TU) is found in the TM. The TransTy"
W12-0106,W03-0313,0,0.0351912,"anslation memory (TM). A TM essentially stores source- and target-language translation pairs for effective reuse of previous translations originally created by human translators. TMs are often used to store examples for EBMT systems. After retrieving a set of examples with associated translations, EBMT systems automatically extract translations of suitable fragments and combine them to produce a grammatical target output. Phrase-based SMT systems (Koehn, 2010a), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to be used in a CAT environment (Simard, 2003; Bic¸ici and Dymetman, 2008; Bourdaillet et al., 2009; Simard and Isabelle, 2009). Koehn and Senellart (2010b) use SMT to produce the translation of the non-matched fragments after obtaining the TMbased match. EBMT phrases have also been used to populate the knowledge database of an SMT system (Groves et al., 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when n"
W12-3128,N07-1051,0,\N,Missing
W12-3128,A00-2018,0,\N,Missing
W12-3128,D10-1014,0,\N,Missing
W12-3128,D10-1054,0,\N,Missing
W12-3128,D11-1079,0,\N,Missing
W12-3128,W05-1506,0,\N,Missing
W12-3128,J03-4003,0,\N,Missing
W12-3128,D11-1020,0,\N,Missing
W12-3128,P11-1065,0,\N,Missing
W12-3128,P11-1001,0,\N,Missing
W12-3128,J04-4002,0,\N,Missing
W12-3128,D09-1008,0,\N,Missing
W12-3128,P07-1005,0,\N,Missing
W12-3128,P96-1021,0,\N,Missing
W12-3128,P08-1114,0,\N,Missing
W12-3128,P05-1033,0,\N,Missing
W12-3128,N03-1017,0,\N,Missing
W12-3128,P02-1038,0,\N,Missing
W12-3128,W06-3119,0,\N,Missing
W12-3128,W04-3250,0,\N,Missing
W12-3128,J07-2003,0,\N,Missing
W12-3128,2011.eamt-1.38,0,\N,Missing
W12-3128,P00-1056,0,\N,Missing
W12-3128,P03-1021,0,\N,Missing
W12-5704,2003.mtsummit-systems.1,0,0.693557,"tands for the number of nulls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more grammatical than Moses or other M"
W12-5704,2010.amta-papers.16,1,0.887417,"Missing"
W12-5704,D09-1115,1,0.886062,"sing (Henderson and Brill, 1999) and speech recognition (Fiscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main probl"
W12-5704,D08-1011,0,0.100136,"aph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main problem of IHMM is that there are numerous one-to-many and one-to-null cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection whic"
W12-5704,W99-0623,0,0.0607824,"oduction This paper describes a new extension to our system combination module in Dublin City University for the participation in the system combination task in the ML4HMT-2012 workshop. We incorporate alignment meta information to the alignment module when building a confusion network. Given multiple translation outputs, a system combination strategy aims at finding the best translations, either by choosing one sentence or generating a new translation from fragments originated from individual systems(Banerjee et al., 2010). Combination methods have been widely used in fields such as parsing (Henderson and Brill, 1999) and speech recognition (Fiscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other"
W12-5704,P07-2045,0,0.00511754,"ls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more grammatical than Moses or other MT systems; 2) accordi"
W12-5704,J03-1002,0,0.0115978,"put. After obtaining the backbone, all other hypotheses are aligned to it. The alignment strategies include IHMM, TER, etc. Note that during the word alignment word reordering and ‘null’ insertion are performed, which is usually called normalization. The confusion network, which can be constructed directly from the normalized alignment is given in Figure 2b, in which case H1 is chosen as the backbone. 2.2 Indirect HMM Alignment In this work we implement the IHMM (He et al., 2008). IHMM is a refined version of HMM alignment (Vogel et al., 1996) which is widely used in bilingual word alignment (Och and Ney, 2003). Let B = (b1 , ..., bJ ) denote the J words in the backbone sentence, H = (h1 , ..., h I ) denote one of the hypothesis, and A = (a1 , ..., aJ ) denote the alignment of each backbone word to the hypothesis word. We use Equation 2 to compute the alignment probability of each word pair. In Equation 2, d represents the distortion model and p denotes the word similarity model. P(H|B) = X Y A j=1...J [d(a j |a j−1 , I)p(h j |ba j )] 39 (2) In order to handle the words which are aligned to an empty word, we also insert a null associated with each backbone word. We follow (Vogel et al., 1996) and us"
W12-5704,2006.tc-1.12,0,0.689469,"of the current path, Nnul ls stands for the number of nulls inserted, and Len is the length of the current path. ν ,µ and ε are the corresponding weights of each feature. A beam search algorithm is employed to find the best path. 3 Experimental Setup 3.1 Data We participate in the ML4HMT-12 shared task ES-EN. Participants are given a development bilingual data set aligned at the sentence level. Each &quot;bilingual sentence&quot; contains: 1) the source sentence, 2) the target (reference) sentence and 3) the corresponding multiple output translations from four systems, based on different MT approaches (Ramırez-Sánchez et al., 2006; Alonso and Thurmair, 2003; Koehn et al., 2007). The output has been annotated with 40 system-internal meta-data information derived from the translation process of each of the systems. In this work we use 1000 sentence pair from the 10K development set to tune the system parameters and all the 3003 sentence pairs in the test set to run the test. 3.2 Backbone Selection Equation 1 describes the traditional backbone selection. However in this work we heuristically set Lucy RBMT (Alonso and Thurmair, 2003) output as the backbone. Our motivations are that: 1) Lucy’s output tends to be more gramma"
W12-5704,N07-1029,0,0.138427,"ull cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection which also affects the combination results. The backbone determines the word order in the final output. Backbone selection is often done by Minimum Bayes Risk (MBR) decoding which selects a hypothesis with minimum average distance among all hypotheses (Rosti et al., 2007a,b). In this work we heuristically choose an RBMT output as the backbone due to its (expected) overall grammatically well-formed output and better human evaluation results. We report our results and provide a comparison with traditional confusion-network-based network approach. The remainder of the paper is organized as follows: We will review the state-of-the-art system combination framework based on confusion networks in Section 2. We describe our experimental setup, how we extract the alignment information from meta-data and how we use it in Section 3. The results and analysis are also giv"
W12-5704,P07-1040,0,0.122071,"ull cases in the alignment results. This alignment noise significantly affects the confusion network construction and the decoding process. In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process. The other crucial factor is the backbone selection which also affects the combination results. The backbone determines the word order in the final output. Backbone selection is often done by Minimum Bayes Risk (MBR) decoding which selects a hypothesis with minimum average distance among all hypotheses (Rosti et al., 2007a,b). In this work we heuristically choose an RBMT output as the backbone due to its (expected) overall grammatically well-formed output and better human evaluation results. We report our results and provide a comparison with traditional confusion-network-based network approach. The remainder of the paper is organized as follows: We will review the state-of-the-art system combination framework based on confusion networks in Section 2. We describe our experimental setup, how we extract the alignment information from meta-data and how we use it in Section 3. The results and analysis are also giv"
W12-5704,C96-2141,0,0.304371,"(1) H∈∇ The backbone is used to decide the word order of the final output. After obtaining the backbone, all other hypotheses are aligned to it. The alignment strategies include IHMM, TER, etc. Note that during the word alignment word reordering and ‘null’ insertion are performed, which is usually called normalization. The confusion network, which can be constructed directly from the normalized alignment is given in Figure 2b, in which case H1 is chosen as the backbone. 2.2 Indirect HMM Alignment In this work we implement the IHMM (He et al., 2008). IHMM is a refined version of HMM alignment (Vogel et al., 1996) which is widely used in bilingual word alignment (Och and Ney, 2003). Let B = (b1 , ..., bJ ) denote the J words in the backbone sentence, H = (h1 , ..., h I ) denote one of the hypothesis, and A = (a1 , ..., aJ ) denote the alignment of each backbone word to the hypothesis word. We use Equation 2 to compute the alignment probability of each word pair. In Equation 2, d represents the distortion model and p denotes the word similarity model. P(H|B) = X Y A j=1...J [d(a j |a j−1 , I)p(h j |ba j )] 39 (2) In order to handle the words which are aligned to an empty word, we also insert a null asso"
W12-5704,P11-1125,0,0.0120179,"iscus, 1997). In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well. The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity. Confusion networks are compact graph-based structures representing multiple hypothesises (Bangalore et al., 2001). It is noted that there are several generalized forms of confusion networks as well. One is a lattice (Feng et al., 2009) and the other is a translation forest (Watanabe and Sumita, 2011). The former employs lattices that can describe arbitrary mappings in hypothesis alignments. A lattice is more general than a confusion network. By contrast, a confusion forest exploits syntactic similarity between individual outputs. Up to now, various state-of-the-art alignment methods have been developed including IndirectHMM (He et al., 2008; Du and Way, 2010) which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance. In this work we focus on the IHMM method. The main problem of IHMM is that there are numerous one-to-many and one-to-null"
W12-5705,P07-1033,0,0.148002,"Missing"
W12-5705,2010.amta-papers.9,0,0.255096,"Missing"
W12-5705,W07-0717,0,0.0804631,"Missing"
W12-5705,2012.amta-monomt.2,0,0.050714,"Missing"
W12-5705,W07-0733,0,0.100456,"Missing"
W12-5705,W02-1019,0,0.0588576,"Missing"
W12-5705,2009.iwslt-evaluation.4,1,0.867836,"Missing"
W12-5705,2011.mtsummit-papers.60,1,0.837006,"Missing"
W12-5705,P09-3009,1,0.805493,"Missing"
W12-5705,okita-2012-annotated,1,0.565616,"Missing"
W12-5705,W10-4006,1,0.50897,"Missing"
W12-5705,P02-1040,0,0.0828711,"Missing"
W12-5705,2012.eamt-1.38,1,0.879438,"Missing"
W12-5705,P07-1040,0,0.162443,"Missing"
W12-5705,W10-2602,0,0.0438618,"Missing"
W12-5706,P09-1064,0,0.121552,"Missing"
W12-5706,I11-1153,0,0.0265379,"Missing"
W12-5706,D08-1011,0,0.0381145,"Missing"
W12-5706,P08-2021,0,0.032303,"Missing"
W12-5706,W02-1019,0,0.0944556,"Missing"
W12-5706,W10-1747,0,0.0286838,"Missing"
W12-5706,E06-1005,0,0.150468,"Missing"
W12-5706,okita-2012-annotated,1,0.586832,"Missing"
W12-5706,W10-4006,1,0.742452,"Missing"
W12-5706,P02-1040,0,0.0835599,"Missing"
W12-5706,P07-1040,0,0.0849459,"Missing"
W12-5706,W12-3117,1,0.888749,"Missing"
W12-5706,2006.amta-papers.25,0,0.031404,"Missing"
W12-5706,W12-3118,0,0.0636222,"Missing"
W12-5706,2009.eamt-1.5,0,0.0836285,"Missing"
W12-5706,D08-1065,0,0.114044,"Missing"
W12-5706,C04-1046,0,\N,Missing
W12-5709,2003.mtsummit-systems.1,0,0.0818841,"Missing"
W12-5709,W11-2107,0,0.0383819,"Missing"
W12-5709,W12-3138,1,0.877314,"Missing"
W12-5709,P07-2045,0,0.00673657,"Missing"
W12-5709,W12-5706,1,0.882913,"Missing"
W12-5709,W12-5705,1,0.848415,"Missing"
W12-5709,P02-1040,0,0.0881819,"Missing"
W12-5709,2006.tc-1.12,0,0.132657,"Missing"
W12-5709,W12-5704,1,0.829445,"Missing"
W12-5709,2009.iwslt-evaluation.8,0,\N,Missing
W13-2222,W12-3141,0,0.0151486,"cess to obtain the best weights based on the n-best lists, (Repeat the decoding / MERT process several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer becomes the input"
W13-2222,P11-1087,0,0.0148513,"ess several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer becomes the input to the higher layer of inputs. Note that such an architecture comes from the Restricted Boltzmann"
W13-2222,J03-1002,0,0.00551079,"model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-ba"
W13-2222,P03-1021,0,0.0155701,"icted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus"
W13-2222,P05-1033,0,0.0965582,"HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus, we used all the resources provided for the translation task at WMT13 for language model, that is parallel corpora (Europarl V7 (Koehn, 2005), Common Crawl corpus, UN corpus, and News Commentary) and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem"
W13-2222,W10-4006,1,0.861831,"utput attached) to predict their BLEU scores for PBSMT and HPBSMT. Our motivation came from the comparison of a sequential learning system and a parser-based system. The typical decoder of the former is a Figure 1: Figure shows the difference of sentencebased performance between PBSMT and HPBSMT systems. Relation of Complexity of Source Sentence and Performance of HPBSMT and PBSMT It is interesting to note that PBSMT tends to be better than HPBSMT for European language pairs as the recent WMT workshop shows, while HPBSMT shows often better performance for distant language pairs such as EN-JP (Okita et al., 2010b) 178 Viterbi decoder while that of the latter is a CockeYounger-Kasami (CYK) decoder (Younger, 1967). The capability of these two systems provides an intuition about the difference of PBSMT and HPBSMT: the CYK decoder-based system has some capability to handle syntactic constructions while the Viterbi decoder-based system has only the capability of learning a sequence. For exties caused by inserted clauses, coordination, long Multiword Expressions, and parentheses, while the sequential learning system does not (This is since this is what the aim of the context-free grammar-based system is.)"
W13-2222,de-marneffe-etal-2006-generating,0,0.0353283,"Missing"
W13-2222,W12-5706,1,0.901057,"Missing"
W13-2222,N03-1017,0,0.00761191,"Missing"
W13-2222,W12-5705,1,0.900608,"Missing"
W13-2222,P07-2045,0,0.00934318,"and Gi is a base measure. Note that these terms belong to the hierarchical Pitman-Yor language model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing"
W13-2222,W12-5707,1,0.909208,"that the sentence is to be translated by genre IDsensitive MT systems, again based on semantics on a sentence level. The context-dependent LM can be interpreted as supplying the local context to a word, capturing semantics on a word level. The architecture presented in this paper is substantially different from multi-engine system combination. Although the system has multiple paths, only one path is chosen at decoding when processing unseen data. Note that standard multi-engine system combination using these three semantics has been presented before (Okita et al., 2012b; Okita et al., 2012a; Okita, 2012). This paper also compares the two approaches. The remainder of this paper is organized as follows. Section 2 describes the motivation for our approach. In Section 3, we describe our proposed systems, while in Section 4 we describe the experimental results. We conclude in Section 5. This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation (WMT 13). The system uses PBSMT and"
W13-2222,W04-3250,0,0.0242566,") and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem-inform) shows our results. The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst. For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results. These improvements over both of PBSMT and HPBSMT are statistically significant by a paired bootstrap test (Koehn, 2004). 5 Conclusion This paper describes shallow semanticallyinformed HPBSMT and PBSMT systems developed at Dublin City University for participation in the translation task at the Workshop on Statistical Machine Translation (WMT 13). Our system has 181 EN-ES BLEU BLEU(11b) BLEU-cased BLEU-cased(11b) NIST Meteor WER PER ES-EN BLEU BLEU(11b) BLEU-cased BLEU-cased(11b) NIST Meteor WER PER sem-inform 30.3 30.3 29.0 29.0 7.91 0.580 53.7 41.3 sem-inform 31.1 31.1 29.7 29.7 7.87 0.615 54.8 41.3 PBSMT 29.5 29.5 28.4 28.4 7.74 0.579 55.4 42.4 PBSMT 30.4 30.4 29.1 29.1 7.79 0.612 55.4 41.8 HPBSMT 28.2 28.2 2"
W13-2222,2005.mtsummit-papers.11,0,0.00611639,"d with SRILM (Stolcke, 2002). For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used. Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005). The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm. We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For the corpus, we used all the resources provided for the translation task at WMT13 for language model, that is parallel corpora (Europarl V7 (Koehn, 2005), Common Crawl corpus, UN corpus, and News Commentary) and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012). Experimental results are shown in Table 2. The left-most column (sem-inform) shows our results. The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst. For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results. These improvements over both of PBSMT and HPBSMT are s"
W13-2222,P07-1040,0,0.0564181,"Missing"
W13-2222,J10-4005,0,0.0147593,". predictQEScore() predictGenreID() predictContextID(wordi , wordi−1 ) If we assume that this is one major difference between these two systems, the complexity of the input sentence will correlate with the difference of translation quality of these two systems. In this subsection, we assume that this is one major difference of these two systems and that the complexity of the input sentence will correlate with the difference of translation quality of these two systems. Based on these assumptions, we build a regressor Table 1: Decoding algorithm: the main algorithm of PBSMT and HPBSMT are from (Koehn, 2010). The modification is related to predictQEScore(), predictGenreID(), and predictContextID(). ample, the (context-free) grammar-based system has the capability of handling various difficul179 for each system for a given input sentence where in a training phase we supply the BLEU score measured using the training set. One remark is that the BLEU score which we predict is only meaningful in a relative manner since we actually generate a translation output in preparation phase (there is a dependency to the mean of BLEU score in the training set). Nevertheless, this is still meaningful as a relativ"
W13-2222,W12-2702,0,0.0199701,"lists, run a MERT process to obtain the best weights based on the n-best lists, (Repeat the decoding / MERT process several iterations. Then, we obtain the best weights for a particular class.) For the test phase, 1. Separate each class of the test set (keep the original index and new index in the allocated separated dataset). 2. Suppose the test sentence belongs to cluster i, run the decoder of cluster i. 3. Repeat the previous step until all the test sentences are decoded. 3.3 Context ID Context ID semantics is used through the reranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012). 2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blunsom and Cohn, 2011) which is a nonparametric 1. For each label c ∈ {1, . . . C}, sample a distribution over word-types φc ∼ Dirichlet(·|β) 2. For each document d ∈ {1, . . . , D} (a) Sample a distribution over its observed labels θd ∼ Dirichlet(·|α) (b) For each word i ∈ {1, . . . , NdW } 1 Currently, we do not have a definite recommendation on this. It needs to be studied more deeply. 180 Bayesian method using hierarchical Pitman-Yor prior. In the 2-layer LM, the hidden sequence of the first layer"
W13-2222,P06-1124,0,0.0130387,"tput layer. The generative model for this is shown below. ¯ t ∼ F (φ¯s ) (1) ht |h t wt |ht ∼ F (φst ) wi |w1:i−1 ∼ PY(di , θi , Gi ) discrete representation N continuous−space language model [Schwenk, 2007] hidden layer N output layer probability estimation neural network 2nd hidden layer 2nd RBM 1st hidden layer 1st RBM output layer (2) ngram language model output layer ngram language model (3) 2−layer ngram−HMM language model where α is a concentration parameter, θ is a strength parameter, and Gi is a base measure. Note that these terms belong to the hierarchical Pitman-Yor language model (Teh, 2006). We used a blocked inference for inference. The performance of 2-layer LM is shown in Table 3. 4 P projection layer 2−layer conditional RBM language model Figure 3: Figure shows the three kinds of contextdependent LM. The upper-side shows continuousspace language model (Schwenk, 2007). The lower-left shows ours, i.e. the 2-layer ngramHMM LM. The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009). Experimental Settings We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments. The GIZA++ implementation (Och and Ney, 2003"
W14-5114,W10-3208,1,0.839958,"al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and"
W14-5114,strapparava-valitutti-2004-wordnet,0,0.263172,"Missing"
W14-5114,D08-1014,0,0.0219678,"cy information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus"
W14-5114,P05-1033,0,0.267679,"Missing"
W14-5114,W04-3248,0,0.0173424,"nment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Ba"
W14-5114,J04-4002,0,0.224891,"Missing"
W14-5114,C08-1125,0,0.0655339,"Missing"
W14-5114,C04-1071,0,0.0377596,"ation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus containing 23,492 parallel sentences comprising of 488,026 word tokens from the travel and tourism domain has been used. We randomly selected 500 sentences each for the development set and the test set from the initial parallel corpus. The rest of the sentences were used as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence le"
W14-5114,P02-1040,0,0.107491,"rie Actions) (Grant No. 317471) and the “Development of English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful"
W14-5114,N10-1029,0,0.0575798,"Missing"
W14-5114,C94-2178,0,0.372118,"Missing"
W14-5114,J93-2003,0,0.107473,"Missing"
W14-5114,N03-1017,0,0.107016,"carried out using the positional information of sentiment components. The rest of the paper is organized in the following manner. Section 2 briefly elaborates the related work. Section 3 provides an overview of the dataset used in our experiments. The proposed system is described in Section 4 while Section 5 provides the system setup for the various experiments. Section 6 includes the experiments and results obtained. Finally, Section 7 concludes and provides avenues for further work. 2 Related Work SMT systems have undergone considerable improvements over the years. Moreover, PB-SMT models (Koehn et al., 2003) outperform wordbased models. The alignment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chin"
W14-5114,W04-3250,0,0.269403,"Missing"
W14-5114,2013.mtsummit-papers.8,1,0.870543,"Missing"
W14-5114,W05-0909,0,0.0587776,"English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful for automatic post-editing the MT output. In either case, we"
W14-5114,C04-1200,0,0.0836786,"g the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectiv"
W14-5114,W06-0301,0,0.104449,"Missing"
W14-5114,baccianella-etal-2010-sentiwordnet,0,0.071767,"English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been dev"
W14-5114,J93-1003,0,0.359133,"Missing"
W14-5114,J93-1004,0,0.625572,"Missing"
W15-3017,W11-2123,0,0.0206342,"lude some NEs that are already there in the parallel NE list, however they might remain untranslated during decoding. Our system post processed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the"
W15-3017,W99-0604,0,0.39787,"Missing"
W15-3017,N03-1017,0,0.0361081,"etup • System 4: System 3 with LM2 . 3.1 Baseline Settings • System 5: System 3 with LM3 . The effectiveness of the present work is demonstrated by using the standard log-linear PB-SMT model as our baseline system. For building the baseline system, we used a maximum phrase length of 7 and a 5-gram language model. The other experimental settings were: SymGIZA++ aligner (Junczys-Dowmunt and Szał, 2012), which is a modified version of GIZA++ word alignment models by updating the symmetrizing models between chosen iterations of the original word alignment training algorithms and phraseextraction (Koehn et al., 2003). The reordering model was trained on hier-mslr-bidirectional (i.e. using both forward and backward models) and • System 6: System 3 with LM4 . System 6 provides the individual best system. System combination (System-7 in Table 1) of the 6 best performing individual systems brings considerable improvements over each of the individual component systems. 5 Conclusions and Future Work A hybrid system (System 6) with NE alignment, EBMT phrases, single-tokenized source MWEs, and MIRA-MERT coupled tuning results in the best performing system. However, confusion 155 Systems Baseline System 1 System 2"
W15-3017,J10-4005,0,0.0383815,"s the best performance of each individual system in a multi-engine pipeline. 1 Introduction In this paper, we present Universit¨at des Saarlandes (UdS) submission (named UdS-Sant) to WMT 2015 using a Hybrid MT framework. We participated in the generic translation shared task for the English-German (EN-DE) language pair. Corpus-based MT (CBMT) has delivered progressively improved quality translations since its inception. There are two main approaches to corpus-based MT – Example Based Machine Translation (EBMT) (Carl and Way, 2003) and Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn, 2010). Out of these two, in terms of large-scale evaluations, SMT is the most successful MT paradigm. However, each approach has its own advantages and disadvantages along with its own methods of applying and acquiring translation knowledge from the bilingual parallel training data. EBMT phrases tend to be more linguistically motivated compared to SMT phrases which essentially operate on n-grams. The knowledge extraction as well as representation process, 152 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 152–157, c Lisboa, Portugal, 17-18 September 2015. 2015 Associati"
W15-3017,N04-1022,0,0.154869,"re tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior"
W15-3017,E06-1005,0,0.0279832,"iple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights (Pal et al., 2014). 3 4 Results and Analysis As described in Section 2.2.1, we developed 16 different systems. Instead of using all these 16 different systems, we apply only the 6 best perfor"
W15-3017,J93-2003,0,0.0424795,"hough, SMT is the most popular MT paradigm, it sometimes fails to deliver sufficient quality in translation output for some languages, since each language has its own difficulties. Multiword Expressions (MWEs) and Named Entities (NEs) offer challenges within a language. MWEs are defined as idiosyncratic interpretations that cross word boundaries (Sag et al., 2002). Named entities on the other hand often consist of more than one word, so that they can be considered as a specific type of MWEs such as noun compounds (Jackendoff, 1997). Traditional approaches to word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a"
W15-3017,P03-1021,0,0.503229,"ssed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) fram"
W15-3017,N12-1047,0,0.0184546,"might remain untranslated during decoding. Our system post processed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Baye"
W15-3017,W10-3707,1,0.907834,"o word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide any additional advantage in terms of translation quality and performance. The remainder of the paper is organised as follows. Section 2 details the components of our system, in particular named entity extraction, translation memory, and EBMT, followed by description of 3 types of Hybrid systems and the system combination module. In Section 3, we outline the comple"
W15-3017,2011.mtsummit-papers.23,1,0.815387,"uch as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide any additional advantage in terms of translation quality and performance. The remainder of the paper is organised as follows. Section 2 details the components of our system, in particular named entity extraction, translation memory, and EBMT, followed by description of 3 types of Hybrid systems and the system combination module. In Section 3, we outline the complete experimental set"
W15-3017,P02-1040,0,0.0921099,"-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is ap"
W15-3017,2006.amta-papers.25,0,0.0454059,"ion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights (Pal et al., 2014). 3 4 Results and Analysis As described in Section 2.2.1, we developed 16 different systems. Instead of using all these 16 different systems, we apply only the 6 best performing systems for system combination. Performance is measured on the devset. Table 1 reports the final evaluation results obtained on the test dataset. The best 6 systems a"
W15-3017,W14-3323,1,0.798583,"ve followed Point-wise Mutual InforOur system is designed with three basic components: (i) preprocessing, (ii) hybrid systems and (iii) system combination. 2.1 Preprocessing Data pre-processing plays a very crucial part in any data-driven approach. We carried out preprocessing in two steps: • Cleaning and clustering sentences based on sentence length. • Effective preprocessing of data in the form of explicit alignment of bilingual terminology (viz. NEs and MWEs). The preprocessing has been shown (cf. Section 2.1.2) to improve the output quality of the baseline PB-SMT system (Pal et al., 2013; Tan and Pal, 2014). 2.1.1 Corpus cleaning We utilized all the parallel training data provided by the WMT 2015 shared task organizers for English–German translation. The training data include Europarl, News Commentary and Common Crawl. The provided corpus is noisy and contains some non-German as well as non-English words and sentences. Therefore, we applied a Language Identifier (Shuyo, 2010) on both bilingual English–German parallel data and monolingual German corpora. We discarded those parallel sentences from the bilingual training data which were detected as belonging to some different language by the langua"
W15-3017,C96-2141,0,0.420797,"ressions (MWEs) and Named Entities (NEs) offer challenges within a language. MWEs are defined as idiosyncratic interpretations that cross word boundaries (Sag et al., 2002). Named entities on the other hand often consist of more than one word, so that they can be considered as a specific type of MWEs such as noun compounds (Jackendoff, 1997). Traditional approaches to word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide"
W15-3017,W07-0734,0,\N,Missing
W15-3017,W09-0416,0,\N,Missing
W15-3017,W10-1720,1,\N,Missing
W15-3026,W06-1607,0,0.072029,"ning algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk ="
W15-3026,D08-1089,0,0.0720062,"nted with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denot"
W15-3026,W11-2123,0,0.102511,"rase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k ="
W15-3026,N03-1017,0,0.00755213,"tings The effectiveness of the present work is demonstrated by using the standard log-linear PBSMT model. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training"
W15-3026,J10-4005,0,0.0634225,"stemmed using the Porter stemmer • WN synonymy: maps if they are considered synonyms in WordNet If multiple alignments exist, METEOR selects the alignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al.,"
W15-3026,N12-1047,0,0.0310726,"rarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk = fb ...fj Data Table 1 presents the statistics of the training, development and test sets released for the English– Spanish SAPE Task orga"
W15-3026,N09-2055,0,0.121014,"Missing"
W15-3026,W07-0734,0,0.742474,"entence aligned training data provided by the organizers of the WMT2015 APE task. The training data consist of 11,272 parallel segments of English to Spanish MT translations as well as the post-edited translations of the MT output. The English source text, 217 the machine translated Spanish output and the corresponding post-edited version contain 238,335, 257,644 and 257,881 tokens respectively. The preprocessing of the training corpus was carried out first by stemming the Spanish MT output and the PE data using Freeling (Padr´o and Stanilovsky, 2012). 3.2 3.2.1 T ER(H, R) = METEOR Alignment (Lavie and Agarwal, 2007) is also an automatic MT evaluation metric which provides an alignment between hypothesis (here the MT output) and reference (here the PE translation). Given a pair of strings such as H and R to be compared, METEOR initially establishes a word alignment between them. The alignment is provided by a mapping method between the words in the hypothesis H an reference R transaltion, which is built incrementally by the following sequence of word-mapping modules: Hybrid Word Alignment Statistical Word Alignment GIZA++ (Och and Ney, 2003) is a statistical word alignment tool which implements maximum li"
W15-3026,N10-1062,0,0.0224964,"ting the MT decoder decide whether the errors should be corrected and about the method of correcting it. Parton et al. (2012) evaluated their approach with human evaluators and found that the adequacy of post-edited MT output improved both for rule-based and feedback APE. In terms of fluency the human evaluation has shown that adequacy increase in feedback APE is related to fluency but not for rule-based APE. Denkowski (2015) has developed a method for integrating in real time post-edited MT output into a translation model, by extracting for each input sentence a grammar. The method, based on Levenberg et al. (2010) and Lopez (2008), allows the indexing of the the source and post-edited MT output, as well as the union of the already existing sentence pairs with the new post-edited data. The system can also remember the rules that are consistent with the post-edited data. This way, rules learned from human corections can be preferred. The experiments Denkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of aut"
W15-3026,N07-1064,0,0.424423,"enkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of automatic post-editing (APE) is to improve the output of MT by post-processing it. One of the first approaches was the one introduced by Chen and Chen (1997) who proposed a combination of rule-based MT (RBMT) and statistical MT (SMT) systems aiming at merging the positive properties of each system type for a better machine translation output. Simard et al. (2007a) and Simard et al. (2007b) have shown how a PBSMT system can be used for automatic post-editing of an RBMT system for translations from English to French and French to English. Because RBMT systems tend to produce repetitive errors, they train a SMT system to correct errors, with the aim of reducing the postediting effort. The SMT system trains on the output of the RBMT system as the source language and the reference human translations as the target language. The evaluation of their system shows that the post-edited output had a better quality than the output of the RBMT system as well as th"
W15-3026,N06-1014,0,0.353606,"in WordNet If multiple alignments exist, METEOR selects the alignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such"
W15-3026,W07-0728,0,0.135807,"enkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of automatic post-editing (APE) is to improve the output of MT by post-processing it. One of the first approaches was the one introduced by Chen and Chen (1997) who proposed a combination of rule-based MT (RBMT) and statistical MT (SMT) systems aiming at merging the positive properties of each system type for a better machine translation output. Simard et al. (2007a) and Simard et al. (2007b) have shown how a PBSMT system can be used for automatic post-editing of an RBMT system for translations from English to French and French to English. Because RBMT systems tend to produce repetitive errors, they train a SMT system to correct errors, with the aim of reducing the postediting effort. The SMT system trains on the output of the RBMT system as the source language and the reference human translations as the target language. The evaluation of their system shows that the post-edited output had a better quality than the output of the RBMT system as well as th"
W15-3026,2006.amta-papers.25,0,0.11155,"ignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such as the reference (in this case the PE translation) and the hyp"
W15-3026,J03-1002,0,0.0404429,"anilovsky, 2012). 3.2 3.2.1 T ER(H, R) = METEOR Alignment (Lavie and Agarwal, 2007) is also an automatic MT evaluation metric which provides an alignment between hypothesis (here the MT output) and reference (here the PE translation). Given a pair of strings such as H and R to be compared, METEOR initially establishes a word alignment between them. The alignment is provided by a mapping method between the words in the hypothesis H an reference R transaltion, which is built incrementally by the following sequence of word-mapping modules: Hybrid Word Alignment Statistical Word Alignment GIZA++ (Och and Ney, 2003) is a statistical word alignment tool which implements maximum likelihood estimators for all the IBM-1 to IBM-5 models, a HMM alignment model as well as the IBM-6 model covering many to many alignments. GIZA++ facilitates fast development of statistical machine translation (SMT) systems. Like GIZA++, the Berkley Aligner (Liang et al., 2006) is also used to align words across sentence pairs. The Berkeley word aligner uses an extension of Cross Expectation Maximization and is jointly trained with HMM models. We use a third statistical word aligner called SymGiza++ (JunczysDowmunt and Szał, 2012)"
W15-3026,P03-1021,0,0.126312,"ordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk = fb ...fj Data Table 1 presents the statistics of the training, development and test"
W15-3026,W15-4921,1,0.884399,"Missing"
W15-3026,padro-stanilovsky-2012-freeling,0,0.0343196,"Missing"
W15-3026,W14-0314,1,0.613986,"ection of repetitive errors in the MT output, various automatic or semi-automatic post-processing or automatic PE techniques have been developed. Although MT output needs to be post-edited by humans to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010), it is faster and cheaper to post-edit MT output than to perform human translation from scratch. In some cases, recent studies have shown that the quality of MT output plus PE can exceed the quality of human translation (Fiederer and O’Brien, 2009; Koehn, 2009; De Palma and Kelly, 2009) as well as the productivity (Zampieri and Vela, 2014). Aimed at cost-effective and timesaving use of MT, the PE process needs to be further optimised (TAUS/CNGL Report, 2010). Post-editing can be also used as a MT evaluation method, implying at least source and target language skills, different from ranking, that does nor require specific skills, a homogeneous group of evaluators beIntroduction In this paper, we present the submission of Saarland University (USAAR) to the WMT2015 APE task. The system combines a hybrid word alignment system implementation with a monolingual PBSMT for the language pair English-Spanish (EN-ES), translating from Eng"
W15-3026,W13-2814,1,0.716633,"links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such as the reference (in this case the PE translation) and the hypothesis (MT output). In the our work, the reference string has been chosen to be the confusion network s"
W15-3026,P02-1040,0,0.100602,"single where h phrase-pair. It thus follows (8): M X m=1 λm K X ˆ m (fˆk , eˆk , sk ) = h k=1 ˆ= where h K X k=1 PK k=1 Tokens ES-MT 257,644 23,213 40,925 ES-PE 257,881 23,098 – Experimental Settings The effectiveness of the present work is demonstrated by using the standard log-linear PBSMT model. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To allevia"
W15-3026,C10-2109,0,0.0179871,"ng raw MT output, before performing human post-editing on it. The objective is to decreases the amount of errors produced by the MT systems, achieving in the end a productivity increase in the translation process. 216 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 216–221, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. methodologies, a rule-abased APE and a feedback APE. The rule-based APE performs either insertions or replacement to address an identified error. The feedback APE, an approach similar to the one proposed by Parton and McKeown (2010), passes the possible correction to the MT system, letting the MT decoder decide whether the errors should be corrected and about the method of correcting it. Parton et al. (2012) evaluated their approach with human evaluators and found that the adequacy of post-edited MT output improved both for rule-based and feedback APE. In terms of fluency the human evaluation has shown that adequacy increase in feedback APE is related to fluency but not for rule-based APE. Denkowski (2015) has developed a method for integrating in real time post-edited MT output into a translation model, by extracting fo"
W15-3026,2012.eamt-1.34,0,0.114117,"Missing"
W15-3026,W12-3146,0,0.195552,"Missing"
W15-3026,W11-2152,0,\N,Missing
W15-3026,O96-2005,0,\N,Missing
W15-3026,2015.eamt-1.22,1,\N,Missing
W15-3047,P11-1103,0,0.0240462,"oes not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and 380 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 380–384, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. For our participation in the WMT-15 task, we used our metric ReVal (Gupta et al., 2015). ReVal metric is based on dense vector spaces and Tree Long Short Term Memory networks. This metric achieved state of the art results for the WMT-14 dataset. The metric including training data is available at https://github.com/rohitguptacs/ReVal. Figure 1 shows simple LSTM and Tree-LSTM architectures. 3 We used the ReVal (Gupta et al., 2015) metric for this ta"
W15-3047,de-marneffe-etal-2006-generating,0,0.0136679,"Missing"
W15-3047,W14-3348,0,0.0716929,"ing to achieve the best results. We use a metric based on dense vector spaces and Long Short Term Memory (LSTM) networks, which are types of Recurrent Neural Networks (RNNs). For WMT15 our new metric is the best performing metric overall according to Spearman and Pearson (Pre-TrueSkill) and second best according to Pearson (TrueSkill) system level correlation. 1 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best performing metrics in the WMT14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DiskoTK-Party-Tuned (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from the"
W15-3047,D14-1162,0,0.0818639,"e similarity score of a training pair. For example, for y = 2.7, pT = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. To compute our training data we automatically convert the human rankings of the WMT-13 evaluation data into similarity scores between the reference and the translation. These translationreference pairs labelled with similarity scores are used for training. We also augment the WMT-13 data with 4500 pairs from the SICK training set (Marelli et al., 2014), resulting in a training dataset of 14059 pairs in total. The metric uses Glove word vectors (Pennington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by Tai et al. (2015). Training is performed using a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The memory dimension is 300, hidden dimension is 100 and compositional parameters are 541,800. Training is performed for 10 epochs. System level scores are computed by aggregating and normalising the segment level scores. Full details can be found in (Gupta et al., 2015).2 and Spearman system-level correlation and second best overall using Pearson (TrueSkill) correlation. Table 2 s"
W15-3047,W14-3350,0,0.0211986,"and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric (Gupta et al., 2015) is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and 380 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 380–384, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. For our participation in the WMT-15 task, we used our metric ReVal (Gupta et al., 2015). ReVal metric is based on dense vector spaces and Tree Long Short Term Memory networks. This metric achieved state of the art results for the WMT-14 dataset. The metric including training data"
W15-3047,P13-1045,0,0.0129288,"metrics and twelve different metrics from the ASIYA MT evaluation toolkit (Gim´enez and M`arquez, 2010). The metric computes the number of common sub-trees between a reference and a translation using a convolution tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or Recurrent Neural Networks (RNNs) are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional measures can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric (Gupta et al., 2015) is simpl"
W15-3047,D15-1124,1,0.232325,"Missing"
W15-3047,D13-1170,0,0.00652067,"Missing"
W15-3047,P15-1150,0,0.0394074,"Missing"
W15-3047,W14-3352,0,0.0432354,"Missing"
W15-3047,W14-3336,0,0.165965,"Missing"
W15-3047,marelli-etal-2014-sick,0,0.025601,".041 .861 ± .051 .893 ± .035 .888 ± .040 .796 ± .052 .768 ± .054 .747 ± .057 for 1 ≤ j ≤ K. Where, y ∈ [1, K] is the similarity score of a training pair. For example, for y = 2.7, pT = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. To compute our training data we automatically convert the human rankings of the WMT-13 evaluation data into similarity scores between the reference and the translation. These translationreference pairs labelled with similarity scores are used for training. We also augment the WMT-13 data with 4500 pairs from the SICK training set (Marelli et al., 2014), resulting in a training dataset of 14059 pairs in total. The metric uses Glove word vectors (Pennington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by Tai et al. (2015). Training is performed using a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The memory dimension is 300, hidden dimension is 100 and compositional parameters are 541,800. Training is performed for 10 epochs. System level scores are computed by aggregating and normalising the segment level scores. Full details can be found in (Gupta et al.,"
W15-3047,P02-1040,0,0.108963,"valuation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve the best results. We use a metric based on dense vector spaces and Long Short Term Memory (LSTM) networks, which are types of Recurrent Neural Networks (RNNs). For WMT15 our new metric is the best performing metric overall according to Spearman and Pearson (Pre-TrueSkill) and second best according to Pearson (TrueSkill) system level correlation. 1 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best performing metrics in the WMT14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DiskoTK-Party-Tuned (Jo"
W15-3047,W13-2201,0,\N,Missing
W15-4105,D08-1089,0,0.351286,"Missing"
W15-4105,W08-0509,0,0.172715,"T system on the WAT test set. The leftmost columns indicate the number of times a dictionary is appended to the parallel training data (Baseline = 0 times, Passive x1 = 1 time). The rightmost columns present the results from both the passive and pervasive use of dictionary translations, with exception to the top-right cell which shows the baseline result of the pervasive dictionary usage without appending any dictionary. • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008) • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn et al., 2005 and Galley and Manning, 2008) Baseline Passive x1 Passive x2 Passive x3 Passive x4 Passive x5 • Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995) • Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoding parameters. + Pervasive 16.87 17.30∗∗ 16.87 17.06 17.38∗∗ 17.29∗∗ Table 2: BLEU Scores for Passive and Pervasive Use of the Dictionary in SMT (Japanese to English) • Fo"
W15-4105,W07-0733,0,0.0411937,"e given the source sentence, p(f|e), and the a priori probability of the translation, pLM (e) (Brown, 1993). ebest = argmax p(e|f ) e = argmax p(f |e) e pLM (e) State-of-art SMT systems rely on (i) large bilingual corpora to train the translation model p(f|e) and (ii) monolingual corpora to build the language model, pLM (e). One approach to improve the translation model is to extend the parallel data with a bilingual dictionary prior to training the model. The primary motivation to use additional lexical information for domain adaptation to overcome the out-ofvocabulary words during decoding (Koehn and Schroeder, 2007; Meng et al. 2014; Wu et al. 2008). Alternatively, adding in-domain lexicon to 30 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 30–34, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics translation. 2 In such a situation, where the dictionary does not provide a translation for the complete multiword string, we set the preference for the dictionary entry with the longest length in the direction from left to right and select “magnetic sensor” + “system” entries for forced translation.1 Finally, we investigate the effe"
W15-4105,N03-1017,0,0.0563605,"the phrase-based SMT system on the WAT test set. The leftmost columns indicate the number of times a dictionary is appended to the parallel training data (Baseline = 0 times, Passive x1 = 1 time). The rightmost columns present the results from both the passive and pervasive use of dictionary translations, with exception to the top-right cell which shows the baseline result of the pervasive dictionary usage without appending any dictionary. • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008) • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn et al., 2005 and Galley and Manning, 2008) Baseline Passive x1 Passive x2 Passive x3 Passive x4 Passive x5 • Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995) • Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoding parameters. + Pervasive 16.87 17.30∗∗ 16.87 17.06 17.38∗∗ 17.29∗∗ Table 2: BLEU Scores for Passive and Pervasive Use of the Dictionary in SMT (Jap"
W15-4105,2005.mtsummit-papers.11,0,0.334378,"Missing"
W15-4105,C08-1125,0,0.0710785,"Missing"
W15-4105,D14-1060,0,0.0166551,", p(f|e), and the a priori probability of the translation, pLM (e) (Brown, 1993). ebest = argmax p(e|f ) e = argmax p(f |e) e pLM (e) State-of-art SMT systems rely on (i) large bilingual corpora to train the translation model p(f|e) and (ii) monolingual corpora to build the language model, pLM (e). One approach to improve the translation model is to extend the parallel data with a bilingual dictionary prior to training the model. The primary motivation to use additional lexical information for domain adaptation to overcome the out-ofvocabulary words during decoding (Koehn and Schroeder, 2007; Meng et al. 2014; Wu et al. 2008). Alternatively, adding in-domain lexicon to 30 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 30–34, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics translation. 2 In such a situation, where the dictionary does not provide a translation for the complete multiword string, we set the preference for the dictionary entry with the longest length in the direction from left to right and select “magnetic sensor” + “system” entries for forced translation.1 Finally, we investigate the effects of using the b"
W15-4105,W14-7001,0,0.214297,"Missing"
W15-4105,J03-1002,0,0.0213841,"lation outputs from the phrase-based SMT system on the WAT test set. The leftmost columns indicate the number of times a dictionary is appended to the parallel training data (Baseline = 0 times, Passive x1 = 1 time). The rightmost columns present the results from both the passive and pervasive use of dictionary translations, with exception to the top-right cell which shows the baseline result of the pervasive dictionary usage without appending any dictionary. • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008) • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn et al., 2005 and Galley and Manning, 2008) Baseline Passive x1 Passive x2 Passive x3 Passive x4 Passive x5 • Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995) • Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoding parameters. + Pervasive 16.87 17.30∗∗ 16.87 17.06 17.38∗∗ 17.29∗∗ Table 2: BLEU Scores for Passive and Pervasive Use of the Di"
W15-4105,W14-3323,1,0.891614,"Missing"
W15-4105,2011.eamt-1.10,0,0.0193629,".de, josef.van genabith@dfki.de, bond@ieee.org Abstract parallel data has also shown to improve SMT. The intuition is that by adding extra counts of bilingual lexical entries, the word alignment accuracy improves, resulting in a better translation model (Skadins et al. 2013; Tan and Pal, 2014; Tan and Bond, 2014). Another approach to use a bilingual dictionary is to hijack the decoding process and force word/phrase translations as per the dictionary entries. Previous researches used this approach to explore various improvements in industrial and academic translation experiments. For instance, Tezcan and Vandeghinste (2011) injected a bilingual dictionary in the SMT decoding process and integrated it with Computer Assisted Translation (CAT) environment to translate documents in the technical domain. They showed that using a dictionary in decoding improves machine translation output and reduces post-editing time of human translators. Carpuat (2009) experimented with translating sentences in discourse context by using a discourse specific dictionary annotations to resolve lexical ambiguities and showed that this can potentially improve translation quality. In this paper, we investigate the improvements made by bot"
W15-4105,vogel-monson-2004-augmenting,0,0.0535445,"Missing"
W15-4105,N06-1001,0,0.0346588,"nd Galley and Manning, 2008) Baseline Passive x1 Passive x2 Passive x3 Passive x4 Passive x5 • Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995) • Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoding parameters. + Pervasive 16.87 17.30∗∗ 16.87 17.06 17.38∗∗ 17.29∗∗ Table 2: BLEU Scores for Passive and Pervasive Use of the Dictionary in SMT (Japanese to English) • For English translations, we trained a truecasing model to keep/reduce tokens’ capitalization to their statistical canonical form (Wang et al., 2006; Lita et al., 2003) and we recased the translation output after the decoding process By repeatedly appending the dictionary to the parallel data, the BLEU scores significantly4 improves from 16.75 to 17.31. Although the system’s performance degrades when adding the dictionary passively thrice, the score remains significantly better than baseline. The pervasive use of the dictionary improves the baseline without the passive of the dictionary. The best performance is achieved when the dictionary is passively added four times with the pervasive use of the dictionary during decoding. The fluctuat"
W15-4105,W09-2404,0,\N,Missing
W15-4105,P03-1020,0,\N,Missing
W15-4105,W11-2123,0,\N,Missing
W15-4105,P13-2121,0,\N,Missing
W15-4905,P02-1040,0,0.108875,"hich use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects t"
W15-4905,P06-1055,0,0.00708211,"at we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of G"
W15-4905,1999.mtsummit-1.48,0,0.202963,"rom scratch when an exact match is not available. However, this retrieval process is still limited to editdistance based measures operating on surface form c 2015 The authors. This article is licensed under a Creative  Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 35 Several researchers have used semantic or syntactic information in TMs, but their evaluations were shallow and most of the time limited to subjective evaluation carried out by the authors. This makes it hard to judge how much a semantically informed TM matching system can beneﬁt a translator. Existing research (Planas and Furuse, 1999; Hod´asz and Pohl, 2005; Pekar and Mitkov, 2007; Mitkov, 2008) pointed out the need for similarity 1 http://www.omegat.org calculations in TMs beyond surface form comparisons. Both Planas and Furuse (1999) and Hodasz and Pohl (2005) proposed to use lemma and parts of speech along with surface form comparison. Hodasz and Pohl (2005) also extend the matching process to a sentence skeleton where noun phrases are either tagged by a translator or by a heuristic NP aligner developed for English-Hungarian translation. Planas and Furuse (1999) tested a prototype model on 50 sentences from the softwar"
W15-4905,aziz-etal-2012-pet,0,0.0168428,"niﬁcantly improves the retrieval results. We have also observed that there are different paraphrases used to bring about this improvement. In the interval [70, 85), 169 different paraphrases are used to retrieve 98 additional segments. To check the quality of the retrieved segments human evaluations are carried out. The sets’ distribution for human evaluation is given in the Table 3. The sets contain randomly selected segments from the additionally retrieved segments using paraphrasing which changed their top ranking.2 TH Set1 Set2 Total 4.1 Familiarisation with the Tool We used the PET tool (Aziz et al., 2012) for all our human experiments. However, settings were changed depending on the experiment. To familiarise translators with the PET tool we carried out a pilot experiment before the actual experiment with the Europarl corpus. This experiment was 100 117 16 13.67 9 24 14 100 2 5 7 [85, 100) 6 4 10 [70, 85) 6 7 13 Total 14 16 30 Table 3: Test Sets for Human Experiments 2 The sets are constructed so that a translator can post-edit a ﬁle in one sitting. There is no differentiation between the evaluations based on sets and all evaluations are carried out in both sets in a similar fashion with diffe"
W15-4905,2012.amta-papers.26,0,0.142654,"TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in pos"
W15-4905,W14-3348,0,0.0312779,"Missing"
W15-4905,2006.amta-papers.25,0,0.154022,"Missing"
W15-4905,N13-1092,0,0.0373292,"is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of German with English language level C1, in the age group of 21 to 40 years with a majority of female"
W15-4905,R11-1014,0,0.014323,"tion metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into different types for efﬁcient implementation based on the matching of the words between the source and corresponding pa"
W15-4905,2014.eamt-1.2,1,0.494135,"Missing"
W15-4905,2011.mtsummit-papers.37,0,0.157075,"eved was considered usable if less than half of the words required editing to match the input sentence. The authors concluded that the approach gives more usable results compared to Trados Workbench used as a baseline. Hodasz and Pohl (2005) claimed that their approach stores simpliﬁed patterns and hence makes it more probable to ﬁnd a match in the TM. Pekar and Mitkov (2007) presented an approach based on syntactic transformation rules. On evaluation of the prototype model using a query sentence, the authors found that the syntactic rules help in retrieving better segments. Recently, work by Utiyama et al. (2011) and Gupta and Or˘asan (2014) presented approaches which use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated us"
W15-4905,2005.mtsummit-papers.11,0,0.0396766,"etter. 17 translators participated in this experiment. Finally, the decision of whether ‘ED is better’ or ‘PP is better’ is made on the basis of how many translators choose one over the other. 3.3 Subjective Evaluation with Three Options (SE3) This evaluation is similar to Evaluation SE2 except that we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing"
W15-4905,2012.amta-wptp.2,0,0.0132314,"chine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into diffe"
W15-4905,W14-0314,1,0.749112,"ves highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into different types for efﬁcient implementation based on the matching of the words between the source and corresponding paraphrase. Using this approach, the fuzzy match score between segments can be calculated in polynomial time despite the inclusion of paraphrases. The method uses dynamic programming along with greedy approximation. The meth"
W15-4905,2012.eamt-1.31,0,\N,Missing
W15-4905,2012.tc-1.5,0,\N,Missing
W15-4916,2012.eamt-1.33,1,0.851202,"Missing"
W15-4916,C04-1046,0,0.147834,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
W15-4916,W12-3156,0,0.103809,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
W15-4916,P14-1065,0,0.0528997,"Missing"
W15-4916,P10-1064,1,0.914135,"Missing"
W15-4916,P14-2047,0,0.0154408,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
W15-4916,W13-3303,0,0.0260337,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
W15-4916,P02-1040,0,0.0919032,"c and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
W15-4916,P09-2004,0,0.0711715,"ndomly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (ﬁlter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further ﬁltered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (ﬁlter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each ﬁlter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also rando"
W15-4916,potet-etal-2012-collection,0,0.104329,"Missing"
W15-4916,2014.eamt-1.21,1,0.850194,"consider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
W15-4916,2006.amta-papers.25,0,0.549835,"es on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the ﬁnal version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quali"
W15-4916,P10-1063,0,0.677539,"than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
W15-4916,2009.eamt-1.5,1,0.889526,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
W15-4916,D14-1025,0,0.053746,"Missing"
W15-4916,C14-2028,0,\N,Missing
W15-4916,W13-2201,1,\N,Missing
W15-4921,W14-3351,0,0.0763214,"lity estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is HTER (Snover et al., 2006) which produces targeted refe"
W15-4921,2012.iwslt-papers.5,0,0.371952,"each ranking of the ﬁve MT outputs has the potential to produce 10 ranking pairs. Before applying the corresponding formulas on the data, the ranking pairs from all evaluators and for all systems are collected in a matrix like the one in Table 1. The matrix records the number of times system Si was ranked better than Sj and vice-versa. For example, if we look at the two systems S1 and S3 in the matrix, we can see that S3 was ranked 2 times higher (from the left triangle) and 4 times lower (from the right triangle) than system S1 . From the matrix, the ﬁnal score for each system - as deﬁned by Koehn (2012) and applied in WMT2013 - can be computed. From the matrix in Table 1 the score for system S1 is computed by counting for each pair of systems (S1 , S2 ), (S1 , S3 ), (S1 , S4 ), (S1 , S5 ) the number of times S1 was ranked higher than the other system divided by the total number of rankings for each pair. The results for each pair of systems including S1 are then 2 The implementation of a new tool was motivated by the accessibility of a server for the evaluators. This way each evaluator had his own evaluation set containing both the tool and the data set. S1 S2 S3 S4 S5 S1 0 0 2 4 1 S2 3 0 2"
W15-4921,P11-1023,0,0.0549574,", 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT output. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by postediting MT output. Human evaluation can also be performed by measuring post-editing time, or by asking evaluators to assess the ﬂuency and adequacy of a hypothesis translation on a Likert scal"
W15-4921,W12-3129,0,0.0514645,"vie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT output. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by postediting MT output. Human evaluation can also be performed by measuring post-editing time, or by asking evaluators to assess the ﬂuency and adequacy of a hypothesis translation on a Likert scale. Another popular human evaluation method is ranking: ord"
W15-4921,P02-1040,0,0.0971607,"chine translation evaluation is an important element in the process of building MT systems. The Workshop for Statistical Machine Translation (WMT) compares new techniques for MT through human and automatic MT evaluation and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and"
W15-4921,2006.amta-papers.25,0,0.0934176,"and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is"
W15-4921,W09-0441,0,0.0527948,"valuation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is HTER (Snover et al., 2006) whic"
W15-4921,W14-3347,0,0.0946567,"ost-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al., 2012) that measures meaning preservation between hypothesis and reference translation on the basis of verb frames and their role ﬁllers. Another method is HTER (Snover et al., 2006) which produces targeted reference translations by post-editing MT ou"
W15-4921,W14-3348,0,0.114013,"he process of building MT systems. The Workshop for Statistical Machine Translation (WMT) compares new techniques for MT through human and automatic MT evaluation and provides also tracks for evaluation metrics, quality estimation of MT as well as post-editing of MT. To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Denkowski and Lavie, 2014), WER (Levenshtein, 1966), position-independent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) and TERp (Snover et al., 2009). Gonzàlez et al. (2014) as well as Comelles and Atserias (2014) introduce their fully automatic approaches to machine translation evaluation using lexical, syntactic and semantic information when comparing the machine translation output with reference translations. Human machine translation evaluation can be performed with different methods. Lo and Wu (2011) propose HMEANT, a metric based on MEANT (Lo et al.,"
W15-5009,D08-1089,0,0.0619819,"th grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008)(Koehn et al., 2003; Och and Ney, 2003; Gao and Vogel, 2008) Baseline System Human evaluations were conducted as pairwise comparisons between translations from our system and the WAT organizers’ phrase-based statistical MT baseline system. Table 1 highlights the parameter differences between the organizers and our phrase-based SMT system. • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn, 2005; Galley and Manning, 2008) 3.1.2 Pairwise Comparison The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform. Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the phrase-based MT system. Five evaluators were asked to judge each document. The crowdsourced evaluators were non-experts, thus their judgements were not necessary precise, especially for patent translations. The evaluators were asked to judge whether the hypothesis or the baseline translati"
W15-5009,W08-0509,0,0.0230202,"al., 2003; Koehn et al., 2007) with the following vanilla Moses experimental settings: 3.1 The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform (WAT, 2014). Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system. 3.1.1 • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008)(Koehn et al., 2003; Och and Ney, 2003; Gao and Vogel, 2008) Baseline System Human evaluations were conducted as pairwise comparisons between translations from our system and the WAT organizers’ phrase-based statistical MT baseline system. Table 1 highlights the parameter differences between the organizers and our phrase-based SMT system. • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn, 2005; Galley and Manning, 2008) 3.1.2 Pairwise Comparison The human judgment scores for the WAT evaluations were acquired using the Lancers crow"
W15-5009,W07-0738,0,0.0231756,"０．００５％以上含有させることが好まし い。 2 Alternatively, researchers would choose to inflate the BLEU score to a range between 0 to 100 to improve readability of the scores without the decimal prefix. 75 2.2 score was introduced by adding a rank correlation coefficient3 prior to unigram matches without the need for higher order n-gram matches (Isozaki et al., 2010). Let us consider another example: Other Shades of BLEU / RIBES It is worth noting that there are other automatic MT evaluation metrics that depend on the same precision-based score with primary differences in how the Countmatch (ngram) is measured; Giménez and Màrquez (2007) described other linguistics features that one could match in place of surface n-grams, such as lexicalized syntactic parse features, semantic entities and roles annotations, etc. As such, the modified BLEU-like metrics can present other aspects of syntactic fluency and semantic adequacy complementary to the string-based BLEU. A different approach to improve upon the BLEU scores is to allow paraphrases or gappy variants and replace the proportion of Countmatch (ngram) / Count(ngram) by a lexical similarity measure. Banerjee and Lavie (2005) introduced the METEOR metric that allows hypotheses’"
W15-5009,P04-1079,0,0.0339468,"have better correlations with human judgements, but in our experiments it also fails to correlate with human judgements. In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores. 1 2 BLEU Papineni et al. (2002) originally define BLEU n-gram precision pn by summing the n-gram matches for every hypothesis sentence S in the test corpus C: Introduction Automatic Machine Translation (MT) evaluation metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006). However, the relatively consistent correlation of higher BLEU scores (Papineni et al., 2002) and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggests a better translation than its lower scoring counterparts (Bojar et al., 2014; Bojar et al., 2015; Nakazawa et al., 2014; Cettolo et al., 2014). Callison-Burch et al. (2006) has anecdotally presented possible failures of BLEU by showing examples of translations with the same BLEU score but of di"
W15-5009,N15-1124,0,0.0514605,"Missing"
W15-5009,W05-0909,0,0.0487073,"fferences in how the Countmatch (ngram) is measured; Giménez and Màrquez (2007) described other linguistics features that one could match in place of surface n-grams, such as lexicalized syntactic parse features, semantic entities and roles annotations, etc. As such, the modified BLEU-like metrics can present other aspects of syntactic fluency and semantic adequacy complementary to the string-based BLEU. A different approach to improve upon the BLEU scores is to allow paraphrases or gappy variants and replace the proportion of Countmatch (ngram) / Count(ngram) by a lexical similarity measure. Banerjee and Lavie (2005) introduced the METEOR metric that allows hypotheses’ n-grams to match paraphrases and stems instead of just the surface strings. Lin and Och (2004) presented the ROUGE-S metrics that uses skip-grams matches. More recently, pre-trained regression models based on semantic textual similarity and neural network-based similarity measures trained on skip-grams are applied to replace the n-gram matching (Vela and Tan, 2015; Gupta et al., 2015). While enriching the surface n-gram matching allows the automatic evaluation metric to handle variant translations, it does not resolves the “prominent cruden"
W15-5009,D15-1124,1,0.874112,"Missing"
W15-5009,W14-3302,0,0.0216278,"m precision pn by summing the n-gram matches for every hypothesis sentence S in the test corpus C: Introduction Automatic Machine Translation (MT) evaluation metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006). However, the relatively consistent correlation of higher BLEU scores (Papineni et al., 2002) and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggests a better translation than its lower scoring counterparts (Bojar et al., 2014; Bojar et al., 2015; Nakazawa et al., 2014; Cettolo et al., 2014). Callison-Burch et al. (2006) has anecdotally presented possible failures of BLEU by showing examples of translations with the same BLEU score but of different translation quality. Through P pn = S∈C P P ngram∈S S∈C P Countmatched (ngram) ngram∈S Count(ngram) (1) BLEU is a precision based metric; to emulate recall, the brevity penalty (BP) is introduced to compensate for the possibility of high precision translation that are too short. The BP is calculated as: 1 Meta-evaluation refers to the measurement of the Pearson correlati"
W15-5009,P13-2121,0,0.116926,"Missing"
W15-5009,W15-3001,0,0.0648471,"mming the n-gram matches for every hypothesis sentence S in the test corpus C: Introduction Automatic Machine Translation (MT) evaluation metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006). However, the relatively consistent correlation of higher BLEU scores (Papineni et al., 2002) and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggests a better translation than its lower scoring counterparts (Bojar et al., 2014; Bojar et al., 2015; Nakazawa et al., 2014; Cettolo et al., 2014). Callison-Burch et al. (2006) has anecdotally presented possible failures of BLEU by showing examples of translations with the same BLEU score but of different translation quality. Through P pn = S∈C P P ngram∈S S∈C P Countmatched (ngram) ngram∈S Count(ngram) (1) BLEU is a precision based metric; to emulate recall, the brevity penalty (BP) is introduced to compensate for the possibility of high precision translation that are too short. The BP is calculated as: 1 Meta-evaluation refers to the measurement of the Pearson correlation R2 between an aut"
W15-5009,W11-2123,0,0.0159337,"he evaluators were asked to judge whether the hypothesis or the baseline translation was better, or they were tied. The translation that was judged better constituted a win and the other a loss. For each, the majority vote between the five evaluators for the hypothesis decided whether the hypothesis won, lost or tied the baseline. The final human judgment score, • To minimize the computing load on the translation model, we compressed the phrasetable and lexical reordering model (JunczysDowmunt, 2012) • Language modeling is trained using KenLM using 5-grams, with modified Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995; Chen and Goodman, 1998). The language model is quantized to reduce filesize and improve querying speed (Heafield et al., 2013; Whittaker and Raj, 2001). • Minimum Error Rate Training (M ERT) (Och, 2003) to tune the decoding parameters. 6 Human Evaluation dev.txt and devtest.txt 77 HUMAN, is calculated as follows: HUMAN = 100 × W −L W +L+T However, the puzzling fact is that our system being 15 BLEU points better than the organizers’ baseline begets a terribly low human judgement score. We discuss this next. (4) By definition, the HUMAN score ranges from −100 to +100, whe"
W15-5009,E06-1032,0,0.723104,"chungszentrum für Künstliche Campus, Saarbrücken, Germany Intelligenz / Saarbrücken, Germany liling.tan@uni-saarland.de {first.last_name}@dfki.de meta-evaluation1 of BLEU scores and human judgements scores of the 2005 NIST MT Evaluation exercise, they have also showed high correlations of R2 = 0.87 (for adequacy) and R2 = 0.74 (for fluency) when an outlier rule-based machine translation system with poor BLEU score and high human score is excluded; when included the correlations drops to 0.14 for adequacy and 0.74 for fluency. Despite showing the poor correlation between BLEU and human scores, Callison-Burch et al. (2006) had only empirically meta-evaluated a scenario where low BLEU score does not necessary result in a poor human judgement score. In this paper, we demonstrate a real-world example of machine translation that yielded high automatic evaluation scores but failed to obtain a good score on manual evaluation in an MT shared task submission. Abstract Automatic evaluation of machine translation (MT) quality is essential in developing high quality MT systems. Despite previous criticisms, BLEU remains the most popular machine translation metric. Previous studies on the schism between BLEU and manual eval"
W15-5009,D10-1092,0,0.112133,"kes reordering into account by rewarding the higher n-gram orders, freely permuted unigrams and bigrams matches are able to sustain a high BLEU score with little penalty caused by tri/fourgram mismatches. To overcome reordering, the RIBES Reference: このような作用を発揮させるためには、夫々 ０．００５％以上含有させることが好まし い。 2 Alternatively, researchers would choose to inflate the BLEU score to a range between 0 to 100 to improve readability of the scores without the decimal prefix. 75 2.2 score was introduced by adding a rank correlation coefficient3 prior to unigram matches without the need for higher order n-gram matches (Isozaki et al., 2010). Let us consider another example: Other Shades of BLEU / RIBES It is worth noting that there are other automatic MT evaluation metrics that depend on the same precision-based score with primary differences in how the Countmatch (ngram) is measured; Giménez and Màrquez (2007) described other linguistics features that one could match in place of surface n-grams, such as lexicalized syntactic parse features, semantic entities and roles annotations, etc. As such, the modified BLEU-like metrics can present other aspects of syntactic fluency and semantic adequacy complementary to the string-based B"
W15-5009,P03-1021,0,0.0918246,"the five evaluators for the hypothesis decided whether the hypothesis won, lost or tied the baseline. The final human judgment score, • To minimize the computing load on the translation model, we compressed the phrasetable and lexical reordering model (JunczysDowmunt, 2012) • Language modeling is trained using KenLM using 5-grams, with modified Kneser-Ney smoothing (Heafield, 2011; Kneser and Ney, 1995; Chen and Goodman, 1998). The language model is quantized to reduce filesize and improve querying speed (Heafield et al., 2013; Whittaker and Raj, 2001). • Minimum Error Rate Training (M ERT) (Och, 2003) to tune the decoding parameters. 6 Human Evaluation dev.txt and devtest.txt 77 HUMAN, is calculated as follows: HUMAN = 100 × W −L W +L+T However, the puzzling fact is that our system being 15 BLEU points better than the organizers’ baseline begets a terribly low human judgement score. We discuss this next. (4) By definition, the HUMAN score ranges from −100 to +100, where higher is better. 4 5 We perform a segment level meta-evaluation by calculating the BLEU and RIBES score difference for each hypothesis-baseline translation. Figures 1 and 2 show the correlations of the BLEU and RIBES score"
W15-5009,P02-1040,0,0.111065,"etric. Previous studies on the schism between BLEU and manual evaluation highlighted the poor correlation between MT systems with low BLEU scores and high manual evaluation scores. Alternatively, the RIBES metric—which is more sensitive to reordering—has shown to have better correlations with human judgements, but in our experiments it also fails to correlate with human judgements. In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores. 1 2 BLEU Papineni et al. (2002) originally define BLEU n-gram precision pn by summing the n-gram matches for every hypothesis sentence S in the test corpus C: Introduction Automatic Machine Translation (MT) evaluation metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006). However, the relatively consistent correlation of higher BLEU scores (Papineni et al., 2002) and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggests a better translation than its"
W15-5009,N03-1017,0,0.0122845,"ese tokenizer LM n-gram order Distortion limit Quantized & binarized LM devtest.txt in LM Binarized phrase tables M ERT runs Organizers 40 MeCab Juman 5 0 no no no 1 Ours 80 KoNLPy MeCab 5 20 yes yes yes 2 Table 1: Differences between Organizer’s and our Phrase-based SMT system mains). Two development datasets6 and one test set each comprises 2000 sentences with 500 sentences from each of the training domains. The Korean and Japanese texts were tokenized using KoNLPy (Park and Cho, 2014) and MeCab (Kudo et al., 2004) respectively. We used the phrase-based SMT implemented in the Moses toolkit (Koehn et al., 2003; Koehn et al., 2007) with the following vanilla Moses experimental settings: 3.1 The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform (WAT, 2014). Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system. 3.1.1 • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and V"
W15-5009,W15-3031,0,0.0410126,"Missing"
W15-5009,P07-2045,0,0.0159002,"ram order Distortion limit Quantized & binarized LM devtest.txt in LM Binarized phrase tables M ERT runs Organizers 40 MeCab Juman 5 0 no no no 1 Ours 80 KoNLPy MeCab 5 20 yes yes yes 2 Table 1: Differences between Organizer’s and our Phrase-based SMT system mains). Two development datasets6 and one test set each comprises 2000 sentences with 500 sentences from each of the training domains. The Korean and Japanese texts were tokenized using KoNLPy (Park and Cho, 2014) and MeCab (Kudo et al., 2004) respectively. We used the phrase-based SMT implemented in the Moses toolkit (Koehn et al., 2003; Koehn et al., 2007) with the following vanilla Moses experimental settings: 3.1 The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform (WAT, 2014). Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system. 3.1.1 • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008)(Koehn et"
W15-5009,W15-4105,1,0.846678,"Missing"
W15-5009,2005.mtsummit-papers.11,0,0.0128051,"nt model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008)(Koehn et al., 2003; Och and Ney, 2003; Gao and Vogel, 2008) Baseline System Human evaluations were conducted as pairwise comparisons between translations from our system and the WAT organizers’ phrase-based statistical MT baseline system. Table 1 highlights the parameter differences between the organizers and our phrase-based SMT system. • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn, 2005; Galley and Manning, 2008) 3.1.2 Pairwise Comparison The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform. Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the phrase-based MT system. Five evaluators were asked to judge each document. The crowdsourced evaluators were non-experts, thus their judgements were not necessary precise, especially for patent translations. The evaluators were asked to judge whether the hypothesi"
W15-5009,W15-3051,1,0.83661,"h to improve upon the BLEU scores is to allow paraphrases or gappy variants and replace the proportion of Countmatch (ngram) / Count(ngram) by a lexical similarity measure. Banerjee and Lavie (2005) introduced the METEOR metric that allows hypotheses’ n-grams to match paraphrases and stems instead of just the surface strings. Lin and Och (2004) presented the ROUGE-S metrics that uses skip-grams matches. More recently, pre-trained regression models based on semantic textual similarity and neural network-based similarity measures trained on skip-grams are applied to replace the n-gram matching (Vela and Tan, 2015; Gupta et al., 2015). While enriching the surface n-gram matching allows the automatic evaluation metric to handle variant translations, it does not resolves the “prominent crudeness"" of BLEU (Callison-Burch, 2006) involving (i) the omission of contentbearing materials not being penalized, and (ii) the inability to calculate recall despite the brevity penalty. Source: Tᄋ ᆼᅲ ᅭ ᆼ ᄋ(DSC) = 89.9℃; Tᄀ ᆯᅥ ᅧ ᆼ ᄌᄒ ᅪ(DSC) = 72℃( 5℃/ ᄇ ᆫᄋ ᅮ ᅦᄉ ᅥDSC ᄅ ᅩᄎ ᆨᄌ ᅳ ᆼ) . ᅥ Hypothesis: Ｔｍｅｌｔ（ＤＳＣ）＝７２℃（５℃／分 でＤＳＣ測定（ＤＳＣ）＝89 . 9 結晶化度 （Ｔ）。 Baseline: Ｔ溶融（ＤＳＣ）＝８９. ９℃；Ｔ結晶化 （ＤＳＣ）＝７２℃（５℃／分でＤＳＣで測 定）。 Reference: Ｔｍｅｌｔ（ＤＳＣ）＝"
W15-5009,W04-3230,0,0.013478,"hypothesis and reference translations 76 Parameters Input document length Korean tokenizer Japanese tokenizer LM n-gram order Distortion limit Quantized & binarized LM devtest.txt in LM Binarized phrase tables M ERT runs Organizers 40 MeCab Juman 5 0 no no no 1 Ours 80 KoNLPy MeCab 5 20 yes yes yes 2 Table 1: Differences between Organizer’s and our Phrase-based SMT system mains). Two development datasets6 and one test set each comprises 2000 sentences with 500 sentences from each of the training domains. The Korean and Japanese texts were tokenized using KoNLPy (Park and Cho, 2014) and MeCab (Kudo et al., 2004) respectively. We used the phrase-based SMT implemented in the Moses toolkit (Koehn et al., 2003; Koehn et al., 2007) with the following vanilla Moses experimental settings: 3.1 The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform (WAT, 2014). Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system. 3.1.1 • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuri"
W15-5009,P04-1077,0,0.0603032,"ce n-grams, such as lexicalized syntactic parse features, semantic entities and roles annotations, etc. As such, the modified BLEU-like metrics can present other aspects of syntactic fluency and semantic adequacy complementary to the string-based BLEU. A different approach to improve upon the BLEU scores is to allow paraphrases or gappy variants and replace the proportion of Countmatch (ngram) / Count(ngram) by a lexical similarity measure. Banerjee and Lavie (2005) introduced the METEOR metric that allows hypotheses’ n-grams to match paraphrases and stems instead of just the surface strings. Lin and Och (2004) presented the ROUGE-S metrics that uses skip-grams matches. More recently, pre-trained regression models based on semantic textual similarity and neural network-based similarity measures trained on skip-grams are applied to replace the n-gram matching (Vela and Tan, 2015; Gupta et al., 2015). While enriching the surface n-gram matching allows the automatic evaluation metric to handle variant translations, it does not resolves the “prominent crudeness"" of BLEU (Callison-Burch, 2006) involving (i) the omission of contentbearing materials not being penalized, and (ii) the inability to calculate"
W15-5009,W15-5001,0,0.071916,"he hypothesis translation was minimally penalized in RIBES and also BLEU. The RIBES score for the hypothesis and baseline translations are 94.04 and 86.33 respectively whereas their BLEU scores are 53.3 and 58.8. In the WAT 2015 evaluation, five evaluators unanimously voted in favor for the baseline translation. Although the RIBES score presents a wider difference between the hypothesis and baseline translation than BLEU, it is insufficient to account for the arrant error that the hypothesis translation made. 3 Experimental Setup We describe our system submission4 to the WAT 2015 shared task (Nakazawa et al., 2015) for Korean to Japanese patent translation.5 . The Japan Patent Office (JPO) Patent Corpus is the official resource provided for the shared task. The training dataset is made up of 1 million sentences (250k each from the chemistry, electricity, mechanical engineering and physics do4 Our Team ID in WAT 2015 is Sense Although, we have also participated in the EnglishJapanese-Chinese scientific text translation subtask using the ASPEC corpus, our results have been presented in Tan and Bond (2014) and Tan et al. (2015) 5 3 normalized Kendall τ of all n-gram pairs between the hypothesis and referen"
W15-5009,J03-1002,0,0.00512596,"emented in the Moses toolkit (Koehn et al., 2003; Koehn et al., 2007) with the following vanilla Moses experimental settings: 3.1 The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform (WAT, 2014). Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation and a baseline translation generated by the baseline phrase-based MT system. 3.1.1 • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction (Och and Ney, 2003; Koehn et al., 2003; Gao and Vogel, 2008)(Koehn et al., 2003; Och and Ney, 2003; Gao and Vogel, 2008) Baseline System Human evaluations were conducted as pairwise comparisons between translations from our system and the WAT organizers’ phrase-based statistical MT baseline system. Table 1 highlights the parameter differences between the organizers and our phrase-based SMT system. • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations (Koehn, 2005; Galley and Manning, 2008) 3.1.2 Pairwise Comparison The human judgment scores for the WAT evalua"
W15-5009,W07-0718,0,\N,Missing
W15-5009,W14-7001,0,\N,Missing
W15-5206,2013.mtsummit-wptp.13,0,0.122457,"2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entitled CATaLog. New features have been integrated into the tool which aim to improve post-editing both in terms of performance and productivity. One of the new features of CATaLog is a color coding scheme that is based on the similarity between a particular input sentence and the segments retrieved from the TM. This color codin"
W15-5206,2012.amta-papers.22,0,0.124202,"presented using English - Bengali data. 2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They"
W15-5206,2014.eamt-1.2,0,0.105775,"retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In th"
W15-5206,2009.mtsummit-papers.14,0,0.0410983,"achines) to decide which output (TM or MT) is most suitable to use for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing diminishing translators’ eﬀort (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are p"
W15-5206,W15-4905,1,0.742271,"Missing"
W15-5206,W09-0441,0,0.0370839,"is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence using the popular Translation Error Rate (TER) metric (Snover et al., 2009). The top 5 most similar segments are chosen and presented to the translator ordered by their similarity. One very important aspect of computing similarity is alignment. Each test (input) segment in the source language (SL) is aligned with the reference SL sentences in the TM and each SL sentence in the TM is aligned to its respective translation. From these two sets 3 BTEC corpus contains tourism-related sentences similar to those that are usually found in phrase books for tourists going abroad 4 Work in progress. 37 of alignments we apply a method to ﬁnd out which parts of the translation ar"
W15-5206,P10-1064,1,0.924209,"Missing"
W15-5206,2011.eamt-1.12,0,0.0128172,"udy are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post-editors either to use TM output or MT output (He et al., 2010). To achieve good performance with this kind of systems, researchers typically train a binary classiﬁer (e.g., Su"
W15-5206,2010.jec-1.3,0,0.508262,"lthough in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entit"
W15-5206,2011.mtsummit-papers.37,0,0.259019,"editors by correcting retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cett"
W15-5206,W14-0314,1,0.778874,"2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers"
W15-5206,2010.jec-1.4,0,0.0590105,"t (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence us"
W15-5206,2008.amta-srw.4,0,0.397037,"e the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post"
W15-5206,N06-1014,0,0.0673427,"he top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave me wrong number . • English: we want to have a table near the window . Source Matches: 1. you ga"
W15-5206,J03-1002,0,0.00872292,"ssign a higher cost for insertion than deletion, and hence such sentences will not be shown as the top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave"
W15-5206,2012.amta-papers.26,0,0.0145925,". | D S S || S | | | we - would like a table by the window . For ﬁnding out the similar and dissimilar parts between the test segment and a matching TM segment, we use TER alignments. TER is an error metric and it gives an edit ratio (often referred to as edit rate or error rate) in terms of how much editing is required to convert a sentence into another with respect to the length of the ﬁrst sentence. Allowable edit operations include insert, delete, substitute and shift. We use the TER metric (using tercom-7.2515 ) to ﬁnd the edit rate between a test sentence and the TM reference sentences. Simard and Fujita (2012) ﬁrst proposed the use of MT evaluation metrics as similarity functions in implementing TM functionality. They experimented with several MT evaluation metrics, viz. BLEU, NIST, Meteor and TER, and studied their behaviors on TM performance. In the TM tool presented here we use TER as the similarity metric as it is very fast and lightweight and it directly mimics the human post-editing eﬀort. Moreover, the tercom-7.251 package also produces the alignments between the sentence pair from which it is very easy to identify which portions in the matching segment match with the input sentence and whic"
W15-5206,2015.eamt-1.6,1,\N,Missing
W15-5411,W13-1728,1,0.737778,"ma a` s indulgˆencias vendidas pelo #NE# na #NE# #NE# quando os fi´eis compravam a redenc¸a˜ o das suas almas dando dinheiro aos padres. Code bs hr sr id my cz sk pt-BR pt-PT es-AR es-ES bg mk xx 4 Approach Given that each team was allowed to submit a maximum of three runs to each track (closed and open), we decided to take this opportunity to test and compare different approaches. To do that, we developed three systems based on team MMS-member’s previous work in language identification and related tasks. The first two systems were previously used for the Native Language Identification (NLI) (Gebre et al., 2013) and the third one has been applied to language variety identification. The following is a list of the three systems and the their corresponding submission runs: Table 1: DSL corpus by language and variety. In detail, the corpus collection contains 308,000 short text excerpts sampled from journalistic texts • Run 1 - Logistic Regression with TF-IDF Weighting 2 For the sake of simplicity, we refer to both languages and language varieties as languages. 67 • Run 2 - SVM with TF-IDF Weighting texts is not a good discriminator, and should be given less weight than one which occurs in fewer texts. T"
W15-5411,W14-5316,0,0.187078,"Missing"
W15-5411,Y08-1042,0,0.329428,"ithms and words and characters as features to solve the task. Unlike general-purpose language identification, most of the systems trained to discriminate between similar languages perform best using high order character n-grams and word n-gram representations. Different groups or pairs of similar languages and language varieties have been studied using data from different sources such as standard contemporary newspapers and social media. Recent studies include: Indian languages (Murthy and Kumar, 2006), Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Mainland, Singapore and Taiwanese Chinese (Huang and Lee, 2008), Brazilian and European Portuguese (Zampieri and Gebre, 2012), South Slavic languages (Tiedemann Introduction Automatic language identification is an important task in Natural Language Processing (NLP), which consists of applying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often f"
W15-5411,W14-5317,0,0.0889212,"tion of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more training data does not necessarily lead to higher performance and that the features learned by the classifiers are, to a certain extent, dataset specific. Therefore, we decided to use only the dataset provided by the organisers and only participate in the closed submission track. The shared task organisers provided all participants with an updated version of the DSL corpus collection v.2.0 (DSLCC) (Tan et al., 2014). This corpus is composed of 14 classes, 13 languages2"
W15-5411,I11-1062,0,0.0209894,"nguages (Tiedemann Introduction Automatic language identification is an important task in Natural Language Processing (NLP), which consists of applying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often fail to achieve satisfactory performance across different datasets and domains (Lui and Baldwin, 2011), particularly with: datasets containing short pieces of texts such as tweets (Zubiaga et al., 2014); code-switching data (Solorio et al., 2014); or when discriminating between very similar languages (Zampieri et al., 2014). Given these challenges, the Discriminating between Similar Languages (DSL) shared task provides an excellent opportunity for researchers interested in evaluating and comparing their 1 MMS is an acronym for our affiliations/locations (Malaga, Munich and Saarland). In the shared task report (Zampieri et al., 2015) the team is displayed as MMS*. The * indicates that a shared"
W15-5411,U13-1003,0,0.432873,"lass are divided into 3 partitions, i.e. 18,000, 2,000 and 2,000 instances for training, development and testing, respectively. The test set is further subdivided into two test sets (A and B), each one containing 1,000 instances. While the test set A contains original texts, the organisers replaced named entities for place holders in the set B in order to decrease thematic bias in the classification process. Below we present an example of a Portuguese instance containing place holders #NE# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between"
W15-5411,W14-5315,0,0.142253,"Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more training data does not necessarily lead to higher performance and that the features learned by the classifiers are, to a certain extent, dataset specific. Therefore, we decided to use only the dataset provided by the organisers and only participate in the closed submission track. The shared task organisers provided all participants with an updated version of the DSL corpus collection v.2.0 (DSLCC) (Tan et al., 2014). This corpus is composed of 14 classes, 13 languages2 and one class containing documents"
W15-5411,W14-4204,0,0.156387,"Missing"
W15-5411,C12-1160,0,0.0994912,"Missing"
W15-5411,W14-5313,0,0.123685,"ining place holders #NE# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the clos"
W15-5411,E14-4004,1,0.879058,"Missing"
W15-5411,zampieri-gebre-2014-varclass,1,0.743895,"Missing"
W15-5411,W14-5904,0,0.0694914,"# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track"
W15-5411,W14-5307,1,0.894268,"Missing"
W15-5411,P14-2125,0,0.0407051,"ed entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more"
W15-5411,W15-5401,1,0.864728,"Missing"
W15-5411,W14-3907,0,0.0381874,"plying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often fail to achieve satisfactory performance across different datasets and domains (Lui and Baldwin, 2011), particularly with: datasets containing short pieces of texts such as tweets (Zubiaga et al., 2014); code-switching data (Solorio et al., 2014); or when discriminating between very similar languages (Zampieri et al., 2014). Given these challenges, the Discriminating between Similar Languages (DSL) shared task provides an excellent opportunity for researchers interested in evaluating and comparing their 1 MMS is an acronym for our affiliations/locations (Malaga, Munich and Saarland). In the shared task report (Zampieri et al., 2015) the team is displayed as MMS*. The * indicates that a shared task organiser is a team member. 66 Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialec"
W15-5411,W13-1706,0,0.0469809,"mostly because they are frequent in both varieties. On the other hand, words like London or rubbish might not be as frequent as the, of, and, yet, they are better discriminative words for British English. Therefore, the actual importance of a term for this task depends on how infrequent the term is in other texts. This can be modelled using Inverse Document Frequency (IDF). IDF is based on the assumption that a term which occurs in many 4.2 Classifiers Systems developed for Run 1 and Run 2 were previously used in the Native Language Identification (NLI) (Gebre et al., 2013) shared task 2013 (Tetreault et al., 2013) by the CologneNijmegen team with good results. They both rely on the TF-IDF weighting scheme combined with two different classifiers. For Run 1, we opt for Logistic Regression using the LIBLINEAR open source library (Fan et al., 2008) from scikit-learn (Pedregosa et al., 2011) and fix the regularisation parameter to 100.0. This regression algorithm has been used in different classification problems including for example temporal text classification (Niculae et al., 2014). 3 The TF-IDF description presented in this section is based on our previous work (Gebre et al., 2013) 4 In our experiments"
W16-2333,D11-1033,0,0.150111,", large amount of additional out-domain data may bias the resultant distribution towards the out-domain. In practice, 442 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 442–448, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 3 Related Work System Description 3.1 Data selection Approach Among the different approaches proposed for data selection, the two most popular and successful methodologies are based on monolingual crossentropy difference (Moore and Lewis, 2010) and bilingual cross-entropy difference (Axelrod et al., 2011). The data selection approach taken in the present work is also motivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm"
W16-2333,W08-0321,0,0.0258029,"sj − tj ] is calculated based on Equation 2. H(Plm ) = − Koehn et al. (2007) used multiple decoding paths for combining multiple domain-specific translation tables in the state-of-the-art PB-SMT decoder MOSES. Banerjee et al. (2013) combined an in-domain model (translation and reordering model) with an out-of-domain model into MOSES and they derived log-linear features to distinguish between phrases of multiple domains by applying the data-source indicator features and showed modest improvement in translation quality. score = |P Pinsl (sj ) − P Posl (sj )| + |P Pintl (tj ) − P Potl (tj ) |(2) Bach et al. (2008) suggested that sentences may be weighted by how much it matches with the target domain. A comparison among different domain adaptation methods for different subject matters in patent translation was carried out by (Ceaus¸fu et al., 2011) which led to a small gain over the baseline. Subsequently, sentence pairs [s − t] from the out-domain corpus (o) are ranked based on this score. 3.2 Interpolation Approach To combine multiple translation and language models, a common approach is to linearly interpolate them. The language model interpolation weights are automatically learnt by minimizing the p"
W16-2333,2011.mtsummit-papers.32,1,0.872589,"Missing"
W16-2333,N03-1017,0,0.00959327,"performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning w"
W16-2333,2013.mtsummit-papers.13,1,0.874847,"Missing"
W16-2333,E12-1045,0,0.0171699,"the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation model"
W16-2333,W04-3250,0,0.0396345,"otivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in"
W16-2333,2005.mtsummit-papers.11,0,0.0178143,"he bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingu"
W16-2333,2011.eamt-1.5,0,0.0592153,"Missing"
W16-2333,W07-0734,0,0.0408698,"e performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and"
W16-2333,N06-1014,0,0.0556417,"an them. We also use out of domain data to accelerate the performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing s"
W16-2333,2010.iwslt-papers.5,0,0.0226244,"is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging f"
W16-2333,W07-0717,0,0.0383431,"erence in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different dir"
W16-2333,D07-1036,0,0.0628601,"Missing"
W16-2333,W06-1607,0,0.20109,"esearch Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary s"
W16-2333,D09-1074,0,0.060014,"Missing"
W16-2333,D08-1089,0,0.0115611,"set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the paramete"
W16-2333,P10-2041,0,0.539223,"ata, India 3 Universit¨at des Saarlandes, Saarbr¨ucken, Germany 4 German Research Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language mode"
W16-2333,W12-3154,0,0.0151824,"s (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific wor"
W16-2333,P03-1021,0,0.00947555,"rdering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain D"
W16-2333,W11-2123,0,0.0263772,"rried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by th"
W16-2333,P02-1040,0,0.099321,"To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and max"
W16-2333,P13-1082,0,0.0831355,"tificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary submission obtained a BLE"
W16-2333,E12-1055,0,0.0841094,"timated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment a"
W16-2333,N06-2037,0,0.0324717,"age model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific word alignment. N 1 X log Plm (wi |wi−k+1 ...wi−1 ) N i=1 (1) We calculated perplexity (P P = 2H ) of individual sentences of out-domain with respect to indomain LM and out-domain LM for both source (sl) a"
W16-2333,2006.amta-papers.25,0,0.0417351,"e table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 80 respectively. Additionally"
W16-2333,W14-3323,1,0.898642,"Missing"
W16-2333,P07-2045,0,\N,Missing
W16-2333,W15-3017,1,\N,Missing
W16-2379,N12-1047,0,0.0301838,"archical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets did not require any preprocessing in terms of encoding or alignment. Train Dev Test 12,000 1,000 2,000 EN 201,505 17,827 31,477 Tokens DE-"
W16-2379,P03-1021,0,0.187832,"rdering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets did not require any preprocessing in terms of encoding or alignment. Train Dev Test 1"
W16-2379,P11-1105,0,0.11743,"Missing"
W16-2379,W15-3026,1,0.834799,"Missing"
W16-2379,J15-2001,0,0.138111,"Missing"
W16-2379,L16-1095,1,0.842747,"Missing"
W16-2379,W06-1607,0,0.0408401,"our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets d"
W16-2379,P02-1040,0,0.109971,"LM-Toolkit (Stolcke, 2002) 760 3 Experiment SEN The effectiveness of the present work is demonstrated by using the standard log-linear PB-SMT model for our phrase based SAPE (PB-SAPE) model. The MT outputs are provided by WMT2016 APE task (c.f Table 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional)"
W16-2379,D08-1089,0,0.0406055,"for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training da"
W16-2379,N07-1064,0,0.859858,"diting Santanu Pal1 , Marcos Zampieri1,2 , Josef van Genabith1,2 1 Saarland University, Saarbr¨ucken, Germany 2 German Research Center for Artificial Intelligence (DFKI), Germany {santanu.pal, marcos.zampieri, josef.vangenabith}@uni-saarland.de Abstract tency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). With this respect the ultimate goal of MT systems is to provide output that can be post-edited with the least effort as possible by human translators. One of the strategies to improve MT output is to apply automatic post-editing (APE) methods (Knight and Chander, 1994; Simard et al., 2007a; Simard et al., 2007b). APE methods work under the assumption that some errors in MT systems are recurrent and they can be corrected automatically in a post-processing stage thus providing output that is more adequate to be post-edited. APE methods are applied before human post-editing increasing translators’ productivity. This paper presents a new approach to APE which was submitted by the USAAR team to the Automatic Post-editing (APE) shared task at WMT-2016. Our system combines two models: monolingual phrase-based and operation sequential model with an edit distance based word alignment b"
W16-2379,W07-0728,0,0.494383,"diting Santanu Pal1 , Marcos Zampieri1,2 , Josef van Genabith1,2 1 Saarland University, Saarbr¨ucken, Germany 2 German Research Center for Artificial Intelligence (DFKI), Germany {santanu.pal, marcos.zampieri, josef.vangenabith}@uni-saarland.de Abstract tency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). With this respect the ultimate goal of MT systems is to provide output that can be post-edited with the least effort as possible by human translators. One of the strategies to improve MT output is to apply automatic post-editing (APE) methods (Knight and Chander, 1994; Simard et al., 2007a; Simard et al., 2007b). APE methods work under the assumption that some errors in MT systems are recurrent and they can be corrected automatically in a post-processing stage thus providing output that is more adequate to be post-edited. APE methods are applied before human post-editing increasing translators’ productivity. This paper presents a new approach to APE which was submitted by the USAAR team to the Automatic Post-editing (APE) shared task at WMT-2016. Our system combines two models: monolingual phrase-based and operation sequential model with an edit distance based word alignment b"
W16-2379,W11-2123,0,0.0478937,"ree different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding"
W16-2379,2006.amta-papers.25,0,0.644646,"system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To com"
W16-2379,N03-1017,0,0.046298,"es our proposed system, in particular PB-SMT coupled OSM model. In Section 3, we outline the data used for experiments and complete experimental setup. Section 4 presents the results of the automatic evaluation, followed by conclusion and future work in Section 5. 2 p(mt, pe) ≈ I Y i=1 p(oi |oi−m+1 ...oi−1 ) (1) The decoder searches best translation in Equation 2 from the model using language model plm (pe) USAAR APE System Our APE system is based on operational N-gram sequential model which integrates translation and reordering operations into the phrase-based APE system. Traditional PB-SMT (Koehn et al., 2003) provides a powerful translation mechanism which can directly be modelled to a phrase-based SAPE (PB-SAPE) system (Simard et al., 2007a; Simard et al., 2007b; Pal et al., 2015) using target language MT output (T LM T ) and their corresponding post-edited version (T LP E ) as a parallel training corpus. Unlike PB-SMT, PB-SAPE also follows similar kind of drawbacks such as dependency across phrases, handling discontinuous phrases etc. Our OSM-APE system is based on phrase based N-gram APE model, however reordering approach is essentially different, it considers all possible orderings of phrases"
W16-2379,P07-2045,0,0.0304214,"Missing"
W16-2379,W14-0314,1,0.877599,"Missing"
W16-2379,W07-0734,0,0.289413,"able 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high pro"
W16-2379,N06-1014,0,0.596721,"e provided by WMT2016 APE task (c.f Table 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the tra"
W16-2379,W15-5206,1,0.834735,"Missing"
W16-2379,J03-1002,0,0.0182983,"a linear sequence of operations such as lexical generation of post-edited translation and their orderings. The translation and reordering decisions are conditioned on n previous translation and reordering decisions. The model also can able to consistently modelled both local and long-range reorderings. Traditional OSM based MT model consists of three sequence of operations: pe∗ = argmaxpe p(mt, pe) × plm (pe) ppr (pe) (2) QI ppr (pe) ≈ i=1 p(wi |wi−m+1 ...wi−1 ), is the prior probability that marginalize the joint probability p(mt, pe). The model is then represented in a log-linear approach (Och and Ney, 2003) (in Equation 3) that makes it useful to incorporate standard features along with several novel features that improve the accuracy. pe∗ = argmaxpe I X λi hi (mt, pe) (3) i=1 where λi is the weight associated with the feature hi (mt, pe): p(mt, pe), ppr (pe) and plm (pe). Apart from this 8 additional features has been included in the log-linear model: 1. Length penalty: Length of the pe in words 2. Deletion penalty 3. Gap bonus: Total number of gap inserted to produce PE sentence 4. Open gap penalty : Number of open gaps, this penalty controls how quickly gap was closed. • Generates a sequence"
W16-2379,P05-1033,0,\N,Missing
W17-5403,D12-1031,0,0.0311039,"Therefore g2p may be possible for low resource languages if this high resource data 19 Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 19–26 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics each time step. The language embeddings their system learned correlate closely to the genetic relationships between languages. However, neither of these models was applied to g2p. with the output of the wFST-based Phonetisaurus system (Novak et al., 2016) did better than either system alone. A different approach came from Kim and Snyder (2012), who used supervised learning with an undirected graphical model to induce the grapheme–phoneme mappings for languages written in the Latin alphabet. Given a short text in a language, the model predicts the language’s orthographic rules. To create phonemic context features from the short text, the model na¨ıvely maps graphemes to IPA symbols written with the same character, and uses the features of these symbols to learn an approximation of the phonotactic constraints of the language. In their experiments, these phonotactic features proved to be more valuable than geographical and genetic fea"
W17-5403,P17-4012,0,0.0841512,"Missing"
W17-5403,E17-2002,0,0.0246148,"Resource g2p Our approach is similar in goal to Deri and Knight (2016)’s model for adapting high resource g2p models for low resource languages. They trained weighted finite state transducer (wFST) models on a variety of high resource languages, then transferred those models to low resource languages, using a language distance metric to choose which high resource models to use and a phoneme distance metric to map the high resource language’s phonemes to the low resource language’s phoneme inventory. These distance metrics are computed based on data from Phoible (Moran et al., 2014) and URIEL (Littell et al., 2017). Other low resource g2p systems have used a strategy of combining multiple models. Schlippe et al. (2014) trained several data-driven g2p systems on varying quantities of monolingual data and combined their outputs with a phoneme-level voting scheme. This led to improvements over the best-performing single system for small quantities of data in some languages. Jyothi and HasegawaJohnson (2017) trained recurrent neural networks for small data sets and found that a version of their system that combined the neural network output Introduction Accurate grapheme-to-phoneme conversion (g2p) is impor"
W17-5403,D15-1166,0,0.0604104,"for its spelling ambiguities. Abjads, used for Arabic and Hebrew, do not give full representation to vowels. Consequently, g2p is harder than simply replacing each grapheme symbol with a corresponding phoneme symbol. It is the problem of replacing a grapheme sequence Table 1: Hyperparameters for multilingual g2p models G = g1 , g2 , ..., gm with a phoneme sequence in forward and reverse directions. We used long short-term memory units (Hochreiter and Schmidhuber, 1997) for both the encoder and decoder. For the attention mechanism, we used the general global attention architecture described by Luong et al. (2015). We implemented2 all models with OpenNMT (Klein et al., 2017). Our hyperparameters, which we determined by experimentation, are listed in Table 1. Φ = φ1 , φ2 , ..., φn where the sequences are not necessarily of the same length. Data-driven g2p is therefore the problem of finding the phoneme sequence that maximizes the likelihood of the grapheme sequence: ˆ = arg max Pr(Φ0 |G) Φ Φ0 4.2 Data-driven approaches are especially useful for problems in which the rules that govern them are complex and difficult to engineer by hand. g2p for languages with ambiguous orthographies is such a problem. Mul"
W17-5403,E17-2102,0,0.329512,"es results on low-resource languages by allowing them to implicitly share parameters with high-resource languages. Our g2p system is inspired by this approach, although it differs in that there is only one target “language”, IPA, and the artificial tokens identify the language of the source instead of the language of the target. Other work has also made use of multilinguallytrained neural networks. Phoneme-level polyglot language models (Tsvetkov et al., 2016) train a single model on multiple languages and additionally condition on externally constructed typolog¨ ical data about the language. Ostling and Tiedemann (2017) used a similar approach, in which a character-level neural language model is trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at th D e @ There are cases in which a single grapheme represents multiple phonemes, such as syllabaries, in which each symbol represents a syllable. In many languages, there are silent letters, as in the word hora in Spanish: h - o o r R a a There are more complicated correspondences, such as the silent e in English that affects the pronunciation of the previous vowel, as seen in the pair of words cape and cap. It is"
W17-5403,N16-1161,0,0.0603257,"r more efficient than building a separate model for each language pair; it allows for translation between languages that share no parallel data; and it improves results on low-resource languages by allowing them to implicitly share parameters with high-resource languages. Our g2p system is inspired by this approach, although it differs in that there is only one target “language”, IPA, and the artificial tokens identify the language of the source instead of the language of the target. Other work has also made use of multilinguallytrained neural networks. Phoneme-level polyglot language models (Tsvetkov et al., 2016) train a single model on multiple languages and additionally condition on externally constructed typolog¨ ical data about the language. Ostling and Tiedemann (2017) used a similar approach, in which a character-level neural language model is trained on a massively multilingual corpus. A language embedding vector is concatenated to the input at th D e @ There are cases in which a single grapheme represents multiple phonemes, such as syllabaries, in which each symbol represents a syllable. In many languages, there are silent letters, as in the word hora in Spanish: h - o o r R a a There are more"
W18-1807,D16-1025,0,0.0130395,"module needs to be robust against errors introduced by the automatic speech recognition (ASR) module. Moreover, NLP should not be vulnerable to adversarial input examples. While all these examples do not pose a real challenge to an experienced human reader, even ”small” perturbations from the canonical form can make a state-of-the-art NLP system fail. To illustrate the typical behavior of state-of-the-art NLP on normalized and nonnormalized text, we discuss an example in the context of neural MT (NMT). Different research groups have shown that NMT can generate natural and ﬂuent translations (Bentivogli et al., 2016), achieving human-like performance in certain settings (Wu et al., 2016). The state-ofthe-art NMT engine Google Translate1 , for example, perfectly translates the English sentence I used my card to purchase a meal on the menu and the total on my receipt was $ 8.95 but when I went on line to check my transaction it shows $ 10.74 . into the German sentence Ich benutzte meine Karte , um eine Mahlzeit auf der Speisekarte zu kaufen und die Gesamtsumme auf meiner Quittung war $ 8,95 , aber als ich online ging , um meine Transaktion zu u¨ berpr¨ufen , zeigt es $ 10,74 . Adding some noise to the sourc"
W18-1807,P16-1160,0,0.0145947,"e arbitrary relationship between the orthography of a word and its meaning in general is a well known assumption in linguistics (de Saussure, 1916). However, the word form often carries additional important information. This is, for example, the case in morphologically rich languages or in non-normalized text where small perturbations result in similar word forms. Recently, sub-word units have attracted some attention in NLP to handle rarely and unseen words and to reduce the computational complexity in neural network approaches (Ling et al., 2015; Gillick et al., 2015; Sennrich et al., 2015; Chung et al., 2016; Heigold et al., 2017). Examples for sub-word units include BPE based units Sennrich et al. (2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al., 2015). A comparison of BPE and characters for machine translation regarding grammaticality can be found in Sennrich (2016). Similarly Sajjad et al. (2017) showed that BPE worked better for MT and char-based models worked better for part-of-speech (POS) tagging. 3 Noise Types In this work, we experiment with three different noise types: character swaps, character ﬂips, and word scrambling. Cha"
W18-1807,R13-1026,0,0.0123233,"tal results are shown and discussed in Section 5. The paper is concluded in Section 6. 2 Related Work A large body of work on regularization techniques to learn robust representations and models exists. Examples include 2 -regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising. Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al., 1987; T¨uske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), work on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP. While invariance transformations such as rotation, translation in vision or vocal tract length, reverberation, and noise in speech have all been harnessed, we do not have a good intuition on useful perturbations for written language yet. Label dropout and ﬂip (cf. typos) have been proposed both on the byte-level (Gillick et al., 2015) and the word-level (Xie et al., 2017). Syntactic and semantic noise for semantic analysis was studied in Yitong et al. (2017). From a human perce"
W18-1807,P11-2008,0,0.0965695,"Missing"
W18-1807,E17-1048,1,0.888568,"Missing"
W18-1807,P16-2058,0,0.046996,"Missing"
W18-1807,D16-1207,0,0.0253124,"al models outperform a conditional random ﬁeld (CRF) based model. The remainder of the paper is organized as follows. Section 2 discusses related work. Section 3 describes the noise types and Section 4 brieﬂy summarizes the modeling approaches used in this paper. Experimental results are shown and discussed in Section 5. The paper is concluded in Section 6. 2 Related Work A large body of work on regularization techniques to learn robust representations and models exists. Examples include 2 -regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising. Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al., 1987; T¨uske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), work on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP. While invariance transformations such as rotation, translation in vision or vocal tract length, reverberation, and noise in speech have all been harnessed, we do not have a good intuition on useful pert"
W18-1807,D15-1176,0,0.0169695,"ng may be of interest (Rawlinson, 1976; Rayner et al., 2006). The arbitrary relationship between the orthography of a word and its meaning in general is a well known assumption in linguistics (de Saussure, 1916). However, the word form often carries additional important information. This is, for example, the case in morphologically rich languages or in non-normalized text where small perturbations result in similar word forms. Recently, sub-word units have attracted some attention in NLP to handle rarely and unseen words and to reduce the computational complexity in neural network approaches (Ling et al., 2015; Gillick et al., 2015; Sennrich et al., 2015; Chung et al., 2016; Heigold et al., 2017). Examples for sub-word units include BPE based units Sennrich et al. (2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al., 2015). A comparison of BPE and characters for machine translation regarding grammaticality can be found in Sennrich (2016). Similarly Sajjad et al. (2017) showed that BPE worked better for MT and char-based models worked better for part-of-speech (POS) tagging. 3 Noise Types In this work, we experiment with three different noise"
W18-1807,N15-1055,0,0.0231155,"Missing"
W18-1807,P17-2095,0,0.0417728,"rd forms. Recently, sub-word units have attracted some attention in NLP to handle rarely and unseen words and to reduce the computational complexity in neural network approaches (Ling et al., 2015; Gillick et al., 2015; Sennrich et al., 2015; Chung et al., 2016; Heigold et al., 2017). Examples for sub-word units include BPE based units Sennrich et al. (2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al., 2015). A comparison of BPE and characters for machine translation regarding grammaticality can be found in Sennrich (2016). Similarly Sajjad et al. (2017) showed that BPE worked better for MT and char-based models worked better for part-of-speech (POS) tagging. 3 Noise Types In this work, we experiment with three different noise types: character swaps, character ﬂips, and word scrambling. Character ﬂips and swaps are rough approximations to typos. Word scrambling is motivated from psycholinguistic studies (Rawlinson, 1976). This choice of noise types allows us to automatically generate noisy text with different type and density distributions from existing properly edited ”clean” corpora. Using synthetic data is clearly suboptimal, but we use sy"
W18-1807,1983.tc-1.13,0,0.751653,"Missing"
W18-1807,E17-2004,0,0.0448889,"ed in Section 6. 2 Related Work A large body of work on regularization techniques to learn robust representations and models exists. Examples include 2 -regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising. Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al., 1987; T¨uske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), work on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP. While invariance transformations such as rotation, translation in vision or vocal tract length, reverberation, and noise in speech have all been harnessed, we do not have a good intuition on useful perturbations for written language yet. Label dropout and ﬂip (cf. typos) have been proposed both on the byte-level (Gillick et al., 2015) and the word-level (Xie et al., 2017). Syntactic and semantic noise for semantic analysis was studied in Yitong et al. (2017). From a human perception perspective, word scrambling may be of interest (Rawlinson, 1"
W18-6468,P15-2026,0,0.164005,". (2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi-source ({src, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture: two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded"
W18-6468,2011.mtsummit-papers.35,1,0.854702,"Missing"
W18-6468,P11-2031,0,0.0221022,"those of the submitted overall ensemble model on test18. 3.4 Results and Discussion Table 2 presents the results for the PBSMT APE task (cf. 3.3.1), where two different transformerbased models, one ensemble of these models and the baseline BLEU scores are shown. The baseline here refers to the original MT output evaluated with respect to the corresponding PE translation. All models yield statistically significant results (p &lt; 0.001) over this baseline. M Savg also provides statistically significant improvement over SSavg . For this and all following significance tests we employ the method by Clark et al. (2011)2 . Generally, reasons for the good performance of our transformer-based MS architecture in comparison to the SS approach for PBSMT-based APE could be the positional encoding that injects information about the relative or absolute position of the tokens in the sequence. This might help to handle word order errors in mt for a given src context. Another possible explanation lies in the self-attention mechanism, which handles local word dependencies for src, mt, and pe. After the individual dependencies are learned by the two encoders’ self-attention mechanisms, another level of self-attention is"
W18-6468,W16-2378,0,0.314737,"ta minimally involves MT output (mt) and the human-post-edited (pe) version of mt, but may additionally make use of the source (src). A more detailed motivation on APE can be found in Bojar et al. (2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi-source ({src, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture: two sepa"
W18-6468,E17-1050,0,0.0501277,"MT output to their corresponding corrections. APE training data minimally involves MT output (mt) and the human-post-edited (pe) version of mt, but may additionally make use of the source (src). A more detailed motivation on APE can be found in Bojar et al. (2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi-source ({src, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018)."
W18-6468,P07-2045,0,0.00465891,"similar TER statistics; and second, the synthetic ‘eSCAPE’ APE corpus (Negri et al., 2018), consisting of more than 7M triples for both NMT and PBSMT. Table 1 presents the statistics of the released data for the English–German APE Task organized in WMT 2018. These datasets, except for the eSCAPE corpus, do not require any preprocessing in terms of encoding or alignment. For cleaning the noisy eSCAPE dataset containing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 80, respectively. After cleaning, we use the Moses tokenizer to tokenize the eSCAPE corpus. To handle outof-vocabulary words, words are preprocessed into subword units (Sennrich et al., 2016) using bytepair encoding (BPE). network are set to 0.2. Each encoder and decoder contains a fully connected feed-forward network having dimensionality dmodel = 256 for the input and output and dimensionality df f = 1024 for the inner layer. This is a similar setting to Vaswani et al. (2017)’s C − model1 . For the scaled dotprod"
W18-6468,W16-2379,1,0.894916,"Missing"
W18-6468,N09-2055,0,0.0773111,"Missing"
W18-6468,2015.mtsummit-wptp.4,0,0.113047,"Missing"
W18-6468,W16-2361,0,0.207937,"Missing"
W18-6468,2015.mtsummit-papers.11,0,0.178999,"Missing"
W18-6468,L18-1004,0,0.201156,"glish– German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a first stage MT system, and the corresponding human-post-edited version (pe), all of them already tokenized. As this released APE dataset is small in size (see Table 1), additional resources are also available: first, the ‘artificial training data’ (Junczys-Dowmunt and Grundkiewicz, 2016) containing 4.5M sentences, 4M of which are weakly similar to the WMT 2016 training data, while 500K show very similar TER statistics; and second, the synthetic ‘eSCAPE’ APE corpus (Negri et al., 2018), consisting of more than 7M triples for both NMT and PBSMT. Table 1 presents the statistics of the released data for the English–German APE Task organized in WMT 2018. These datasets, except for the eSCAPE corpus, do not require any preprocessing in terms of encoding or alignment. For cleaning the noisy eSCAPE dataset containing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set t"
W18-6468,W12-3146,0,0.053688,"Missing"
W18-6468,C16-1172,0,0.295776,"Missing"
W18-6468,P16-1162,0,0.061481,"tasets, except for the eSCAPE corpus, do not require any preprocessing in terms of encoding or alignment. For cleaning the noisy eSCAPE dataset containing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 80, respectively. After cleaning, we use the Moses tokenizer to tokenize the eSCAPE corpus. To handle outof-vocabulary words, words are preprocessed into subword units (Sennrich et al., 2016) using bytepair encoding (BPE). network are set to 0.2. Each encoder and decoder contains a fully connected feed-forward network having dimensionality dmodel = 256 for the input and output and dimensionality df f = 1024 for the inner layer. This is a similar setting to Vaswani et al. (2017)’s C − model1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension dk , and values of dimension dv . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensionality of dk = dv = dmodel /h = 32. For op"
W18-6468,W15-3017,1,0.871153,"Missing"
W18-6468,N07-1064,0,0.220716,"efore be viewed as a 2nd -stage MT system, translating predictable error patterns in MT output to their corresponding corrections. APE training data minimally involves MT output (mt) and the human-post-edited (pe) version of mt, but may additionally make use of the source (src). A more detailed motivation on APE can be found in Bojar et al. (2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi-source ({src, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing"
W18-6468,C16-1241,1,0.806247,"Missing"
W18-6468,W07-0728,0,0.0476416,"efore be viewed as a 2nd -stage MT system, translating predictable error patterns in MT output to their corresponding corrections. APE training data minimally involves MT output (mt) and the human-post-edited (pe) version of mt, but may additionally make use of the source (src). A more detailed motivation on APE can be found in Bojar et al. (2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi-source ({src, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing"
W18-6468,P16-2046,1,0.927101,"Missing"
W18-6468,W17-4777,0,0.632079,"rc, mt} → pe) approaches. In general, the field of APE covers a wide methodological range, including SMTbased approaches (Simard et al., 2007a,b; Lagarda et al., 2009; Rosa et al., 2012; Pal et al., 2016c; Chatterjee et al., 2017b), and neural APE (Pal et al., 2016b; Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2017) based on neural machine translation (NMT). Some of the stateof-the-art multi-source approaches, both statistical (B´echara et al., 2011; Chatterjee et al., 2015) and recently neural (Libovick´y et al., 2016; Chatterjee et al., 2017a; Junczys-Dowmunt and Grundkiewicz, 2016; Varis and Bojar, 2017), explore learning from {src, mt} → pe (multi-source, MS) This paper presents our English–German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture: two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt ) for generating the post-edited sentence. We compare this multi-source architecture (i.e, {src, mt} → pe)"
W18-6468,E17-2056,1,0.885018,"Missing"
W19-5332,D18-1399,0,0.115166,"ta is not enough to train a neural system for such a low resource language pair. Therefore, preparation for large volume of parallel corpus is required which can be produced either by manual translation by professional translators or scraping parallel data from the internet. However, these processes are costly, tedious and sometimes inefficient (in case of scraping from internet). As the released data was insufficient, to generate more training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing mode"
W19-5332,1983.tc-1.13,0,0.639264,"Missing"
W19-5332,2000.eamt-1.5,0,0.626376,"Missing"
W19-5332,N18-4016,0,0.0187063,"atistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding (Sennrich et al., 2016b) to handle OutOf-Vocabulary Words. 3 Data Preparation For our experiments we used both parallel and mon"
W19-5332,W18-6459,0,0.0163515,"n be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding (Sennrich et al., 2016b) to handle OutOf-Vocabulary Words. 3 Data Preparation For our experiments we used both parallel and monolingual corpus released by the WMT 2019 Organizers. We back-translate the monolingual corpus and use it as additional synthetic parallel corpus to train our NMT system. The detailed statistics of the corpus is given in Table 1. We performed our experiments on two datasets, one using the parallel corpus provide"
W19-5332,P16-1009,0,0.200866,"training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding"
W19-5332,W14-3308,0,0.0232331,"ne translation (MT) that uses artificial neural network to directly model the conditional probability p(y|x) of translating a source sentence (x1 ,x2 ,...,xn ) into a target sentence (y1 ,y2 ,...,ym ). NMT has consistently performed better than the phrase-based statistical MT (PB-SMT) approaches and has provided state-ofthe-art results in the last few years. However, one of the major constraints of using supervised NMT is that it is not suitable for low resource language pairs. Thus, to use supervised NMT, low resource pairs need to resort to other techniques 2 Related Works Dungarwal et al. (Dungarwal et al., 2014) developed a statistical method for machine translation, where phrase based method for Hindi-English and factored based method for English-Hindi SMT system was used. They had shown improvements to the existing SMT systems using pre-procesing and post-processing components that generated morphological inflections correctly. Imankulova et al. (Imankulova et al., 2017) showed how backtranslation and filtering from monolingual data can be used to build an effective translation system for a low-resourse language pair like Japanese∗ These three authors have contributed equally. 308 Proceedings of th"
W19-5332,P16-1162,0,0.189162,"training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding"
W19-5332,W17-5704,0,0.0159843,"r, one of the major constraints of using supervised NMT is that it is not suitable for low resource language pairs. Thus, to use supervised NMT, low resource pairs need to resort to other techniques 2 Related Works Dungarwal et al. (Dungarwal et al., 2014) developed a statistical method for machine translation, where phrase based method for Hindi-English and factored based method for English-Hindi SMT system was used. They had shown improvements to the existing SMT systems using pre-procesing and post-processing components that generated morphological inflections correctly. Imankulova et al. (Imankulova et al., 2017) showed how backtranslation and filtering from monolingual data can be used to build an effective translation system for a low-resourse language pair like Japanese∗ These three authors have contributed equally. 308 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 308–313 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Dataset Parallel Corpora Cleaned Parallel Corpora Back-translated Data Development Data Gujarati Test Data English Test Data Pairs is important in the splitting part too as it is impo"
W19-5332,2006.amta-papers.25,0,0.0451986,"ask for English–Gujarati and Gujarati–English. The released training data set is completely different in-domain compared to the development set and the size is not anywhere close to the sizable amount of training data which is typically required for the success of NMT systems. We use additional synthetic data produced through backtranslation from the monolingual corpus. This provides significant improvements in translation performance for both our English–Gujarati and Gujarati–English NMT systems. Our English– Gujarati system was ranked second in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) in the shared task. In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task subtrack. Our baseline and primary submissions are built using a Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism followed by fine-tuning using in-domain data. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, b"
W19-5332,P17-4012,0,0.0223876,"s initially set to 1.0. Table 2 shows the hyper-parameter configurations for our Gujarati–English translation system. We initially trained our model with the cleaned parallel corpus provided by WMT 2019 up to 100K training steps. Thereafter, we fine-tune our generic model on domain specific corpus (containing 219K sentences back-translated using Doc Translator API) changing the learning rate to 0.5 and decay started from 130K training steps with a decay factor of 0.5 and keeping the other hyperparameters same as mentioned in Table 2. Data Postprocessing Postprocessing, such as detokenization (Klein et al., 2017), punctuation normalization4 (Koehn et al., 2007), was performed on our translated data (on the test set) to produce the final translated data. 4 Primary System description Experiment Setup We have explained our experimental setups in the next two sections. The first section contains the setup used for our final submission and the next section describes all the other supporting experimental setups. We use the OpenNMT toolkit (Klein et al., 2017) for our experiments. We performed several experiments where the parallel corpus is sent to the model as space separated character format, space separa"
W19-5332,P07-2045,0,0.00941137,"nslation pairs makes the model prone to overfitting and hence prevents it from recognizing new features. Thus, one of the sentence pair is kept while the other redundant pairs are removed. Some sentence pairs had combinations of both language pairs which were also identified as redundant. These pairs strictly need elimination as the vocabularies of the individual languages consist of alphanumeric characters of the other language which results in inconsistent encoding and decoding during encoderdecoder application steps on the considered language pair. We tokenize the English side using Moses (Koehn et al., 2007) tokenizer and for Gujarati, we use the Indic NLP library tokenization tool3 . Punctuation normalization was also done. 3.2 Value text fp32 2 8 500 256 160,000 50,000 50,000 warm-up+decay* softmax wordpiece LSTM Table 2: Hyper-parameter configurations for Gujarati– English translation using unidirectional RNN (Cho et al., 2014)), *learning-rate was initially set to 1.0. Table 2 shows the hyper-parameter configurations for our Gujarati–English translation system. We initially trained our model with the cleaned parallel corpus provided by WMT 2019 up to 100K training steps. Thereafter, we fine-t"
W19-5332,P02-1040,0,0.104065,"the WMT 2019 news translation task for English–Gujarati and Gujarati–English. The released training data set is completely different in-domain compared to the development set and the size is not anywhere close to the sizable amount of training data which is typically required for the success of NMT systems. We use additional synthetic data produced through backtranslation from the monolingual corpus. This provides significant improvements in translation performance for both our English–Gujarati and Gujarati–English NMT systems. Our English– Gujarati system was ranked second in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) in the shared task. In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task subtrack. Our baseline and primary submissions are built using a Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism followed by fine-tuning using in-domain data. Given the fact that the two languages belong to different language families and there is not enough parallel"
W19-5350,D18-1040,0,0.0201294,"kely to be in News domain. Equation 1 is used to score each sentence pair in the out-ofdomain corpus. In Equation 1, Ps is the language model probability of the source sentence; Ns is the length of the source sentence; Pt is the language model probability of the target sentence; Nt is the length of the target sentence. We selected the top 15M scored sentence pairs from out-ofdomain data for training our systems. log Ps log Pt + Ns Nt 2.2 Back-translation We back-translated the 2018 part of the large monolingual in-domain News crawl data as additional training data for our translation systems. Fadaee and Monz (2018) showed that it is more beneficial to back-translate sentences that contain difficult words. In our experiments, we consider words which occur less than 1000 times in the bilingual training data as difficult words. Then we randomly selected 10M sentences which contain difficult words for back-translation. The mod(1) The neural language models trained for data 441 Stage 1 Stage 2 Stage 3 in-domain 356k X X X out-of-domain 15M X X back-translated 10M Stage 1 Stage 2 Stage 3 Ensemble X Table 1: Training data used in different training stages. Stage 1 Stage 2 Stage 3 en-de base big 7.3 7.6 0.3 0.4"
W19-5350,W11-2123,0,0.0311609,"h can be trained very fast. Figure 1 (a) shows the structure of the standard Transformer translation model (Vaswani et al., 2017) and we removed the encoder and the attention layer in the decoder from the Transformer translation model to create our Transformer language model as shown in Figure 1 (b). For training efficiency, we used byte pair encoding (Sennrich et al., 2016b) to learn a vocabulary of 50k for English and German respectively. formance due to domain dismatch. Therefore, we performed data selection on out-of-domain data. Inspired by Schamper et al. (2018)’s work which used KenLM (Heafield, 2011) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively. Because these neural language models are trained on the News domain, we can use them to score out-ofdomain data. Sentences with higher probabilities are more likely to be in News domain. Equation 1 is used to score each sentence pair in the out-ofdomain corpus. In Equation 1, Ps is the language model probability of the source sentence; Ns is the length of the source sentence; Pt is the language model pro"
W19-5350,E17-3017,0,0.0312043,"analysis of the performance of our system. 1 In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (Luong and Manning, 2015; Schamper et al., 2018). Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors. Introduction This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-toEnglish directions. We trained Transformer models (Vaswani et al., 2017) using Sockeye1 (Hieber et al., 2017). Compared to RNN-based translation models (Bahdanau et al., 2014), Transformer models can be trained very fast due to parallelizable self-attention networks. We applied several very useful techniques for effectively training our models. Model Ensemble We trained two Transformer models with different sizes, Transformer-base and Transformer-big. Our final submission is an ensemble of both models (Schamper et al., 2018). The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task. 2 2.1 Da"
W19-5350,P07-2045,0,0.0131636,"does not 2.4 Results and Analysis Table 2 shows the numbers of training epochs for different training stages and Table 3 shows the performance of our systems after different training stages. As we can see, back-translation (Stage 2) and in-domain fine-tuning (Stage 3) both improved the translation quality on a significant level. An ensemble of Stage 3 Transformer-base and Transformer-big achieved further improvements. We also tried to ensemble different checkpoints of Transformer-big, but achieved little improvement, likely because different checkpoints of 2 For preprocessing, we used Moses (Koehn et al., 2007) scripts normalize-punctuation.perl, tokenizer.perl, lowercase.perl. We trained a recaser using train-recaser.perl to recase translations. 442 Example 1 Src Ref Ours Example 2 Src Ref Ours wei@@ dez@@ aun@@ projekt ist element@@ ar past@@ ure fence project is fundamental electric sound project is elementary jetzt nimmt sich das weiße haus von trump die freiheits@@ statue vor now trump &apos;s white house is targeting the statue of liberty now trump &apos;s white house takes the statue of liberty Table 4: Translation examples. “@@” means segmented by byte pair encoding. Acknowledgments the same"
W19-5350,W18-6426,0,0.316158,"rticipated in both English-to-German and German-to-English directions. We trained standard Transformer models and adopted various techniques for effectively training our models, including data selection, backtranslation, in-domain fine-tuning and model ensemble. We show that these training techniques improved the performance of our Transformer models up to 5 BLEU points. We give a detailed analysis of the performance of our system. 1 In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (Luong and Manning, 2015; Schamper et al., 2018). Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors. Introduction This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-toEnglish directions. We trained Transformer models (Vaswani et al., 2017) using Sockeye1 (Hieber et al., 2017). Compared to RNN-based translation models (Bahdanau et al., 2014), Transformer models can be trained very fast due to parallelizable self-attention networks. We applied several very useful techniques for effec"
W19-5350,P16-1009,0,0.175012,"(a)Transformer translation model (b)Transformer language model Figure 1: Structures of Transformer translation models and Transformer language models used in our experiments. selection in our experiments are based on selfattention networks which can be trained very fast. Figure 1 (a) shows the structure of the standard Transformer translation model (Vaswani et al., 2017) and we removed the encoder and the attention layer in the decoder from the Transformer translation model to create our Transformer language model as shown in Figure 1 (b). For training efficiency, we used byte pair encoding (Sennrich et al., 2016b) to learn a vocabulary of 50k for English and German respectively. formance due to domain dismatch. Therefore, we performed data selection on out-of-domain data. Inspired by Schamper et al. (2018)’s work which used KenLM (Heafield, 2011) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively. Because these neural language models are trained on the News domain, we can use them to score out-ofdomain data. Sentences with higher probabilities are more likely to"
W19-5350,P16-1162,0,0.284862,"(a)Transformer translation model (b)Transformer language model Figure 1: Structures of Transformer translation models and Transformer language models used in our experiments. selection in our experiments are based on selfattention networks which can be trained very fast. Figure 1 (a) shows the structure of the standard Transformer translation model (Vaswani et al., 2017) and we removed the encoder and the attention layer in the decoder from the Transformer translation model to create our Transformer language model as shown in Figure 1 (b). For training efficiency, we used byte pair encoding (Sennrich et al., 2016b) to learn a vocabulary of 50k for English and German respectively. formance due to domain dismatch. Therefore, we performed data selection on out-of-domain data. Inspired by Schamper et al. (2018)’s work which used KenLM (Heafield, 2011) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively. Because these neural language models are trained on the News domain, we can use them to score out-ofdomain data. Sentences with higher probabilities are more likely to"
W19-5350,P17-2089,0,0.0203592,"ansformer models with different sizes, Transformer-base and Transformer-big. Our final submission is an ensemble of both models (Schamper et al., 2018). The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task. 2 2.1 Data Selection The parallel training data provided for German-English is quite large (38M sentence pairs). Most of the parallel data is crawled from the Internet and is not in News domain. Out-ofdomain training data can hurt the translation performance on News test sets (Wang et al., 2017) and also significantly increase training time. Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (Schamper et al., 2018). Data Selection The parallel data provided for the German-toEnglish and English-to-German tasks includes Europarl v9, ParaCrawl v3, Common Crawl corpus, News Commentary v14, Wiki Titles v1 and Document-split Rapid corpus. We also used old test sets (newstest2008 to newstest2017) for training our systems. We consider News Commentary v14 and old test sets as in-domain data and the rest as out-of-domain data. Compared to"
W19-5414,D11-1033,0,0.0253945,"split the released data (13.4K) into two sets; we use the first 12K for training and the remaining 1.4K as validation data. The development set (Dev) released by WMT20192 is used as test data for our experiment. We build two models transference4M and transferenceALL using slightly different training procedures. For transference4M, we first train on a training set called eScape4M combined with the first 12k of the provided NMT training data. This eScape4M data is prepared using in-domain (for our case the 12K training data) bilingual cross-entropy difference for data selection as described in Axelrod et al. (2011). The difference in cross-entropy is computed based on two language models (LM): a domain-specific LM is estimated from the indomain (12K) PE corpus (lmi ) and the out-domain LM (lmo ) is estimated from the eScape corpus. We rank the eScape corpus by assigning a score to each of the individual sentences which is the sum of the three cross-entropy (H) differences. For a j th sentence pair srcj –mtj –pej , the score is calculated based on Equation 1. 3.3 Hyper-parameter Setup We follow a similar hyper-parameter setup for all reported systems. All encoders (for {src, mt}tr → pe), and the decoder,"
W19-5414,P11-2031,0,0.0398634,"e EN-DE NMT task. Exp No. Models BLEU ↑ Test TER ↓ Baseline 1 2 3 74.73 Submission transference4M (CONTRASTIVE) 73.97 (-0.76) transferenceALL (PRIMARY) 75.75 (+1.02) raw MT 16.84 17.31 (+0.47) 16.15 (-0.69) Table 3: Evaluation results on the WMT APE 2019 test set for the EN-DE NMT task. 4.1 yield statistically significant results (p < 0.001) over the raw MT baseline. transferenceALL (PRIMARY, our primary submission in WMT2019 APE task) (Exp 7) also provides statistically significant improvement over transference4M (Exp 6). For these and all following significance tests we employ the method by Clark et al. (2011)3 . Table 2 shows that our APE architecture transferenceALL (PRIMARY) (Exp 7) significantly improves over the already very good NMT system by about +0.91 BLEU and -0.56 TER. Table 3 presents the results of our submissions on the Test set in the WMT 2019 ENDE APE task. We submitted transference4M (CONTRASTIVE) system – a weak model having performance close to the baseline, (i) to check whether in-domain data provides any gain in performance on the Test set or not, (ii) to create another baseline trained on in-domain data, by which we could analyze our PRIMARY transference model’s capability of"
W19-5414,W16-2378,0,0.0440917,"nabith1,2 Santanu Pal1,2 , Hongfei Xu1,2 , Nico Herbig2 , Antonio Kruger 1 Department of Language Science and Technology, Saarland University, Germany 2 German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus, Germany {santanu.pal, josef.vangenabith}@uni-saarland.de {hongfei.xu, nico.herbig, krueger, josef.van genabith}@dfki.de Abstract source ({src, mt} → pe) approaches. This integration of source-language information in APE is intuitively useful in conveying context information to improve APE performance. Neural APE was first proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src and mt and passing the concatenation of both encoders’ final states to the decoder (Libovick´y et al., 2016). A small number of multi-source neural APE approaches were proposed in the WMT 2017 APE shared task. The two-encoder architecture (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Varis and Bojar, 2017) of multi-source models utilizes both the source text (src) and the MT output"
W19-5414,W18-6467,0,0.0567849,"the source (src) has been shown to provide further benefits (Bojar et al., 2015, 2016, 2017). Based on the training process, APE systems can be categorized as either single-source (mt → pe) or multi124 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 124–131 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics after another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should remain in pe. The WMT 2018 winner for the PBSMT task (Junczys-Dowmunt and Grundkiewicz, 2018) also presented transformer-based multisource APE called a dual-source transformer architecture. They use two encoders and stack an additional cross-attention component for src → pe above the previous cross-attention for mt → pe. Comparing Shin and Lee (2018)’s approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of src → mt and src → pe in the decoder, and (ii) the winner system additionally shares parameters between two encoders. In this work, we present a multi-source neural APE architecture called transference1 . Our model conta"
W19-5414,W17-4773,1,0.922074,"to improve APE performance. Neural APE was first proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src and mt and passing the concatenation of both encoders’ final states to the decoder (Libovick´y et al., 2016). A small number of multi-source neural APE approaches were proposed in the WMT 2017 APE shared task. The two-encoder architecture (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Varis and Bojar, 2017) of multi-source models utilizes both the source text (src) and the MT output (mt) to predict the postedited output (pe) in a single end-to-end neural architecture. In the WMT 2018 APE shared task, further multi-source APE architectures based on the transformer model (Vaswani et al., 2017) have been presented. The winning team for the NMT task in WMT 2018 Tebbifakhr et al. (2018) employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. (Pal et al., 2018) proposed an APE model that us"
W19-5414,P02-1040,0,0.10395,"ons for future work. 2 Figure 1: The transference model architecture for APE ({src, mt}tr → pe). layer and an additional cross-attention layer for src → mt. Here, the encsrc encoder and the decpe decoder are equivalent to the original transformer for neural MT (Vaswani et al., 2017). Put differently, our multi-source APE implementation extends Vaswani et al. (2017) by introducing an additional encoding block by which src and mt communicate with the decoder. 3 Experiments We compare our approach against the raw MT output provided by the 1st -stage MT system. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). 3.1 Data For our experiments, we use the English–German WMT 2019 (Chatterjee et al., 2018) neural APE data. All released APE datasets consist of English–German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a 1st stage NMT system, and the corresponding humanpost-edited version (pe). Table 1 presents the statistics of the released data. As this released APE dataset is small in size (see Table 1), the synthetic eScape APE corpus (Negri et al., 2018), consisting of more than 7M triples, is available"
W19-5414,2015.mtsummit-wptp.4,0,0.021683,"Missing"
W19-5414,W16-2361,0,0.134007,"Missing"
W19-5414,2015.mtsummit-papers.11,0,0.0332464,"Missing"
W19-5414,L18-1004,0,0.125487,"MT system. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). 3.1 Data For our experiments, we use the English–German WMT 2019 (Chatterjee et al., 2018) neural APE data. All released APE datasets consist of English–German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a 1st stage NMT system, and the corresponding humanpost-edited version (pe). Table 1 presents the statistics of the released data. As this released APE dataset is small in size (see Table 1), the synthetic eScape APE corpus (Negri et al., 2018), consisting of more than 7M triples, is available as an additional resource. All datasets, except for the eScape corpus, do not require any preprocessing in terms of encoding, tokenization or alignment. For cleaning the noisy eScape dataset containing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. Transference Model for APE We propose a multi-source"
W19-5414,P16-1162,0,0.0761845,"implementation is available at https:// github.com/santanupal1980/Transference. git 125 Corpus Train Dev Test eScape Sentences Overall Cleaning 13,442 1,000 1,023 7.2M 6.5M Both models are then fine-tuned towards the real data, by training again solely on the first 12k segments of the provided data. For both models, we perform checkpoint averaging using the 8 best checkpoints. We report the results on the development set provided by WMT2019, which we use as a test set. To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the src, mt and pe, we define BPE tokens by jointly processing all triplets. Thus, src, mt and pe derive a single BPE vocabulary. Since mt and pe belong to the same language (DE) and src is a close language (EN), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. Table 1: Statistics of the WMT 2019 English-German APE Shared Task Dataset. (iii) After cleaning, we perform punctuation normalization, and then use the Moses tokenizer to tokenize"
W19-5414,C16-1172,0,0.118961,"Missing"
W19-5414,W18-6470,0,0.398369,"APE architectures based on the transformer model (Vaswani et al., 2017) have been presented. The winning team for the NMT task in WMT 2018 Tebbifakhr et al. (2018) employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. (Pal et al., 2018) proposed an APE model that uses two separate self-attention-based encoders to encode mt and src, followed by a self-attended joint encoder that attends over a combination of the two encoded sequences and is used by the decoder for generating the post-edited sentence pe. Shin and Lee (2018) propose that each encoder has its own selfattention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for src → mt and another for src → pe. ThereIn this paper we present an English–German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019. Our transference model is based on a multi-encoder transformer architecture. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a transformer decoder block, but without masking"
W19-5414,W18-6468,1,0.733685,"Missing"
W19-5414,2006.amta-papers.25,0,0.0291767,"The transference model architecture for APE ({src, mt}tr → pe). layer and an additional cross-attention layer for src → mt. Here, the encsrc encoder and the decpe decoder are equivalent to the original transformer for neural MT (Vaswani et al., 2017). Put differently, our multi-source APE implementation extends Vaswani et al. (2017) by introducing an additional encoding block by which src and mt communicate with the decoder. 3 Experiments We compare our approach against the raw MT output provided by the 1st -stage MT system. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). 3.1 Data For our experiments, we use the English–German WMT 2019 (Chatterjee et al., 2018) neural APE data. All released APE datasets consist of English–German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a 1st stage NMT system, and the corresponding humanpost-edited version (pe). Table 1 presents the statistics of the released data. As this released APE dataset is small in size (see Table 1), the synthetic eScape APE corpus (Negri et al., 2018), consisting of more than 7M triples, is available as an additional resource. Al"
W19-5414,W15-3017,1,0.900274,"Missing"
W19-5414,W18-6471,0,0.345226,"ibovick´y et al., 2016). A small number of multi-source neural APE approaches were proposed in the WMT 2017 APE shared task. The two-encoder architecture (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Varis and Bojar, 2017) of multi-source models utilizes both the source text (src) and the MT output (mt) to predict the postedited output (pe) in a single end-to-end neural architecture. In the WMT 2018 APE shared task, further multi-source APE architectures based on the transformer model (Vaswani et al., 2017) have been presented. The winning team for the NMT task in WMT 2018 Tebbifakhr et al. (2018) employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. (Pal et al., 2018) proposed an APE model that uses two separate self-attention-based encoders to encode mt and src, followed by a self-attended joint encoder that attends over a combination of the two encoded sequences and is used by the decoder for generating the post-edited sentence pe. Shin and Lee (2018) propose that each encoder has its own selfattention and feed-forward layer to process each input separately. On the decoder side, they add two a"
W19-5414,C16-1241,1,0.894741,"Missing"
W19-5414,W17-4777,0,0.133652,"ce. Neural APE was first proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src and mt and passing the concatenation of both encoders’ final states to the decoder (Libovick´y et al., 2016). A small number of multi-source neural APE approaches were proposed in the WMT 2017 APE shared task. The two-encoder architecture (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Varis and Bojar, 2017) of multi-source models utilizes both the source text (src) and the MT output (mt) to predict the postedited output (pe) in a single end-to-end neural architecture. In the WMT 2018 APE shared task, further multi-source APE architectures based on the transformer model (Vaswani et al., 2017) have been presented. The winning team for the NMT task in WMT 2018 Tebbifakhr et al. (2018) employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. (Pal et al., 2018) proposed an APE model that uses two separate self-att"
W19-5414,P16-2046,1,0.881439,"Missing"
W19-5417,W17-4772,0,0.0192749,"t BLEU scores on the development set, another one was the model saved for each training epoch with lowest validation perplexity. Models MT as PE Processed MT Base Gaussian Uniform Ensemble x5 Models MT as PE Gaussian Uniform Ensemble x5 TER 16.84 16.79 16.80 16.77 BLEU 74.73 75.03 75.03 75.03 BLEU Table 3: Results on the Test Set 76.76 76.61 76.91 ∼ 77.13 76.94 ∼ 77.08 77.01 ∼ 77.10 77.22 4 Related Work Pal et al. (2016) applied a multi-source sequenceto-sequence neural model for APE, and Vu and Haffari (2018) jointly trained machine translation with the post editing sequence prediction task (Berard et al., 2017). Though all previous approaches get significant improvements over Statistical Machine Translation outputs, benefits with APE on top of Neural Machine Translation outputs are not very significant (Chatterjee et al., 2018). On the other hand, advanced neural machine translation approaches may also improve the APE task, such as: combining advances of the recurrent decoder (Chen et al., 2018), the Evolved Transformer architecture (So et al., 2019), Layer Aggregation (Dou et al., 2018) and Dynamic Convolution structures (Wu et al., 2019). Table 2: BLEU Scores on the Development Set Table 2 shows t"
W19-5417,W18-6467,0,0.401437,"ar to raw machine translation outputs. In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder. 1 Introduction The Automatic Post-Editing (APE) task is to automatically correct errors in machine translation outputs. This paper describes our submission to the English-German APE shared task at WMT 2019. Based on recent research on the APE task (Junczys-Dowmunt and Grundkiewicz, 2018) and an architecture for the utilization of documentlevel context information in neural machine translation (Zhang et al., 2018b), we re-implement a multi-source transformer model for the task. Inspired by Cheng et al. (2018), we try to train a more robust model by introducing a multi-task learning approach which jointly trains APE with a de-noising encoder. We made use of the artificial eScape data set (Negri et al., 2018) provided for the task, since the multi-source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can he"
W19-5417,P07-2045,0,0.00666265,"beam size of 4 without any length penalty. loss = λ ∗ lossape + (1 − λ) ∗ lossde−noising (2) i.e. the loss between the APE task and the denoising encoder task are balanced by λ in this way. 3 Experiments We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) for transformer-based neural machine translation. 3.1 Data and Settings We only participated in the English to German task, and we used both the training set provided by WMT and the synthetic eSCAPE corpus (Negri et al., 2018). We first re-tokenized1 and truecased both data sets with tools provided by Moses (Koehn et al., 2007), then cleaned the data sets with scripts ported from the Neutron implementation, and the original training set was up-sampled 20 times as in (Junczys-Dowmunt and Grundkiewicz, 2018). We applied joint Byte-Pair Encoding (Sennrich et al., 2016) with 40k merge operations and 50 as the vocabulary threshold for the BPE. We only kept sentences with a max of 256 sub-word tokens for training, and obtained a training set of about 6.5M triples with a shared vocabulary of 42476. We did not apply any domain adaptation approach for our submission considering that (Junczys-Dowmunt and Grundkiewicz, 2018) s"
W19-5417,P18-1008,0,0.0236351,"Related Work Pal et al. (2016) applied a multi-source sequenceto-sequence neural model for APE, and Vu and Haffari (2018) jointly trained machine translation with the post editing sequence prediction task (Berard et al., 2017). Though all previous approaches get significant improvements over Statistical Machine Translation outputs, benefits with APE on top of Neural Machine Translation outputs are not very significant (Chatterjee et al., 2018). On the other hand, advanced neural machine translation approaches may also improve the APE task, such as: combining advances of the recurrent decoder (Chen et al., 2018), the Evolved Transformer architecture (So et al., 2019), Layer Aggregation (Dou et al., 2018) and Dynamic Convolution structures (Wu et al., 2019). Table 2: BLEU Scores on the Development Set Table 2 shows that the performance got slightly hurt (comparing “Processed MT” with “MT as PE”) with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task. We think this is perhaps because"
W19-5417,2015.iwslt-evaluation.11,0,0.0467376,"e original training set was up-sampled 20 times as in (Junczys-Dowmunt and Grundkiewicz, 2018). We applied joint Byte-Pair Encoding (Sennrich et al., 2016) with 40k merge operations and 50 as the vocabulary threshold for the BPE. We only kept sentences with a max of 256 sub-word tokens for training, and obtained a training set of about 6.5M triples with a shared vocabulary of 42476. We did not apply any domain adaptation approach for our submission considering that (Junczys-Dowmunt and Grundkiewicz, 2018) shows few improvements, but advanced domain adaption (Wang et al., 2017) or fine-tuning (Luong and Manning, 2015) methods may still bring some improvements. The training set was shuffled for each training epoch. Like Junczys-Dowmunt and Grundkiewicz (2018), all embedding matrices were bound with the weight of the classifier. But for tokens which in fact do never appear in post-editing outputs in the shared vocabulary, we additionally remove their weights in the label smoothing loss and set corresponding biases in the decoder classifier to −1032 . Unlike Zhang et al. (2018b), the source encoder, the machine translation encoder and the decoder had 6 layers. The hidden dimension of the 1 3.2 Results We firs"
W19-5417,P18-1163,0,0.10338,"t this in our own transformer model and explore joint training of the APE task with a de-noising encoder. 1 Introduction The Automatic Post-Editing (APE) task is to automatically correct errors in machine translation outputs. This paper describes our submission to the English-German APE shared task at WMT 2019. Based on recent research on the APE task (Junczys-Dowmunt and Grundkiewicz, 2018) and an architecture for the utilization of documentlevel context information in neural machine translation (Zhang et al., 2018b), we re-implement a multi-source transformer model for the task. Inspired by Cheng et al. (2018), we try to train a more robust model by introducing a multi-task learning approach which jointly trains APE with a de-noising encoder. We made use of the artificial eScape data set (Negri et al., 2018) provided for the task, since the multi-source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can help regularize its parameters and make the model more general. We then tested the BLEU scores between machine translation results and corresponding gold standard post-editing results on the original development set, the traini"
W19-5417,L18-1004,0,0.330592,"ne translation outputs. This paper describes our submission to the English-German APE shared task at WMT 2019. Based on recent research on the APE task (Junczys-Dowmunt and Grundkiewicz, 2018) and an architecture for the utilization of documentlevel context information in neural machine translation (Zhang et al., 2018b), we re-implement a multi-source transformer model for the task. Inspired by Cheng et al. (2018), we try to train a more robust model by introducing a multi-task learning approach which jointly trains APE with a de-noising encoder. We made use of the artificial eScape data set (Negri et al., 2018) provided for the task, since the multi-source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can help regularize its parameters and make the model more general. We then tested the BLEU scores between machine translation results and corresponding gold standard post-editing results on the original development set, the training set and the synthetic data as shown in Table 1. dev 77.15 train 77.42 2 Our Approach We simplify and employ a multi-source transformer model (Zhang et al., 2018b) for the APE task, and try to train a"
W19-5417,D18-1457,0,0.0211903,"and Vu and Haffari (2018) jointly trained machine translation with the post editing sequence prediction task (Berard et al., 2017). Though all previous approaches get significant improvements over Statistical Machine Translation outputs, benefits with APE on top of Neural Machine Translation outputs are not very significant (Chatterjee et al., 2018). On the other hand, advanced neural machine translation approaches may also improve the APE task, such as: combining advances of the recurrent decoder (Chen et al., 2018), the Evolved Transformer architecture (So et al., 2019), Layer Aggregation (Dou et al., 2018) and Dynamic Convolution structures (Wu et al., 2019). Table 2: BLEU Scores on the Development Set Table 2 shows that the performance got slightly hurt (comparing “Processed MT” with “MT as PE”) with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task. We think this is perhaps because there is a gap between the generated machine translation outputs with noise and the real worl"
W19-5417,P16-2046,1,0.922964,"Missing"
W19-5417,P16-1162,0,0.0328678,"on the Neutron implementation (Xu and Liu, 2019) for transformer-based neural machine translation. 3.1 Data and Settings We only participated in the English to German task, and we used both the training set provided by WMT and the synthetic eSCAPE corpus (Negri et al., 2018). We first re-tokenized1 and truecased both data sets with tools provided by Moses (Koehn et al., 2007), then cleaned the data sets with scripts ported from the Neutron implementation, and the original training set was up-sampled 20 times as in (Junczys-Dowmunt and Grundkiewicz, 2018). We applied joint Byte-Pair Encoding (Sennrich et al., 2016) with 40k merge operations and 50 as the vocabulary threshold for the BPE. We only kept sentences with a max of 256 sub-word tokens for training, and obtained a training set of about 6.5M triples with a shared vocabulary of 42476. We did not apply any domain adaptation approach for our submission considering that (Junczys-Dowmunt and Grundkiewicz, 2018) shows few improvements, but advanced domain adaption (Wang et al., 2017) or fine-tuning (Luong and Manning, 2015) methods may still bring some improvements. The training set was shuffled for each training epoch. Like Junczys-Dowmunt and Grundki"
W19-5417,D18-1048,0,0.0165346,"improve machine translation outputs measured in BLEU (+0.46). We think human post-editing results may contain valuable information to guide neural machine translation models in some way like Reinforcement-Learning, but unfortunately, due to the high quality of the original neural machine translation output, only a small part of the real training data in the APE task are actually corrections from post editors, and most data are generated from the neural machine translation system, which makes it like adversarial training of neural machine translation (Yang et al., 2018) or multipass decoding (Geng et al., 2018). All our submissions were made by jointly trained models because the performance gap between the best and the worst model of jointly trained models is smaller, which means that jointly trained models may have smaller variance. Results on the test set from the APE shared task organizers are shown in Table 3. Even the ensemble of 5 models did not result in significant differ5 Conclusion In this paper, we described details of our approaches for our submission to the WMT 19 APE task. We borrowed a multi-source transformer model from the context-dependent machine translation task and applied joint"
W19-5417,W16-2378,0,0.179867,"Missing"
W19-5417,D18-1341,0,0.0607625,"r Approach We simplify and employ a multi-source transformer model (Zhang et al., 2018b) for the APE task, and try to train a more robust model through multi-task learning. 2.1 Our Model The transformer-based model proposed by Zhang et al. (2018b) for utilizing document-level context information in neural machine translation has two source inputs which can also be a source sentence along with the corresponding machine translation output and therefore caters for the requirements of APE. Since both source sentence and machine translation outputs are important for the APE task (Pal et al., 2016; Vu and Haffari, 2018), we remove the context gate used to restrict the information flow from the first input to the final output in their architecture, and obtain the model we used for our submission shown in Figure 1. The model first encodes the given source sentence with stacked self-attention layers, then “post-edits” the corresponding machine translation result through repetitively encoding the machine translation result (with a self-attention eScape 37.68 Table 1: BLEU Scores of Data Sets 145 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 145–150"
W19-5417,P17-2089,0,0.0208522,"the Neutron implementation, and the original training set was up-sampled 20 times as in (Junczys-Dowmunt and Grundkiewicz, 2018). We applied joint Byte-Pair Encoding (Sennrich et al., 2016) with 40k merge operations and 50 as the vocabulary threshold for the BPE. We only kept sentences with a max of 256 sub-word tokens for training, and obtained a training set of about 6.5M triples with a shared vocabulary of 42476. We did not apply any domain adaptation approach for our submission considering that (Junczys-Dowmunt and Grundkiewicz, 2018) shows few improvements, but advanced domain adaption (Wang et al., 2017) or fine-tuning (Luong and Manning, 2015) methods may still bring some improvements. The training set was shuffled for each training epoch. Like Junczys-Dowmunt and Grundkiewicz (2018), all embedding matrices were bound with the weight of the classifier. But for tokens which in fact do never appear in post-editing outputs in the shared vocabulary, we additionally remove their weights in the label smoothing loss and set corresponding biases in the decoder classifier to −1032 . Unlike Zhang et al. (2018b), the source encoder, the machine translation encoder and the decoder had 6 layers. The hidd"
W19-5417,N18-1122,0,0.0419614,"Missing"
W19-5417,P18-1166,0,0.0519023,"Missing"
W19-5417,D18-1049,0,0.306008,"dapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder. 1 Introduction The Automatic Post-Editing (APE) task is to automatically correct errors in machine translation outputs. This paper describes our submission to the English-German APE shared task at WMT 2019. Based on recent research on the APE task (Junczys-Dowmunt and Grundkiewicz, 2018) and an architecture for the utilization of documentlevel context information in neural machine translation (Zhang et al., 2018b), we re-implement a multi-source transformer model for the task. Inspired by Cheng et al. (2018), we try to train a more robust model by introducing a multi-task learning approach which jointly trains APE with a de-noising encoder. We made use of the artificial eScape data set (Negri et al., 2018) provided for the task, since the multi-source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can help regularize its parameters and make the model more general. We then tested the BLEU scores between machine translation result"
W19-5430,P07-2045,0,0.0110993,"(or generaldomain) data only and it differs substantially from the released development set which is part of a TED corpus. The parallel data includes Europarl v9, Wiki-titles v1, and JRC-Acquis. We combine all the released data and prepare a large outdomain dataset. 3.1 score = |Hsrc (srcj , lmi ) − Hsrc (srcj , lmo )| + |Htrg (trgj , lmi ) − Htrg (trgj , lmo ) |(1) 4 Pre-processing The out-domain data is noisy for our purposes, so we apply methods for cleaning. We performed the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then we use the Moses tokenizer to tokenize the out-domain corpus with ‘no-escape’ option. Finally, we apply true-casing. The cleaned version of the released data, i.e., the General corpus containing 1,394,319 sentences, is sorted based on the score in Equation 1. Thereafter, We split the entire data (1,394,319) into two sets; we use the first 1,000 for validation and the remaining as training data. The released development set (Dev) is us"
W19-5430,P02-1040,0,0.107429,"verhampton, UK 3 German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus, Germany santanu.pal@uni-saarland.de 1 Abstract ing, development, and testing data from the following language pairs: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Participant could submit system outputs to any of the three language pairs in any direction. The shared task attracted a good number of participants and the performance of all entries was evaluated using popular MT automatic evaluation metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we describe the UDS-DFKI system to the WMT 2019 Similar Language Translation task. The system achieved competitive performance and ranked second among ten entries in Czech to Polish translation in terms of BLEU score. In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tr"
W19-5430,W15-3017,1,0.860655,"Missing"
W19-5430,D11-1033,0,0.0388467,"h, and two pairs of similar languages Croatian–Serbian 219 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 219–223 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics et al., 2011), which are very similar to the indomain data (for our case the development set), and (ii) transferenceALL, utilizing all the released out-domain data sorted by Equation 1. The transference500Ktraining set is prepared using in-domain (development set) bilingual cross-entropy difference for data selection as was described in Axelrod et al. (2011). The difference in cross-entropy is computed based on two language models (LM): a domain-specific LM is estimated from the in-domain (containing 2050 sentences) corpus (lmi ) and the out-domain LM (lmo ) is estimated from the eScape corpus. We rank the eScape corpus by assigning a score to each of the individual sentences which is the sum of the three cross-entropy (H) differences. For a j th sentence pair srcj –trg j , the score is calculated based on Equation 1. and Indonesian–Malay. Processing similar languages and language varieties has attracted attention not only in the MT community but"
W19-5430,P16-1162,0,0.0274215,"initially train on the complete out-of-domain dataset (General). The General data is sorted based on their in-domain similarities as described in Equation 1. transferenceALLmodels are then fine-tuned towards the 500K (in-domain-like) data. Finally, we perform checkpoint averaging using the 8 best checkpoints. We report the results on the provided development set, which we use as a test set before the submission. Additionally we also report the official test set result. To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the Czech (CS) and Polish (PL), we define BPE tokens by jointly processing all parallel data. Thus, CS and PL derive a single BPE vocabulary. Since CS and PL belong to the similar language, they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. We pass word level information on the first encoder and the BPE information to the second one. On the decoder side of the transference model we pass only BPE text. We evaluate our approach with developmen"
W19-5430,2006.amta-papers.25,0,0.499063,"Center for Artificial Intelligence (DFKI), Saarland Informatics Campus, Germany santanu.pal@uni-saarland.de 1 Abstract ing, development, and testing data from the following language pairs: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Participant could submit system outputs to any of the three language pairs in any direction. The shared task attracted a good number of participants and the performance of all entries was evaluated using popular MT automatic evaluation metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we describe the UDS-DFKI system to the WMT 2019 Similar Language Translation task. The system achieved competitive performance and ranked second among ten entries in Czech to Polish translation in terms of BLEU score. In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs"
W19-5430,W17-1207,0,0.031028,"Missing"
W19-5430,W18-3931,1,0.847571,"Missing"
W19-5430,2014.eamt-1.34,0,0.0234245,"nterest in training systems to translate between languages other than English (Costa-juss`a, 2017). One reason for this is the growing need of direct translation between pairs of similar languages, and to a lesser extent language varieties, without the use of English as a pivot language. The main challenge is to overcome the limitation of available parallel data taking advantage of the similarity between languages. Studies have been published on translating between similar languages (e.g. Catalan - Spanish (Costa-juss`a, 2017)) and language varieties such as European and Brazilian Portuguese (Fancellu et al., 2014; Costa-juss`a et al., 2018). The study by Lakew et al. (2018) tackles both training MT systems to translate between European–Brazilian Portuguese and European–Canadian French, and two pairs of similar languages Croatian–Serbian 219 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 219–223 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics et al., 2011), which are very similar to the indomain data (for our case the development set), and (ii) transferenceALL, utilizing all the released out-domain data s"
W19-5430,W18-6316,0,\N,Missing
W19-6702,C14-2028,0,0.137444,", CATaLog Online uses the Nutch2 information retrieval (IR) system. Nutch follows the standard IR model of Lucene3 with document parsing, document Indexing, TF-IDF calculation, query parsing and finally searching/document retrieval and document ranking. In our implementation, each document contains (a) a TM source segment, (b) its corresponding translation and (c) the word alignments. Machine Translation and Automatic Post Editing Along with TM matches, CATaLog Online provides MT output (Pal et al., 2015a) to the translator, an option provided by many state-of-the-art CAT tools (e.g. MateCat (Federico et al., 2014)). Besides the retrieved TM segment and the MT output CATaLog Online provides also a third option to the translator: the output of an automatic post-editing system meant to be post-edited as the MT output. The APE system is based in an OSM model (Pal et al., 2016b) and proved to deliver competitive performance in previous editions of the Automatic Post Editing (APE) shared task at WMT Bojar et al. (2016). 2 3 http://nutch.apache.org/ http://lucene.apache.org/ Proceedings of MT Summit XVII, volume 2 Editing Logs For a given input segment, CATaLog Online provides four different options: TM, MT,"
W19-6702,W15-4905,1,0.909403,"Missing"
W19-6702,W12-3123,0,0.598965,"Missing"
W19-6702,W15-5206,1,0.888157,"Missing"
W19-6702,W15-3017,1,0.90096,"Missing"
W19-6702,W15-3026,1,0.902711,"Missing"
W19-6702,L16-1095,1,0.855593,"Missing"
W19-6702,W16-2379,1,0.892057,"Missing"
W19-6702,2015.tc-1.15,0,0.150837,"Missing"
W19-6702,W14-0314,1,0.712543,"the users preferred using CATaLog Online over existing CAT tools in some respects, especially by selecting the output of the MT system and taking advantage of the color scheme for TM suggestions. 1 Introduction The use of computer software is an important part of the modern translation workflow (Zaretskaya et al., 2015; Schneider et al., 2019). A number of tools are widely used by professional translators, most notably CAT tools and terminology management software. These tools increase translators’ productivity, improve consistency in translation and, in turn, reduce the cost of translation (Zampieri and Vela, 2014). The most important compo© 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND. Proceedings of MT Summit XVII, volume 2 nent in state-of-the-art CAT tools are translation memories (TM). The translators can either accept, reject or modify the suggestions received from the TM engine. As the process is done iteratively, every new translation increases the size of the translation memory making it more useful for future translations. The idea behind TMs is relatively simple, however, the process of matching and retrieval of so"
W19-6702,2015.eamt-1.6,1,\N,Missing
Y04-1016,P04-1041,1,0.861636,"Missing"
Y04-1016,C02-1126,0,0.0611897,"Missing"
Y04-1016,P02-1043,0,0.030143,"robust, state-of-art resources. However, (with few exceptions) the grammars induced are mostly &quot;shallow&quot;, i.e. without the deep syntactic (dependency) or semantic information captured by deep, constraint-based grammar formalisms such as LFG or HPSG. A recent body of research had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short descripti"
Y04-1016,W03-2401,0,0.0943368,"Missing"
Y04-1016,H94-1020,0,0.543341,"ch had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure a"
Y04-1016,P04-1047,1,0.631032,"Missing"
Y04-1016,P02-1035,0,0.530094,"In order to assess the quality of the extracted grammars we carried out three types of parsing experiments: • In experiment 1 we evaluate the CFG tree output of our parsers against the original trees for strings length &lt;= 40 in articles 301-325 CTB, reporting f-scores for labelled and unlabelled bracketings using evalb. • In experiment 2 we evaluate the f-structures generated by our grammars against the manually annotated 50 gold-standard f-structures for randomly selected trees from articles 301-325 using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). • In experiment 3 we evaluate the f-structures generated by our grammars against the fstructures for the full 318 test strings as generated by the automatic f-structure annotation algorithm for the original trees in articles 301-325 CTB using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). - 168 - PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo 7.2.1 Experiment 1 (Tree-Based Evaluation) Table 5 describes the results obtained in experiment 1. In this experiment we evaluate the parse output generated by our grammars ag"
Y04-1016,C02-1145,0,0.217827,"ill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure as they do not yet resolve long-distance dependencies. Section 4 outlines the architecture underlying the automatic fstructure annotation algorithm and how it was applied to the CTB. Section 5 provides an evaluation of the f-s"
Y04-1016,C04-1024,0,\N,Missing
Y04-1016,P03-1046,0,\N,Missing
Y04-1016,P03-1056,0,\N,Missing
Y07-1039,Y04-1016,1,0.892328,"Missing"
Y07-1039,W02-1503,0,0.0142172,"to a constraint solver to generate f-structures for these sentences. Long-distance dependencies (LDD) are resolved on f-structures using LDD path frequencies acquired from the f-structure annotated treebank and automatically acquired subcategorization frames (O’Donovan et al. 2004). This method has been applied to several languages other than English, including Chinese and German (Burke et al. 2004; Cahill et al. 2003). 3. Acquisition of LFG Resources from a Japanese Text Corpus A wide-coverage LFG grammar for Japanese (Masuichi et al.2003) has been manually developed in the ParGram project (Butt et al. 2002) along with grammars for a number of other languages. To the best of our knowledge our research is the first method for the automatic treebank-based acquisition of deep Japanese LFG resources, focusing on morphological information and on unlabelled dependency relationships among the syntactic units in a sentence, as provided by an existent wide-coverage Japanese corpus. We use KTC4 as the corpus from which wide-coverage LFG resources are acquired. The method we develop implements the idea that the part-of-speech tags on each morpheme and the unlabelled dependency tags on each syntactic unit in"
Y07-1039,P04-1041,1,0.840252,"Missing"
Y07-1039,hockenmaier-steedman-2002-acquiring,0,0.0456171,"Missing"
Y07-1039,P06-1063,1,0.890337,"Missing"
Y07-1039,C02-1122,0,0.0716418,"Missing"
Y07-1039,W02-2016,0,0.111292,"Missing"
Y07-1039,H94-1020,0,0.0707325,"Missing"
Y07-1039,Y03-1034,0,0.0381009,"Missing"
Y07-1039,P05-1011,0,0.0197058,"o be able to induce linguistic resources that are deep, including not only syntactic properties of given sentences but also semantic properties such as predicate-argument structures and long-distance dependencies (LDDs). Several methods to 376 achieve this goal have been developed to date, based on different grammatical formalisms like Combinatory Categorial Grammar (CCG) (Steedman, 2000), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994), and LFG. For example, Hockenmaier and Steedman (2002) presented an algorithm to translate the Penn-II Treebank into a CCG-style Treebank. Miyao and Tsujii (2005) developed probabilistic models for parsing with HPSG grammars acquired from the Penn-II treebank. Cahill et al. (2002, 2003, 2004) developed a method for automatic annotation of LFG f-structure on the Penn-II Treebank. The approach of Cahill et al. (2002, 2003, 2004) is as follows: first, LFG functional equations are automatically annotated on the phrase-structure trees in the English treebank. The equations specify the constraints on the f-structure mapping from the c-structure. The equations are collected and sent to a constraint solver to generate f-structures for these sentences. Long-dis"
Y07-1039,P04-1047,1,0.888247,"Missing"
Y07-1039,W07-0714,1,0.877763,"Missing"
Y07-1039,C04-1050,0,\N,Missing
Y09-2027,A94-1010,0,0.38398,"rder (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain tr"
Y09-2027,W07-0722,0,0.097568,"g the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain"
Y09-2027,eck-etal-2004-language,0,0.312628,"mbination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora w"
Y09-2027,W08-0334,0,0.0119442,"m on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data through phrase generalization in order to improve the Italian-English translation quality. They showed a noticeable improvement in translation quality. Finch and Sumita (2008) employed probabilistic mixture weights to combine two models for questions and declarative sentences with a general model. Foster and Kuhn (2007) used distance based weights in a mixture model. In contrast to their work, Finch and Sumita (2008) used a probabilistic classifier to determine a vector of probability representing classmembership. They performed experiments on a number of language pairs and experimental results showed the usefulness of their method. Domain adaptation techniques can be broadly divided into two categories: (i) adaptation techniques to improve word alignment models; s"
Y09-2027,W07-0717,0,0.165775,"ectly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-d"
Y09-2027,2005.eamt-1.17,0,0.514059,"etrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain training data. We apply adaptation techniques to combine sub-corpora with indomain small-scale training data into a unified framework. The remainder of the paper is organized as follows. In section 2 we discuss related work. Secti"
Y09-2027,2005.eamt-1.19,0,0.0349785,"in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular e"
Y09-2027,N03-1017,0,0.00855717,"cally (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced trans"
Y09-2027,P07-2045,0,0.0154384,"o account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve tran"
Y09-2027,W07-0733,0,0.509107,"e semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work"
Y09-2027,W02-1405,0,0.709646,"section 2 we discuss related work. Section 3 describes experimental results using our baseline SMT model. In section 4 we describe the domain adaptation techniques which are employed to combine multiple models. Section 5 presents the results obtained, together with some analysis. Section 6 concludes, and provides avenues for further work. 2 Related Work Topic-dependent modeling was effectively applied in speech recognition to improve the quality of models (Carter, 1994). Adaptation technology has been widely used in language modeling in the same filed over the last decade (Iyer et al., 1997). Langlais (2002) was the first to introduce domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approa"
Y09-2027,W08-0320,0,0.487175,"eneous topics. These topics usually define a set of terminological lexicons. Terminologies need to be translated taking into account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Rece"
Y09-2027,P03-1021,0,0.0301251,"ated EILMT and TIDES language models and translation models using a log-linear combination. 4.1 Language Model Adaptation We used the language modeling toolkit SRILM (Stolke, 2002) to build two language models from the target side of the EILMT and TIDES training data. We performed log-linear interpolation of multi-domain translation models. This results in a straight-forward combination of in-domain and out-of-domain language models. Fortunately, the PB-SMT Moses decoder supports log-linear combinations of language models. Language model weights are optimized with minimum error rate training (Och, 2003). 4.2 Translation Model Adaptation In general, translation models are built separately for each of the domain specific corpora. These models are then combined using two techniques: (i) linear interpolation (ii) log-linear interpolation. We performed the log-linear interpolation of multi-domain translation models. There are two ways of performing log-linear interpolation: Multiple Decoding Paths: a recent feature of Moses is multiple decoding paths. This alternate decoding path model was developed by Birch et al. (2007). Here we use Moses’ capabilities to use different decoding paths for transl"
Y09-2027,D08-1090,0,0.0207945,"ted repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data t"
Y09-2027,P07-1004,0,0.0249606,"SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffin"
Y09-2027,P05-1058,0,0.27129,"prove translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, a"
Y09-2027,C08-1125,0,0.0743408,"ence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the tran"
Y09-2027,D07-1054,0,0.353807,"f language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adap"
Y09-2027,C04-1059,0,0.0553937,"e domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific"
Y09-2027,wu-wang-2004-improving-domain,0,\N,Missing
Y09-2027,J05-4003,0,\N,Missing
Y09-2027,P08-1000,0,\N,Missing
