2002.tmi-papers.5,J00-1004,0,0.0343957,"Missing"
2002.tmi-papers.5,J93-2003,0,0.00314432,"for making multilingual Bibles available for research via the University of Maryland Parallel Corpus Project, to Dan Melamed and Franz Josef Och for discussions regarding word alignment resources, and to Hiyan Alshawi regarding evaluation metrics. This work benefited from the comments of Bob Kasper, Erhard Hinrichs, Detmar Meurers, Dale Russell, and three anonymous reviewers. Paul C. Davis is the recipient of a Motorola University Partnerships in Research Grant. Portions of this research were also funded by an Ohio State University Summer GRA in Cognitive Science. machine translation (SMT) is Brown et al. (1993), which models the alignment between words in two languages. There are, however, a number of SMT approaches which seek to impose additional structure on the language and translation models via the use of finite-state devices. These range from techniques which attempt to replicate the pure SMT approach by means of composed transducers, such as Knight & Al-Onaizan (1998), to those which use subsequential transducers on limited domain translation tasks, such as Vilar et al. (1999) , and finally to those which employ more powerful finite-state devices, such as weighted head transducers (Alshawi et"
2002.tmi-papers.5,P98-1102,0,0.0247204,"precludes the use of discontinuous alignments with source words, i.e., those where the source words of the alignment do not all occur in sequence. This is intentional, since the word-alignment algorithms tested do not allow such alignments. Nothing in the model’s architecture, however, needs to be changed to use them, and we will present an algorithm for translating with such alignments in a future paper. word is used exactly once to generate each complete target transition sequence. We accomplish this by adding another component to transitions, called the source-wordstore (sws), inspired by Johnston (1998), which contains a numerical representation of the source words which generated the transition. A complete parse, then, (i.e., a potential translation), is a sequence of transitions which begins at the start-state, ends at a final state, and has a full sws (i.e., one that covers all the source words). The best translation is the combined labels (i.e., the words) of the highest scoring sequence. 4 Generalization The model as described is limited to only the exact bitexts on which it was trained. We use a number of techniques to increase its ability to generalize (as well as reduce its size, a n"
2002.tmi-papers.5,knight-al-onaizan-1998-translation,0,0.0538692,"Missing"
2002.tmi-papers.5,1993.tmi-1.4,0,0.0227314,"represented in the linked automata MT model languages, while natural languages are standardly assumed to be at least contextfree. But by using techniques for approximation and generalization, and heuristics for working with partial results, the model may indeed prove to be adequate for translation tasks.1 The model thus also bears a relationship to another type of data-driven MT, example-based machine translation, since when working with partial results, decisions need to be made with regard to where segments should be matched and how their translations should be recombined (see, for example, Nirenburg et al. (1993)). In the remainder of this paper, we will, in section 2, briefly describe the training process for the model, including our use of probabilities and their motivation. In section 3 we describe the translation process, detailing how the linked automata model is used. In section 4 we outline methods we use for increased generalization as well as sizereduction of the system. In the final section, we describe our evaluation methodology, and present preliminary results and future research directions. 2 Training A linked automata translation system consists of a source language automaton, a target l"
2002.tmi-papers.5,P00-1056,0,0.0748281,"only option here, but has the merit of simplicity. &lt; qn−1 , qn , wn , pn−1,n &gt; with alignment probabilities a1 , a2 , . . . , an will have the overall translation probability: (2) Pis f inal (qn ) Qn i=1 ai pi−1,i construction The linked automata model is constructed from word-aligned bitexts. For the results reported in this paper, we used English and Spanish versions of the Judeo-Christian Bible.6 We experimented with two different automatic word-aligners: a rather poor-performing word-aligner we created for the task, and the higher quality (word alignments of the) Giza++ translation model (Och & Ney 2000).7 Construction from the word-aligned bitexts is straightforward: From a file of bitext triples (a source sentence, a target sentence, and a representation of the alignment between them), for each triple, add the appropriate states and transitions to the source automaton so that the source sentence can be recognized, perform the analogous additions to the target automaton with respect to the target sentence, and for each alignment pair of the form: ((source trani ...source tranj ) (target trank ...target tranl )), add an entry to the table for the source sequence if one does not already exist."
2002.tmi-papers.5,white-2000-contemplating,0,0.0236594,"d in either direction: If the distance from the start-state to A is less than the distance to B, then A precedes B in the translation. Using these techniques, we can guarantee that the system always produces something; a property that simplifies the task of evaluation. 5 Evaluation and Future Directions There appear to be as many different evaluation methods for machine translation as there are machine translation methods. This arises from the fact that there is little agreement on how to define what a correct translation is, much less how to measure it, be the measurement automatic or human (White 2000). MT evaluation (MTE) has become a small research niche in its own right, but these efforts have yielded few uncontroversial results. It comes down to one of the fundamental truths of MTE: “There is no such thing as the correct translation” (King (1997:261), emphasis added). In an evaluation of system such as that proposed in this paper, we are primarily interested in determining if the overall approach is feasible, as well as being able to assess to what degree the generalization methods improve performance. As such, automatic evaluations are suitable (White 2000), especially since small chan"
2007.tmi-papers.15,W05-0909,0,0.677319,"set, scoring contiguous sequences higher than discontiguous matches. Kulesza and Shieber (2004) describe a machine learning-based approach to combining various metrics such as B LEU-style n-gram precision (1 ≤ n ≤ 5), word error rate, position-independent word error rate, etc. These values are passed as features to a support vector machine (Vapnik, 1995) which learns to discriminate human from machine-generated translations. The farther a candidate translation’s feature encoding is on the human side of the hyperplane separating human from machine translations, the better it is judged to be. (Banerjee and Lavie, 2005) describes METEOR, a word-based generalised unigram matching approach that rewards sentence alignments between references and candidates that minimise the number of crossing word alignments. Stemming and WordNet synonyms are used to improve the match between translations that may differ only in their lexical choice or grammatical use of a particular base word form. All of these approaches, however, are still based on matching a candidate to a reference at the word level, and, as such, they are ultimately still susceptible to reduced performance due to syntactically acceptable variation. Thus,"
2007.tmi-papers.15,E06-1032,0,0.0672912,"Even though this candidate has only 1 bigram (and no 3- and 4-grams) in common with the reference (thus, giving it a low B LEU ˆ score), it still receives a fairly high B LEU ATRE score of 0.75, since only ‘please’ and ‘fill’ are out of the order specified by the parse of the reference. This accords with our intuitions that ‘Fill please your name in’ is only mildly “Dutch-sounding” and conveys the gist of the reference. 3 Related Work There is a growing concern in the MT research community as to the correlation of B LEU with human judgments of translation quality, even at the document level (Callison-Burch et al., 2006). This is of particular concern, as statistical MT systems are now trained to minimise error with respect to ATE metrics (Och, 2003). There have been many attempts to improve upon the performance of B LEU. The NIST metric mentioned above (Doddington, 2002) uses n-gram precision scores as B LEU does, but it weights the information contributed by certain n-grams. In this approach, rare n-grams count more than frequent n-grams in a candidate’s precision score. Turian et al.’s (2003) approach (called General Text Matcher or GTM) is to compute both precision and recall of a candidate’s match to the"
2007.tmi-papers.15,P04-1014,0,0.0511079,"anslation should be allowed to display. It seems reasonable, then, to explore approaches to ATE that exploit syntactic information so as not penalise legitimate syntactic variation. This paper describes such an approach. We describe here a prototype sys1 (“bluish”), a novel apˆ tem called B LEU ATRE proach to syntactically-informed automatic machine translation evaluation that uses syntactic word-word dependencies from parses of ref1 Standing for BLEU’s Tectogrammatical RElations. Associate with erence translations. In this approach, we use a statistical Combinatory Categorial Grammar parser (Clark and Curran, 2004) to parse the reference set and extract word-word dependencies based on hierarchical head-dependent relationships (or “tectogrammatical” relationships). These dependencies are then compiled out into bags of dependent words that must appear to the left and right of each head word — essentially enforcing a partial linear ordering of dependents with respect to their heads. The quality of a candidate translation is then evaluated according to the number of these head word-dependent word partial orderings that it recalls. This approach is novel in that it only requires parses of reference translati"
2007.tmi-papers.15,2004.tmi-1.8,0,0.261694,"nimise error with respect to ATE metrics (Och, 2003). There have been many attempts to improve upon the performance of B LEU. The NIST metric mentioned above (Doddington, 2002) uses n-gram precision scores as B LEU does, but it weights the information contributed by certain n-grams. In this approach, rare n-grams count more than frequent n-grams in a candidate’s precision score. Turian et al.’s (2003) approach (called General Text Matcher or GTM) is to compute both precision and recall of a candidate’s match to the reference set, scoring contiguous sequences higher than discontiguous matches. Kulesza and Shieber (2004) describe a machine learning-based approach to combining various metrics such as B LEU-style n-gram precision (1 ≤ n ≤ 5), word error rate, position-independent word error rate, etc. These values are passed as features to a support vector machine (Vapnik, 1995) which learns to discriminate human from machine-generated translations. The farther a candidate translation’s feature encoding is on the human side of the hyperplane separating human from machine translations, the better it is judged to be. (Banerjee and Lavie, 2005) describes METEOR, a word-based generalised unigram matching approach t"
2007.tmi-papers.15,W05-0904,0,0.723558,"oach that rewards sentence alignments between references and candidates that minimise the number of crossing word alignments. Stemming and WordNet synonyms are used to improve the match between translations that may differ only in their lexical choice or grammatical use of a particular base word form. All of these approaches, however, are still based on matching a candidate to a reference at the word level, and, as such, they are ultimately still susceptible to reduced performance due to syntactically acceptable variation. Thus, some authors have attempted to use syntactic information in ATE. Liu and Gildea (2005) parse both reference and candidate translations. The count of subtrees up to a fixed, uniform depth that the candidate recalls is one metric used. Also, by decomposing each parse tree into a vector of counts of all subtrees, the authors compute the cosine between the reference and candidate vectors. Both metrics are also computed for dependency parses, as extracted from the phrase-structure parses of the candidate and reference translations. Finally, the authors compute the fraction of dependency chains (up to some fixed length) in the reference that are also in the candidate. The authors rep"
2007.tmi-papers.15,P03-1021,0,0.00638542,"eives a fairly high B LEU ATRE score of 0.75, since only ‘please’ and ‘fill’ are out of the order specified by the parse of the reference. This accords with our intuitions that ‘Fill please your name in’ is only mildly “Dutch-sounding” and conveys the gist of the reference. 3 Related Work There is a growing concern in the MT research community as to the correlation of B LEU with human judgments of translation quality, even at the document level (Callison-Burch et al., 2006). This is of particular concern, as statistical MT systems are now trained to minimise error with respect to ATE metrics (Och, 2003). There have been many attempts to improve upon the performance of B LEU. The NIST metric mentioned above (Doddington, 2002) uses n-gram precision scores as B LEU does, but it weights the information contributed by certain n-grams. In this approach, rare n-grams count more than frequent n-grams in a candidate’s precision score. Turian et al.’s (2003) approach (called General Text Matcher or GTM) is to compute both precision and recall of a candidate’s match to the reference set, scoring contiguous sequences higher than discontiguous matches. Kulesza and Shieber (2004) describe a machine learni"
2007.tmi-papers.15,W06-3112,0,0.420772,"Missing"
2007.tmi-papers.15,W07-0411,0,0.366096,"Missing"
2007.tmi-papers.15,P02-1040,0,0.0759627,"ymy, paraphrasing or inflectional morphological information, all of which would be easy to add. 1 Introduction Effective automatic translation evaluation (ATE) systems are crucial to the development of machine translation (MT) systems, as the relative performance gain of each minor system modification must be tested quickly and cheaply. A professional human evaluation of MT system output after each such modification is too expensive and time-consuming for rapid, cost-effective deployment of translation software. For the past few years, n-gram precision metrics for MT evaluation such as B LEU (Papineni et al., 2002) and the related NIST metric (Doddington, 2002) have been the standard approach to ATE. In essence, B LEU and NIST measure the quality of a candidate translation as a function of the number of n-grams (typically, 1 ≤ n ≤ 4) it shares with a set of (one 122 or more) reference translations. These metrics require a one-time investment of creating a reference corpus of translations to test the system against, but are fully automatic once this corpus has been created and are very portable, requiring only word tokenisers for the reference set (if it is not already tokenised). The portability of n-gr"
2007.tmi-papers.15,2006.amta-papers.25,0,0.0412751,"o parse both the reference and candidate translations. They directly compute the dependency precision and recall of the candidate translation with respect to the reference. These authors perform an extensive comparison of their system to various ATE metrics over the Linguistic Data Consortium’s Multiple Translation Chinese corpus (parts 2 and 4). When supplementing the dependency matches with WordNet synonyms, they achieve the highest correlation to human judgments in fluency and second place in an average of fluency and accuracy, as compared to B LEU, NIST, GTM, Translation Error Rate (TER, (Snover et al., 2006)) and METEOR. We have used this same corpus and, as such, can compare our results to theirs, as well as the other approaches they tested over this corpus. Our approach is distinguished from these last two approaches in that we do not attempt to parse candidate translations. 4 Preliminary Experiments To test our system, we used sections 2 and 4 of the TIDES 2003 Chinese-to-English Multiple Translation corpus (MTC) of newswire text (released by the LDC). This corpus contains various commercial off-the-shelf (COTS) and research MT systems’ translations of a set of Chinese source sentences. There"
2007.tmi-papers.15,2003.mtsummit-papers.51,0,0.156719,"Missing"
2020.alw-1.6,W17-3013,0,0.0207857,"to capture nuanced abusive speech as they fail to contextualize the word meanings. For instance, depending on the context, the word gay can be used to denote either ebullience or sexual preference. Only the latter is a candidate attack. Recently, deep learning models are also proposed that leverage pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) to capture the semantics of the tweets. These models aggregate individual word embeddings in a contextaware manner to compute tweet embeddings and later use them for classification. Earlier studies have either used the CNN (Gambäck and Sikdar, 2017; Park and Fung, 2017) or RNN (Badjatiya et al., 2017; Agrawal and Awekar, 2018) to compute these embeddings. The syntactic structure of the text was also used to help identify the target group and the intensity of hate speech (Warner and Hirschberg, 2012; Silva et al., 2016). The primary difficulty of these works is that the space of possibly relevant rules is too large to be comprehensive. Besides, it verges on the impossible to specify a set of rules that will cover possible implicit attacks. On the other hand, Burnap and Williams (2016); Alorainy et al. (2019) proposed computational models"
2020.alw-1.6,D14-1108,0,0.033777,"periments did not show the superior performance of the BERT model on the abusive language datasets. We also noticed that prior literature on comparable tasks is variable, with some successes for BERT-like models but few robust trends. There are numerous reports of difficulties in training these large neural networks on the small, imbalanced annotated datasets typical of such tasks (Zampieri et al., 2019; Liu et al., 2019). The challenges are likely an effect of over-fitting and lead to inconsistent results. 8 9 51 nlp.stanford.edu/projects/glove/ https://spacy.io/ specially built for Twitter (Kong et al., 2014) but believe that using Twitter-specific parsers might improve our results further. 6 Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Eleventh international aaai conference on web and social media. Conclusion Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo"
2020.alw-1.6,W18-4401,0,0.0120556,"h and Krestel (2020) proposed to use an ensemble of BERT models to control the variance of these large models over small datasets. Ensembling is expensive, so there remains a need for computationally efficient methods that approach the same performance. Because GCN has far fewer parameters, it is less likely to need these countermeasures against overfitting. Experiments on the relatively larger abusive language dataset of 100K tweets (Founta et al., 2018) by Lee et al. (2018) highlighted that sequential models such as RNN perform well but are still hard to train on this dataset size. Further, Kumar et al. (2018) concluded that with optimal feature selection, classifiers like SVM and Random forest performs at par with neural networks. Dataset quality and annotator bias: Recent works have highlighted that majority of the abusive language datasets suffer from poor quality (Wiegand et al., 2019; Vidgen and Derczynski, 2020) or show evidence that annotator decisions were inappropriately affected by surface markers of speaker race (Sap et al., 2019). We believe that corrections of these deficiencies and biases in annotation are essential for research progress in the field. Better dataset collection, howeve"
2020.alw-1.6,D14-1082,0,0.0636152,"g the BiLSTM module before the DepGCN only increases the inference time slightly. On the other hand, BERT takes an order of magnitude longer than any of these approaches. Note that, for operational reasons, the inference time does not take into account the time taken to extract the tweets’ parse tree. We did the parsing step ahead of time, once, and reused the results for each experiment. A real production system would do this at inference time, adding a small time cost for each new tweet. State-of-the-art dependency parsers can currently achieve around 1000 sentences per second per CPU core (Chen and Manning, 2014; Kong and Smith, 2014). We estimate that on modern multi-CPU machines we can keep the parse cost under 0.05 seconds per 1000 tweets. This still keeps GCN methods more than competitive with BERT at inference time. The same trend can be observed at training time too in Figure 3b. However, the jump from DepGCN to BiLSTM training time is a little higher than during inference. In summary, our parser-based DepGCN approach is much more efficient than the BERT model. Also, including BiLSTM module to the DepGCN model leads to only a slight drop in efficiency. Racist Sexist Benign Overall AUC 0.75 0.78"
2020.alw-1.6,W18-5113,0,0.016592,"plementary attack types. 50 4 Related Work Remedies include careful hyperparameter tuning, early stopping, and the use of ensembles. Risch and Krestel (2020) proposed to use an ensemble of BERT models to control the variance of these large models over small datasets. Ensembling is expensive, so there remains a need for computationally efficient methods that approach the same performance. Because GCN has far fewer parameters, it is less likely to need these countermeasures against overfitting. Experiments on the relatively larger abusive language dataset of 100K tweets (Founta et al., 2018) by Lee et al. (2018) highlighted that sequential models such as RNN perform well but are still hard to train on this dataset size. Further, Kumar et al. (2018) concluded that with optimal feature selection, classifiers like SVM and Random forest performs at par with neural networks. Dataset quality and annotator bias: Recent works have highlighted that majority of the abusive language datasets suffer from poor quality (Wiegand et al., 2019; Vidgen and Derczynski, 2020) or show evidence that annotator decisions were inappropriately affected by surface markers of speaker race (Sap et al., 2019). We believe that cor"
2020.alw-1.6,W19-4828,0,0.0261967,"ve language detection use either n-gram features (Waseem and Hovy, 2016; Davidson et al., 2017) or employ sequential deep learning models like CNN or LSTM (Zhang et al., 2018b; Badjatiya et al., 2017). These methods do not work well to capture semantic word meanings or long-range attack in text (such as long clauses or complex scoping shown in online attacks). Large pre-trained language models like BERT (Devlin et al., 2019) achieve high accuracy after fine-tuning on supervised tasks. However, these methods are computationally expensive and are, thus, unfit to be used for real-time detection. Clark et al. (2019) observed that some attention heads of the pre-trained BERT model are learning syntactic dependencies between words such as direct objects of verbs etc. It is similar to a dependency parser that represents the structure of syntactic dependence between words in the sentence. Recently, Burnap and Williams (2016) and Alorainy et al. (2019) also showed that including syntactic dependency as features improves classifier performance in abusive language detection tasks. However, adding features can only provide weak supervision compared to encoding these dependence explicitly in the model. On the oth"
2020.alw-1.6,S19-2011,0,0.0215164,"r the bag-of-words features. Our model builds on this work and explicitly models the dependency between words using graph convolution operations. 5 Discussion Worse performance of BERT: Our experiments did not show the superior performance of the BERT model on the abusive language datasets. We also noticed that prior literature on comparable tasks is variable, with some successes for BERT-like models but few robust trends. There are numerous reports of difficulties in training these large neural networks on the small, imbalanced annotated datasets typical of such tasks (Zampieri et al., 2019; Liu et al., 2019). The challenges are likely an effect of over-fitting and lead to inconsistent results. 8 9 51 nlp.stanford.edu/projects/glove/ https://spacy.io/ specially built for Twitter (Kong et al., 2014) but believe that using Twitter-specific parsers might improve our results further. 6 Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Eleventh international aaai conference on web and social media. Conclusion Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bid"
2020.alw-1.6,W12-2103,0,0.0576381,"learning models are also proposed that leverage pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) to capture the semantics of the tweets. These models aggregate individual word embeddings in a contextaware manner to compute tweet embeddings and later use them for classification. Earlier studies have either used the CNN (Gambäck and Sikdar, 2017; Park and Fung, 2017) or RNN (Badjatiya et al., 2017; Agrawal and Awekar, 2018) to compute these embeddings. The syntactic structure of the text was also used to help identify the target group and the intensity of hate speech (Warner and Hirschberg, 2012; Silva et al., 2016). The primary difficulty of these works is that the space of possibly relevant rules is too large to be comprehensive. Besides, it verges on the impossible to specify a set of rules that will cover possible implicit attacks. On the other hand, Burnap and Williams (2016); Alorainy et al. (2019) proposed computational models using the dependency labels as features and reported significant gains over the bag-of-words features. Our model builds on this work and explicitly models the dependency between words using graph convolution operations. 5 Discussion Worse performance of"
2020.alw-1.6,N16-2013,0,0.312203,"e pre-trained BERTbase model from the transformers 6 library for our experiments. Preliminary experiments showed that average pooling from the last four layers of BERT as the aggregate sentence representation performed better than the [CLS] token representation from the final layer. We append a linear layer on top of the model to compute class-wise scores. Further, we fine-tune this classifier on our training dataset. BiLSTM is a sequential model to compute sentence embeddings useful for many downstream classification tasks (Zhou et al., 2016). We use Categories Dataset Davidson et al. (2017) Waseem and Hovy (2016) Davidson extended Hate 1,430 Racism 1,939 Hate 1,430 Offensive 19,190 Sexism 3,148 Offensive 19,190 Benign 4,163 Benign 11,115 Benign 15,278 Table 1: Dataset Statistics. For comparability, we experiment with both the datasets from Davidson et al. (2017) and Waseem and Hovy (2016). We do not claim that our results will necessarily transfer to more naturalistic settings. Table 1 lists the per class distribution in the collected dataset. Note that the Davidson et al. (2017) is highly skewed with majority of tweets 4 5 https://bit.ly/3gN2RsD https://bit.ly/3dpoTzJ 6 47 github.com/huggingface/tran"
2020.alw-1.6,N19-1060,0,0.061137,". 2.2 (6) 3 Experiments In this section, we first describe our experimental setup, followed by the results. We then present a detailed error analysis of dependency-based model vs. a widely-used sequential model for sentence classification. Sentence representation 3.1 In the previous section, we computed contextualized word embeddings using syntactic relationships. However, we still need to aggregate these vertex embeddings to compute a graph-level embedding (sentence in our case). In particular, we Experimental Setup We first describe our datasets, followed by comparative baselines. Datasets: Wiegand et al. (2019) emphasizes the difficulty of selecting representative datasets for studying abusive language. At the heart of this difficulty is the relative rarity of abusive language in large-scale user-generated text. It is not unusual for 3 Our experiments did not show performance gain when using recent variants of GCN that uses attention (Veliˇckovi´c et al., 2018) or different aggregators (Xu et al., 2018). 46 Multi-layer Graph Convolution FC layers Contextualized Embeddings Sentence Representation LSTM Input Word Graph LSTM Predicted Class ... LSTM Masked Pooling LSTM Figure 2: Overview of our propose"
2020.alw-1.6,W17-3006,0,0.0209087,"speech as they fail to contextualize the word meanings. For instance, depending on the context, the word gay can be used to denote either ebullience or sexual preference. Only the latter is a candidate attack. Recently, deep learning models are also proposed that leverage pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) to capture the semantics of the tweets. These models aggregate individual word embeddings in a contextaware manner to compute tweet embeddings and later use them for classification. Earlier studies have either used the CNN (Gambäck and Sikdar, 2017; Park and Fung, 2017) or RNN (Badjatiya et al., 2017; Agrawal and Awekar, 2018) to compute these embeddings. The syntactic structure of the text was also used to help identify the target group and the intensity of hate speech (Warner and Hirschberg, 2012; Silva et al., 2016). The primary difficulty of these works is that the space of possibly relevant rules is too large to be comprehensive. Besides, it verges on the impossible to specify a set of rules that will cover possible implicit attacks. On the other hand, Burnap and Williams (2016); Alorainy et al. (2019) proposed computational models using the dependency"
2020.alw-1.6,D14-1162,0,0.0892717,"STM module before the DepGCN only increases the inference time slightly. On the other hand, BERT takes an order of magnitude longer than any of these approaches. Note that, for operational reasons, the inference time does not take into account the time taken to extract the tweets’ parse tree. We did the parsing step ahead of time, once, and reused the results for each experiment. A real production system would do this at inference time, adding a small time cost for each new tweet. State-of-the-art dependency parsers can currently achieve around 1000 sentences per second per CPU core (Chen and Manning, 2014; Kong and Smith, 2014). We estimate that on modern multi-CPU machines we can keep the parse cost under 0.05 seconds per 1000 tweets. This still keeps GCN methods more than competitive with BERT at inference time. The same trend can be observed at training time too in Figure 3b. However, the jump from DepGCN to BiLSTM training time is a little higher than during inference. In summary, our parser-based DepGCN approach is much more efficient than the BERT model. Also, including BiLSTM module to the DepGCN model leads to only a slight drop in efficiency. Racist Sexist Benign Overall AUC 0.75 0.78"
2020.alw-1.6,2020.trac-1.9,0,0.0262647,"tionships in the text in some cases. For instance, the parse tree of the racist tweet, ""How can &lt;PG> say they want equality when they see &lt;PG> as lesser beings? "" shown in Section 3.4 is correct. However, the parse tree misses the coreference of pronoun they to belong to &lt;PG>. In these cases, powerful language models like BERT will be able to extract these relationships. These analyses show that both approaches have their own merits and often perform well for complementary attack types. 50 4 Related Work Remedies include careful hyperparameter tuning, early stopping, and the use of ensembles. Risch and Krestel (2020) proposed to use an ensemble of BERT models to control the variance of these large models over small datasets. Ensembling is expensive, so there remains a need for computationally efficient methods that approach the same performance. Because GCN has far fewer parameters, it is less likely to need these countermeasures against overfitting. Experiments on the relatively larger abusive language dataset of 100K tweets (Founta et al., 2018) by Lee et al. (2018) highlighted that sequential models such as RNN perform well but are still hard to train on this dataset size. Further, Kumar et al. (2018)"
2020.alw-1.6,S19-2010,0,0.0137178,"d significant gains over the bag-of-words features. Our model builds on this work and explicitly models the dependency between words using graph convolution operations. 5 Discussion Worse performance of BERT: Our experiments did not show the superior performance of the BERT model on the abusive language datasets. We also noticed that prior literature on comparable tasks is variable, with some successes for BERT-like models but few robust trends. There are numerous reports of difficulties in training these large neural networks on the small, imbalanced annotated datasets typical of such tasks (Zampieri et al., 2019; Liu et al., 2019). The challenges are likely an effect of over-fitting and lead to inconsistent results. 8 9 51 nlp.stanford.edu/projects/glove/ https://spacy.io/ specially built for Twitter (Kong et al., 2014) but believe that using Twitter-specific parsers might improve our results further. 6 Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Eleventh international aaai conference on web and social media. Conclusion Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-t"
2020.alw-1.6,P19-1163,0,0.0247734,"nta et al., 2018) by Lee et al. (2018) highlighted that sequential models such as RNN perform well but are still hard to train on this dataset size. Further, Kumar et al. (2018) concluded that with optimal feature selection, classifiers like SVM and Random forest performs at par with neural networks. Dataset quality and annotator bias: Recent works have highlighted that majority of the abusive language datasets suffer from poor quality (Wiegand et al., 2019; Vidgen and Derczynski, 2020) or show evidence that annotator decisions were inappropriately affected by surface markers of speaker race (Sap et al., 2019). We believe that corrections of these deficiencies and biases in annotation are essential for research progress in the field. Better dataset collection, however, is complementary to our computational approach. We believe our dependency parser-based approach should be able to perform competitively on future datasets. This is because the model is designed to be insensitive to the cues from unrelated single words. Social media-specific tools: Social media text tends to be very noisy and thus, NLP tools trained on general corpus do not perform well on these datasets. However, our preliminary expe"
2020.alw-1.6,D18-1244,0,0.0990268,"that some attention heads of the pre-trained BERT model are learning syntactic dependencies between words such as direct objects of verbs etc. It is similar to a dependency parser that represents the structure of syntactic dependence between words in the sentence. Recently, Burnap and Williams (2016) and Alorainy et al. (2019) also showed that including syntactic dependency as features improves classifier performance in abusive language detection tasks. However, adding features can only provide weak supervision compared to encoding these dependence explicitly in the model. On the other hand, Zhang et al. (2018a) leveraged the dependency path between the subject and object of the sentence to achieve state-of-the-art results on relation extraction task on the TACRED dataset. They encoded the dependency parse graph using efficient graph convolution operations (Kipf and Welling, 2017). However, a direct usage of Zhang et al. (2018a)’s method is not straightforward for abusive language datasets due to the complexity of the possibilities for expressing an attack in text. An attack may be Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work wel"
2020.alw-1.6,P16-2034,0,0.076169,"Missing"
A97-1034,A97-1054,0,0.0636692,"Missing"
A97-1034,E95-1027,0,\N,Missing
boxwell-brew-2010-pilot,maamouri-etal-2008-enhancing,0,\N,Missing
boxwell-brew-2010-pilot,J93-2004,0,\N,Missing
boxwell-brew-2010-pilot,W09-0806,0,\N,Missing
boxwell-brew-2010-pilot,P05-2013,0,\N,Missing
boxwell-brew-2010-pilot,P06-1064,0,\N,Missing
boxwell-brew-2010-pilot,P09-1005,1,\N,Missing
boxwell-brew-2010-pilot,J07-3004,0,\N,Missing
C90-3007,C88-1063,0,0.382629,"r sure that the pronoun described is &quot;she&quot; and the number has to be singular, since the network dictates, in a way which will be explained in detail below, that the choice between feminine, masculine, and neuter is only applicable to third person singular pronouns. The network thus provides the raw material for particular sorts of limited inference about the behaviour of pronouns. We wanted to investigate the mathematical properties of systemic networks in order to better understand the nature of the constraints on feature structure which they are capaMe of expressing. Both Mellish and Kasper [7] provide translations of systemic networks into non-graphical formMisms: Mellish expresses constraints as axioms within a simple subset of predicate logic, while Kasper uses an extended version&apos;of Functional Unification Grammar [10]. Unfortunately the methods which they then use to check for consistency are powerful general methods which may incur considerable computational cost. We initially hoped to show that systemic networks constitute a low-power constraint language which combines the twin goals of linguistic credibility and computationM tractability. While the main result we present in t"
C90-3007,P87-1033,0,0.05988,"Missing"
C90-3007,P86-1038,0,0.0528014,"Missing"
C90-3007,J88-1004,0,\N,Missing
C90-3007,P84-1008,0,\N,Missing
C92-2092,C92-2117,0,0.157977,"Missing"
D10-1072,P09-1005,1,0.934821,"e category np/n, or “the category of words that become noun phrases when combined with a noun to the right”. The rightmost category indicates the argument that the category is seeking, the leftmost category indicates the result of combining this category with its argument, and the slash (/ or ) indicates the direction of combination. Categories can be nested within each other: a transitive verb like devoured belongs to the category The man devoured np/n n np > (s
p)/npx the steak 5 Semantic Role Labeling npx /nx nx > npx We use a modified version of the Brutus semantic role labeling system (Boxwell et al., 2009)2 . The original vers
p sion of this system takes complete CCG derivations as in< s put, and predicts semantic roles over them. For our purposes, however, it is necessary to modify the system to Figure 3: A simple CCG derivation. make semantic predictions at parse time, inside a packed chart, before the complete derivation is available. For this reason, it is necessary to remove the global features from the system (that is, features that rely on the comThe steak that the man devoured plete parse), leaving only local features (features that are np (npx 
px )/(s/npx ) np (s
p)/npx known at th"
D10-1072,P01-1017,0,0.0239462,"mantic roles. When a semantic role labeler predicts an incorrect role, it is often due to an error in the parse tree. Consider the erroneously annotated sentence from the Penn Treebank corpus shown in Figure 1. If a semantic role labeling system relies heavily upon syntactic attachment decisions, then it will likely predict that in 1956 describes the time that asbestos was used, rather than when it ceased to be used. Errors of this kind are common in treebanks and in automatic parses. It is telling, though, that while the handannotated Penn Treebank (Marcus et al., 1993), the Charniak parser (Charniak, 2001), and the C&C parser (Clark and Curran, 2004) all produce the erroneous parse from Figure 1, the hand-annotated Propbank corpus of verbal semantic roles (Palmer et al., 2005) correctly identifies in 1956 as a temporal modifier of stopped, rather than using. This demonstrates that while syntactic attachment decisions like these are difficult for humans and for automatic parsers, a human reader has little difficulty identifying the correct semantic relationship between the temporal modifier and the verbs. This is likely due to the fact that the meaning suggested by the parse in Figure 1 is unlik"
D10-1072,P04-1014,0,0.752503,"beler predicts an incorrect role, it is often due to an error in the parse tree. Consider the erroneously annotated sentence from the Penn Treebank corpus shown in Figure 1. If a semantic role labeling system relies heavily upon syntactic attachment decisions, then it will likely predict that in 1956 describes the time that asbestos was used, rather than when it ceased to be used. Errors of this kind are common in treebanks and in automatic parses. It is telling, though, that while the handannotated Penn Treebank (Marcus et al., 1993), the Charniak parser (Charniak, 2001), and the C&C parser (Clark and Curran, 2004) all produce the erroneous parse from Figure 1, the hand-annotated Propbank corpus of verbal semantic roles (Palmer et al., 2005) correctly identifies in 1956 as a temporal modifier of stopped, rather than using. This demonstrates that while syntactic attachment decisions like these are difficult for humans and for automatic parsers, a human reader has little difficulty identifying the correct semantic relationship between the temporal modifier and the verbs. This is likely due to the fact that the meaning suggested by the parse in Figure 1 is unlikely – the reader instinctively feels that a t"
D10-1072,N09-1037,0,0.0502923,"Hajiˇc et al., 2009). Many of these systems perform joint syntactic and semantic analysis by generating an n-best list of syntactic parses, labeling semantic roles on all of them, then re-ranking these parses by some means. Our approach differs from this strategy by abandoning the preliminary ranking and predicting semantic roles at parse time. By doing this, we effectively open semantic roles in the entire parse forest to examination by the ranking model, rather than restricting the model to an n-best list generated by a baseline parser. The spirit of this work more closely resembles that of Finkel and Manning (2009) , which improves both parsing and named entity recognition by combining the two tasks. 3 Why Predicting Semantic Roles in a Packed Chart is Difficult Predicting semantic roles in the environment of a packed chart is difficult when using an atomic CFG. In order to achieve the polynomial efficiency appropriate for widecoverage parsing, it is necessary to “pack” the chart – that is, to combine distinct analyses of a given span of words that produce the same category. The only other widely used option for wide-coverage parsing is to use beam search with a narrow beam, which runs the risk of searc"
D10-1072,W03-1008,0,0.560159,"above to aid the eye in following the relevant version of Brutus are as follows: > [devoured-steak] dependency. (s
p)/np, or “the category that would become a sentence if it could combine with a noun phrase to the right and another noun phrase to the left”. An example of how categories combine to make sentences is shown in Figure 3. CCG has many capabilities that go beyond that of a typical context-free grammar. First, it has a sophisticated internal system of managing syntactic heads and dependencies1 . These dependencies are used to great effect in CCG-based semantic role labeling systems (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), as they do not suffer the same data-sparsity effects encounted with treepath features in CFG-based SRL systems. Secondly, CCG permits these dependencies to be passed through intermediary categories in grammatical structures like relative clauses. In Figure 4, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is what makes CCG resistant to the problem described in Section 3 – because the dependency is for"
D10-1072,W09-1201,0,0.0302058,"Missing"
D10-1072,J93-2004,0,0.0338057,"use the syntactic information to predict semantic roles. When a semantic role labeler predicts an incorrect role, it is often due to an error in the parse tree. Consider the erroneously annotated sentence from the Penn Treebank corpus shown in Figure 1. If a semantic role labeling system relies heavily upon syntactic attachment decisions, then it will likely predict that in 1956 describes the time that asbestos was used, rather than when it ceased to be used. Errors of this kind are common in treebanks and in automatic parses. It is telling, though, that while the handannotated Penn Treebank (Marcus et al., 1993), the Charniak parser (Charniak, 2001), and the C&C parser (Clark and Curran, 2004) all produce the erroneous parse from Figure 1, the hand-annotated Propbank corpus of verbal semantic roles (Palmer et al., 2005) correctly identifies in 1956 as a temporal modifier of stopped, rather than using. This demonstrates that while syntactic attachment decisions like these are difficult for humans and for automatic parsers, a human reader has little difficulty identifying the correct semantic relationship between the temporal modifier and the verbs. This is likely due to the fact that the meaning sugge"
D10-1072,W04-2705,0,0.0379306,"ibe this feature in detail. The Maximum-Entropy models were trained to 500 iterations. To prevent overfitting, we used Gaussian priors with global variances of 1 and 5 for the identifier and the labeler, respectively. Table 1 shows SRL performance for the local model described above, and the full global CCG-system described by Boxwell et al. (2009). We use the method for calculating the accuracy of Propbank verbal semantic roles described in the CoNLL-2008 shared task on semantic role labeling (Surdeanu et al., 2008). Because the Brutus SRL system is not designed to accommodate Nombank roles (Meyers et al., 2004), we restrict ourselves to predicting Propbank roles in the present work. The local system has the same precision as the global one, but trails it on recall and F-measure. Note that this performance is achieved with gold standard parses. 6 Performing Semantic Role Predictions at Parse Time Recall that the reasoning for using a substantially pared down version of the Brutus SRL system is to allow it to predict semantic roles in the context of a packed chart. Because we predict semantic roles for each constituent immediately after the constituent is formed and before it is added to the chart, we"
D10-1072,J05-1004,0,0.0475801,"e Penn Treebank corpus shown in Figure 1. If a semantic role labeling system relies heavily upon syntactic attachment decisions, then it will likely predict that in 1956 describes the time that asbestos was used, rather than when it ceased to be used. Errors of this kind are common in treebanks and in automatic parses. It is telling, though, that while the handannotated Penn Treebank (Marcus et al., 1993), the Charniak parser (Charniak, 2001), and the C&C parser (Clark and Curran, 2004) all produce the erroneous parse from Figure 1, the hand-annotated Propbank corpus of verbal semantic roles (Palmer et al., 2005) correctly identifies in 1956 as a temporal modifier of stopped, rather than using. This demonstrates that while syntactic attachment decisions like these are difficult for humans and for automatic parsers, a human reader has little difficulty identifying the correct semantic relationship between the temporal modifier and the verbs. This is likely due to the fact that the meaning suggested by the parse in Figure 1 is unlikely – the reader instinctively feels that a temporal modifier fits better with the verb stop than with the verb use. In this paper, we will use the idea that semantic roles p"
D10-1072,W08-2121,0,0.118551,"Missing"
D10-1072,J08-2005,0,\N,Missing
E95-1012,P94-1044,0,0.0124495,"o more interesting g r a m m a r s than has hitherto been the case. The paper is structured as follows. We start by reviewing the training and use of probabilistic context-free g r a m m a r s (PCFGs). We then de: velop a technique to allow analogous probabilistic annotations on type hierarchies. This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy. We conclude by sketching a technique which does treat such structures. While we know of previous work which associates scores with feature structures (Kim, 1994) are not aware of any previous t r e a t m e n t which makes explicit the link to classical probability theory. We take a slightly unconventional perspective • P(N1) = 1. • If T is a partial phrase marker, and T ' is a partial phrase marker which differs from it only in that a single non-terminal node N k in T has been expanded to ~'~ in T ', then P(T') = P(T) × P(N~ ~ ~'~). In this definition R acts as a specification of the accessibility relationships which can hold between nodes of the trees admitted by the grammar. The rule probabilities specify the cost of 1Our description is closely base"
E95-1012,J88-1004,0,0.0292914,"Missing"
feldman-etal-2006-cross,A00-2013,0,\N,Missing
feldman-etal-2006-cross,P00-1027,0,\N,Missing
feldman-etal-2006-cross,A00-1031,0,\N,Missing
feldman-etal-2006-cross,W04-3229,1,\N,Missing
feldman-etal-2006-cross,W06-2005,1,\N,Missing
H94-1019,J93-1003,0,0.0480678,"Missing"
I11-1022,D10-1072,1,0.760614,"ermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the dependency is formed when the two heads combine, it is available to be used as a local feature by the semantic role labeler. This property of CCG and its impact on packed-chart SRL is described extensively in Boxwell et al. (2010). This ability to predict dependencies (and semantic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in"
I11-1022,C04-1041,0,0.204923,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,P04-1014,0,0.203309,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,E09-1026,0,0.0598503,"Missing"
I11-1022,W03-1008,0,0.195301,"entence if it could combine with a noun phrase to the right and another noun phrase to the left”. The process of automatically assigning CCG categories to words is called “supertagging”, and CCG categories are sometimes informally referred to as “supertags”. An example of how categories combine to make sentences is shown in Figure 1. CCG has many capabilities that go beyond that of a typical context-free grammar. First, it has a sophisticated internal system of managing syntactic heads and dependencies1 . These dependencies are used to great effect in CCG-based semantic role labeling systems (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), as they do not suffer the same data-sparsity effects encountered with treepath features in CFG-based SRL systems. Secondly, CCG permits these dependencies to be passed through intermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the depen"
I11-1022,J07-3004,0,0.0683133,"esource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conferen"
I11-1022,N10-1137,0,0.0213533,"like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 192–200, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP The phrases when combined with a noun to the right”. The rightmost category indicates the argument that the category is seeking, the leftmost category indicates the res"
I11-1022,C08-1008,1,0.899679,"Missing"
I11-1022,J93-2004,0,0.0366651,"cting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential."
I11-1022,boxwell-white-2008-projecting,1,0.896164,"ntic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in terminal indexation (Honnibal and Curran, 2007; Boxwell and White, 2008). The system is organized in a twostage pipeline of maximum entropy models3 , following the organization of a previous CFG-style approach (Punyakanok et al., 2008). The first stage is the identification stage, where, for each predicate in the sentence, each word is tagged as either a role or a nonrole (figure 3). The second stage is the classification stage, where the roles are sorted into A RG 0, A RG 1, and so on (figure 4). The identification model and the classification model share the same features, but they are trained and run separately. For the results presented here, we use a version"
I11-1022,J05-1004,0,0.0128898,"ics relationship, and then predicting roles on novel syntactic analyses. The gold standard syntactic training data can be eliminated from the process by extracting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combin"
I11-1022,P10-1051,1,0.895534,"Missing"
I11-1022,J08-2005,0,\N,Missing
I11-1022,P09-1005,1,\N,Missing
J01-3007,A92-1022,0,0.0544926,"Missing"
J01-3007,J00-1003,0,0.0389308,"Missing"
J03-1007,copestake-flickinger-2000-open,0,0.0656215,"Missing"
J04-1003,A97-1052,0,0.0536178,"ointed out by Kipper, Dang, and Palmer (2000), Levin classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. This means that some ambiguities may also arise as a result of accidental errors or inconsistencies. The classification was created not with computational uses in mind, but for human readers, so it has not been necessary to remedy all the errors and omissions that might cause trouble for machines. Similar issues arise in almost all efforts to make use of preexisting lexical resources for computational purposes (Briscoe and Carroll 1997), so none of the above comments should be taken as criticisms of Levin’s achievement. The objective of this article is to show how to train and use a probabilistic version of Levin’s classification in verb sense disambiguation. We treat errors and inconsistencies in the classification as noise. Although all our tests have used Levin’s classes and the British National Corpus, the method itself depends neither on the details of Levin’s classification nor on parochial facts about the English language. Our future work will include tests on other languages, other classifications, and other corpora."
J04-1003,W98-1505,0,0.0283921,"Missing"
J04-1003,C00-1023,0,0.544541,"s has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take precedence over the alternations"
J04-1003,C00-1028,0,0.426494,"rnations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take preceden"
J04-1003,W02-1005,0,0.0501738,"Missing"
J04-1003,C96-1055,0,0.0785818,"evin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic"
J04-1003,P92-1032,0,0.12757,"Missing"
J04-1003,J93-1005,0,0.0704721,"(e.g., Thameslink presently carries 20,000 passengers daily) is larger than the Carry class, it will be given a higher probability (.45 versus .4). Our estimation scheme is clearly a simplification, but it is an empirical question how much it matters. Tables 5 and 6 show the ten most frequent classes as estimated using (15) and (16). We explore the contribution of the two estimation schemes for P(c) in Experiments 1 and 2. The probabilities P(f |c) and P(f |v) will be unreliable when the frequencies F(f , v) and F(f , c) are small and will be undefined when the frequencies are zero. Following Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7. F(f ,V) When F(f , v) is zero, the estimate used is proportional to the average F(V) across 54 Lapata and Brew Verb Class Disambiguation Using Informative Priors Table 7 Smoothed estimates. (a) (c) P(f |v) ≈ P(f |c) ≈ F(f , v) + F(f ,V) F(V) F(v) + 1 F(f , c) + Ff ,C) F(C) F(c) + 1 (b) F(f , V) =  F( f , vi ) i (d) F(C) =  F( f , ci ) i all verbs. Similarly, when F(f , c) is zero, our estimate is proportional to the average F(f ,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have a"
J04-1003,J98-1001,0,0.0339313,"Missing"
J04-1003,P98-1112,0,0.0727917,"lausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic constraints. More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the sem"
J04-1003,P99-1051,0,0.188935,"iteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for desc"
J04-1003,A00-2034,0,0.160933,"opt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal me"
J04-1003,J01-3003,0,0.595211,"aightforward syntactic and syntactico-semantic criteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification p"
J04-1003,W98-0703,0,0.0514272,"Missing"
J04-1003,W96-0208,0,0.0918429,"Missing"
J04-1003,W97-0323,0,0.0540735,"Missing"
J04-1003,A00-2009,0,0.050523,"Missing"
J04-1003,N01-1011,0,0.0131953,"ion experiments as it was represented solely by the verb kick (50 instances). In this study we compare a naive Bayesian classifier that relies on a uniform prior (see (20)) against two classifiers that make use of nonuniform prior models: The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2, whereas the classifier in (23) relies on the more informative model P(c, f , v). As a baseline for the disambiguation task, we simply assign the most common class in the training data to every instance in the test data, ignoring context and any form of prior information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an upper bound on disambiguation performance by measuring how well human judges agree with one another (percentage agreement) on the class assignment task. Recall from Section 4.1 that our corpus was annotated by two judges with Levin-compatible verb classes. 6.2 Results The results of our class disambiguation experiments are summarized in Figures 2–5. In order to investigate differences among different frames, we show how the naive Bayesian classifiers perform for each frame individually. Figures 2–5 (x-axis) also reveal the influence of col"
J04-1003,C00-2108,0,0.403214,"Missing"
J04-1003,J98-3003,0,0.0261116,"semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic"
J04-1003,P94-1013,0,0.163515,"Missing"
J04-1003,P95-1026,0,0.104611,"Missing"
J04-1003,P00-1065,0,\N,Missing
J04-1003,J06-2001,0,\N,Missing
J04-1003,J99-4002,0,\N,Missing
J04-1003,P98-1046,0,\N,Missing
J04-1003,C98-1046,0,\N,Missing
J04-1003,C98-1108,0,\N,Missing
J04-1003,J98-1004,0,\N,Missing
J04-1003,1997.mtsummit-workshop.2,0,\N,Missing
J91-4002,P88-1035,0,0.0434162,"Missing"
J91-4002,W17-6307,0,0.0222065,"Missing"
J91-4002,1986.tc-1.15,0,0.0666217,"Missing"
J91-4002,C88-1063,0,0.0203607,"d the utterances they license, it might be possible to use their grammars in either direction. For Patten and Ritchie a systemic grammar is encoded as a set of rules for a production system, and the generation process involves the application of these rules. Kasper (1987a, 1987b) has designed algorithms that involve the encoding of both system networks and the associated realization rules as constraints expressed within a feature logic involving disjunction. He then implements the key operation of unification by applying a general technique for unification of disjunctive feature descriptions (Kasper 1988). These techniques decompose the problem of disjunctive unification into three stages, only the last of which is exponentially hard. We are not attempting to match his general algorithms for disjunctive unification, but rather to design special purpose algorithms that are particularly suitable for use with system networks, and particularly for those networks that play a role in Houghton&apos;s system. In practice the set of constraints that Kasper develops from his systemic grammar is too large to solve conveniently with the constraint-solving technology available to him, so he augments the constra"
J91-4002,P87-1033,0,0.134476,"which choices are made as well as the networks that are the present concern. Patten and Ritchie are primarily concerned with language-generation 382 Chris Brew Systemic Classification and its Efficiency rather than comprehension, but since they aim for a declarative specification of the generation relationship between systemic grammars and the utterances they license, it might be possible to use their grammars in either direction. For Patten and Ritchie a systemic grammar is encoded as a set of rules for a production system, and the generation process involves the application of these rules. Kasper (1987a, 1987b) has designed algorithms that involve the encoding of both system networks and the associated realization rules as constraints expressed within a feature logic involving disjunction. He then implements the key operation of unification by applying a general technique for unification of disjunctive feature descriptions (Kasper 1988). These techniques decompose the problem of disjunctive unification into three stages, only the last of which is exponentially hard. We are not attempting to match his general algorithms for disjunctive unification, but rather to design special purpose algori"
J91-4002,J88-1004,0,0.484049,"ional constraints imposed by the process of realization. In this paper we focus on the use of the networks as a knowledge source in their own right. Systemic networks form a terminological representation system in the sense of the term used in work on KL-ONE and its successors (Brachman and Schmolze 1985; Nebel 1990). We would like to understand the nature of the knowledge they express well enough to be able to use system networks as the basis of a general, and perhaps computationally convenient, terminological language for use in linguistic applications. Our approach is very close to that of Mellish (1988), as well as to a multitude of other formalisms involving partial descriptions, such as Shieber (1986). By way of example, consider the word your, as it is described by the network and rules in Figures 1 and 2. Without further information we do not know whether it is s i n g u l a r or p l u r a l , although we can be sure that it is second, p e r s o n a l and possdet. This information will sometimes be provided by context. In the sentence You should not take my word for it, but bolster your own intuitions by inspecting the realization rules for yourselves, we eventually discover that the wri"
J91-4002,H89-1033,0,0.0311611,"those --* I --* me --* myself ---* mine --* my ~ you --* you --* yourself --* yours ~ your ~ we -* us --* ourselves --* ours --. our --* you -* you --* yourselves --+ yours ~ your ~ she --* he --* it --+ her ~ him --* it --* herself ~ himself --* itself --* hers --+ his --* its --* her ~ his --+ its --* they --* them --* themselves ~ theirs --* their Figure 2 Realization rules for Winograd&apos;s pronoun network. 377 Computational Linguistics Volume 17, Number 4 2. System Networks in NLP 2.1 Systemic Language Production In many language-generation systems, such as those described by Davey (1978), Winograd (1972), Houghton (1986), Houghton and Isard (1987), and Mann and Matthiessen (1985), system networks are used to structure the decisions the system needs to make in the course of producing a syntactic constituent. Each network lays out a set of interconnected options, but does not specify how the system chooses between the options in a particular situation. In systemic grammar, language production involw,~s the task of traversing a network, gradually making choices that incrementally specify various aspects of the form of an utterance. As choices are made the system collects features that are eventu"
N12-1021,P08-1118,0,0.0143027,"Missing"
N12-1021,P10-4003,1,0.648755,"r a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student"
N12-1021,E12-1048,1,0.800568,"sed on lexical similarity, which we report in order to offer a substantial example of applying the same classifier to both portions of the dataset. 3.1 B EETLE II system baseline The interpretation component of the B EETLE II system uses a syntactic parser and a set of handauthored rules to extract the domain-specific semantic representations of student utterances from the text. These representations were then matched against the semantic representations of expected correct answers supplied by tutors. The resulting system output was automatically mapped into our target labels as discussed in (Dzikovska et al., 2012). 3.2 Lexical similarity baseline To provide a higher baseline that is comparable across both subsets of the data, we built a simple decision tree classifier using the Weka 3.6.2 implementation of C4.5 pruned decision trees (weka.classifiers.trees.J48 class), with default parameters. As features, we used lexical similarity scores computed by the Text::Similarity package with default parameters2 . The code computes four similarity metrics – the raw number of overlapping words, F1 score, Lesk score and cosine score. We compared the learner response to the expected answer(s) and the question, res"
N12-1021,W07-1401,0,0.0130615,"//www.cs.york.ac.uk/semeval-2013/task4/ 208 numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in B EETLE and S CI E NTS BANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language const"
N12-1021,H91-1061,0,0.189184,"and F1 are defined similarly. Micro-averaging takes class sizes into account, so a system that performs well on the most common classes will have a high micro-average score. This is the most commonly used classifier evaluation metric. Note that, in particular, overall classification accuracy (defined as the number of correctly classified instances out of all instances) is mathematically equivalent to micro-averaged recall (Abudawood and Flach, 2011). However, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., (Lewis, 1991)). We report both values in our results. 205 656 86 204 2729 0.24 0.03 0.07 526 1175 24 5251 0.10 0.22 0.005 Table 1: Distribution of annotated labels in the data In addition, we report the system scores on the binary decision of whether or not the corrective feedback should be issued (denoted “corrective feedback” in the results table). It assumes that a tutoring system using a classifier will give corrective feedback if the classifiers returns any label other than “correct”. Thus, every instance classified as “partially correct incomplete”, “contradictory”, “irrelevant” or “non domain” is co"
N12-1021,P11-1076,0,0.0396476,"ons and Unseen Domains allow researchers to evaluate how well their systems generalize to near and far domains, respectively. The primary target application for this work is intelligent tutoring systems, where the classification of responses is intended to facilitate specific pedagogic feedback. Beneath the surface, the baseline systems reported here are more similar to grading systems that use the approach of (Leacock and Chodorow, 2003), which uses classifier technology to detect expressions of facet-like concepts, then converts the result to a numerical score, than to grading systems like (Mohler et al., 2011), which directly produces a 4 See http://www.cs.york.ac.uk/semeval-2013/task4/ 208 numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in B EETLE and S CI E NTS BANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing w"
N12-1021,nielsen-etal-2008-annotating,1,0.794312,"Missing"
N12-1021,D08-1002,0,0.012212,"niques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in B EETLE and S CI E NTS BANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language constructs coming from (typed) dialogue context. We chose to use lexical similarity as a baseline specificall"
N15-3021,D14-1067,0,0.0396151,"een frequently adopted for retrieving information from KBs. However, users have to figure out the most effective queries in order to retrieve relevant information. Furthermore, without appropriate ranking methods, users may be overwhelmed by the information available in the search results. Early Natural Language Interfaces (NLIs) required a handcrafted interface solution for each database thereby restricting its portability (Green et al., 1961; Hendrix et al., 1978; Woods, 1973). Recent research has focused more on developing open domain systems (Kwiatkowski et al., 2013; Yao and Durme, 2014; Bordes et al., 2014), but there remains a need for specialized NLIs (Minock, 2005). One unique feature of our system is to help users to build a complete question by providing suggestions according to a partial question and a grammar. Much of prior work translates a natural language question into SPARQL and retrieves answers from a 101 Proceedings of NAACL-HLT 2015, pages 101–105, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics triple store (Lopez et al., 2005; Unger et al., 2012; Lehmann et al., 2012; Yahya et al., 2013; He et al., 2014); however, SPARQL queries have bee"
N15-3021,D14-1116,0,0.0193534,"., 2013; Yao and Durme, 2014; Bordes et al., 2014), but there remains a need for specialized NLIs (Minock, 2005). One unique feature of our system is to help users to build a complete question by providing suggestions according to a partial question and a grammar. Much of prior work translates a natural language question into SPARQL and retrieves answers from a 101 Proceedings of NAACL-HLT 2015, pages 101–105, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics triple store (Lopez et al., 2005; Unger et al., 2012; Lehmann et al., 2012; Yahya et al., 2013; He et al., 2014); however, SPARQL queries have been criticized to have unsatisfying query response time. In this work, we maintain flexibility by first parsing a question into First Order Logic, which is further translated into both SQL and SPARQL. This enables us to easily adapt to new query languages and allows us to choose the most appropriate query language technology for a given use case. Finally, to the best of our knowledge, none of existing NLIs provide dynamic analytics for the results. Our system performs descriptive analytics and comparisons on various dimensions of the data, conducts sentiment ana"
N15-3021,D13-1161,0,0.033914,"(Zhang et al., 2013; Zhang et al., 2014) have been frequently adopted for retrieving information from KBs. However, users have to figure out the most effective queries in order to retrieve relevant information. Furthermore, without appropriate ranking methods, users may be overwhelmed by the information available in the search results. Early Natural Language Interfaces (NLIs) required a handcrafted interface solution for each database thereby restricting its portability (Green et al., 1961; Hendrix et al., 1978; Woods, 1973). Recent research has focused more on developing open domain systems (Kwiatkowski et al., 2013; Yao and Durme, 2014; Bordes et al., 2014), but there remains a need for specialized NLIs (Minock, 2005). One unique feature of our system is to help users to build a complete question by providing suggestions according to a partial question and a grammar. Much of prior work translates a natural language question into SPARQL and retrieves answers from a 101 Proceedings of NAACL-HLT 2015, pages 101–105, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics triple store (Lopez et al., 2005; Unger et al., 2012; Lehmann et al., 2012; Yahya et al., 2013; He et a"
N15-3021,P14-5010,0,0.00311753,"question “show me all drugs targeting pain”, our system shows the distribution of all technologies used for such drugs in the result set. We also compare the drugs in the result set on different dimensions (e.g., diseases). Moreover, we compute trends via exponential smoothing for entities that have a temporal dimension. By linking entities from our KB to entity mentions in a large news corpus (14 million articles and 147 million sentences), we are able to perform additional analytics based on named entity recognition and sentiment analysis techniques. We adopted the Stanford CoreNLP toolkit (Manning et al., 2014) for recognizing person, organization, and location from the news corpus. Given an entity, we show its frequency count and how its sentiment may change over time. This information may provide further insights to users in order to support their own analysis. 8 Demonstration Script Outline Figure 2 shows the beginning of the sample query: companies developing drugs having an indication of . . . ? While the user is typing, a variety of possible extensions to the query are offered, and the user se104 lects Hypertension (1). Our system shows a pie chart of each company’s market share for hypertensi"
N15-3021,P14-1090,0,0.0610032,"Missing"
P02-1029,C96-1055,0,0.157719,"nts. The theoretical foundation has been established in extensive work on semantic verb classes such as (Levin, 1993) for English and (Vázquez et al., 2000) for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There"
P02-1029,P98-1112,0,0.417224,"in extensive work on semantic verb classes such as (Levin, 1993) for English and (Vázquez et al., 2000) for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse"
P02-1029,P99-1051,0,0.216635,"aning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase inform"
P02-1029,C00-2108,1,0.895871,"tical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus preposition). In both conditions v"
P02-1029,schulte-im-walde-2002-subcategorisation,1,0.716294,"long term goal is to support the development of high-quality and large-scale lexical resources. 2 Syntactic Descriptors for Verb Frames The syntactic subcategorisation frames for German verbs were obtained by unsupervised learning in a statistical grammar framework (Schulte im Walde et al., 2001): a German context-free grammar containing frame-predicting grammar rules and information about lexical heads was trained on 25 million words of a large German newspaper corpus. The lexicalised version of the probabilistic grammar served as source for syntactic descriptors for verb frames (Schulte im Walde, 2002b). The verb frame types contain at most three arguments. Possible arguments in the frames are nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns (r), prepositional phrases (p), expletive es (x), non-finite clauses (i), finite clauses (s-2 for verb second clauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and copula constructions (k). For example, subcategorising a direct (accusative case) object and a non-finite clause would be represented by nai. We defined a total of 38 subcategorisation frame types, according to the verb subca"
P02-1029,E99-1007,0,0.2346,"heir syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus prepositio"
P02-1029,C00-2094,0,0.0429675,"ns for generalising over these objects. In our case, clustering is realised on verbs: the data objects are represented by verbs, and the data features for describing the objects are realised by a probability distribution over syntactic verb frame descriptions. Clustering is applicable to a variety of areas in Natural Language Processing, e.g. by utilising class type descriptions such as in machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998), or by applying clusters for smoothing such as in machine translation (Prescher et al., 2000), or probabilistic grammars (Riezler et al., 2000). We performed clustering by the k-Means algorithm as proposed by (Forgy, 1965), which is an unsupervised hard clustering method assigning data  objects to exactly clusters. Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster (centroid) and re-calculating cluster centroids until no further changes take place. One parameter of the clustering process is the distance measure used. Standard choices include the cosine, Euclidean distance, Manhattan metric, and variants of the Kullback-Leibler (KL) diverg"
P02-1029,M95-1005,0,0.00967065,"od related to hierarchical clustering (Schulte im Walde, 2000). 5 Clustering Evaluation The task of evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires us to assess the similarity between two sets of equivalence relations. As noted by (Strehl et al., 2000), it is useful to have an evaluation measure that does not depend on the choice of similarity measure or on the original dimensionality of the input data, since that allows meaningful comparison of results for which these parameters vary. This is similar to the perspective of (Vilain et al., 1995), who present, in the context of the MUC co-reference evaluation scheme, a model-theoretic measure of the similarity between equivalence classes. Strehl et al. consider a clustering that partitions   objects (  ) into clusters;  the  clusters  of are the sets for which     . 1 We also tried various transformations and variations of the probabilities, such as frequencies and binarisation, but none proved as effective as the probabilities. We call the cluster result  and the desired goldstandard  . For measuring the quality of an indi-  vidual cluster, the cluster"
P02-1029,P00-1061,0,0.0111788,", clustering is realised on verbs: the data objects are represented by verbs, and the data features for describing the objects are realised by a probability distribution over syntactic verb frame descriptions. Clustering is applicable to a variety of areas in Natural Language Processing, e.g. by utilising class type descriptions such as in machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998), or by applying clusters for smoothing such as in machine translation (Prescher et al., 2000), or probabilistic grammars (Riezler et al., 2000). We performed clustering by the k-Means algorithm as proposed by (Forgy, 1965), which is an unsupervised hard clustering method assigning data  objects to exactly clusters. Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster (centroid) and re-calculating cluster centroids until no further changes take place. One parameter of the clustering process is the distance measure used. Standard choices include the cosine, Euclidean distance, Manhattan metric, and variants of the Kullback-Leibler (KL) divergence. We concentrated on two variants of KL in Equ"
P02-1029,C98-1108,0,\N,Missing
P06-1007,W87-0110,0,0.134669,"Missing"
P06-1007,J01-2004,0,0.0357415,"account for the garden-path effect observed in empirical studies, specifically, that the slower processing times associated with garden-path sentences are due in part to their relatively unlikely POS sequences in comparison with those of non-garden-path sentences and in part to differences in the emission probabilities that the tagger learns. One attractive future direction is to carry out simulations that compare the evolution of probabilities in the tagger with that in a theoretically more powerful model trained on the same data, such as an incremental statistical parser (Kim et al., 2002; Roark, 2001). In so doing we can find the places where the prediction problem faced both by the HSPM and the machines that aspire to emulate it actually warrants the greater power of structurally sensitive models, using this knowledge to mine large corpora for future experiments with human subjects. We have not necessarily cast doubt on the hypothesis that the HSPM makes crucial use of structural information, but we have demonstrated that much of the relevant behavior can be captured in a simple model. The ’structural’ regularities that we observe are reasonably well encoded into this model. For purposes"
P06-1007,J99-2004,0,0.0124341,"of lexical category disambiguation based on a bigram statistical POS tagger. Kim et al. (2002) suggest the feasibility of modeling human syntactic processing as lexical ambiguity resolution using a syntactic tagging system called Super-Tagger 49 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 49–56, c Sydney, July 2006. 2006 Association for Computational Linguistics Using the Viterbi algorithm (Viterbi, 1967), the tagger finds the most likely POS sequence for a given word string as shown in (2). (Joshi and Srinivas, 1994; Bangalore and Joshi, 1999). Probabilistic parsing techniques also have been used for sentence processing modeling (Jurafsky, 1996; Narayanan and Jurafsky, 2002; Hale, 2001; Crocker and Brants, 2000). Jurafsky (1996) proposed a probabilistic model of HSPM using a parallel beam-search parsing technique based on the stochastic context-free grammar (SCFG) and subcategorization probabilities. Crocker and Brants (2000) used broad coverage statistical parsing techniques in their modeling of human syntactic parsing. Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and"
P06-1007,N01-1021,0,0.0611666,"lexical ambiguity resolution using a syntactic tagging system called Super-Tagger 49 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 49–56, c Sydney, July 2006. 2006 Association for Computational Linguistics Using the Viterbi algorithm (Viterbi, 1967), the tagger finds the most likely POS sequence for a given word string as shown in (2). (Joshi and Srinivas, 1994; Bangalore and Joshi, 1999). Probabilistic parsing techniques also have been used for sentence processing modeling (Jurafsky, 1996; Narayanan and Jurafsky, 2002; Hale, 2001; Crocker and Brants, 2000). Jurafsky (1996) proposed a probabilistic model of HSPM using a parallel beam-search parsing technique based on the stochastic context-free grammar (SCFG) and subcategorization probabilities. Crocker and Brants (2000) used broad coverage statistical parsing techniques in their modeling of human syntactic parsing. Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry. These previous studies have used small numbers of examples of, for example, the Reduced-relative clause am"
P06-1007,C94-1024,0,\N,Missing
P06-2067,J93-2002,0,0.0761995,"is the relative value for WSJ and Switchboard. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). 4 Extraction of SCFs from Spoken Language Our experiments indicate that the SCCs generated by the parser from spoken language are as accurate as those generated from written texts. Hence, we would expect that the current technology for extracting SCFs, initially designed for written texts, should work equally well for spoken language. We previously built a system for automatically extracting SCFs from spoken BNC, and reported accuracy comparable to previous systems that work with only written texts (Li and Brew, 2005). However, Korhonen (2002"
P06-2067,A97-1052,0,0.262731,"erview As noted above, previous studies on automatic extraction of SCFs from corpora usually proceed in 519 SCCs NP-PP-before NP-S-when NP-PP-at-S-before NP-PP-to-S-when NP-PP-to-PP-at NP-PP-to-S-because-ADVP gold standard corpus type precision type recall F-measure SCFs NP WC 115,524 5,234 1,102 2,688 2.43 recall and F-measure. Type precision is the percentage of SCF types that our system proposes which are correct according some gold standard and type recall is the percentage of correct SCF types proposed by our system that are listed in the gold standard. We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al., 1994) and a manually constructed SCF set from the training data. It makes sense to use a manually constructed SCF set while calculating type precision and recall because some of the SCFs in a syntax dictionary such as COMLEX might not occur in the training data at all. We constructed separate SCF sets for the written and spoken BNC. The results are summarized in Table 7. As shown in Table 7, the accuracy achieved for WC and SC are very comparable: Our system achieves a slightly bet"
P06-2067,A00-2018,0,0.0404783,"tracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two 1. Test the performance of Bikel’s parser in parsing written and spoken language. 2. Compare the accuracy level of SCCs generated from parsed written and spoken lan515 Proceedings of th"
P06-2067,W02-1007,0,0.0477695,"Missing"
P06-2067,C94-1042,0,0.0560652,"re NP-S-when NP-PP-at-S-before NP-PP-to-S-when NP-PP-to-PP-at NP-PP-to-S-because-ADVP gold standard corpus type precision type recall F-measure SCFs NP WC 115,524 5,234 1,102 2,688 2.43 recall and F-measure. Type precision is the percentage of SCF types that our system proposes which are correct according some gold standard and type recall is the percentage of correct SCF types proposed by our system that are listed in the gold standard. We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al., 1994) and a manually constructed SCF set from the training data. It makes sense to use a manually constructed SCF set while calculating type precision and recall because some of the SCFs in a syntax dictionary such as COMLEX might not occur in the training data at all. We constructed separate SCF sets for the written and spoken BNC. The results are summarized in Table 7. As shown in Table 7, the accuracy achieved for WC and SC are very comparable: Our system achieves a slightly better result for WC when using COMLEX as the gold standard and for SC when using manually constructed SCF set as gold sta"
P06-2067,J04-1003,1,0.82648,"Missing"
P06-2067,J93-2004,0,0.0268156,"e. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 1 Introduction Robust statistical syntactic parsers, made possible by new statistical techniques (Collins, 1999; Charniak, 2000; Bikel, 2004) and by the availability of large, hand-annotated training corpora such as WSJ (Marcus et al., 1993) and Switchboard (Godefrey et al., 1992), have had a major impact on the field of natural language processing. There are many ways to make use of parsers’ output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two 1. Test the performance of Bikel’s parser in parsing written and spoken language. 2. Compare the accuracy level of SCCs generated from parsed written and spoken lan515 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 515–522, c Sydney, July 2006. 2006 Association for Comput"
P06-2067,J01-3003,0,0.0300217,"Missing"
P06-2067,W05-0639,0,0.0131699,"a Jianguo Li and Chris Brew Department of Linguistics The Ohio State University Columbus, OH, USA {jianguo|cbrew}@ling.ohio-state.edu Abstract forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Jurafsky (1998) have suggested that there are substantial subcategorization difference"
P06-2067,W96-0213,0,0.0451513,"Missing"
P06-2067,C00-2100,0,0.0223904,"chboard. 2. An SCF Extractor: An extractor is use to extract SCCs from the parser’s output. 3. An English Lemmatizer: MORPHA (Minnen et al., 2000) is used to lemmatize each verb. 4. An SCF Evaluator: An evaluator is used to filter out false SCCs based on their likelihood. An SCC generated by the parser and extractor may be a correct SCC, or it may contain an adjunct, or it may simply be wrong due to tagging or parsing errors. We therefore need an SCF evaluator capable of filtering out false cues. Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). 4 Extraction of SCFs from Spoken Language Our experiments indicate that the SCCs generated by the parser from spoken language are as accurate as those generated from written texts. Hence, we would expect that the current technology for extracting SCFs, initially designed for written texts, should work equally well for spoken language. We previously built a system for automatically extracting SCFs from spoken BNC, and reported accuracy comparable to previous systems that work with only written texts (Li and Brew, 2005). However, Korhonen (2002) has shown that a direct comparison of different"
P06-2067,C00-2108,0,0.0489508,"Missing"
P06-2067,W04-3212,0,0.237196,"Subcategorization Data Jianguo Li and Chris Brew Department of Linguistics The Ohio State University Columbus, OH, USA {jianguo|cbrew}@ling.ohio-state.edu Abstract forms: subcategorization frame (SCF) and subcategorization cue (SCC). SCFs differ from SCCs in that SCFs contain only arguments while SCCs contain both arguments and adjuncts. Both SCFs and SCCs have been crucial to NLP tasks. For example, SCFs have been used for verb disambiguation and classification (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Lapata and Brew, 2004; Merlo et al., 2005) and SCCs for semantic role labeling (Xue and Palmer, 2004; Punyakanok et al., 2005). Current technology for automatically acquiring subcategorization data from corpora usually relies on statistical parsers to generate SCCs. While great efforts have been made in parsing written texts and extracting subcategorization data from written texts, spoken corpora have received little attention. This is understandable given that spoken language poses several challenges that are absent in written texts, including disfluency, uncertainty about utterance segmentation and lack of punctuation. Roland and Jurafsky (1998) have suggested that there are substantial su"
P06-2067,J04-4004,0,\N,Missing
P06-2067,J03-4003,0,\N,Missing
P06-2067,P98-2184,0,\N,Missing
P06-2067,C98-2179,0,\N,Missing
P06-2067,P93-1032,0,\N,Missing
P08-1050,J93-2002,0,0.0880462,"Missing"
P08-1050,P07-1032,0,0.0136526,"h type of features, we use the Gigaword Corpus, which consists of samples of recent newswire text data collected from four distinct international sources of English newswire. 4.2 Feature Extraction We evaluate six different feature sets for their effectiveness in AVC: SCF, DR, CO, ACO, SCF+CO, and JOANIS07. SCF contains mainly syntactic information, whereas CO lexical information. The other four feature sets include both syntactic and lexical information. SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C&C CCG parser (Clark and Curran, 2007). Take He broke the door with a hammer as an example. The grammatical relations generated are given in table 2. he broke the door with a hammer. (det door 3 the 2) (dobj broke 1 door 3) (det hammer 6 a 5) (dobj with 4 hammer 6) (iobj broke 1 with 4) (ncsubj broke 1 He 0 ) 4.3 Table 2: grammatical relations generated by the parser We first build a lexicalized frame for the verb break: NP1(he)-V-NP2(door)-PP(with:hammer). This is done by matching each grammatical label onto one of the traditional syntactic constituents. The set of syntactic constituents we use is summarized in table 3. constitue"
P08-1050,J04-1003,1,0.884145,"ith respect to their usage. When the information about a verb type is not available or sufficient for us to draw firm conclusions about its usage, the information about the class to which the verb type belongs can compensate for it, addressing the pervasive problem of data sparsity in a wide range of NLP tasks, such as automatic extraction of subcategorization frames (Korhonen, 2002), semantic role labeling (Swier and Stevenson, 2004; Gildea and Jurafsky, 2002), natural language generation for machine translation (Habash et al., 2003), and deriving predominant verb senses from unlabeled data (Lapata and Brew, 2004). Although there exist several manually-created verb lexicons or ontologies, including Levin’s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002). AVC helps avoid the expensive hand-coding of such information, but appropriate features must be identified and demonstrated to be effective. In this work, our primary goal is not nece"
P08-1050,P98-2127,0,0.0891107,"study, we explore a wider range of features for AVC, focusing particularly on various ways to mix syntactic with lexical information. Dependency relation (DR): Our way to overcome data sparsity is to break lexicalized frames into lexicalized slots (a.k.a. dependency relations). Dependency relations contain both syntactic and lexical information (4). (4) to data sparsity. We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004), their utility in AVC still remains untested. Co-occurrence (CO): CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures (Rohde et al., 2004). Nevertheless, it is worthwhile testing whether the meaning components that are brought out by syntactic alternations are also correlated to the neighboring words. In other words, Levin verbs may be distinguished on the dimension of neighboring words, in addition to argument structures. A test on this claim can help answer the question of whether v"
P08-1050,P04-1036,0,0.0170208,"wider range of features for AVC, focusing particularly on various ways to mix syntactic with lexical information. Dependency relation (DR): Our way to overcome data sparsity is to break lexicalized frames into lexicalized slots (a.k.a. dependency relations). Dependency relations contain both syntactic and lexical information (4). (4) to data sparsity. We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004), their utility in AVC still remains untested. Co-occurrence (CO): CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures (Rohde et al., 2004). Nevertheless, it is worthwhile testing whether the meaning components that are brought out by syntactic alternations are also correlated to the neighboring words. In other words, Levin verbs may be distinguished on the dimension of neighboring words, in addition to argument structures. A test on this claim can help answer the question of whether verbs in the same Levin class als"
P08-1050,J01-3003,0,0.0608925,"NT] b. I left with a friend. [ACCOMPANIMENT] c. I sang with confidence. [MANNER] This deficiency of unlexicalized subcategorization frames leads researchers to make attempts to incorporate lexical information into the feature representation. One possible improvement over subcategorization frames is to enrich them with lexical information. Lexicalized frames are usually obtained 435 JOANIS07: Incorporating lexical information directly into subcategorization frames has proved inadequate for AVC. Other methods for combining syntactic information with lexical information have also been attempted (Merlo and Stevenson, 2001; Joanis et al., 2007). These studies use a small collection of features that require some degree of expert linguistic analysis to devise. The deeper linguistic analysis allows their feature set to cover a variety of indicators of verb semantics, beyond that of frame information. Joanis et al. (2007) reports an experiment that involves 15 Levin verb classes. They define a general feature space that is supposed to be applicable to all Levin classes. The features they use fall into four different groups: syntactic slots, slot overlaps, tense, voice and aspect, and animacy of NPs. • Syntactic slo"
P08-1050,J07-2002,0,0.0150289,"each row corresponds to a Levin verb and each column represents a given feature. We construct a semantic space with each feature set. Except for JONAIS07 which only contains 224 features, all the other feature sets lead to a very high-dimensional space. For instance, the semantic space with CO features contains over one million columns, which is too huge and cumbersome. One way to avoid these high-dimensional spaces is to assume that most of the features are irrelevant, an assumption adopted by many of the previous studies working with high-dimensional semantic spaces (Burgess and Lund, 1997; Pado and Lapata, 2007; Rohde et al., 2004). Burgess and Lund (1997) suggests that the semantic space can be reduced by keeping only the k columns (features) with the highest variance. However, Rohde et al. (2004) have found it is simpler and more effective to discard columns on the basis of feature frequency, with little degradation in performance, and often some improvement. Columns representing low-frequency features tend to be noisier because they only involve few examples. We therefore apply a simple frequency cutoff for feature selection. We only use features that occur with a frequency over some threshold in"
P08-1050,C00-2108,0,0.164924,"Missing"
P08-1050,E03-1037,0,0.0351352,"Missing"
P08-1050,W04-2606,0,0.0166975,"the pervasive problem of data sparsity in a wide range of NLP tasks, such as automatic extraction of subcategorization frames (Korhonen, 2002), semantic role labeling (Swier and Stevenson, 2004; Gildea and Jurafsky, 2002), natural language generation for machine translation (Habash et al., 2003), and deriving predominant verb senses from unlabeled data (Lapata and Brew, 2004). Although there exist several manually-created verb lexicons or ontologies, including Levin’s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002). AVC helps avoid the expensive hand-coding of such information, but appropriate features must be identified and demonstrated to be effective. In this work, our primary goal is not necessarily to obtain the optimal classification, but rather to investigate 434 Proceedings of ACL-08: HLT, pages 434–442, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the linguistic conditions which are crucial for"
P08-1050,P06-1044,0,0.0124568,"f subcategorization frames (Korhonen, 2002), semantic role labeling (Swier and Stevenson, 2004; Gildea and Jurafsky, 2002), natural language generation for machine translation (Habash et al., 2003), and deriving predominant verb senses from unlabeled data (Lapata and Brew, 2004). Although there exist several manually-created verb lexicons or ontologies, including Levin’s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002). AVC helps avoid the expensive hand-coding of such information, but appropriate features must be identified and demonstrated to be effective. In this work, our primary goal is not necessarily to obtain the optimal classification, but rather to investigate 434 Proceedings of ACL-08: HLT, pages 434–442, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the linguistic conditions which are crucial for lexical semantic classification of verbs. We develop feature sets that combine syntactic and le"
P08-1050,P03-1009,0,0.106434,"not generally improve classification performance (Schulte im Walde, 2000; Joanis, 2002). (3) a. NP(PERSON)-V-PP(with:ARTIFACT) b. NP(PERSON)-V-PP(with:PERSON) 2 Related Work c. NP(PERSON)-V-PP(with:FEELING) Earlier work on verb classification has generally adopted one of the two approaches for devising statistical, corpus-based features. Subcategorization frame (SCF): Subcategorization frames are obviously relevant to alternation behaviors. It is therefore unsurprising that much work on verb classification has adopted them as features (Schulte im Walde, 2000; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). However, relying solely on subcategorization frames also leads to the loss of semantic distinctions. Consider the frame NP-V-PPwith. The semantic interpretation of this frame depends to a large extent on the NP argument selected by the preposition with. In (1), the same surface form NP-V-PPwith corresponds to three different underlying meanings. However, such semantic distinctions are totally lost if lexical information is disregarded. (1) a. I ate with a fork. [INSTRUMENT] b. I left with a friend. [ACCOMPANIMENT] c. I sang with confidence. [MANNER] This deficiency of unlexicalized subcatego"
P08-1050,C02-1146,0,0.0160233,"and Stevenson, 2004; Gildea and Jurafsky, 2002), natural language generation for machine translation (Habash et al., 2003), and deriving predominant verb senses from unlabeled data (Lapata and Brew, 2004). Although there exist several manually-created verb lexicons or ontologies, including Levin’s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002). AVC helps avoid the expensive hand-coding of such information, but appropriate features must be identified and demonstrated to be effective. In this work, our primary goal is not necessarily to obtain the optimal classification, but rather to investigate 434 Proceedings of ACL-08: HLT, pages 434–442, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the linguistic conditions which are crucial for lexical semantic classification of verbs. We develop feature sets that combine syntactic and lexical information, which are in principle useful for any Levinstyle verb"
P08-1050,W04-3213,0,\N,Missing
P08-1050,W02-1016,1,\N,Missing
P08-1050,J02-3001,0,\N,Missing
P08-1050,C98-2122,0,\N,Missing
P09-1005,W03-1008,0,0.519121,"roles of different verbs. We follow the approach in (Punyakanok et al., 2008) in framing the SRL problem as a two-stage pipeline: identification followed by labeling. During identification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to th"
P09-1005,J99-2004,0,0.0360425,"onding syntactic argument of the verb. As an example, consider the sentence The boy loves a girl. (figure 4). By examining the arguments that the verbal category combines with in the treebank, we can identify the corresponding semantic role for each argument that is marked on the verbal category. We then use these tags to train the Argument Mapping model, which will predict likely argument mappings for verbal categories based on their local surroundings and the headwords of their arguments, similar to the supertagging approaches used to label the informative syntactic categories of the verbs (Bangalore and Joshi, 1999; Clark, 2002), except tagging “one level above” the syntax. The Argument Mapping Predictor uses the following features: (23) Predicate. The lemma of the predicate, as before. (24) Words. Words drawn from a 5 word window around the target word, with each word associated with a binary indicator feature, as before. (25) Parts of Speech. Part of Speech tags drawn from a 5 word window around the target word, with each tag associated with a binary indicator feature, as before. (26) CCG Categories. CCG categories drawn from a 5 word window around the target word, with each category associated with a"
P09-1005,J07-3004,0,0.0538892,"from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and linWe describe a semantic role labeling system that makes primary use of CCG-based features. Most previously developed systems are CFG-based and make extensive u"
P09-1005,J96-1002,0,0.00711511,"ures, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and linWe describe a semantic role labeling system that makes primary use of"
P09-1005,W08-2123,0,0.0429012,"tem employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued Two features which are less frequently used in SRL research play a major role in the Brutus system: The PARG feature (Gildea and Hocke"
P09-1005,boxwell-white-2008-projecting,1,0.939242,"n (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and linWe describe a semantic role labeling system that makes primary use of CCG-based features. Most previously developed systems are CFG-based and make extensive use of a treepath feature, which suffers from data sparsity due to its use of explicit tree configu"
P09-1005,J08-2001,0,0.0247271,"Missing"
P09-1005,W08-2101,0,0.0833011,"Missing"
P09-1005,P01-1017,0,0.349339,"ntification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach e"
P09-1005,W06-2303,0,0.161966,"Missing"
P09-1005,J07-4004,0,0.306886,"s a two-stage pipeline: identification followed by labeling. During identification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion"
P09-1005,W02-2203,0,0.0284853,"f the verb. As an example, consider the sentence The boy loves a girl. (figure 4). By examining the arguments that the verbal category combines with in the treebank, we can identify the corresponding semantic role for each argument that is marked on the verbal category. We then use these tags to train the Argument Mapping model, which will predict likely argument mappings for verbal categories based on their local surroundings and the headwords of their arguments, similar to the supertagging approaches used to label the informative syntactic categories of the verbs (Bangalore and Joshi, 1999; Clark, 2002), except tagging “one level above” the syntax. The Argument Mapping Predictor uses the following features: (23) Predicate. The lemma of the predicate, as before. (24) Words. Words drawn from a 5 word window around the target word, with each word associated with a binary indicator feature, as before. (25) Parts of Speech. Part of Speech tags drawn from a 5 word window around the target word, with each tag associated with a binary indicator feature, as before. (26) CCG Categories. CCG categories drawn from a 5 word window around the target word, with each category associated with a binary indica"
P09-1005,J05-1004,0,0.0675316,"of assigning semantic roles to strings of words in a sentence according to their relationship to the semantic predicates expressed in the sentence. The task is difficult because the relationship between syntactic relations like “subject” and “object” do not always correspond to semantic relations like “agent” and “patient”. An effective semantic role labeling system must recognize the differences between different configurations: (a) [The man]Arg0 opened [the door]Arg1 [for him]Arg3 [today]ArgM −T M P . (b) [The door]Arg1 opened. (c) [The door]Arg1 was opened by [a man]Arg0 . We use Propbank (Palmer et al., 2005), a corpus of newswire text annotated with verb predicate semantic role information that is widely used in the SRL literature (M`arquez et al., 2008). Rather than describe semantic roles in terms of “agent” or “patient”, Propbank defines semantic roles on a verb-by-verb basis. For example, the verb open encodes the O PENER as Arg0, the O PENEE as Arg1, and the beneficiary of the O PENING action as Arg3. Propbank also defines a set of adjunct 1 This is guaranteed by our string-to-string mapping from the original Propbank to the CCGbank. 37 Proceedings of the 47th Annual Meeting of the ACL and t"
P09-1005,J08-2002,0,0.049901,"ber of factors, including the fact that our system employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued Two features which are less frequently used in SRL research play a major role in t"
P09-1005,J04-4004,0,\N,Missing
P09-1005,J08-2005,0,\N,Missing
P09-1005,J03-4003,0,\N,Missing
P94-1003,P90-1021,0,0.246776,"e case. Our solution to this problem is to dispense with the MSCD operation and to use generalization instead. However, we do propose that generalization should take inputs whose parallelism dependent anaphors have already been resolved. 1 In the case of the combination of (5a) and (5d), this will give 1As described in the next section, we use priority union to resolve these anaphors in both lists and contrasts. The use of generalization as a step towards checking that there is sufficient common ground is subsequent to the use of priority ration as the resolution mechanism. ~See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). 19 default structure, hence our preference to refer to it by the n a m e priority union. Below we demonstrate the results of priority union for the examples in ( l a ) - ( l c ) . Note t h a t the target is the strict structure and the source is the defeasible one. (11) Hannah likes beetles. So does Thomas. Source: Target: (12) 5a 5b (15) Jessy likes her Priority[ AGENT th°mas ] Union: PATIENT beetle like Hannah likes beetles. She alsolikes caterpillars. Source: Target: Priority Union: (13) The situations where the credulous version of the oper"
P94-1003,P93-1009,0,0.0722164,"eetle) = like(hannah, beetle) b. P(like) = like(hannah, beetle) Source: Target: Equation: Sol.1 ($1): Sol.2 (S2): Apply SI: Apply $2: like(jessy, brother-of (jessy) ) P( hannah ) P(jessy) = like(jessy, brother-of (jessy) ) P = ~x.like(x, brother-of(jessy)) e = Ax.like(x, brother-of(x)) like(hannah, brother-of (jessy) ) like(hannah, brother-of(hannah)) Parallelism In the DSP approach to vP-ellipsis and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useful discussion of the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like contrast but it is not sufficient for building a coherent list. DSP claim that a significant attribute of their account is that they can provide the two readings in strict/sloppy ambiguities without having to postulate ambiguity in the source. They claim this as a virtue which is matched by few other"
P94-1003,P84-1085,0,0.0111088,"parallelism-dependent anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anap"
P94-1003,C88-2120,0,0.0211539,"anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretati"
S13-2045,S12-1059,0,0.00894071,"Missing"
S13-2045,P10-4003,1,0.705578,"in, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and c"
S13-2045,N12-1021,1,0.552195,"ing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions. 1 Introduction One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could 263 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 263–274, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Example 1 Q UESTION R EF. A NS . S TUD . A NS . Example 2 Q UESTION R EF. A NS . S TUD . A NS . You used several methods to separate and identify the"
S13-2045,P11-1076,0,0.189172,"ar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way stud"
S13-2045,nielsen-etal-2008-annotating,1,0.393893,"@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the"
S13-2045,W05-0202,0,0.185532,"Clark Vulcan Inc. USA peterc@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. T"
S13-2045,R11-1063,1,0.693982,"ighted Average Precision, Recall and F1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as ‘Unaddressed’. Its performance is presented in Section 5.4 jointly with the system results. 5.4 Participants and results Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures (B¨ar et al., 2012), and (ii) recognizing textual entailment with BIUTEE (Stern and Dagan, 2011). The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with S CI E NTS BANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table 7 shows Weighted Average and Macro Average F1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performanc"
S13-2045,C00-2137,0,0.0467226,"m one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs.3 In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test (Yeh, 2000) with 10,000 iterations; p ≤ 0.05 was considered statistically significant. 4.4 Dataset: B EETLE 5way Run UA UQ CELI1 0.315 0.300 CNGL2 0.431 0.382 CoMeT1 0.569 0.300 EHUALM2 0.526 0.3703 ETS1 0.444 0.461 ETS2 0.619 0.552 LIMSIILES1 0.327 0.280 SoftCardinality1 0.455 0.436 UKP-BIU1 0.423 0.285 Median 0.444 0.370 Baselines: Lexical 0.424 0.414 Majority 0.114 0.118 Five-way Task The results for the five-way task are shown in Tables 2 and 3. Comparison to baselines All of the systems performed substantially better than the majority class baseline (“correct” for both B EETLE and S CI E NTS BANK),"
S13-2045,W07-1401,1,\N,Missing
S18-1145,J92-4003,0,0.0735357,"Missing"
S18-1145,D15-1162,0,0.0143135,"subtask 2. The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task. The methods DO chose do not attempt to make use of this linkage, which may be a missed opportunity. This motivates a post-hoc error analysis. It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences. 1 2 Digital Operatives Systems The Digital Operatives submission used spaCy (Honnibal and Johnson, 2015). to generate features for each token, then aggregated the features from the whole sentence. To estimate performance, we used 5-fold cross-validation on the combined training and development sets. As an example, consider the word ”ago” in the sentence: ”A few days ago we detected a watering hole campaign in a website owned by one big industrial company .” We extract: Introduction The SecureNLP challenge is motivated by (Lim et al., 2017) and further described in (Phandi et al., 2018), it aims to provide automation for malware analysts who might otherwise be overwhelmed by the task of finding k"
S18-1145,S18-1113,0,0.0355728,"Missing"
S18-1145,D08-1027,0,0.113444,"Missing"
W02-1016,W98-1505,0,0.0225794,"Missing"
W02-1016,1997.mtsummit-workshop.2,0,0.0624191,"Missing"
W02-1016,C96-1055,0,0.0164073,"Missing"
W02-1016,W99-0632,1,0.900684,"Missing"
W02-1016,P99-1051,0,0.154179,"Missing"
W02-1016,A00-2034,0,0.0219475,"Missing"
W02-1016,J01-3003,0,0.151964,"Missing"
W02-1016,P02-1029,1,0.742129,"Missing"
W02-1016,C00-2108,1,0.923673,"Missing"
W02-1016,schulte-im-walde-2002-subcategorisation,1,0.881087,"Missing"
W04-3229,A00-1031,0,0.957179,"demanding tagging task. Because no large annotated corpora of Russian are available to us, we instead chose to use an annotated corpus of Czech. Czech is sufficiently similar to Russian that it is reasonable to suppose that information about Czech will be relevant in some way to the tagging of Russian. The languages share many linguistic properties (free word order and a rich morphology which plays a considerable role in determining agreement and argument relationships). We created a morphological analyzer for Russian, combined the results with information derived from Czech and used the TnT (Brants, 2000) tagger in a number of differ1 All Russian examples in this paper are transcribed in the Roman alphabet. Our system is able to analyze Russian texts in both Cyrillic and various transcriptions. beautiful (short adjective, feminine) husband (noun, masc., sing., genitive) husband (noun, masc., sing., accusative) window (noun, neuter, sing., genitive) window (noun, neuter, pl., nominative) window (noun, neuter, pl., accusative) book (noun, fem., sing., nominative) house (noun, masc., sing., genitive) house (noun, masc., pl., nominative) house (noun, masc., pl., accusative) say (verb, fem., sing.,"
W04-3229,J01-2001,0,0.0148259,"is very elaborate and specifically devised to serve multiple needs, while our tagset is designed solely to capture the core of Russian morphology, as we need it for our primary purpose of demonstrating the portability and feasibility of our technique. Still, our tagset is much larger than the Penn Treebank tagset, which uses only 36 non-punctuation tags (Marcus et al., 1993). 4.2 Morphological analysis In this section we describe our approach to a resource-light encoding of salient facts about the Russian lexicon. Our techniques are not as radical as previously explored unsupervised methods (Goldsmith, 2001; Yarowsky and Wicentowski, 2000), but are designed to be feasible for languages for which serious morphological expertise is unavailable to us. We use a paradigm-based morphology that avoids the need to explicitly create a large lexicon. The price that we pay for this is overgeneration. Most of these analyses look very implausible to a Russian speaker, but significantly increasing the precision would be at the cost of greater development time than our resource-light approach is able to commit. We wish our work to be portable at least to other Slavic languages, for which we assume that elabora"
W04-3229,P01-1035,0,0.017007,"Missing"
W04-3229,A00-2013,0,0.0386823,"Missing"
W04-3229,N01-1014,0,0.0239687,"he Czech to the Russian emission probabilities. Just as a knowledge of English words is sometimes helpful (modulo sound changes) when reading German, a knowledge of the Czech lexicon should be helpful (modulo character set issues) when reading Russian. We are seeking the right way to operationalize this intuition in our system, bearing in mind that we want a sufficiently general algorithm to make the method portable to other languages, for which we assume we have neither the time nor the expertise to undertake knowledge-intensive work. A potentially suitable cognate algorithm is described by (Kondrak, 2001). Finally, we would like to extend our work to Slavic languages for which there are even fewer available resources than Russian, such as Belarusian, since this was the original motivation for undertaking the work in the first place. Acknowledgements We thank Erhard Hinrichs and Eric Fosler-Lussier for giving us feedback on previous versions of the paper and providing useful suggestions for subtaggers and voting; Jan Hajiˇc for the help with the Czech tag system and the morphological analyzer; to the Clippers discussion group for allowing us to interview ourselves in front of them, and for ensu"
W04-3229,J93-2004,0,0.0224659,"Russian morphological categories usually have fewer values (e.g., 6 cases in Russian vs. 7 in Czech; Czech often has formal and colloquial variants of the same morpheme); but there is also an immediate practical reason – the Czech tag system is very elaborate and specifically devised to serve multiple needs, while our tagset is designed solely to capture the core of Russian morphology, as we need it for our primary purpose of demonstrating the portability and feasibility of our technique. Still, our tagset is much larger than the Penn Treebank tagset, which uses only 36 non-punctuation tags (Marcus et al., 1993). 4.2 Morphological analysis In this section we describe our approach to a resource-light encoding of salient facts about the Russian lexicon. Our techniques are not as radical as previously explored unsupervised methods (Goldsmith, 2001; Yarowsky and Wicentowski, 2000), but are designed to be feasible for languages for which serious morphological expertise is unavailable to us. We use a paradigm-based morphology that avoids the need to explicitly create a large lexicon. The price that we pay for this is overgeneration. Most of these analyses look very implausible to a Russian speaker, but sig"
W04-3229,P00-1027,0,0.024883,"e and specifically devised to serve multiple needs, while our tagset is designed solely to capture the core of Russian morphology, as we need it for our primary purpose of demonstrating the portability and feasibility of our technique. Still, our tagset is much larger than the Penn Treebank tagset, which uses only 36 non-punctuation tags (Marcus et al., 1993). 4.2 Morphological analysis In this section we describe our approach to a resource-light encoding of salient facts about the Russian lexicon. Our techniques are not as radical as previously explored unsupervised methods (Goldsmith, 2001; Yarowsky and Wicentowski, 2000), but are designed to be feasible for languages for which serious morphological expertise is unavailable to us. We use a paradigm-based morphology that avoids the need to explicitly create a large lexicon. The price that we pay for this is overgeneration. Most of these analyses look very implausible to a Russian speaker, but significantly increasing the precision would be at the cost of greater development time than our resource-light approach is able to commit. We wish our work to be portable at least to other Slavic languages, for which we assume that elaborate morphological analyzers will n"
W04-3229,dzeroski-etal-2000-morphosyntactic,0,\N,Missing
W05-0103,W02-0105,0,0.062929,"sification tasks, such as language identification) • Writers’ aids (Spelling and grammar correction) • Machine translation (2 weeks) • Dialogue systems (2 weeks) • Computer-aided language learning • Social context of language technology use In contrast to the courses of which we are aware that offer computational linguistics to undergraduates, our Language and Computers is supposed to be accessible without prerequisites to students from every major (a requirement for GEC courses). For example, we cannot assume any linguistic background or language awareness. Like Lillian Lee’s Cornell course (Lee, 2002), the course cannot presume programming 17 • Reasoning about finite-state automata and regular expressions (in the contexts of web searching and of information management). Students reason about relationships between specific and general search terms. • Reasoning about more elaborate syntactic representations (such as context-free grammars) and semantic representations (such as predicate calculus), in order to better understand grammar checking and machine translation errors. • Reasoning about the interaction between components of natural language systems (in the contexts of machine translatio"
W05-1529,A00-2018,0,\N,Missing
W05-1529,J04-4004,0,\N,Missing
W05-1529,C00-2100,0,\N,Missing
W05-1529,C94-1042,0,\N,Missing
W05-1529,A97-1052,0,\N,Missing
W05-1529,J93-2002,0,\N,Missing
W05-1529,P98-2184,0,\N,Missing
W05-1529,C98-2179,0,\N,Missing
W06-2005,N01-1020,0,0.237405,"Missing"
W06-2005,J94-2001,0,0.0983344,"xisting monolingual text corpus in the language, but they also use a medium-sized bilingual dictionary. In our work, we use a paradigm-based morphology, including only the basic paradigms from a standard grammar textbook. Cucerzan and Yarowsky (2002) create a dictionary of regular inflectional affix changes and their associated POS and on the basis of it, generate hypothesized inflected forms following the regular paradigms. Related work Previous research in resource-light language learning has defined resource-light in different ways. Some have assumed only partially tagged training corpora (Merialdo, 1994); some have begun with small tagged seed wordlists (Cucerzan and Yarowsky, 1999) for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in a 38 languages are often close enough to others within their language family so that cognate pairs between the two are common, and significant portions of the translation lexicon can be induced with high accuracy where no bilingual dictionary or parallel corpora may exist. Clearly, these hypothesized forms are inaccurate and overgenerated. Therefore, the authors perform a probabilistic match fr"
W06-2005,A00-1031,0,0.297719,"Missing"
W06-2005,J97-3003,0,0.0701552,"ork. Raw Portuguese corpus. For automatic lexicon acquisition, we use NILC corpus,3 containing 1.2M tokens. 3.3 Portuguese closed class words 4.3 Lexicon Acquisition The morphological analyzer supports a module or modules employing a lexicon containing information about lemmas, stems and paradigms. There is always the possibility to provide this information manually. That, however, is very costly. Instead, we created such a lexicon automatically. Usually, automatically acquired lexicons and similar systems are used as a backup for large high-precision high-cost manually created lexicons (e.g. Mikheev, 1997; Hlav´acˇ ov´a, 2001). Such systems extrapolate the information about the words known by the lexicon (e.g. distributional properties of endings) to unknown words. Since our approach is resource light, we do not have any such large lexicon to extrapolate from. The general idea of our system is very simple. The paradigm-based Guesser, provides all the possible analyses of a word consistent with Portuguese paradigms. Obviously, this approach masMorphological Analysis Our morphological analyzer (Hana, 2005) is an open and modular system. It allows us to combine modules with different levels of ma"
W06-2005,E03-1038,0,0.198466,"Missing"
W06-2005,P00-1016,0,0.0125086,"olumn e-cognates of Table 4. p0p (t) = 6 e-even e-cognates Tag: 56.9 77.2 82.1 POS: SubPOS: gender: number: case: possessor’s num: form: person: tense: mood: participle: 65.3 61.7 70.4 78.3 93.8 85.4 92.9 74.5 90.7 91.5 99.9 84.2 83.3 87.3 95.3 96.8 96.7 99.2 91.2 95.1 95.0 100.0 87.6 86.9 90.2 96.0 97.2 97.0 99.2 92.7 96.1 96.0 100.0 Table 4: Tagging Brazilian Portuguese different genres or a different language (e.g. crosslanguage projection of morphological and syntactic information in (Yarowsky et al., 2001; Yarowsky and Ngai, 2001), requiring no direct supervision in the target language). Ngai and Yarowsky (2000) observe that the total weighted human and resource costs is the most practical measure of the degree of supervision. Cucerzan and Yarowsky (2002) observe that another useful measure of minimal supervision is the additional cost of obtaining a desired functionality from existing commonly available knowledge sources. They note that for a remarkably wide range of languages, there exist a plenty of reference grammar books and dictionaries which is an invaluable linguistic resource. Evaluation & Comparison The best way to evaluate our results would be to compare it against the TnT tagger used the"
W06-2005,W99-0612,0,0.0279067,"a medium-sized bilingual dictionary. In our work, we use a paradigm-based morphology, including only the basic paradigms from a standard grammar textbook. Cucerzan and Yarowsky (2002) create a dictionary of regular inflectional affix changes and their associated POS and on the basis of it, generate hypothesized inflected forms following the regular paradigms. Related work Previous research in resource-light language learning has defined resource-light in different ways. Some have assumed only partially tagged training corpora (Merialdo, 1994); some have begun with small tagged seed wordlists (Cucerzan and Yarowsky, 1999) for named-entity tagging, while others have exploited the automatic transfer of an already existing annotated resource in a 38 languages are often close enough to others within their language family so that cognate pairs between the two are common, and significant portions of the translation lexicon can be induced with high accuracy where no bilingual dictionary or parallel corpora may exist. Clearly, these hypothesized forms are inaccurate and overgenerated. Therefore, the authors perform a probabilistic match from all lexical tokens actually observed in a monolingual corpus and the hypothes"
W06-2005,W02-2006,0,0.0190716,"n: tense: mood: participle: 65.3 61.7 70.4 78.3 93.8 85.4 92.9 74.5 90.7 91.5 99.9 84.2 83.3 87.3 95.3 96.8 96.7 99.2 91.2 95.1 95.0 100.0 87.6 86.9 90.2 96.0 97.2 97.0 99.2 92.7 96.1 96.0 100.0 Table 4: Tagging Brazilian Portuguese different genres or a different language (e.g. crosslanguage projection of morphological and syntactic information in (Yarowsky et al., 2001; Yarowsky and Ngai, 2001), requiring no direct supervision in the target language). Ngai and Yarowsky (2000) observe that the total weighted human and resource costs is the most practical measure of the degree of supervision. Cucerzan and Yarowsky (2002) observe that another useful measure of minimal supervision is the additional cost of obtaining a desired functionality from existing commonly available knowledge sources. They note that for a remarkably wide range of languages, there exist a plenty of reference grammar books and dictionaries which is an invaluable linguistic resource. Evaluation & Comparison The best way to evaluate our results would be to compare it against the TnT tagger used the usual way – trained on Portuguese and applied on Portuguese. We do not have access to a large Portuguese corpus annotated with detailed tags. Howe"
W06-2005,N01-1026,0,0.0655786,"Missing"
W06-2005,H01-1035,0,0.0663962,"Missing"
W06-2005,W04-3229,1,0.922048,"m is not tied to these particular languages. The method is easily portable to other (inflected) languages. Our method assumes that an annotated corpus exists for the source language (here, Spanish) and that a text book with basic linguistic facts about the source language is available (here, Portuguese). We want to test the generality and specificity of the method. Can the systematic commonalities and differences between two genetically related languages be exploited for crosslanguage applications? Is the processing of Portuguese via Spanish different from the processing of Russian via Czech (Hana et al., 2004; Feldman et al., 2006)? Introduction Part of speech (POS) tagging is an important step in natural language processing. Corpora that have been POS-tagged are very useful both for linguistic research, e.g. finding instances or frequencies of particular constructions (Meurers, 2004) and for further computational processing, such as syntactic parsing, speech recognition, stemming, wordsense disambiguation. Morphological tagging is the process of assigning POS, case, number, gender and other morphological information to each word in a corpus. Despite the importance of morphological tagging, there"
W06-2005,P00-1027,0,0.125732,"Missing"
W07-1013,N06-1009,0,0.0209183,"Missing"
W09-3304,J06-1003,0,0.0316139,"n had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Both Wikipedia and Wiktionary are organized around a basic “page” unit, containing information about an individual word, phrase or entity in the world – definitions, thesaurus en"
W99-0632,A97-1052,0,0.387206,"Missing"
W99-0632,W98-1505,0,0.207588,"the most frequent (e.g., Smoking can impair the blood which (e.g., Thameslink presently carr/es 20,000 passengers daily) is larger than the CARRY class, it will be given a higher probability (0.45 versus 0.4). This is clearly wrong, but it is an empirical question how much it matters. Finally, we wanted to estimate the probability of a given frame, P(frame). We could have done this by acquiring Levin compatible subcategorization frames from the BNC. Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). But the present study was less ambitious, and narrowly focused on the frames representing the dative and the benefactive alternation. In default of the more ambitious study, which we plan for the future, the estimation of P(frame) was carried out on types and not on tokens. The mapping of Levin's linguistic specifications into surface syntactic information resulted in 79 different frame types. By counting the number of times a given frame is licensed by several semantic classes we get a distribution of frames, a sample of which is shown in figure 3. The probabilities P(frmnelclass) and P(fra"
W99-0632,P98-1046,0,0.0579833,"Missing"
W99-0632,J93-1005,0,0.0864499,"(framelclass) P (class) P (frame) 268 It is easy to obtain f(verb) from the lemmatized BNC. For the experiments reported here, syntactic frames for the dative and benefactive alternations were automatically extracted from the BNC using Gsearch (Keller et al., 1999), a tool which facilitates search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified contextfree grammar and a syntactic query. The acquisition and filtering process is detailed in Lapata (1999). We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information nearly as useful as that provided by Fidditch. We estimated f(verb,frame) as the number of times a verb co-occurred with a particular frame in the corpus. We cannot read off P(frame[class) from the corpus, because it is not annotated with verb classes. Nevertheless we can use the information listed in Levin with respect to the syntactic frames exhibited by the verbs of a given class. For each class C"
W99-0632,P99-1051,1,0.687511,"t making reference to the individual verbs. By applying Bayes Law we write P(classlframe) as: (8) P(class[frame)= P (framelclass) P (class) P (frame) 268 It is easy to obtain f(verb) from the lemmatized BNC. For the experiments reported here, syntactic frames for the dative and benefactive alternations were automatically extracted from the BNC using Gsearch (Keller et al., 1999), a tool which facilitates search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified contextfree grammar and a syntactic query. The acquisition and filtering process is detailed in Lapata (1999). We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information nearly as useful as that provided by Fidditch. We estimated f(verb,frame) as the number of times a verb co-occurred with a particular frame in the corpus. We cannot read off P(frame[class) from the corpus, because it is not annotated with verb classes. Nevertheless we can use"
W99-0632,P98-2177,0,0.0131453,"ted here, syntactic frames for the dative and benefactive alternations were automatically extracted from the BNC using Gsearch (Keller et al., 1999), a tool which facilitates search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified contextfree grammar and a syntactic query. The acquisition and filtering process is detailed in Lapata (1999). We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information nearly as useful as that provided by Fidditch. We estimated f(verb,frame) as the number of times a verb co-occurred with a particular frame in the corpus. We cannot read off P(frame[class) from the corpus, because it is not annotated with verb classes. Nevertheless we can use the information listed in Levin with respect to the syntactic frames exhibited by the verbs of a given class. For each class Class Frames Class size(class) p(class[amb_class) f (verb, class) THROW 27 0.40 7783.6 SEND 20 0.27 5253.9 GIVE 1"
W99-0632,C98-1046,0,\N,Missing
W99-0632,P93-1032,0,\N,Missing
Y98-1021,J95-4004,0,0.268351,"Missing"
Y98-1021,P97-1041,0,0.213475,"nt by the system is the final output of the learning process' . - We now have a sequence of transformations which can be applied any text which has been passed through the initial state annotator. This scheme is sufficiently general to apply to many tasks, including Chinese word segmentation, although each task will require the choice of a suitable set of rule templates, which serve to define the space of transformations to be searched by the algorithm. Performance will hinge crucially on the choice of appropriate templates. 3 Transformation-Based Learning applied to word segmentation Palmer ([4]) reports four experiments on Chinese word segmentation. Adopting Brill's scheme, he used four distinct initial-state annotators: a naive character-as-word segmentation - an initial segmentation obtained by maximum-matching - an initial segmentation by maximum matching where unknown character sequences were treated by a character-as-word segmentation. - the output of a high-accuracy word segmenter For all experiments, Palmer used a set of rule templates with three types of actions: - concatenate two characters. - spl i t two characters - s 1 i de a word boundary to the adjacent character. Rule"
Y98-1021,J96-3004,0,0.254686,"Missing"
