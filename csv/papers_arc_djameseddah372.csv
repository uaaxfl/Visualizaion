2021.wnut-1.22,Understanding the Impact of {UGC} Specificities on Translation Quality,2021,-1,-1,2,1,166,jose nunez,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT. Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality. That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology. Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible."
2021.wnut-1.23,Noisy {UGC} Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models,2021,-1,-1,3,1,166,jose nunez,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"This work explores the capacities of character-based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero-shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation."
2021.wnut-1.47,Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?,2021,-1,-1,3,0,249,arij riabi,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings."
2021.naacl-main.38,When Being Unseen from m{BERT} is just the Beginning: Handling New Languages With Multilingual Language Models,2021,-1,-1,4,1,3345,benjamin muller,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages."
2021.iwpt-1.15,From Raw Text to Enhanced {U}niversal {D}ependencies: The Parsing Shared Task at {IWPT} 2021,2021,-1,-1,2,0,5827,gosse bouma,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"We describe the second IWPT task on end-to-end parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task."
2021.insights-1.19,Challenging the Semi-Supervised {VAE} Framework for Text Classification,2021,-1,-1,3,0,5901,ghazi felhi,Proceedings of the Second Workshop on Insights from Negative Results in NLP,0,"Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for data efficient learning. In this paper, we question the adequacy of the standard design of sequence SSVAEs for the task of text classification as we exhibit two sources of overcomplexity for which we provide simplifications. These simplifications to SSVAEs preserve their theoretical soundness while providing a number of practical advantages in the semi-supervised setup where the result of training is a text classifier. These simplifications are the removal of (i) the Kullback-Liebler divergence from its objective and (ii) the fully unobserved latent variable from its probabilistic model. These changes relieve users from choosing a prior for their latent variables, make the model smaller and faster, and allow for a better flow of information into the latent variables. We compare the simplified versions to standard SSVAEs on 4 text classification tasks. On top of the above-mentioned simplification, experiments show a speed-up of 26{\%}, while keeping equivalent classification scores. The code to reproduce our experiments is public."
2021.emnlp-main.562,Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering,2021,-1,-1,5,0,249,arij riabi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr)."
2021.eacl-main.189,"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual {BERT}",2021,-1,-1,4,1,3345,benjamin muller,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model{'}s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis."
2020.lrec-1.645,Treebanking User-Generated Content: A Proposal for a Unified Representation in {U}niversal {D}ependencies,2020,-1,-1,9,0,16433,manuela sanguinetti,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The paper presents a discussion on the main linguistic phenomena of user-generated texts found in web and social media, and proposes a set of annotation guidelines for their treatment within the Universal Dependencies (UD) framework. Given on the one hand the increasing number of treebanks featuring user-generated content, and its somewhat inconsistent treatment in these resources on the other, the aim of this paper is twofold: (1) to provide a short, though comprehensive, overview of such treebanks - based on available literature - along with their main features and a comparative analysis of their annotation criteria, and (2) to propose a set of tentative UD-based annotation guidelines, to promote consistent treatment of the particular phenomena found in these types of texts. The main goal of this paper is to provide a common framework for those teams interested in developing similar resources in UD, thus enabling cross-linguistic consistency, which is a principle that has always been in the spirit of UD."
2020.jeptalnrecital-taln.5,Les mod{\\`e}les de langue contextuels Camembert pour le fran{\\c{c}}ais : impact de la taille et de l{'}h{\\'e}t{\\'e}rog{\\'e}n{\\'e}it{\\'e} des donn{\\'e}es d{'}entrainement ({C} {AMEM} {BERT} Contextual Language Models for {F}rench: Impact of Training Data Size and Heterogeneity ),2020,-1,-1,8,0,17824,louis martin,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue neuronaux contextuels sont d{\'e}sormais omnipr{\'e}sents en traitement automatique des langues. Jusqu{'}{\`a} r{\'e}cemment, la plupart des mod{\`e}les disponibles ont {\'e}t{\'e} entra{\^\i}n{\'e}s soit sur des donn{\'e}es en anglais, soit sur la concat{\'e}nation de donn{\'e}es dans plusieurs langues. L{'}utilisation pratique de ces mod{\`e}les {---} dans toutes les langues sauf l{'}anglais {---} {\'e}tait donc limit{\'e}e. La sortie r{\'e}cente de plusieurs mod{\`e}les monolingues fond{\'e}s sur BERT (Devlin et al., 2019), notamment pour le fran{\c{c}}ais, a d{\'e}montr{\'e} l{'}int{\'e}r{\^e}t de ces mod{\`e}les en am{\'e}liorant l{'}{\'e}tat de l{'}art pour toutes les t{\^a}ches {\'e}valu{\'e}es. Dans cet article, {\`a} partir d{'}exp{\'e}riences men{\'e}es sur CamemBERT (Martin et al., 2019), nous montrons que l{'}utilisation de donn{\'e}es {\`a} haute variabilit{\'e} est pr{\'e}f{\'e}rable {\`a} des donn{\'e}es plus uniformes. De fa{\c{c}}on plus surprenante, nous montrons que l{'}utilisation d{'}un ensemble relativement petit de donn{\'e}es issues du web (4Go) donne des r{\'e}sultats aussi bons que ceux obtenus {\`a} partir d{'}ensembles de donn{\'e}es plus grands de deux ordres de grandeurs (138Go)."
2020.iwpt-1.16,Overview of the {IWPT} 2020 Shared Task on Parsing into Enhanced {U}niversal {D}ependencies,2020,-1,-1,2,0,5827,gosse bouma,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"This overview introduces the task of parsing into enhanced universal dependencies, describes the datasets used for training and evaluation, and evaluation metrics. We outline various approaches and discuss the results of the shared task."
2020.acl-main.51,"Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora",2020,-1,-1,3,0,3399,hila gonen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew)."
2020.acl-main.107,Building a User-Generated Content {N}orth-{A}frican {A}rabizi Treebank: Tackling Hell,2020,-1,-1,1,1,167,djame seddah,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches."
2020.acl-main.645,{C}amem{BERT}: a Tasty {F}rench Language Model,2020,-1,-1,7,0,17824,louis martin,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."
W19-6101,Comparison between {NMT} and {PBSMT} Performance for Translating Noisy User-Generated Content,2019,0,0,2,1,166,jose nunez,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"This work compares the performances achieved by Phrase-Based Statistical Machine Translation systems (PB-SMT) and attention-based Neuronal Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models."
W19-4705,Contextualized Diachronic Word Representations,2019,0,0,2,1,12015,ganesh jawahar,Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,0,"Diachronic word embeddings play a key role in capturing interesting patterns about how language evolves over time. Most of the existing work focuses on studying corpora spanning across several decades, which is understandably still not a possibility when working on social media-based user-generated content. In this work, we address the problem of studying semantic changes in a large Twitter corpus collected over five years, a much shorter period than what is usually the norm in diachronic studies. We devise a novel attentional model, based on Bernoulli word embeddings, that are conditioned on contextual extra-linguistic (social) features such as network, spatial and socio-economic variables, which are associated with Twitter users, as well as topic-based features. We posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem. Our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods."
P19-1356,What Does {BERT} Learn about the Structure of Language?,2019,0,47,3,1,12015,ganesh jawahar,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures."
D19-5539,Enhancing {BERT} for Lexical Normalization,2019,0,0,3,1,3345,benjamin muller,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Language model-based pre-trained representations have become ubiquitous in natural language processing. They have been shown to significantly improve the performance of neural models on a great variety of tasks. However, it remains unclear how useful those general models can be in handling non-canonical text. In this article, focusing on User Generated Content (UGC), we study the ability of BERT to perform lexical normalisation. Our contribution is simple: by framing lexical normalisation as a token prediction task, by enhancing its architecture and by carefully fine-tuning it, we show that BERT can be a competitive lexical normalisation model without the need of any UGC resources aside from 3,000 training sentences. To the best of our knowledge, it is the first work done in adapting and analysing the ability of this model to handle noisy UGC data."
D19-5553,Phonetic Normalization for Machine Translation of User Generated Content,2019,0,0,2,1,166,jose nunez,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"We present an approach to correct noisy User Generated Content (UGC) in French aiming to produce a pretreatement pipeline to improve Machine Translation for this kind of non-canonical corpora. In order to do so, we have implemented a character-based neural model phonetizer to produce IPA pronunciations of words. In this way, we intend to correct grammar, vocabulary and accentuation errors often present in noisy UGC corpora. Our method leverages on the fact that some errors are due to confusion induced by words with similar pronunciation which can be corrected using a phonetic look-up table to produce normalization candidates. These potential corrections are then encoded in a lattice and ranked using a language model to output the most probable corrected phrase. Compare to using other phonetizers, our method boosts a transformer-based machine translation system on UGC."
L18-1608,{C}o{NLL}-{UL}: Universal Morphological Lattices for {U}niversal {D}ependency Parsing,2018,0,2,6,0,25397,amir more,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Following the development of the universal dependencies (UD) framework and the CoNLL 2017 Shared Task on end-to-end UD parsing, we address the need for a universal representation of morphological analysis which on the one hand can capture a range of different alternative morphological analyses of surface tokens, and on the other hand is compatible with the segmentation and morphological annotation guidelines prescribed for UD treebanks. We propose the CoNLL universal lattices (CoNLL-UL) format, a new annotation format for word lattices that represent morphological analyses, and provide resources that obey this format for a range of typologically different languages. The resources we provide are harmonized with the two-level representation and morphological annotation in their respective UD v2 treebanks, thus enabling research on universal models for morphological and syntactic parsing , in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages."
L18-1718,Cheating a Parser to Death: Data-driven Cross-Treebank Annotation Transfer,2018,0,0,1,1,167,djame seddah,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present an efficient and accurate method for transferring annotations between two different treebanks of the same language. This method led to the creation of a new instance of the French Treebank (Abeille et al., 2003), which follows the Universal Dependency annotation scheme and which was proposed to the participants of the CoNLL 2017 Universal Dependency parsing shared task (Zeman et al., 2017). Strong results from an evaluation on our gold standard (94.75% of LAS, 99.40% UAS on the test set) demonstrate the quality of this new annotated data set and validate our approach."
K18-2023,{ELM}o{L}ex: Connecting {ELM}o and Lexicon Features for Dependency Parsing,2018,0,1,7,1,12015,ganesh jawahar,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"In this paper, we present the details of the neural dependency parser and the neural tagger submitted by our team {`}ParisNLP{'} to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies. We augment the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features to perform competitively: we utilize an indomain version of ELMo features (Peters et al., 2018) which provide context-dependent word representations; we utilize disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018), which complements the existing feature set. Henceforth, we call our system {`}ELMoLex{'}. In addition to incorporating character embeddings, ELMoLex benefits from pre-trained word vectors, ELMo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology. ELMoLex ranked 11th by Labeled Attachment Score metric (70.64{\%}), Morphology-aware LAS metric (55.74{\%}) and ranked 9th by Bilexical dependency metric (60.70{\%})."
W17-6507,Enhanced {UD} Dependencies with Neutralized Diathesis Alternation,2017,19,3,4,0.510062,16504,marie candito,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,"The 2.0 release of the Universal Dependency treebanks demonstrates the effectiveness of the UD scheme to cope with very diverse languages. The next step would be to get more of syntactic analysis , and the  enhanced dependencies  sketched in the UD 2.0 guidelines is a promising attempt in that direction. In this work we propose to go further and enrich the enhanced dependency scheme along two axis: extending the cases of recovered arguments of non-finite verbs, and neutralizing syntactic alternations. Doing so leads to both richer and more uniform structures, while remaining at the syntactic level, and thus rather neutral with respect to the type of semantic representation that can be further obtained. We implemented this proposal in two UD treebanks of French, using deterministic graph-rewriting rules. Evaluation on a 200 sentence gold standard shows that deep syntactic graphs can be obtained from surface syntax annotations with a high accuracy. Among all arguments of verbs in the gold standard, 13.91% are impacted by syntactic alternation normalization, and 18.93% are additional deep edges."
K17-3026,The {P}aris{NLP} entry at the {C}on{LL} {UD} Shared Task 2017: A Tale of a {\\#}{P}arsing{T}ragedy,2017,4,0,3,0,17825,eric clergerie,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present the ParisNLP entry at the UD CoNLL 2017 parsing shared task. In addition to the UDpipe models provided, we built our own data-driven tokenization models, sentence segmenter and lexicon-based morphological analyzers. All of these were used with a range of different parsing models (neural or not, feature-rich or not, transition or graph-based, etc.) and the best combination for each language was selected. Unfortunately, a glitch in the shared task{'}s Matrix led our model selector to run generic, weakly lexicalized models, tailored for surprise languages, instead of our dataset-specific models. Because of this {\#}ParsingTragedy, we officially ranked 27th, whereas our real models finally unofficially ranked 6th."
W16-3905,From Noisy Questions to {M}inecraft Texts: Annotation Challenges in Extreme Syntax Scenario,2016,24,0,2,0,20634,hector alonso,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"User-generated content presents many challenges for its automatic processing. While many of them do come from out-of-vocabulary effects, others spawn from different linguistic phenomena such as unusual syntax. In this work we present a French three-domain data set made up of question headlines from a cooking forum, game chat logs and associated forums from two popular online games (MINECRAFT {\&} LEAGUE OF LEGENDS). We chose these domains because they encompass different degrees of lexical and syntactic compliance with canonical language. We conduct an automatic and manual evaluation of the difficulties of processing these domains for part-of-speech prediction, and introduce a pilot study to determine whether dependency analysis lends itself well to annotate these data. We also discuss the development cost of our data set."
L16-1375,Hard Time Parsing Questions: Building a {Q}uestion{B}ank for {F}rench,2016,13,1,1,1,167,djame seddah,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present the French Question Bank, a treebank of 2600 questions. We show that classical parsing model performance drop while the inclusion of this data set is highly beneficial without harming the parsing of non-question data. when facing out-of- domain data with strong structural diver- gences. Two thirds being aligned with the QB (Judge et al., 2006) and being freely available, this treebank will prove useful to build robust NLP systems."
L16-1566,Accurate Deep Syntactic Parsing of Graphs: The Case of {F}rench,2016,0,0,3,1,30903,corentin ribeyre,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Parsing predicate-argument structures in a deep syntax framework requires graphs to be predicted. Argument structures represent a higher level of abstraction than the syntactic ones and are thus more difficult to predict even for highly accurate parsing models on surfacic syntax. In this paper we investigate deep syntax parsing, using a French data set (Ribeyre et al., 2014a). We demonstrate that the use of topologically different types of syntactic features, such as dependencies, tree fragments, spines or syntactic paths, brings a much needed context to the parser. Our higher-order parsing model, gaining thus up to 4 points, establishes the state of the art for parsing French deep syntactic structures."
N15-1007,Because Syntax Does Matter: Improving Predicate-Argument Structures Parsing with Syntactic Features,2015,44,2,3,1,30903,corentin ribeyre,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Parsing full-fledged predicate-argument structures in a deep syntax framework requires graphs to be predicted. Using the DeepBank (Flickinger et al., 2012) and the Predicate-Argument Structure treebank (Miyao and Tsujii, 2005) as a test field, we show how transition-based parsers, extended to handle connected graphs, benefit from the use of topologically different syntactic features such as dependencies, tree fragments, spines or syntactic paths, bringing a much needed context to the parsing models, improving notably over long distance dependencies and elided coordinate structures. By confirming this positive impact on an accurate 2nd-order graph-based parser (Martins and Almeida, 2014), we establish a new state-of-the-art on these data sets."
W14-6111,Introducing the {SPMRL} 2014 Shared Task on Parsing Morphologically-rich Languages,2014,31,43,1,1,167,djame seddah,Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,0,"This first joint meeting on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical English (SPMRL-SANCL) featured a shared task on statistical parsing of morphologically rich languages (SPMRL). The goal of the shared task is to allow to train and test different participating systems on comparable data sets, thus providing an objective measure of comparison between state-of-the-art parsing systems on data data sets from a range of different languages. This 2014 SPMRL shared task is a continuation and extension of the SPMRL shared task, which was co-located with the SPMRL meeting at EMNLP 2013 (Seddah et al., 2013). This paper provides a short overview of the 2014 SPMRL shared task goals, data sets, and evaluation setup. Since the SPMRL 2014 largely builds on the infrastructure established for the SPMRL 2013 shared task, we start by reviewing the previous shared task (xc2xa72) and then proceed to the 2014 SPMRL evaluation settings (xc2xa73), data sets (xc2xa74), and a task summary (xc2xa75). Due to organizational constraints, this overview is published prior to the submission of all system test runs, and a more detailed overview including the description of participating systems and the analysis of their results will follow as part of (Seddah et al., 2014), once the shared task is completed."
S14-2012,{A}lpage: Transition-based Semantic Graph Parsing with Syntactic Features,2014,24,4,3,1,30903,corentin ribeyre,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes the systems deployed by the ALPAGE team to participate to the SemEval-2014 Task on Broad-Coverage Semantic Dependency Parsing. We developed two transition-based dependency parsers with extended sets of actions to handle non-planar acyclic graphs. For the open track, we worked over two orthogonal axes xe2x80x90 lexical and syntactic xe2x80x90 in order to provide our models with lexical and syntactic features such as word clusters, lemmas and tree fragments of different types."
candito-etal-2014-deep,Deep Syntax Annotation of the Sequoia {F}rench Treebank,2014,27,7,6,0.748025,16504,marie candito,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We define a deep syntactic representation scheme for French, which abstracts away from surface syntactic variation and diathesis alternations, and describe the annotation of deep syntactic representations on top of the surface dependency trees of the Sequoia corpus. The resulting deep-annotated corpus, named deep-sequoia, is freely available, and hopefully useful for corpus linguistics studies and for training deep analyzers to prepare semantic analysis."
F14-2031,Annotation scheme for deep dependency syntax of {F}rench (Un sch{\\'e}ma d{'}annotation en d{\\'e}pendances syntaxiques profondes pour le fran{\\c{c}}ais) [in {F}rench],2014,0,0,6,0,5832,guy perrier,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
W13-4905,The {LIGM}-{A}lpage architecture for the {SPMRL} 2013 Shared Task: Multiword Expression Analysis and Dependency Parsing,2013,20,8,3,0,23668,matthieu constant,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper describes the LIGM-Alpage system for the SPMRL 2013 Shared Task. We only participated to the French part of the dependency parsing track, focusing on the realistic setting where the system is informed neither with gold tagging and morphology nor (more importantly) with gold grouping of tokens into multi-word expressions (MWEs). While the realistic scenario of predicting both MWEs and syntax has already been investigated for constituency parsing, the SPMRL 2013 shared task datasets offer the possibility to investigate it in the dependency framework. We obtain the best results for French, both for overall parsing and for MWE recognition, using a reparsing architecture that combines several parsers, with both pipeline architecture (MWE recognition followed by parsing), and joint architecture (MWE recognition performed by the parser)."
W13-4917,Overview of the {SPMRL} 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages,2013,110,38,1,1,167,djame seddah,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios."
J13-1003,Parsing Morphologically Rich Languages: Introduction to the Special Issue,2013,32,40,2,0.339575,5249,reut tsarfaty,Computational Linguistics,0,"Parsing is a key task in natural language processing. It involves predicting, for each natural language sentence, an abstract representation of the grammatical entities in the sentence and the relations between these entities. This representation provides an interface to compositional semantics and to the notions of who did what to whom. The last two decades have seen great advances in parsing English, leading to major leaps also in the performance of applications that use parsers as part of their backbone, such as systems for information extraction, sentiment analysis, text summarization, and machine translation. Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results. In particular, parsing languages with complex word structure and flexible word order has been shown to require non-trivial adaptation. This special issue reports on methods that successfully address the challenges involved in parsing a range of morphologically rich languages MRLs. This introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the contributions of the articles in the special issue. These contributions present up-to-date research efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs addresses challenges that transcend particular representational and algorithmic choices."
W12-4625,A linguistically-motivated 2-stage Tree to Graph Transformation,2012,-1,-1,2,1,30903,corentin ribeyre,Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+11),0,None
W12-3408,Statistical Parsing of {S}panish and Data Driven Lemmatization,2012,15,5,3,0,5824,joseph roux,Proceedings of the {ACL} 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,0,"Although parsing performances have greatly improved in the last years, grammar inference from treebanks for morphologically rich lan- guages, especially from small treebanks, is still a challenging task. In this paper we in- vestigate how state-of-the-art parsing perfor- mances can be achieved on Spanish, a lan- guage with a rich verbal morphology, with a non-lexicalized parser trained on a treebank containing only around 2,800 trees. We rely on accurate part-of-speech tagging and data- driven lemmatization in order to cope with lexical data sparseness. Providing state-of- the-art results on Spanish, our methodology is applicable to other languages."
seddah-etal-2012-ubiquitous,Ubiquitous Usage of a Broad Coverage {F}rench Corpus: Processing the {E}st {R}epublicain corpus,2012,0,1,1,1,167,djame seddah,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we introduce a set of resources that we have derived from the EST R{\'E}PUBLICAIN CORPUS, a large, freely-available collection of regional newspaper articles in French, totaling 150 million words. Our resources are the result of a full NLP treatment of the EST R{\'E}PUBLICAIN CORPUS: handling of multi-word expressions, lemmatization, part-of-speech tagging, and syntactic parsing. Processing of the corpus is carried out using statistical machine-learning approaches - joint model of data driven lemmatization and part- of-speech tagging, PCFG-LA and dependency based models for parsing - that have been shown to achieve state-of-the-art performance when evaluated on the French Treebank. Our derived resources are made freely available, and released according to the original Creative Common license for the EST R{\'E}PUBLICAIN CORPUS. We additionally provide an overview of the use of these resources in various applications, in particular the use of generated word clusters from the corpus to alleviate lexical data sparseness for statistical parsing."
F12-2024,Le corpus Sequoia : annotation syntaxique et exploitation pour l{'}adaptation d{'}analyseur par pont lexical (The Sequoia Corpus : Syntactic Annotation and Use for a Parser Lexical Domain Adaptation Method) [in {F}rench],2012,0,3,2,0.914973,16504,marie candito,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
C12-1149,The {F}rench {S}ocial {M}edia {B}ank: a Treebank of Noisy User Generated Content,2012,34,19,1,1,167,djame seddah,Proceedings of {COLING} 2012,0,"In recent years, statistical parsers have reached high performance levels on well-edited texts. Domain adaptation techniques have improved parsing results on text genres differing from the journalistic data most parsers are trained on. However, such corpora usually comply with standard linguistic, spelling and typographic conventions. In the meantime, the emergence of Web 2.0 communication media has caused the apparition of new types of online textual data. Although valuable, e.g., in terms of data mining and sentiment analysis, such user-generated content rarely complies with standard conventions: they are noisy. This prevents most NLP tools, especially treebank based parsers, from performing well on such data. For this reason, we have developed the French Social Media Bank, the first user-generated content treebank for French, a morphologically rich language (MRL). The first release of this resource contains 1,700 sentences from various Web 2.0 sources, including data specifically chosen for their high noisiness. We describe here how we created this treebank and expose the methodology we used for fully annotating it. We also provide baseline POS tagging and statistical constituency parsing results, which are lower by far than usual results on edited texts. This highlights the high difficulty of automatically processing such noisy data in a MRL."
W11-2905,A Word Clustering Approach to Domain Adaptation: Effective Parsing of Biomedical Texts,2011,22,20,3,0.914973,16504,marie candito,Proceedings of the 12th International Conference on Parsing Technologies,0,"We present a simple and effective way to perform out-of-domain statistical parsing by drastically reducing lexical data sparseness in a PCFG-LA architecture. We replace terminal symbols with unsupervised word clusters acquired from a large newspaper corpus augmented with biomedical targetdomain data. The resulting clusters are effective in bridging the lexical gap between source-domain and target-domain vocabularies. Our experiments combine known self-training techniques with unsupervised word clustering and produce promising results, achieving an error reduction of 21% on a new evaluation set for biomedical text with manual bracketing annotations."
W10-4413,"Control Verb, Argument Cluster Coordination and Multi Component {TAG}",2010,0,1,1,1,167,djame seddah,Proceedings of the 10th International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+10),0,None
W10-1401,"Statistical Parsing of Morphologically Rich Languages ({SPMRL}) What, How and Whither",2010,60,61,2,0.339575,5249,reut tsarfaty,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite language-specific idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations."
W10-1409,Parsing Word Clusters,2010,22,37,2,0.767938,16504,marie candito,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We present and discuss experiments in statistical parsing of French, where terminal forms used during training and parsing are replaced by more general symbols, particularly clusters of words obtained through un-supervised linear clustering. We build on the work of Candito and Crabbe (2009) who proposed to use clusters built over slightly coarsened French inflected forms. We investigate the alternative method of building clusters over lemma/part-of-speech pairs, using a raw corpus automatically tagged and lemmatized. We find that both methods lead to comparable improvement over the baseline (we obtain F1=86.20% and F1=86.21% respectively, compared to a baseline of F1=84.10%). Yet, when we replace gold lemma/POS pairs with their corresponding cluster, we obtain an upper bound (F1=87.80) that suggests room for improvement for this technique, should tag-ging/lemmatisation performance increase for French.n n We also analyze the improvement in performance for both techniques with respect to word frequency. We find that replacing word forms with clusters improves attachment performance for words that are originally either unknown or low-frequency, since these words are replaced by cluster symbols that tend to have higher frequencies. Furthermore, clustering also helps significantly for medium to high frequency words, suggesting that training on word clusters leads to better probability estimates for these words."
W10-1410,Lemmatization and Lexicalized Statistical Parsing of Morphologically-Rich Languages: the Case of {F}rench,2010,22,15,1,1,167,djame seddah,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results. We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words."
seddah-2010-exploring,Exploring the Spinal-{STIG} Model for Parsing {F}rench,2010,28,5,1,1,167,djame seddah,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We evaluate statistical parsing of French using two probabilistic models derived from the Tree Adjoining Grammar framework: a Stochastic Tree Insertion Grammars model (STIG) and a specific instance of this formalism, called Spinal Tree Insertion Grammar model which exhibits interesting properties with regard to data sparseness issues common to small treebanks such as the Paris 7 French Treebank. Using David ChiangÂs STIG parser (Chiang, 2003), we present results of various experiments we conducted to explore those models for French parsing. The grammar induction makes use of a head percolation table tailored for the French Treebank and which is provided in this paper. Using two evaluation metrics, we found that the parsing performance of a STIG model is tied to the size of the underlying Tree Insertion Grammar, with a more compact grammar, a spinal STIG, outperforming a genuine STIG. We finally note that a ''``spinal'''' framework seems to emerge in the literature. Indeed, the use of vertical grammars such as Spinal STIG instead of horizontal grammars such as PCFGs, afflicted with well known data sparseness issues, seems to be a promising path toward better parsing performance."
W09-3824,Cross parser evaluation : a {F}rench Treebanks study,2009,13,8,1,1,167,djame seddah,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,None
W09-1008,On Statistical Parsing of {F}rench with Supervised and Semi-Supervised Strategies,2009,19,19,3,0.64396,16504,marie candito,Proceedings of the {EACL} 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference,0,"This paper reports results on grammatical induction for French. We investigate how to best train a parser on the French Treebank (Abeille et al., 2003), viewing the task as a trade-off between generaliz-ability and interpretability. We compare, for French, a supervised lexicalized parsing algorithm with a semi-supervised un-lexicalized algorithm (Petrov et al., 2006) along the lines of (Crabbe and Candito, 2008). We report the best results known to us on French statistical parsing, that we obtained with the semi-supervised learning algorithm. The reported experiments can give insights for the task of grammatical learning for a morphologically-rich language, with a relatively limited amount of training data, annotated with a rather flat structure."
2009.jeptalnrecital-court.1,Adaptation de parsers statistiques lexicalis{\\'e}s pour le fran{\\c{c}}ais : Une {\\'e}valuation compl{\\`e}te sur corpus arbor{\\'e}s,2009,-1,-1,1,1,167,djame seddah,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente les r{\'e}sultats d{'}une {\'e}valuation exhaustive des principaux analyseurs syntaxiques probabilistes dit {``}lexicalis{\'e}s{''} initialement con{\c{c}}us pour l{'}anglais, adapt{\'e}s pour le fran{\c{c}}ais et {\'e}valu{\'e}s sur le CORPUS ARBOR{\'E} DU FRAN{\c{C}}AIS (Abeill{\'e} et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter {\&} van Genabith, 2007). Confirmant les r{\'e}sultats de (Crabb{\'e} {\&} Candito, 2008), nous montrons que les mod{\`e}les lexicalis{\'e}s, {\`a} travers les mod{\`e}les de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le mod{\`e}le des TIG Stochastiques (Chiang, 2000), pr{\'e}sentent des performances moindres face {\`a} un analyseur PCFG {\`a} Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d{'}un jeu d{'}annotations issus de tel ou tel treebank oriente fortement les r{\'e}sultats d{'}{\'e}valuations tant en constituance qu{'}en d{\'e}pendance non typ{\'e}e. Compar{\'e}s {\`a} (Schluter {\&} van Genabith, 2008; Arun {\&} Keller, 2005), tous nos r{\'e}sultats sont state-of-the-art et infirment l{'}hypoth{\`e}se d{'}une difficult{\'e} particuli{\`e}re qu{'}aurait le fran{\c{c}}ais en terme d{'}analyse syntaxique probabiliste et de sources de donn{\'e}es."
W08-2311,The use of {MCTAG} to Process Elliptic Coordination,2008,14,5,1,1,167,djame seddah,Proceedings of the Ninth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+9),0,"In this paper, we introduce a formalization of various elliptical coordination structures within the Multi-Component TAG framework. Numerous authors describe elliptic coordination as parallel constructions where symmetric derivations can be observed from a desired predicate-argument structure analysis. We show that most famous coordinate structures, including zeugma constructions, can be analyzed simply with the addition of a simple synchronous mechanism to the MCTAG framework ."
W07-2204,Adapting {WSJ}-Trained Parsers to the {B}ritish {N}ational {C}orpus using In-Domain Self-Training,2007,10,18,3,0,819,jennifer foster,Proceedings of the Tenth International Conference on Parsing Technologies,0,"We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnson's reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set."
W06-1522,Modeling and Analysis of Elliptic Coordination by Dynamic Exploitation of Derivation Forests in {LTAG} Parsing,2006,13,3,1,1,167,djame seddah,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"In this paper, we introduce a generic approach to elliptic coordination modeling through the parsing of Ltag grammars. We show that erased lexical items can be replaced during parsing by informations gathered in the other member of the coordinate structure and used as a guide at the derivation level. Moreover, we show how this approach can be indeed implemented as a light extension of the LTAG formalism throuh a so-called xe2x80x9cfusionxe2x80x9d operation and by the use of tree schemata during parsing in order to obtain a dependency graph."
2006.jeptalnrecital-poster.25,Mod{\\'e}lisation et analyse des coordinations elliptiques par l{'}exploitation dynamique des for{\\^e}ts de d{\\'e}rivation,2006,-1,-1,1,1,167,djame seddah,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Nous pr{\'e}sentons dans cet article une approche g{\'e}n{\'e}rale pour la mod{\'e}lisation et l{'}analyse syntaxique des coordinations elliptiques. Nous montrons que les lex{\`e}mes {\'e}lid{\'e}s peuvent {\^e}tre remplac{\'e}s, au cours de l{'}analyse, par des informations qui proviennent de l{'}autre membre de la coordination, utilis{\'e} comme guide au niveau des d{\'e}rivations. De plus, nous montrons comment cette approche peut {\^e}tre effectivement mise en oeuvre par une l{\'e}g{\`e}re extension des Grammaires d{'}Arbres Adjoints Lexicalis{\'e}es (LTAG) {\`a} travers une op{\'e}ration dite de fusion. Nous d{\'e}crivons les algorithmes de d{\'e}rivation n{\'e}cessaires pour l{'}analyse de constructions coordonn{\'e}es pouvant comporter un nombre quelconque d{'}ellipses."
2005.jeptalnrecital-long.35,Des arbres de d{\\'e}rivation aux for{\\^e}ts de d{\\'e}pendance : un chemin via les for{\\^e}ts partag{\\'e}es,2005,-1,-1,1,1,167,djame seddah,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}objectif de cet article est de montrer comment b{\^a}tir une structure de r{\'e}presentation proche d{'}un graphe de d{\'e}pendance {\`a} l{'}aide des deux structures de repr{\'e}sentation canoniques fournies par les Grammaires d{'}Arbres Adjoints Lexicalis{\'e}es . Pour illustrer cette approche, nous d{\'e}crivons comment utiliser ces deux structures {\`a} partir d{'}une for{\^e}t partag{\'e}e."
2002.jeptalnrecital-recital.4,"Conceptualisation d{'}un syst{\\`e}me d{'}informations lexicales, une interface param{\\'e}trable pour le {T}.{A}.{L}",2002,-1,-1,1,1,167,djame seddah,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,"La n{\'e}cessit{\'e} de ressources lexicales normalis{\'e}es et publiques est av{\'e}r{\'e}e dans le domaine du TAL. Cet article vise {\`a} montrer comment, sur la base d{'}une partie du lexique MULTEXT disponible sur le serveur ABU, il serait possible de construire une architecture permettant tout {\`a} la fois l{'}acc{\`e}s aux ressources avec des attentes diff{\'e}rentes (lemmatiseur, parseur, extraction d{'}informations, pr{\'e}diction, etc.) et la mise {\`a} jour par un groupe restreint de ces ressources. Cette mise {\`a} jour consistant en l{'}int{\'e}gration et la modification, automatique ou manuelle, de donn{\'e}es existantes. Pour ce faire, nous cherchons {\`a} prendre en compte {\`a} la fois les besoins et les donn{\'e}es accessibles. Ce mod{\`e}le est {\'e}valu{\'e} conceptuellement dans un premier temps en fonction des syst{\`e}mes utilis{\'e}s dans notre {\'e}quipe : un analyseur TAG, un constructeur de grammaires TAGs, un extracteur d{'}information."
W00-2003,Practical aspects in compiling tabular {TAG} parsers,2000,4,2,2,0,31545,miguel alonso,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,This paper describes the extension of the system DyALog to compile tabular parsers from Feature Tree Adjoining Grammars. The compilation process uses intermediary 2-stack automata to encode various parsing strategies and a dynamic programming interpretation to break automata derivations into tabulable fragments.
