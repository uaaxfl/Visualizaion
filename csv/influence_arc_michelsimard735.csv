1992.tmi-1.7,P91-1022,0,0.474028,"of the smallest possible couples. The type of alignment we will be discussing takes the sentence to be the segmentation unit. Figure 1 illustrates such an alignment. Clearly, a corpus of properly aligned bitext constitutes an extremely valuable source of information, not only to researchers in bilingual lexicography and terminology, but also for a range of applications. While producing alignments by hand is extremely time-consuming and requires the skills of individuals with a good knowledge of both languages, there exist programs that produce relatively reliable alignments at a minimal cost [Brown et al. 1991, Gale and Church 1991]. And in fact, for some applications, it is sufficient that the alignment for a given bitext be only partially correct, as long as there is a way of automatically extracting a subset of that bitext for the alignment of which there is a high level of confidence. For other applications however, much can be gained from a program that is capable of producing high-quality alignments for an entire piece of bitext. This is the case for translation revision and evaluation [Isabelle, 1991]. The first of these gains is obvious: to allow one to visualize a text and its translation"
1992.tmi-1.7,J90-2002,0,0.146448,"Missing"
1992.tmi-1.7,P91-1023,0,0.279504,"sible couples. The type of alignment we will be discussing takes the sentence to be the segmentation unit. Figure 1 illustrates such an alignment. Clearly, a corpus of properly aligned bitext constitutes an extremely valuable source of information, not only to researchers in bilingual lexicography and terminology, but also for a range of applications. While producing alignments by hand is extremely time-consuming and requires the skills of individuals with a good knowledge of both languages, there exist programs that produce relatively reliable alignments at a minimal cost [Brown et al. 1991, Gale and Church 1991]. And in fact, for some applications, it is sufficient that the alignment for a given bitext be only partially correct, as long as there is a way of automatically extracting a subset of that bitext for the alignment of which there is a high level of confidence. For other applications however, much can be gained from a program that is capable of producing high-quality alignments for an entire piece of bitext. This is the case for translation revision and evaluation [Isabelle, 1991]. The first of these gains is obvious: to allow one to visualize a text and its translation side-by-side, with exp"
1996.amta-1.14,J93-2003,0,0.0180193,"ose width is proportional to the distance between the points; only those pairs of sentence-boundaries that fall within these regions are included (circled intersections). Second step: Final Sentence Alignment From this point on, any sentence alignment program that is capable of working within such a restricted search-space can be used to finish up the job. Following the ideas of Chen [4] and Dagan et al. [6], we have developed a method that could probably be referred to as ""heavy artillery"" in this context: it is based on a statistical lexical translation model, namely Brown et al.'s ""Model 1""[1]. Essentially, the model consists in a set of parameters Tf,e, that estimate the probability of observing word f in one text, given that word e appears in the other. The parameters are normally estimated from frequencies observed in a large collection of pairs of text segments known to be mutual translations (typically, these segments are sentences). The parameters of the model can be combined so as to estimate the probability of observing some arbitrary set of words in one language, given some other set in the other language. In particular, this may be applied to estimate how likely it is to"
1996.amta-1.14,P91-1022,0,0.469366,"n a coherent context, so that the user can evaluate the relevance of each returned item with regard to his own problem. In our ongoing efforts to develop sentence alignment methods that are both reliable and accurate, we have developed a hybrid approach, that combines the best of existing methods. This work is described in the following pages. Background As far as we know, interest in bitext correspondence began sometime in the mid-eighties, at which time independent efforts were being pursued concurrently in many places, most notably at Xerox PARC [11], IBM's Thomas J. Watson research centre [2], AT&T Bell Laboratories in Murray Hill [8] and in Geneva, at ISSCO [3]. Interestingly, all these early efforts focussed on sentence alignments rather than bitext maps. The first communications on the subject were published in 1991. Paradoxically, there were two of them, they appeared back-to-back in the proceedings of the Conference of the Association for Computational Linguistics, and described alignment methods that were virtually identical. Both were based on a statistical modelization of translations that only took into account the length of the text segments (sentences, paragraphs), and"
1996.amta-1.14,P93-1002,0,0.687665,"much sense, the program produces a bitext map. The second issue is that of accuracy: even when the input texts are ""clean"", alignment programs are sometimes faced with hard decisions. In order to obtain the best possible alignments, one will eventually have to throw in the whole armada of NLP and AI techniques: dictionaries, grammars, semantic networks, stochastic language models, common-sense reasoning, intelligent agents - you name it. So far, the most promising avenues in dealing with this problem make use of stochastic translation models. For example, to compute sentence alignments, Chen [4] replaces the simple length-based models of earlier methods by a more elaborate model that takes into account the words of the text. Dagan et al [6] use a similar model to obtain word-level mappings. To this day, most research on the BCP has focussed on either one of these two problems (robustness and accuracy). This work is an attempt to tackle the two together. Robust and Accurate Sentence Alignments We now describe our approach to the sentence alignment problem. Our idea is to combine the robustness of ""character-based"" methods, such as char_align, and the accuracy of stochastic translation"
1996.amta-1.14,P93-1001,0,0.505503,"t correspond to one another. This is illustrated in figure 1. Bitext correspondences are of vital interest to anyone who wishes to exploit existing translations as an active source of information. Which is best between an alignment and a bitext map usually depends on the intended application. By definition, an alignment covers the totality of the bitext. In this sense, it is both exhaustive and exact: for each segment of text, it says something like ""the translation of this segment is exactly that segment"". The same cannot be said of bitext maps. Some methods, such as those proposed by Church [5] or Fung and McKeown [7], produce approximate maps (i.e. not exact), that say something like ""The translation of the text around this point is somewhere around that point"". Other methods, such as those 135 proposed by Dagan et al. [6] or Melamed [12] produce maps that are exact (""the translation of the object at position x is the object at position y"") but not exhaustive. On the other hand, what they lack in exactness or exhaustiveness, bitext map usually make up for in resolution: they give a ""closer view"" on the correspondence. There are many situations where alignments are preferable, howev"
1996.amta-1.14,W93-0301,0,0.858295,"d a bitext map usually depends on the intended application. By definition, an alignment covers the totality of the bitext. In this sense, it is both exhaustive and exact: for each segment of text, it says something like ""the translation of this segment is exactly that segment"". The same cannot be said of bitext maps. Some methods, such as those proposed by Church [5] or Fung and McKeown [7], produce approximate maps (i.e. not exact), that say something like ""The translation of the text around this point is somewhere around that point"". Other methods, such as those 135 proposed by Dagan et al. [6] or Melamed [12] produce maps that are exact (""the translation of the object at position x is the object at position y"") but not exhaustive. On the other hand, what they lack in exactness or exhaustiveness, bitext map usually make up for in resolution: they give a ""closer view"" on the correspondence. There are many situations where alignments are preferable, however. In particular, this appears to be true of applications where the bitext correspondence is directly intended for a human. An example of such an application is the bilingual concordance system developed at CITI [16]. This system all"
1996.amta-1.14,1994.amta-1.11,0,0.425801,"her. This is illustrated in figure 1. Bitext correspondences are of vital interest to anyone who wishes to exploit existing translations as an active source of information. Which is best between an alignment and a bitext map usually depends on the intended application. By definition, an alignment covers the totality of the bitext. In this sense, it is both exhaustive and exact: for each segment of text, it says something like ""the translation of this segment is exactly that segment"". The same cannot be said of bitext maps. Some methods, such as those proposed by Church [5] or Fung and McKeown [7], produce approximate maps (i.e. not exact), that say something like ""The translation of the text around this point is somewhere around that point"". Other methods, such as those 135 proposed by Dagan et al. [6] or Melamed [12] produce maps that are exact (""the translation of the object at position x is the object at position y"") but not exhaustive. On the other hand, what they lack in exactness or exhaustiveness, bitext map usually make up for in resolution: they give a ""closer view"" on the correspondence. There are many situations where alignments are preferable, however. In particular, this"
1996.amta-1.14,P91-1023,0,0.11653,"evaluate the relevance of each returned item with regard to his own problem. In our ongoing efforts to develop sentence alignment methods that are both reliable and accurate, we have developed a hybrid approach, that combines the best of existing methods. This work is described in the following pages. Background As far as we know, interest in bitext correspondence began sometime in the mid-eighties, at which time independent efforts were being pursued concurrently in many places, most notably at Xerox PARC [11], IBM's Thomas J. Watson research centre [2], AT&T Bell Laboratories in Murray Hill [8] and in Geneva, at ISSCO [3]. Interestingly, all these early efforts focussed on sentence alignments rather than bitext maps. The first communications on the subject were published in 1991. Paradoxically, there were two of them, they appeared back-to-back in the proceedings of the Conference of the Association for Computational Linguistics, and described alignment methods that were virtually identical. Both were based on a statistical modelization of translations that only took into account the length of the text segments (sentences, paragraphs), and relied on a dynamic programming scheme to f"
1996.amta-1.14,1993.tmi-1.17,0,0.14931,"s was how length was measured: while Brown et al. [2] counted words, Gale and Church [8] counted characters. 136 The early successes obtained using these methods almost gave the impression that the problem had been solved. Of course, this was not the case, and although it is true that sentence alignment is mostly an easy problem, anyone who has attempted to manually align a sufficient amount of text knows that there are situations where even humans have a hard time making a decision. The truth of the matter is that BCP is just one instance of the more general translation analysis problem (see [9]), which turns out to be ""AIcomplete"". In other words, to solve the BCP entirely, you would first have to solve all the other ""hard"" AI problems - and conversely, if you solve the BCP, then you have just put the whole AI community out of work! What is it that is so difficult with sentence alignment? The first issue is that of robustness. For a long time, almost everybody in the field was working with the same set of data, namely the Hansards (Canadian parliamentary proceedings). As other multilingual corpora became available, it quickly appeared that the Hansards were exceptionally ""clean"" tra"
1996.amta-1.14,J93-1006,0,0.583942,"turn the expression and its translation within a coherent context, so that the user can evaluate the relevance of each returned item with regard to his own problem. In our ongoing efforts to develop sentence alignment methods that are both reliable and accurate, we have developed a hybrid approach, that combines the best of existing methods. This work is described in the following pages. Background As far as we know, interest in bitext correspondence began sometime in the mid-eighties, at which time independent efforts were being pursued concurrently in many places, most notably at Xerox PARC [11], IBM's Thomas J. Watson research centre [2], AT&T Bell Laboratories in Murray Hill [8] and in Geneva, at ISSCO [3]. Interestingly, all these early efforts focussed on sentence alignments rather than bitext maps. The first communications on the subject were published in 1991. Paradoxically, there were two of them, they appeared back-to-back in the proceedings of the Conference of the Association for Computational Linguistics, and described alignment methods that were virtually identical. Both were based on a statistical modelization of translations that only took into account the length of the"
1996.amta-1.14,W96-0201,0,0.254545,"usually depends on the intended application. By definition, an alignment covers the totality of the bitext. In this sense, it is both exhaustive and exact: for each segment of text, it says something like ""the translation of this segment is exactly that segment"". The same cannot be said of bitext maps. Some methods, such as those proposed by Church [5] or Fung and McKeown [7], produce approximate maps (i.e. not exact), that say something like ""The translation of the text around this point is somewhere around that point"". Other methods, such as those 135 proposed by Dagan et al. [6] or Melamed [12] produce maps that are exact (""the translation of the object at position x is the object at position y"") but not exhaustive. On the other hand, what they lack in exactness or exhaustiveness, bitext map usually make up for in resolution: they give a ""closer view"" on the correspondence. There are many situations where alignments are preferable, however. In particular, this appears to be true of applications where the bitext correspondence is directly intended for a human. An example of such an application is the bilingual concordance system developed at CITI [16]. This system allows a user to qu"
1996.amta-1.14,A94-1013,0,0.013518,"-segmentation"" reduces alignment recall, while ""under-segmentation"" reduces alignment precision. It should be noted, however, that for an application such as bilingual concordance, such errors are not necessarily catastrophic. From our experience, alignment errors resulting from oversegmentation usually separate unrelated portions of sentences, while under-segmentation simply results in a ""dilution"" of the information rather than in genuine misalignments. We are nevertheless exploring the possibility of using more sophisticated segmentation methods, such as those proposed by Palmer and Hearst [14] for disambiguating periods. However, it would seem that ambiguous periods is not the only issue at stake here. In fact, most of our problems come from ""sentences"" that simply do not end with a period, or any punctuation mark for that matter: titles, section headings, list and table items, etc. What remains to be demonstrated, however, is that plain ASCII text such as those that we have been working with, are really what real-life texts will be like in the near future... 143 Acknowledgments We would like to thank Pierre Isabelle and Dan Melamed, for inspirational discussions and constructive c"
1996.amta-1.14,1992.tmi-1.7,1,0.905597,"came available, it quickly appeared that the Hansards were exceptionally ""clean"" translations. As Church points out, ""Real texts are noisy"" ([5]). Earlier methods are likely to wander off track when faced with deviations from the standard ""linear"" progression of translation, such as those that occur when parts of the source text do not make their way into the translation (omissions), or end up in a different order (inversions). To deal with the robustness issue, Church took a very straightforward and intuitive approach, exploiting an alignment criterion that was first proposed by Simard et al [15]: cognate words. Cognates are pairs of words of different languages that have close etymological ties. Often this tie will be reflected both in the meanings and orthography of these words. As a result, they are likely mutual translations, and they are fairly easy to detect, even for someone who is not familiar with either of the languages involved. Church's program, called char_align, does not rely on a formal definition of cognates, but rather on a more general notion of ""resemblance"" between source-text and translation. Interestingly, what char_align does could very well be compared to what"
2001.jeptalnrecital-long.22,C96-1030,0,0.121126,"Missing"
2001.jeptalnrecital-long.22,1993.tmi-1.17,0,0.0293361,"ies is often limited by the nature of the text segments that they connect, generally whole sentences. This article examines the potential of a type of system that would be able to recuperate the translation of arbitrary sequences of words. Mots cl´es : m´emoire de traduction sous-phrastique, traduction assist´ee par ordinateur, traduction automatique a` base d’exemples. 1 Introduction L’ensemble des nombres r´eels contient plus de solutions a` plus de probl`emes d’arithm´etique que toute autre ressource... Le lecteur averti aura reconnu, sous un d´eguisement grossier, la c´el`ebre citation de Isabelle et al. (1993): en remplac¸ant l’arithm´etique par la traduction et l’ensemble des nombres r´eels par celui des traductions existantes, on reconnaˆıt bien l’affirmation qui a servi de leitmotiv a` quantit´e de travaux sur les outils d’aide a` la traduction. La premi`ere question que cette boutade soul`eve est celle que se pose tout e´ colier aux prises avec un devoir de math´ematiques: comment aller chercher ces solutions ? Dans le cas de la traduction, toute une gamme d’approches ont e´ t´e propos´ees, allant de la simple consultation interactive (par exemple, avec le syst`eme TransSearch (Macklovitch et a"
2001.jeptalnrecital-long.22,macklovitch-russell-2000-whats,0,0.674436,"Missing"
2001.jeptalnrecital-long.22,macklovitch-etal-2000-transsearch,1,0.771304,"Missing"
2001.jeptalnrecital-long.22,niessen-etal-2000-evaluation,0,0.140903,"Missing"
2001.jeptalnrecital-long.22,1999.mtsummit-1.48,0,0.475518,"Missing"
2001.jeptalnrecital-long.22,W00-0726,0,0.0973981,"Missing"
2001.jeptalnrecital-long.22,1992.tmi-1.7,1,0.860661,"en italique identifient les parties des propositions que l’utilisateur s´electionne. 4 Exp´eriences 4.1 M´emoire de traduction La m´emoire de traduction que nous avons utilis´ee ici est constitu´ee de textes des d´ebats parlementaires canadiens ; textes commun´ement regroup´es sous le nom Hansard. Notre corpus couvre environ 15 ans de d´ebats (de 1986 a` 2000 inclusivement) et totalise pr`es de 100 millions de mots de chaque langue. Le tout a e´ t´e automatiquement segment´e en phrases, lesquelles ont ensuite e´ t´e align´ees avec le programme SFIAL (une version am´elior´ee de la m´ethode de Simard et al. (1992)), produisant ainsi plus de 5 millions de paires de segments. 4.2 Corpus de test Nous avons isol´e deux bitextes assez diff´erents de nature pour mesurer les taux de pr´ecision et de rappel obtenus. Chaque bitexte est constitu´e de 100 paires de phrases extraites al´eatoirement 3 La pertinence d’un copi´e/coll´e est dans cette e´ tude d´efinie par: une brique traductionnelle contient au moins deux mots de la traduction oracle. Philippe Langlais et Michel Simard de l’un des corpus suivants: Hansard, constitu´e de textes du Hansard non rencontr´es dans la m´emoire de traduction et Verne, le roma"
2001.jeptalnrecital-long.22,J97-3002,0,0.0704067,"de r´eduire la quantit´e de mat´eriel cible propos´e a` l’utilisateur consiste a` apparier les couples traductionnels a` un niveau sousphrastique (mots ou autres). Plusieurs m´ethodes ont e´ t´e propos´ees pour localiser a` l’int´erieur de traductions la sous-s´equence cible en relation de traduction avec une s´equence source donn´ee. Certaines ont fait l’objet d’une e´ valuation comparative, dans le cadre de l’ARC-A2 (V´eronis and Langlais2000). Dans ce travail nous avons implant´e une m´ethode d’alignement de mots bas´ee sur les transducteurs grammaticaux invers´es (ITG) propos´es par Dekai Wu (1997). En bref, cette proc´edure prend en entr´ee une paire de segments et une grammaire sp´ecialis´ee (sp´ecifi´ee sous formes de r`egles probabilistes). Elle tente alors de segmenter de mani`ere r´ecursive et parall`ele la paire de segments en identifiant a` chaque e´ tape l’alignement le plus probable entre deux sous-s´equences. La figure 4 illustre le r´esultat d’un alignement produit par cette m´ethode pour un couple traductionnel et une requˆete donn´es. S´equence source: the recommendations made by Couple traductionnel: source: What we find in this bill are things that are directly from the"
2001.mtsummit-papers.60,C96-1030,0,0.206884,"Missing"
2001.mtsummit-papers.60,1997.mtsummit-papers.1,0,0.0284968,"her a new translation can be a laborious activity, and whether users would find it easier just to type the text instead is not clear. To evaluate this, we measured the performance of a GTMS under 2 alternative scenarios. In the first of these, the user picks only those TL proposals that he can use “as is” in his new translation, without cutting. In the second scenario, the user also considers proposals whose prefix is usable as is. The intention of this last scenario is to evaluate the potential of a “typing completion” mechanism, such as that proposed in the TransType interactive MT project (Foster et al., 1997). The results of these experiments on the Hansard bitext appear in Table 3 below. Scenario Precision Recall cut and paste 37.14 28.09 prefix 26.01 19.66 as is 17.96 13.58 Table 3: GTMS performance under different user-editing scenarios What this shows is that, within our current GTMS implementation, a lot of cutting and pasting would be required to take full advantage of the TM’s content. Nevertheless, using only those proposals that fit “as is” is still viable. Multiple Translation Evaluation Using “oracle translations” to evaluate usability does not necessarily reflect the full potential of"
2001.mtsummit-papers.60,2001.jeptalnrecital-long.22,1,0.741808,"the Pangloss constitutes an acceptable translation for the sourcemulti-engine machine-translation system comprises an language (SL) sentence. As for the query, it is constructed automatically by the TMS, from the SL sentence to be 1 http://www.google.com translated. The retrieval operation is carried out by 2 Note that even for large segments of text such as sentences and matching this query as closely as possible. paragraphs, the general re-usability of past translations is being In this perspective, the default strategy of existing TMS's seriously questioned by some translators; see Bédard (2001), for is an extreme form of high-precision, low-recall search: example. return only the best matching document, and only if it 3 http://www-rali.iro.umontreal.ca/TransSearch EBMT component based on a very simple mechanism (Brown, 1996). The system’s database of examples consists in a large collection of aligned sentences (in other words, a standard translation memory). Given a new sentence to translate, the system looks up all possible subsequences of words of this sentence in the database. It then relies on a simple word-alignment mechanism to locate the translation of each matching sequence"
2001.mtsummit-papers.60,macklovitch-russell-2000-whats,0,0.0616014,"l usually look for does not contain smaller segments that could be useful to couples whose source-language sentence matches the new the translator. sentence in its entirety. This match need not be exact (all The TransSearch system (Macklovitch et al. 2000) is one the above systems feature some sort of “fuzzy” matching), radically different type of TMS based on this idea. It but obviously, better-matching couples stand a better allows translators to interactively query a large database chance of being re-usable. of past translations for specific terms, expressions, or any It is instructive, as Macklovitch and Russell (2000) sequence of words. If current usage statistics are an suggest, to view TM applications in an information argument in favor of exploiting TM’s at a sub-sentential retrieval (IR) perspective: when using a TMS, the level, the system has been online3 for almost 5 years, and translator is actually just searching for documents that currently processes over 50 000 queries per month (March might help in translating a given sentence. In this case, 2001). these “documents” happen to be pairs of mutually Another argument in support of this idea comes in the translated sentences, and the translator is li"
2001.mtsummit-papers.60,macklovitch-etal-2000-transsearch,1,0.726713,"slation Manager/2), Atril (Déja Vu) and Star-AG Nevertheless, this state of affairs is somewhat frustrating: (Transit). In all these systems, the couples in the intuitively, just because a sentence has never been translation memory are normally pairs of sentences. Given translated before does not necessarily mean that the TM a new sentence to be translated, they will usually look for does not contain smaller segments that could be useful to couples whose source-language sentence matches the new the translator. sentence in its entirety. This match need not be exact (all The TransSearch system (Macklovitch et al. 2000) is one the above systems feature some sort of “fuzzy” matching), radically different type of TMS based on this idea. It but obviously, better-matching couples stand a better allows translators to interactively query a large database chance of being re-usable. of past translations for specific terms, expressions, or any It is instructive, as Macklovitch and Russell (2000) sequence of words. If current usage statistics are an suggest, to view TM applications in an information argument in favor of exploiting TM’s at a sub-sentential retrieval (IR) perspective: when using a TMS, the level, the sy"
2001.mtsummit-papers.60,niessen-etal-2000-evaluation,0,0.0554197,"Missing"
2001.mtsummit-papers.60,1999.mtsummit-1.48,0,0.399588,"Missing"
2001.mtsummit-papers.60,1992.tmi-1.7,1,0.677229,"osera-t-il] les [recommandations faites par] les deux agences? Figure 5: Proposed TL sub-sequences and optimal cover of the oracle translation. Test Material The translation memory we used for the tests was constructed from a corpus of proceedings of the Canadian parliamentary debates (Hansard). This corpus covers 15 years of debates (from 1986 to 2000 inclusively) and totals over 100 million words of each language. All pairs of documents were automatically segmented into sentences, which were then aligned using the SFIAL program (an improved implementation of the alignment method proposed by Simard et al., 1992), thus producing over 5 million pairs of segments. Two different test bitexts were used for our experiments, each consisting in 100 pairs of sentences, randomly selected from two quite different documents: The Hansard bitext comes from a parliamentary debate outside our translation memory corpus, while the Verne bitext was extracted from Jules Verne's novel “De la terre à la lune”. In both bitexts, the alignments were verified by hand. Basic Results The initial objective of our work was to determine to what extent it was possible to improve the performance of existing TMS's in terms of recall."
2001.mtsummit-papers.60,J97-3002,0,0.0202272,"sing the selected couples, the system must produce the TL text to be proposed to the user. Since each selected couple (s,t) may correspond to only a fraction of the source sentence e, large segments of the TL sentences t will usually not be relevant to the translation of e. Limiting the amount of information presented to the user requires identifying some specific portion in t that best corresponds to the common sub-sequence sij between e and s. To perform this operation, we have experimented with a word-alignment procedure adapted from Wu’s statistical inversion transduction grammars (SITG − Wu, 1997). Briefly, this procedure takes as input the pair of sentences s and t, and recursively segments both texts in parallel, identifying at each step the most probable alignment between sub-sequences. By forcing this procedure to “stop” around the matching sub-sequence sij in s, we can locate the sub-sequence tkl of t to which it most likely corresponds in this “parallel derivation” of s and t. Figure 4 below shows an example. SL sub-sequence : <the recommendations made by&gt; Matching couple : SL : “ What we find in this bill are things that are directly from the recommendations made by these groups"
2003.jeptalnrecital-long.18,J93-2003,0,0.00472084,"Missing"
2003.jeptalnrecital-long.18,C96-1030,0,0.0322551,"Missing"
2003.jeptalnrecital-long.18,1994.amta-1.10,0,0.0994015,"Missing"
2003.jeptalnrecital-long.18,P01-1030,0,0.0608848,"Missing"
2003.jeptalnrecital-long.18,2002.jeptalnrecital-long.2,1,0.863824,"Missing"
2003.jeptalnrecital-long.18,P01-1050,0,0.0318827,"Missing"
2003.jeptalnrecital-long.18,P98-2158,0,0.0542169,"Missing"
2003.jeptalnrecital-long.18,P00-1056,0,0.119625,"Missing"
2003.jeptalnrecital-long.18,W00-0731,0,0.0611225,"Missing"
2003.jeptalnrecital-long.18,P02-1040,0,0.0807418,"Missing"
2003.jeptalnrecital-long.18,J97-3002,0,0.0334788,"Missing"
2003.jeptalnrecital-long.18,C98-2153,0,\N,Missing
2003.mtsummit-papers.15,J93-2003,0,0.00571176,"isted, sentences were split at an arbitrary token boundary. For the take2 experiments, however, we decided to simplify this process by discarding segment pairs for which the source sentence was longer than 40 words. 4 Translation Engines The problem of statistical translation can be viewed as an optimization problem where, given a source string F = hf1 , . . . , fj i, a translation model Ptm (·) and a language model Plm (·), we try to find a target string E = he1 , . . . , ei i that maximizes the joint probability Plm (E) ∗ Ptm (F |E). As such, it is an instance of the noisy channel approach (Brown et al., 1993), in which an output signal is ‘decoded’ in order to recover the original input; algorithms which perform this task are known as decoders. The probabilities Plm (·) and Ptm (·) are derived by training on monolingual and bilingual corpora. The language model used in this work was a trigram model trained by an existing in-house package, and the translation models considered were the IBM model 2, also trained using an in-house tool, and IBM model 4, trained using the GIZA++ package (Och and Ney, 2000).4 One important choice for rescoring is how to represent the sets of translations that are outpu"
2003.mtsummit-papers.15,1994.amta-1.10,0,0.0296286,"Missing"
2003.mtsummit-papers.15,P01-1030,0,0.497158,"gov/ speech/tests/mt/resources/. As time quickly passed we rapidly learned the second lesson of this exercise: building a statistical MT engine, even largely from pre-existing components, is by no means straightforward. Several unexpected problems arose which in retrospect are quite interesting. First, training very large models with publicly available packages is feasible only to the extent that the corresponding memory demands can be met (see section 4.1). Second, dealing with multiple decoders inevitably complicates development. However, thanks to the multiple decoder strategy suggested by Germann et al. (2001) we managed to get a decent IBM 4 decoder. Third, we observed that decoding in itself involves a compromise between quality of results and the length of the delay before results are emitted: tuning for performance is a delicate and time-consuming process. The immediate aim of the NIST exercise we participated in was to translate 919 Chinese sentences (length varying from 5 to 101 words, with an average of 29) into English. Additionally, the rescoring strategy required the translation of some 20,000 sentences for the purpose of training the rescoring layer. In the event, slower than expected pr"
2003.mtsummit-papers.15,P98-2158,0,0.279627,"de approximation to the joint probability of those words). This last heuristic was introduced as a normalization in order to moderate the strong influence that frequent words in the training corpus tend to have at translation time. This means that we only compare paths using the same number of source and target words, and whose source words have similar a priori probabilities. 4.2.3 Inverted Alignment Decoder In an attempt to gauge the influence of both decoders and models on the overall translation quality, we also implemented a version of the inverted alignment search algorithm described by Nießen et al. (1998) for IBM 2 models. The basic idea of this method is to expand hypotheses along the positions of the target string while progressively covering the source. The algorithm allows any target word to be aligned to none, one or several consecutive source words (up to a maximum that was set to 3 in the reported experiments); thus, this search accounts for the notion of fertility which is not explicitly captured by IBM 2 models. A hypothesis is fully determined by four parameters: the source and target positions, the source coverage in words and the target word found at the target position. Therefore,"
2003.mtsummit-papers.15,E99-1010,0,0.0211525,"tures. Establishing the limits of the package (maximum input size, etc.) took at least another week of monitoring, excluding computation time. We soon found that using GIZA++ to train a translation model on a corpus of more than one million sentence pairs was impractical. Beyond this point, memory is saturated and the system is forced to swap, drastically increasing the time required. IBM model 4 conditions the distortion probabilities on the class of the centroid source and target words. To acquire these classes, we used the program mkcls.5 In contrast to the bilingual procedure described by Och (1999), for which no ready-made solution was immediately available, this permits only the creation of monolingual classes. Using this program, source and target vocabulary were processed into 50 classes; this required around ten hours of computation. In total, the full training of an IBM model 4 on a corpus of around one million sentence pairs requires two to three days of computation on a 1GB memory Pentium 4 desk computer. 4.2 Multiple Decoders Three different decoders, all previously described in the statistical MT literature, were implemented. One advantage of having several decoders at one’s di"
2003.mtsummit-papers.15,P00-1056,0,0.115271,"int probability Plm (E) ∗ Ptm (F |E). As such, it is an instance of the noisy channel approach (Brown et al., 1993), in which an output signal is ‘decoded’ in order to recover the original input; algorithms which perform this task are known as decoders. The probabilities Plm (·) and Ptm (·) are derived by training on monolingual and bilingual corpora. The language model used in this work was a trigram model trained by an existing in-house package, and the translation models considered were the IBM model 2, also trained using an in-house tool, and IBM model 4, trained using the GIZA++ package (Och and Ney, 2000).4 One important choice for rescoring is how to represent the sets of translations that are output from the base model. There are at least two possibilities: n-best lists— explicit enumerations of the candidate translations—and word graphs (Ueffing et al., 2002), which are capable of storing much larger sets of translations implicitly. Word graphs are potentially more powerful than n-best lists, but they are also more complex to implement. Furthermore, they constrain the rescoring layer to respect the factorization inherent in the graph. Because of these problems, we chose a simple n-best list"
2003.mtsummit-papers.15,P02-1038,0,0.0861279,"in the NIST evaluation was a desire to learn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted b"
2003.mtsummit-papers.15,soricut-etal-2002-using,0,0.0119268,"tion was a desire to learn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted by an obvious corollary"
2003.mtsummit-papers.15,W02-1021,0,0.0431082,"earn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted by an obvious corollary of the statistical MT"
2005.jeptalnrecital-long.24,J93-2003,0,0.0126525,"Missing"
2005.jeptalnrecital-long.24,P01-1030,1,0.892376,"Missing"
2005.jeptalnrecital-long.24,P04-1064,1,0.882028,"Missing"
2005.jeptalnrecital-long.24,P99-1041,0,0.0877922,"Missing"
2005.jeptalnrecital-long.24,W02-1018,0,0.0264918,"Missing"
2005.jeptalnrecital-long.24,P00-1056,0,0.327309,"Missing"
2005.jeptalnrecital-long.24,J04-4002,0,0.0723566,"Missing"
2005.jeptalnrecital-long.24,W99-0604,0,0.111557,"Missing"
2005.jeptalnrecital-long.24,P03-1021,0,0.0362064,"Missing"
2005.jeptalnrecital-long.24,P02-1040,0,0.072645,"Missing"
2005.jeptalnrecital-long.24,N03-2036,0,0.0275543,"Missing"
2007.mtsummit-papers.34,J93-2003,0,0.0170215,"Missing"
2007.mtsummit-papers.34,2006.eamt-1.27,0,0.103424,"it documents, see which errors crop up over and over (these will be different for any given system/domain pair), and begin to emulate what the human is doing.” (p. 779) Their paper does not explore that possibility in any detail, though. (Allen and Hogan, 2000) propose the development of a “processing engine that could automatically fix up machine translation raw output before such texts are even given to a human posteditor”. They are not very explicit about how such an engine would operate but their discussion suggests that it would be fed with manually developed sets of post-editing rules. (Elming, 2006) presents the first published results on the use of an APE module to correct MT output. He uses transformation-based learning to correct the output of the Patrans RBMT system and reports a 4.6 point increase in BLEU score. In (Simard et al., 2007a) we presented a set of experiments on using standard phrase-based SMT technology to build an APE module for a classical rule-based MT system. The APE task is then viewed as a machine translation task in which our SMT system “translates” from the language of RBMT outputs into the language of their manually post-edited counterparts. In the experiments"
2007.mtsummit-papers.34,W06-1607,0,0.0266641,"Missing"
2007.mtsummit-papers.34,N03-1017,0,0.0661419,"Missing"
2007.mtsummit-papers.34,koen-2004-pharaoh,0,0.0341227,"steps: preprocessing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven re-scoring to choose the best hypothesis. For languages such as French and English, the first of these steps (tokenization) is mostly a straightforward process and we do not describe it any further here. Decoding is the central task in SMT, involving a search for the hypotheses that have highest probabilities of being translations of the current source sentence according to a model of . PORTAGE implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose source-language part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. PORTAGE’s model for is a log-linear combination of four main components: one or more -gram targetlanguage models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrase-based translation model is similar to that of"
2007.mtsummit-papers.34,W02-1018,0,0.0269009,"osteditor under a range of different MT conditions: SMT versus RBMT, non-adapted versus adapted RBMT and presence or absence of an APE module. Our main finding is that in combination with the SMT-based APE module, the vanilla RBMT system performs almost as well as its manually adapted version. In other words, the APE has succeeded in capturing whatever benefits were brought by manual system adaptation. In section 5 we discuss that result before concluding the paper. 2 SMT and Automatic Post-Editing The work reported here is based on the paradigm of phrasebased statistical machine translation (Marcu and Wong, 2002; Koehn et al., 2003). Our PORTAGE SMT system (Sadat et al., 2005) is a typical exemplification of that paradigm. In recent months, we tested it in the role of an APE module for a commercial RBMT system (Simard et al., 2007a), and in this paper we show that APE constitutes a good way to adapt the RBMT system to a new domain. 2.2 Previous results on SMT-based APE 2.1 The PORTAGE SMT system PORTAGE is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005).1 Like other SMT systems, it learns to translate from existin"
2007.mtsummit-papers.34,P03-1021,0,0.0269477,"-linear combination of four main components: one or more -gram targetlanguage models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrase-based translation model is similar to that of Koehn, with the exception that phrase probability estimates are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the log-linear model are set using Och’s minimum error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate -best translation hypotheses, that are representative of the entire decoding            1 search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. To improve raw output from decoding, PORTAGE relies on a re-scoring strategy: given a list of -best"
2007.mtsummit-papers.34,P02-1040,0,0.0898536,"Missing"
2007.mtsummit-papers.34,W05-0822,0,0.035014,", non-adapted versus adapted RBMT and presence or absence of an APE module. Our main finding is that in combination with the SMT-based APE module, the vanilla RBMT system performs almost as well as its manually adapted version. In other words, the APE has succeeded in capturing whatever benefits were brought by manual system adaptation. In section 5 we discuss that result before concluding the paper. 2 SMT and Automatic Post-Editing The work reported here is based on the paradigm of phrasebased statistical machine translation (Marcu and Wong, 2002; Koehn et al., 2003). Our PORTAGE SMT system (Sadat et al., 2005) is a typical exemplification of that paradigm. In recent months, we tested it in the role of an APE module for a commercial RBMT system (Simard et al., 2007a), and in this paper we show that APE constitutes a good way to adapt the RBMT system to a new domain. 2.2 Previous results on SMT-based APE 2.1 The PORTAGE SMT system PORTAGE is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005).1 Like other SMT systems, it learns to translate from existing parallel corpora. The system translates text in three main steps"
2007.mtsummit-papers.34,W07-0728,1,0.876832,"llen and Hogan, 2000) propose the development of a “processing engine that could automatically fix up machine translation raw output before such texts are even given to a human posteditor”. They are not very explicit about how such an engine would operate but their discussion suggests that it would be fed with manually developed sets of post-editing rules. (Elming, 2006) presents the first published results on the use of an APE module to correct MT output. He uses transformation-based learning to correct the output of the Patrans RBMT system and reports a 4.6 point increase in BLEU score. In (Simard et al., 2007a) we presented a set of experiments on using standard phrase-based SMT technology to build an APE module for a classical rule-based MT system. The APE task is then viewed as a machine translation task in which our SMT system “translates” from the language of RBMT outputs into the language of their manually post-edited counterparts. In the experiments reported in the paper, the addition of such an APE module resulted in very substantial gains in translation quality (figures will be provided below). Once we realize that APE is feasible, we are led to ask to what extent it could be used as a mea"
2007.mtsummit-papers.34,2006.amta-papers.25,0,0.104635,"Missing"
2009.mtsummit-papers.14,W09-0405,0,0.0126804,"for each input sentence q, we find the single best matching pair p = hs, ti from the TM, using the TMem program. We then compute the set Tp of all admissible phrase pairs from p as in Och & Ney (2004), but without limiting phrase size. In particular, this means that if we find an exact matching sentence in the TM, then this is presented as a “pretranslated phrase” to the MT system, regardless of its size. The content of this TM-based phrasetable is used at decoding time as an additional source of phrases, and its weight in the loglinear model is determined by MERT on a heldout set, as usual. (Chen et al. (2009) propose a similar method for combining the output of multiple MT systems.) We can use a similar trick to provide the MT system with a TM-based language model: use the target-language portion of the single best match from the TM to train a sentence-specific language model, which we use as an additional feature function in the model. Finally, while the MT system may have all the right phrases to put together a perfect match from the TM, it may opt for alternate phrase translations, or simply order the phrases differently. One way to coerce the system into generating something that resembles a T"
2009.mtsummit-papers.14,W06-1607,0,0.0209925,"Configuration All instances of the MT system discussed here were essentially trained in the same way. Distinct systems were trained on each of the three corpora. Phrase tables and language models were extracted from each corpus’s train sets. HMM translation models were used to perform word alignments on the training corpus, which were symmetrized by the usual “diagand” algorithm prior to phrase extraction. Phrases were limited to a maximum of 8 words. In addition to raw joint translation probabilities, various smoothed conditional probabilities were used as distinct phrase feature functions (Foster et al., 2006). The target-language models were 4-gram models with Kneyser-Ney smoothing. The dev sets were then used to optimize the model’s decoding and rescoring parameters. In addition to the decoding feature functions, rescoring also relied on IBM-Model based features, plus some nbest-post features, some features that check for mismatched parentheses, quotes etc. Parameter optimization was performed using minimum error-rate training (MERT) on BLEU scores (Och, 2003). 4.3 Translation Results Table 1 presents the performance of the main systems we tested on each corpus’ test set. Results are presented bo"
2009.mtsummit-papers.14,W02-1020,0,0.0320072,"th that of a TM when high-similarity material exists in the training data; it also implies providing the MT system with a component that is capable of filtering out machine translations that are less likely to be useful. We propose solutions to both problems, and evaluate their impact on three different data sets. Our results indicate that the proposed approach leads to systems that produce better output than a TM, for a larger portion of the source text. 1 Introduction While much research effort has been devoted to finding ways of combining the strengths of human and machine translation (see Foster (2002) for an overview), in the end, many human translators still find MT technology to be unhelpful when it comes to producing high-quality translations. As a result, MT is not yet well-established within computerassisted translation (CAT) environments, at least when compared to the much simpler technology of translation memory (TM). While it is tempting to view TM as a simplistic form of example-based MT, or a variation on phrasebased MT (or, conversely, to see phrase-based MT as a natural evolution of TM), TM has some notable advantages over most data-driven MT systems. The most obvious is its ab"
2009.mtsummit-papers.14,1996.eamt-1.12,0,0.683893,"achine translation is to succesfully integrate the CAT environment, it should begin by catching up with TM on these aspects. We argue that this requires two things: (1) the MT system should behave more like a TM in the presence of high-similarity matches. In practice, this can be achieved by combining the two technologies, i.e. by building a combination MT system that incorporates a TM component. And (2) just like existing TM systems, the combined MT system should provide the user with means to filter out translations that are less likely to be useful. It has sometimes been proposed (see e.g. Heyn (1996)) that MT should be used within a CAT environment only when the TM fails to retrieve something useful. Unfortunately, this has the effect of relegating the MT system to the task of translating only the sentences that are most unlike previously seen ones. For data-driven systems, this turns out to mean translating only the “harder” sentences and missing the chance to do a better job than the TM. The reason why MT is often treated as a last resort lies in the fact that translators tend to see its performance as unpredictable and, as a result, overly likely to waste their time. In other words, to"
2009.mtsummit-papers.14,N03-1017,0,0.0294183,"starting point. In what follows, we propose relatively simple ways of attaining the two above goals: Section 2 deals with combining TM with phrase-based MT, and Section 3 discusses estimating translation usefulness. Our experiments and results are presented in Section 4. 2 Combining Machine Translation and Translation Memory In this section, we examine the question of how to combine a TM with a phrase-based MT system. Our goal is to obtain an MT system that is capable of taking advantage of exact or close matches in the TM. The methods described here assume a standard phrase-based SMT system (Koehn et al., 2003) employing a log-linear combination of feature functions. Unfortunately, there are no such “standard” TM systems; therefore we had to construct our own, which we now describe before discussing combination strategies. 2.1 the query and each source-language sentence in the corpus, and retains the source-target pair from the corpus with the smallest distance. This is admittedly very inefficient (although manageable if one resorts to simple optimizations and some parallelization), and while Levenshtein distance may not be the nec plus ultra in TM technology, this is possibly compensated by the fac"
2009.mtsummit-papers.14,2005.mtsummit-papers.11,0,0.029457,"lines were excessively long and caused problems in the MT system, so we kept only those pairs of sentences shorter than 800 characters (that’s still a healthy 150-200 words...); in the process, we also removed pairs in which one sentence was empty. Finally, the data was split into three parts: train, dev and test. The last two contain 3000 pairs of sentences each, which leaves close to 330k sentence pairs in train. Europarl The Europarl corpus is a collection of text from the proceedings of the European Parliament. We used the French and English versions of this corpus as prepared by Philipp Koehn (2005) for training SMT systems. This is a well-known dataset that is commonly used in MT experiments. The Eu4.2 System Configuration All instances of the MT system discussed here were essentially trained in the same way. Distinct systems were trained on each of the three corpora. Phrase tables and language models were extracted from each corpus’s train sets. HMM translation models were used to perform word alignments on the training corpus, which were symmetrized by the usual “diagand” algorithm prior to phrase extraction. Phrases were limited to a maximum of 8 words. In addition to raw joint trans"
2009.mtsummit-papers.14,leplus-etal-2004-weather,0,0.0230826,"d MT system. Somewhat parallel to this, a number of authors have examined specifically the MT-TM tandem. Much work in this line actually aims at producing better MT systems, as opposed to integrating MT into a CAT environment. For instance, Vogel et al. (2004) present a SMT system which incorporates a translation memory component. The system outputs exact matches from the TM without further processing. Automatic “repairs” are performed for matches that display a single “error” (insertion, deletion or substitution): this operates essentially like a onestep greedy modification on the TM target. Leplus et al. (2004) show how a translation memory equipped with a minimal hand-built alteration mechanism for numbers, etc., can be quite successfully used as a MT system for repetitive texts such as weather reports. The Dynamic Translation Memory (DTM) method (Bic¸ici and Dymetman, 2008) also aims at improving the output of a phrase-based SMT system using a TM: Given a new sentence q to be translated, they: 1. Find the best matching pair for q in the TM: hs, ti; 2. Identify the longest common subsequence between q and s: Ps ; 3. Using word-alignment, identify the corresponding subsequence in t: Pt ; 4. Dynamica"
2009.mtsummit-papers.14,P04-1063,0,0.0430742,"rudimentary translation model. A step forward in the same direction is proposed by Gotti et al. (2005), who suggest restricting phrases to syntactic chunks or treelets. 2.3 Combination Strategies In what follows, we propose two different strategies which we believe are better suited to the particular characteristics of TM and phrase-based MT. 2.3.1 System selection: β-combination The simplest form of MT-TM combination is probably one in which either the MT or the TM output is produced, depending on the context. This strategy has been proposed for combining multiple MT systems, for example in Nomoto (2004). Many factors can be taken into account when deciding which system is best for a given input; this decision process can be viewed as a standard classification task, and many standard machine learning methods can be applied. An extremely simple, yet effective approach is to base the decision solely on the similarity between the query and the closest match from the translation memory, as proposed by Vogel et al. (2004): above a given similarity threshold β, the combined system outputs the translation from the TM, otherwise it produces the MT output. We call the systems based on this combination"
2009.mtsummit-papers.14,J04-4002,0,0.0522699,"g. BLEU or WER) on a held-out sample. 2.3.2 TM-based Translation Feature Functions As mentioned earlier, the method of Bic¸ici and Dymetman (2008) is not directly applicable to standard phrase-based systems, because it relies crucially on discontiguous phrases. However, we can propose a very similar approach, in which multiple phrases are extracted from the best TM match and fed to the MT system. More precisely, for each input sentence q, we find the single best matching pair p = hs, ti from the TM, using the TMem program. We then compute the set Tp of all admissible phrase pairs from p as in Och & Ney (2004), but without limiting phrase size. In particular, this means that if we find an exact matching sentence in the TM, then this is presented as a “pretranslated phrase” to the MT system, regardless of its size. The content of this TM-based phrasetable is used at decoding time as an additional source of phrases, and its weight in the loglinear model is determined by MERT on a heldout set, as usual. (Chen et al. (2009) propose a similar method for combining the output of multiple MT systems.) We can use a similar trick to provide the MT system with a TM-based language model: use the target-languag"
2009.mtsummit-papers.14,P03-1021,0,0.020821,"raw joint translation probabilities, various smoothed conditional probabilities were used as distinct phrase feature functions (Foster et al., 2006). The target-language models were 4-gram models with Kneyser-Ney smoothing. The dev sets were then used to optimize the model’s decoding and rescoring parameters. In addition to the decoding feature functions, rescoring also relied on IBM-Model based features, plus some nbest-post features, some features that check for mismatched parentheses, quotes etc. Parameter optimization was performed using minimum error-rate training (MERT) on BLEU scores (Och, 2003). 4.3 Translation Results Table 1 presents the performance of the main systems we tested on each corpus’ test set. Results are presented both in terms of BLEU scores and word error-rate (WER). The first line shows the results of using only a TM, namely our TMem program. These results reveal some fundamental differences between our different corpora with regard to internal repetition. Europarl is a typical example of a corpus for which TM’s are mostly useless; as it contains very little sentence repetition, TMem’s performance is extremely low. In comparison, TMem performance on the Hansard corp"
2009.mtsummit-papers.14,quirk-2004-training,0,0.0334733,"While there are different ways to measure this, the Levenshtein-based sim function of Section 2.1 is appealing because it is simple to compute and has an intuitive interpretation. Furthermore, when applied to target-language translations, it is related in obvious ways to the wellknown word-error rate (WER) metric: sim(t, r) = 1 − W ER(t, r), where r is a reference (correct) translation and t is the translation whose usefulness we wish to estimate. Estimating target similarity is quite obviously related to work on confidence estimation for machine translation. As proposed for the latter task (Quirk, 2004; Gandrabur et al., 2006), we take a supervised machine learning approach to the problem: the idea is to base a target quality estimation (QE) function on a variety of features of the input and output texts, and learn output values from training data annotated with target similarity values. We consider the following input features: • length of the source input q, best-matching TM source s, corresponding target t and MT output p; • probabilities PSLM (q), PSLM (s), PT LM (t) and PT LM (p), according to N -gram language models, trained on the source (“SLM”) or target (“TLM”) portions of the tran"
2009.mtsummit-papers.14,2001.mtsummit-ebmt.5,0,0.141449,"Missing"
2009.mtsummit-papers.14,2001.mtsummit-papers.60,1,0.829259,"ith “gaps” both in the source and the target. This makes it possible in step 2 above to build a phrase pair from subsequences (not substrings) which covers as much commonalities as possible between q and s. Then there is also some work that aims at improving TM systems, using MT technology. Sch¨aler (2001) proposes the idea of a (syntactic) phrase-based translation memory that would be able to mix-and-match phrases from different TM matches, in order to piece together a proposal for a previously unseen sentence, EBMT fashion. But in practice, this is more a step for TM in the direction of MT. Simard and Langlais (2001) evaluate the potential of complementing a translation memory with a phrasetable. Phrases are proposed to the user based on a maximum cover of the input, taking into account a rudimentary translation model. A step forward in the same direction is proposed by Gotti et al. (2005), who suggest restricting phrases to syntactic chunks or treelets. 2.3 Combination Strategies In what follows, we propose two different strategies which we believe are better suited to the particular characteristics of TM and phrase-based MT. 2.3.1 System selection: β-combination The simplest form of MT-TM combination is"
2009.mtsummit-papers.14,H05-1095,1,0.863203,"i and Dymetman, 2008) also aims at improving the output of a phrase-based SMT system using a TM: Given a new sentence q to be translated, they: 1. Find the best matching pair for q in the TM: hs, ti; 2. Identify the longest common subsequence between q and s: Ps ; 3. Using word-alignment, identify the corresponding subsequence in t: Pt ; 4. Dynamically add the phrase pair hPs , Pt i to the translation system’s phrasetable They then translate as usual. This strategy crucially depends on an essential characteristic of the MT system in which it is implemented: the MATRAX phrase-based SMT system (Simard et al., 2005) can handle non-contiguous phrase pairs, i.e. phrases with “gaps” both in the source and the target. This makes it possible in step 2 above to build a phrase pair from subsequences (not substrings) which covers as much commonalities as possible between q and s. Then there is also some work that aims at improving TM systems, using MT technology. Sch¨aler (2001) proposes the idea of a (syntactic) phrase-based translation memory that would be able to mix-and-match phrases from different TM matches, in order to piece together a proposal for a previously unseen sentence, EBMT fashion. But in practi"
2009.mtsummit-papers.14,steinberger-etal-2006-jrc,0,0.0597963,"approximately 1500 sentences, while the train set contains approximately 5.2 million sentence pairs (just over 100 million English words). 4.1 Experiments Corpora Our experimental data consists in French-English bilingual corpora. In all our experiments, we assumed English to be the source language and French to be the target. All experiments were performed on three distinct data sets, namely the Hansard, Acquis, and Europarl corpora. Acquis The Acquis corpus is the European Community’s “Acquis communautaire”, prepared and distributed by the Community’s Joint Research Centre in Ispra, Italy (Steinberger et al., 2006), version 2.2. We chose this corpus because it is rather technical, with lots of internal repetition. This makes it a good candidate for TM. While the corpus is available in over 20 languages, only the English and French versions were used here. The data had to be: converted from XML to plain line-for-line sentence alignment format; tokenized and re-segmented into sentences; re-aligned at the sentence level, using a variant of the Gale & Church method (the provided alignment was at the paragraph level); and lowercased. Even after re-alignment, some lines were excessively long and caused proble"
2009.mtsummit-papers.14,2004.iwslt-evaluation.11,0,0.134983,"e of sim as α (alpha): if sim(q, s) ≥ α, then TMem outputs the translation of s, otherwise it outputs nothing. 2.2 Related Work There is a rapidly growing body of work on MT system combination (see e.g. Callison-Burch et al. (2009)), and many of the methods proposed in the literature could be applied to the specific task of combining a TM with a phrase-based MT system. Somewhat parallel to this, a number of authors have examined specifically the MT-TM tandem. Much work in this line actually aims at producing better MT systems, as opposed to integrating MT into a CAT environment. For instance, Vogel et al. (2004) present a SMT system which incorporates a translation memory component. The system outputs exact matches from the TM without further processing. Automatic “repairs” are performed for matches that display a single “error” (insertion, deletion or substitution): this operates essentially like a onestep greedy modification on the TM target. Leplus et al. (2004) show how a translation memory equipped with a minimal hand-built alteration mechanism for numbers, etc., can be quite successfully used as a MT system for repetitive texts such as weather reports. The Dynamic Translation Memory (DTM) metho"
2012.amta-papers.26,C00-1006,0,0.347165,"n-fr fWER (q, s) = 1 − min(1, WER(s, q)) NIST also produces values that are unbounded in the positive. The solution we propose is to divide the outcome by the largest possible obtainable value for the current query, i.e. the value that would be produced by an exact match: fNIST (q, s) = 5 5.1 EMEA JRC-Acquis Experiments Data Evaluation Methodology Little attention has so far been devoted to the evaluation of TM systems. Gow (2003) proposes a general methodology, but which is very much usercentered, and that focuses on complete CAT environments rather than specifically on the TM functionality; Baldwin and Tanaka (2000) and Whyman and Somers (1999) both propose methods that are based on recall and precision, but these have not been widely used or evaluated. Test words 28 817 28 365 26 715 30 471 28 054 27 426 16 514 19 260 Table 1: Experimental Data NIST(s, q) NIST(q, q) We performed experiments to assess the performance of each MT evaluation metric as TM similarity function. Experiments were done under different conditions, corresponding to different corpora and language pairs. While most of the metrics we use are language-agnostic, Meteor relies on languagespecific resources which are not available in all"
2012.amta-papers.26,P01-1004,0,0.184721,"obody really knows how they work. In particular, the details of the similarity function at the heart of TMs are well-kept commercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It shoul"
2012.amta-papers.26,W05-0909,0,0.060822,"nts the number of chunks t and r would need to be broken into to allow them to be rearranged with no crossing alignments, Pβ,γ = 1 − γ(chunks/matches)β . Meteorα,β,γ = Fα × Pβ,γ Word-alignments are not restricted to surfacesimilar forms, as Meteor can rely on a lemmatiser and other linguistic resources (Wordnet and paraphrase tables) to account for semantic equivalence. In this study, we experiment with this metric used in two different modes6 : • without any linguistic resources; we refer to this as Vanilla-Meteor (or VMeteor). In this mode, the metric behaves more like its earlier versions (Banerjee and Lavie, 2005). • with all linguistic resources; we refer to this simply as Meteor. 4 MT Evaluation Metrics as TM Similarity Functions Since automatic MT evaluation methods are based on text-similarity metrics, it seems natural to use them in TMs as well. The idea is to replace the reference r and the system output t in the evaluation metrics (Section 3) with the TM’s query q and source-language segment s (Section 2), respectively. The four evaluation metrics described above are well-known to the MT community, and represent different perspectives on textual similarity. Given the large number of existing met"
2012.amta-papers.26,N10-1080,0,0.017332,"st similarity. Much more efficient implementations are of course possible, based on a two-pass strategy, where an efficient search method – for example (Koehn and Senellart, 2010b) – produces a reduced set of candidates, which are then reranked using one of the similarity functions proposed here. 7 TER would arguably have been a better choice, being a well-established metric in the MT community. In practice, however, when attempting to use existing TER implementations for our purposes, we ran into a number of technical difficulties; furthermore, WER and TER are known to behave very similarly (Cer et al., 2010). Open-source implementations exist in the public domain for all our MT evaluation metrics. Using these directly within an exhaustive TM search is certainly not optimal, but it is usually feasible. For instance, the publicly available Meteor software has options -nBest and -oracle, which allow to compare the reference (in our case, the query) to multiple system translations (in our case: candidate TM source-language matches) and output the one that produces the highest score. We used that implementation of Meteor for our experiments, but produced our own implementations of BLEU, NIST and WER."
2012.amta-papers.26,W11-2107,0,0.0192533,"he candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP · exp N ). (1) n=1 NIST: (Doddington, 2002) A variant of BLEU, in which n-gram precisions are averaged with harmonic rather than geometric mean; it also uses a slightly different brevity penalty   |t| (2) BP = exp β log min( , 1) |r| and, more importantly, weights n-gram matches by how informative they are; the informativeness of an 1 ...wn−1 ) n-gram w1 ...wn is estimated as log count(w count(w1 ...wn ) where counts are typically obtained from the reference translations. Meteor: (Denkowski and Lavie, 2011) Based on a one-to-one alignment between words of t and r, giving preference to alignments with less crossing alignments, Meteor computes unigram precision P and recall R, which are then combined in a weighted harmonic mean, Fα = P R/(αP + (1 − α)R), and scaled by a reordering penalty, which counts the number of chunks t and r would need to be broken into to allow them to be rearranged with no crossing alignments, Pβ,γ = 1 − γ(chunks/matches)β . Meteorα,β,γ = Fα × Pβ,γ Word-alignments are not restricted to surfacesimilar forms, as Meteor can rely on a lemmatiser and other linguistic resources"
2012.amta-papers.26,D12-1058,1,0.83046,"best lists of matches, which means that a general optimization scheme such as Minimum-Error Rate Training (MERT) could be applied in a relatively straightforward manner to optimize numerical parameters with regard to a given evaluation metric9 . Meteor also relies on a paraphrase table to discover semantic similarities. One possible way of optimizing the performance of that metric as a TM similarity function is to provide it with domain-specific paraphrases. In a preliminary experiment along this line, we created domain-specific paraphrase tables from each TM using the technique described in (Fujita et al., 2012), and used these with Meteor instead of the provided table. In practice, in-domain paraphrases do not lead to measurable gains or losses 9 Here, we overlook the integral nature of some parameters, such as N for BLEU and NIST, which raises problems for standard optimization techniques. Then again, using MERT to optimize N for BLEU is clearly overkill. Example 1 This is the process we are commencing. I suggest that we perhaps continue the work we have started. CMeteor This is the point at which we must start. WER This is the stage we are at. BLEU This is the stage we are at. NIST This is the sta"
2012.amta-papers.26,C10-2043,0,0.0897039,"Missing"
2012.amta-papers.26,2010.jec-1.4,0,0.392243,"mercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at this stage, we are not concer"
2012.amta-papers.26,2010.amta-papers.2,0,0.387511,"mercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at this stage, we are not concer"
2012.amta-papers.26,P07-2045,0,0.00481744,"produced it, and from there establish whether or not it is reliable. In other words, the TM technology is one that users can understand and trust. As a result, TMs have succeeded where most of MT has failed so far: in establishing themselves as a must-have item in translators’ toolboxes. Recently, however, this situation appears to be changing, as we see a surge of interest for MT among the translation community. This renewed enthusiasm is fueled in part by the increased quality of the output of MT systems, but also by the availability of reliable free software, most notably the Moses system (Koehn et al., 2007). Yet, because of the inherent qualities of TM outlined above, and their solid entrenchment in the translation community, it is unlikely that MT will completely replace TM, at least not in the near future. Instead, we will probably see them co-exist for some time. This phenomenon is already visible, as many work environments for translators now incorporate both TM and MT technology. See, for example, the products of SDL Trados1 , Multicorpora2 or the Google Transla1 2 http://www.trados.com http://www.multicorpora.com tor Toolkit3 . As researchers tackle the question of properly integrating the"
2012.amta-papers.26,2005.mtsummit-papers.11,0,0.0185458,"ds 28 817 28 365 26 715 30 471 28 054 27 426 16 514 19 260 Table 1: Experimental Data NIST(s, q) NIST(q, q) We performed experiments to assess the performance of each MT evaluation metric as TM similarity function. Experiments were done under different conditions, corresponding to different corpora and language pairs. While most of the metrics we use are language-agnostic, Meteor relies on languagespecific resources which are not available in all languages. For this reason, we restricted our experiments to English, French, German and Spanish. Our four datasets are drawn from the Europarl v.6 (Koehn, 2005), OPUS corpus (Tiedemann, 2009) (corpora ECB, featuring content from the European Central Bank and EMEA, from the European Medicines Agency) and the JRC-Acquis v.2.2 (Steinberger et al., 2006). All bilingual corpora are available aligned at the sentence level. From each corpus, we randomly sampled 1000 pairs of segments, to be used as test data; the rest was used to build translation memories. All experiments were performed “in-domain”, i.e. for any given experiment, test and TM data always come from the same corpus. Table 1 provides additional details. 5.2 ECB TM (“Train”) segments words 1.8M"
2012.amta-papers.26,C04-1072,0,0.0231565,"cased texts. The differences with true-cased texts are minimal, however; in the end, it’s probably a matter of user-preference. In all that follows, we assume lower-cased source-language texts8 . Both BLEU and NIST are designed to evaluate document-level MT quality, and are not well adapted to finer-grained evaluation; for example, if t and r do not have at least one 4-gram in common, then the product in Equation (1) goes to zero, and therefore the whole BLEU score. To compensate for this problem, it is common to use a “smoothed” version of the score, in which 1 is added to all n-gram counts (Lin and Och, 2004). BLEU and Meteor naturally produce scores comprised between 0 and 1, and can be used directly as similarity functions in a TM. NIST and WER are not as well-behaved: WER can produce scores larger than 1, when the number of edits required to convert t into r (or s into q) is larger than the number of words in r. In a TM setting, where such matches are unlikely to be useful, this problem is easily remedied by capping the value at 1. Also, because WER 8 Note, however, that all target-language evaluations were performed on true-cased texts. is a distance metric rather than a similarity metric, we"
2012.amta-papers.26,N07-1006,0,0.0184426,"that rely on linguistic resources, such as Meteor. In practice, they are easy to implement and produce results comparable to WER (on which existing commercial systems are believed to be based), especially in high-similarity situations, where it counts for real-life TM usage. Most metrics can be tuned, to optimize performance of TM systems for specific text domains. Optimization methods commonly used in statistical machine translation could easily be adapted to this task. One related aspect that we have not yet examined is the combination of different metrics into a single similarity function (Liu and Gildea, 2007). In a similar vein, preliminary experiments suggest that customizing linguistic resources such as paraphrase tables could help in better leveraging the contents of the TM when appropriate metrics are used, such as Meteor or TERp. Extracting domain-specific paraphrases is one possible avenue, but in a TM perspective, it would be interesting to extend similarity to other semantic relations besides synonymy, e.g. antonymy, hyponymy, etc. These are areas we hope to explore further in the near future. When evaluating the performance of TM systems using MT evaluation metrics, in general, we find th"
2012.amta-papers.26,N12-1019,0,0.0458782,"Missing"
2012.amta-papers.26,P03-1021,0,0.0114132,"f the query (average length ratios are given in Table 3). This contrasts with WER (red dots), which naturally favors segments that are much shorter than the query, and with the Meteor metrics (purple and black), which tend to produce source segments that are much longer than the query. On the target side, shorter TM matches such as those produced by the WER similarity function will be penalized at evaluation time by the BLEU and NIST 150 Lengh of best match To those with a background in statistical machine translation, who are familiar with the general approach of Minimum Error-Rate Training (Och, 2003), this may seem like a very natural outcome at first sight. After all, using any given metric as a similarity function is somewhat like optimizing the behavior of the system for that evaluation metric. The subtle difference here is that in a TM, similarity is measured in the source language. If we take the example of BLEU, this means that maximizing ngram precision relative to the source-language query somehow results in the n-gram precision being maximized in the target-language as well. 120 90 60 30 0 0 30 60 90 120 150 Length of query Figure 1: Length (in words) of TM best match source segm"
2012.amta-papers.26,P02-1040,0,0.105449,"en used extensively in speech recognition. It is computed as the word-based Levenshtein distance between t and r, divided by the number of words in the reference: |r|. Several variants exist, most notably TER (Translation Edit – or Error – Rate), in which local swaps of sequences of words are allowed. TER itself has two variants: HTER, in which the reference translation is manually produced by minimally post-editing the MT output under evaluation (Snover et al., 2006); and TERp, which also relies on a table of paraphrases to detect semantically equivalent matches (Snover et al., 2009). BLEU: (Papineni et al., 2002) The mother of all MT evaluation metrics: BLEU measures n-gram precision, i.e. the proportion of word n-grams of t that are also found in r. These n-gram precisions pn are calculated separately for values of n ranging from 1 to N (typically N = 4), and then combined using a geometric mean. The score is scaled by a brevity penalty if the candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP · exp N ). (1) n=1 NIST: (Doddington, 2002) A variant of BLEU, in which n-gram precisions are averaged with harmonic rather than geometric mean; it also"
2012.amta-papers.26,2009.mtsummit-papers.14,1,0.947549,"rt of TMs are well-kept commercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at th"
2012.amta-papers.26,2006.amta-papers.25,0,0.11355,"focus on four well-known MT evaluation metrics: WER, BLEU, NIST and Meteor. We briefly review these here: WER (Word Error Rate): This metric has been used extensively in speech recognition. It is computed as the word-based Levenshtein distance between t and r, divided by the number of words in the reference: |r|. Several variants exist, most notably TER (Translation Edit – or Error – Rate), in which local swaps of sequences of words are allowed. TER itself has two variants: HTER, in which the reference translation is manually produced by minimally post-editing the MT output under evaluation (Snover et al., 2006); and TERp, which also relies on a table of paraphrases to detect semantically equivalent matches (Snover et al., 2009). BLEU: (Papineni et al., 2002) The mother of all MT evaluation metrics: BLEU measures n-gram precision, i.e. the proportion of word n-grams of t that are also found in r. These n-gram precisions pn are calculated separately for values of n ranging from 1 to N (typically N = 4), and then combined using a geometric mean. The score is scaled by a brevity penalty if the candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP ·"
2012.amta-papers.26,steinberger-etal-2006-jrc,0,0.0607128,"Missing"
2012.amta-papers.26,H93-1040,0,0.355121,"Missing"
2012.amta-papers.26,1993.mtsummit-1.24,0,\N,Missing
2013.mtsummit-papers.24,2011.mtsummit-papers.35,0,0.333245,"Missing"
2013.mtsummit-papers.24,W12-3156,1,0.792219,"r addresses this gap, by proposing a mechanism that automatically propagates post-editor corrections to further machine-translated sentences within a document. We call this process Post-edit Propagation, or PEPr for short. One recurrent complain from post-editors is that they often have to fix the same error repeatedly. Repeated errors can happen for a number of reasons: if the MT system’s training data was too small or heterogeneous, or if it was from a different domain as the document under consideration, then it is not uncommon for a given word or phrase to be systematically mistranslated. Carpuat & Simard (2012) have shown that SMT systems tend to be highly consistent, meaning that multiple occurrences of any given source-language word or phrase will tend to be translated by the same targetlanguage phrase. If that translation happens not to be appropriate in the current context, i.e. if the system is consistently wrong, then the post-editor will need to fix that translation several times. As pointed out by Lagoudaki (2008), post-editing systems should have the ability to “learn from the decisions/choices made by users (e.g. which potential translations are preferred, which were rejected and why), so"
2013.mtsummit-papers.24,N10-1080,0,0.0117456,"We further discuss the effect of varying this parameter in Section 3.4. Table 2 presents the results of these experiments. The impact of PEPr is measured in terms of WER and BLEU gain (for convenience, we report WER scores as 100-WER, so that larger values denote better translations, and negative “gains” can be interpreted as “losses”) 3 . For each corpus and language, we first report the scores obtained by the 3 TER would arguably have been a better metric to evaluate the potential of our approach in a post-editing setting; in practice, however, WER is known to behave very similarly to TER (Cer et al., 2010). Corpus ECB en→fr 100-WER BLEU MT +PEPr-MTLM +PEPr-GLM MT +PEPr-MTLM +PEPr-GLM 32.24 +5.71 +6.53 32.65 +3.45 +5.38 25.87 +6.16 +7.08 22.56 +7.42 +9.27 MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM Science Abstracts en→fr MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM Science Digests en→fr MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM 32.75 +3.83 +4.76 30.05 +3.27 +4.56 25.05 +5.37 +6.56 25.13 +5.35 +7.44 37.00 -0.94 -0.10 39.64 +0.84 -0.50 24.00 +0.73 -0.16 24.82 +0.74 -0.04 36.96 +0.37 -1.88 39.68 +0.56 -0.64 23.93 +0.37 +0.09 24.81 +0.57 -0.16 fr→en EMEA en→fr System"
2013.mtsummit-papers.24,N12-1047,1,0.880515,"SMT, the reordering (or distortion) model controls the relative order of SL and TL words. Local word reorderings are typically captured within phrase pairs in the phrase table, and need not be handled by these models. Phrase-level reordering is already a complicated matter, and it is doubtful that we can reliably model it in this extreme sparse-data environment. We also conjecture that phrase-level reorderings are a rare event in post-editing, and therefore completely inhibit them for PEPr. Parameter Optimization Parameters of the APE systems’ log-linear model are optimized using batch-MIRA (Cherry and Foster, 2012). This procedure normally assumes a development set, which is repeatedly translated with a single translation system. In a PEPr setting, the system dynamically changes after each post-edited sentence. We nevertheless make the assumption that it is possible to find a set of parameters that is globally optimal under these varying conditions. We modify the decoding step in the optimization loop so that each development set sentence is translated by a different system, as described above. In our current setting, the parameters controlling the LM mixture are optimized manually on the development se"
2013.mtsummit-papers.24,W07-0717,1,0.83565,"Missing"
2013.mtsummit-papers.24,D11-1084,0,0.0151339,"updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely small amounts of training data, we believe that it can be implemented into an efficient,"
2013.mtsummit-papers.24,2010.amta-papers.21,0,0.568393,"Automatic post-editing using SMT was proposed in Simard et al. (2007). The idea of dynamically updating an APE system after each sentence revised by a translator was the subject of an early proposal by Knight and Chander (1994). As far as we are aware, it has not been investigated experimentally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-b"
2013.mtsummit-papers.24,P07-2045,0,0.00533868,"n for the do-nothing option, because it is inhibited as soon as a word appears in the phrase table: if on its first occurrence a word w is post-edited to v, then that correction will automatically be applied the second time w is encountered. Instead, we explicitly include in the phrase table unedited versions of all input phrases. This can be achieved by adding the pair htk , tk i to Ak ’s training set; this way, the phrase table contains phrase pairs that explicitly sanction leaving a word or phrase unedited in the APE output. While phrase pair extraction is performed using standard methods (Koehn et al., 2007), a wordlevel alignment between machine translations ti and reference translations ri is required, which, in standard SMT (or APE) approaches, is computed using IBM-style translation models. In a PEPr scenario, there is typically too little data to train such a model: in many cases, the training corpus will be ridiculously small. But since the “source” and “target” are the same language, a straightforward alternative is to obtain the alignment from the minimal sequence of edits e = e1 ...em corresponding to the word-level Levenshtein distance between ti and ri . To achieve more precise alignme"
2013.mtsummit-papers.24,2008.amta-srw.4,0,0.0397365,"terogeneous, or if it was from a different domain as the document under consideration, then it is not uncommon for a given word or phrase to be systematically mistranslated. Carpuat & Simard (2012) have shown that SMT systems tend to be highly consistent, meaning that multiple occurrences of any given source-language word or phrase will tend to be translated by the same targetlanguage phrase. If that translation happens not to be appropriate in the current context, i.e. if the system is consistently wrong, then the post-editor will need to fix that translation several times. As pointed out by Lagoudaki (2008), post-editing systems should have the ability to “learn from the decisions/choices made by users (e.g. which potential translations are preferred, which were rejected and why), so that errors in future translation assemblies are reduced.” Developers of commercial translation memory (TM) systems have already taken note of this requirement. The idea behind TM technology is that translators should not have to translate the same material more than once. To achieve this, all translations are archived, new texts are systematically compared to the contents of the archive, and when segments are found"
2013.mtsummit-papers.24,N10-1062,0,0.204901,"entally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely"
2013.mtsummit-papers.24,W04-3225,1,0.68915,"nd it could also be interesting to leave its setting to the user, as a way of controlling how aggressive or conservative the system is. 4 Related Work Automatic post-editing using SMT was proposed in Simard et al. (2007). The idea of dynamically updating an APE system after each sentence revised by a translator was the subject of an early proposal by Knight and Chander (1994). As far as we are aware, it has not been investigated experimentally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editin"
2013.mtsummit-papers.24,N07-1064,1,0.942338,"weight of prior (background) and newly acquired (local) knowledge. We argue that the PEPr task can be effectively carried out by a phrase-based statistical MT system. The system is then effectively an automatic post-editing (APE) system with online learning capabilities. We detail how this idea can be realized (Section 2), then validate our design with experiments that simulate PE sessions (Section 3). Related work is reviewed in Section 4. 2 PEPr using Phrase-based SMT An APE system is one that performs transformations on MT output. If this APE system is based on a SMT system, as proposed in Simard et al. (2007), then it can be seen as a system that attempts to translate target-language (TL) MT output into proper (human quality) TL text. Typically, it will be trained on a corpus of post-edited MT: pairs of machine-translated sentences along with their post-edited counterparts. To perform post-edit propagation, we propose to use an SMT system in an APE setting: the system will be trained incrementally, using pairs of machine-translated and post-edited segments as they are produced. The whole process is assumed to take place within the context of a single document D. The general set-up is illustrated i"
2013.mtsummit-papers.24,W10-2602,0,0.012337,"ems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely small amounts of training data, we believe that it can be implemente"
2014.amta-researchers.6,2007.mtsummit-papers.1,0,0.0614351,"Missing"
2014.amta-researchers.6,W09-0432,0,0.0457879,"Missing"
2014.amta-researchers.6,W08-0309,0,0.0297546,"und in Table 1. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 71 Dataset Europarl EMEA GALE Language English-French French- English Chinese-English Segments 2M 837K 547K Source Tokens 55.5M 12.7M 17.7M Table 1: Experimental data Europarl This is release v7 of the well-known Europarl corpus (Koehn, 2005), in English and French. We arbitrarily set the source language to be English. We set aside 2000 randomly picked pairs of segments for tuning purposes. The test set for this dataset is the Europarl test set from the WMT 2008 shared task (Callison-Burch et al., 2008). EMEA This is release 0.3 of the EMEA dataset, from the OPUS Corpus (Tiedemann, 2009) in English and French. In this case, we chose French to be the source language. This is a parallel corpus made out of PDF documents from the European Medicines Agency. While highly technical, it is very repetitive by nature. The data is organized into individual documents. We respected these natural divisions when subsampling the data: we randomly picked 60 documents from the complete dataset, and assigned 30 for tuning and 30 for testing. We took at most 100 segments from each document, to avoid having a fe"
2014.amta-researchers.6,W12-3156,1,0.887507,"Missing"
2014.amta-researchers.6,N12-1047,0,0.0340726,"f phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. The following feature functions are used in the log-linear model: 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995); lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Table 2 shows the performance of these contaminator systems on the test sets used in our Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 72 Dataset Europarl EMEA GALE Test Segments 2000 1832 1082 Tokens 59 936 32 601 33 064 BLEU 26.05 31.31 19.65 WER 65.66 56.90 76.03 Table 2: MT Contaminator Performance Corpus Europarl EMEA GALE Baseline segments tokens 222K 6.2M 93K 1.5M 60.6K 2.0M +8× aux. segments tokens 2.0M 55.5M 835K 12.6M 545K 17"
2014.amta-researchers.6,W07-0732,0,0.0524901,"Missing"
2014.amta-researchers.6,W08-0327,0,0.0572331,"Missing"
2014.amta-researchers.6,W11-2139,0,0.035614,"Missing"
2014.amta-researchers.6,W07-0717,0,0.0110274,"seful. For example, one could easily identify pairs of segments that contain previously unseen vocabulary. Similarly, one could filter out segments or phrase-pairs that contain domainspecific terminology, or unusual or inconsistent translations of this terminology. Yet another approach with less-reliable training data is to turn to standard domainadaptation techniques. For example, auxiliary data could be used to generate distinct phrase tables and language models, whose relative weight in the trained systems would be determined empirically based on performance on suitably chosen tuning sets (Foster and Kuhn, 2007). This is something we plan to integrate into our experimental framework in the near future. When considering using bilingual data automatically harvested from the Web as training data for MT systems, it is important to take the final application into consideration: whether the resulting MT is intended for post-editing terminology-sensitive material, or for gisting and knowledge acquisition. In the end, the best advice is probably to sample the data for quality, and more importantly to monitor the quality of the resulting MT systems, both by appropriate use of standard benchmarks and metrics,"
2014.amta-researchers.6,2012.amta-papers.7,0,0.394287,"Missing"
2014.amta-researchers.6,P07-1019,0,0.0388511,", with a maximum phrase length of 7 tokens. The following feature functions are used in the log-linear model: 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995); lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Table 2 shows the performance of these contaminator systems on the test sets used in our Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 72 Dataset Europarl EMEA GALE Test Segments 2000 1832 1082 Tokens 59 936 32 601 33 064 BLEU 26.05 31.31 19.65 WER 65.66 56.90 76.03 Table 2: MT Contaminator Performance Corpus Europarl EMEA GALE Baseline segments tokens 222K 6.2M 93K 1.5M 60.6K 2.0M +8× aux. segments tokens 2.0M 55.5M 835K 12.6M 545K 17.7M Table 3: Sizes of training datasets: “Baseline” contains clean (un"
2014.amta-researchers.6,2010.eamt-1.26,0,0.0338192,"Missing"
2014.amta-researchers.6,2005.mtsummit-papers.11,0,0.0512797,"training data. For the experiments described here, we opt for the latter strategy. We describe our experimental data and the MT systems used below. 2.2 Data We describe here the three distinct data sets used in our experiments. More details can be found in Table 1. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 71 Dataset Europarl EMEA GALE Language English-French French- English Chinese-English Segments 2M 837K 547K Source Tokens 55.5M 12.7M 17.7M Table 1: Experimental data Europarl This is release v7 of the well-known Europarl corpus (Koehn, 2005), in English and French. We arbitrarily set the source language to be English. We set aside 2000 randomly picked pairs of segments for tuning purposes. The test set for this dataset is the Europarl test set from the WMT 2008 shared task (Callison-Burch et al., 2008). EMEA This is release 0.3 of the EMEA dataset, from the OPUS Corpus (Tiedemann, 2009) in English and French. In this case, we chose French to be the source language. This is a parallel corpus made out of PDF documents from the European Medicines Agency. While highly technical, it is very repetitive by nature. The data is organized"
2014.amta-researchers.6,2005.iwslt-1.8,0,0.0369241,"Missing"
2014.amta-researchers.6,2009.mtsummit-papers.9,0,0.0265774,"that of detecting MT. But their approach is based on comparing human production with MT output, an approach which would be difficult to apply in our case. As mentioned in the introduction, watermarking has also been considered for detecting MT-contaminated data (Venugopal et al., 2011). Unfortunately, this is not a general solution to MT detection because it only allows one producer of MT (e.g. Google) to recognize output from its own systems. In fact, Venugopal et al. suggest that no generally-applicable automatic method exists to distinguish between human- and machinegenerated translations. Kurokawa et al. (2009) propose a method to determine whether a given version of a text is the original or a translation; a similar approach could arguably be used for our problem, given suitable training data. To our knowledge, this has not been done, and no such dataset is available. But what if we knew how to detect MT? What if it was possible to reliably detect contaminated segments or documents, and filter them out of the auxiliary training data? To examine the potential of such methods, we trained systems with artificially decontaminated datasets. Figure 3 compares the behavior of these systems with those obta"
2014.amta-researchers.6,W11-2132,0,0.0342459,"Missing"
2014.amta-researchers.6,W10-1717,0,0.141851,"Missing"
2014.amta-researchers.6,J03-3002,0,0.199899,"Missing"
2014.amta-researchers.6,2009.mtsummit-posters.17,0,0.052242,"Missing"
2014.amta-researchers.6,N07-1064,1,0.759748,"Missing"
2014.amta-researchers.6,2006.eamt-1.6,0,0.0627134,"Missing"
2014.amta-researchers.6,N04-4026,0,0.0127104,"(domain gc.ca), containing over 500M words in each language. Phrase extraction was done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. The following feature functions are used in the log-linear model: 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995); lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Table 2 shows the performance of these contaminator systems on the test sets used in our Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 72 Dataset Europarl EMEA GALE Test Segments 2000 1832 1082 Tokens 59 936 32 601 33 064 BLEU 26.05 31.31 19.65 WER 65"
2014.amta-researchers.6,2006.iwslt-papers.3,0,0.0775174,"Missing"
2014.amta-researchers.6,D11-1126,0,0.14017,"ting large amounts of bilingual documents from the Web, there is a growing risk that some unknown proportion of these documents will actually be machine translated. In general, the presence of machine-generated data in training data that is otherwise assumed to have been produced by humans is seen as a bad thing. Using this kind of data to build systems aimed at simulating human behavior is likely to reinforce the errors committed by other systems, or even earlier versions of the same system. This is what motivates efforts such as those of Google to “watermark” the output of their MT systems (Venugopal et al., 2011), thus making it possible for them to recognize the output of their own systems, and to exclude it from their training sets. In practice, however, it is not clear that machine-generated translations are always harmful in MT training data. In fact, in some situations, MT has been used specifically to compensate for the lack of adequate bilingual training data. For example, Ueffing (2006) proposed a selftraining procedure, in which machine-translated versions of the test data itself was used as an additional source of knowledge in a phrase-based SMT system. This approach, as well as a number of"
2014.amta-researchers.6,W03-1729,0,0.0100716,"s. For tuning and test, we used the NIST 04 (1788 segments) and NIST 05 (1082 segments) Chinese-English test sets respectively. Four reference translations are provided for each of these sets. 2.3 MT Contamination In our experimental setup, MT systems play two different roles: they are first used as “contaminators”, i.e. to produce machine-translated target-language versions of training segments; then, they are used as “machine learners” in the rest of the study. To generate MT contamination in the GALE dataset, we used an older version of SYSTRAN’s Chinese-English machine translation system (Yang et al., 2003). This is a rule-based system, customized for the domains of science and technology. It uses a rule-based wordsegmenter and a bilingual lexicon with about 1.2M entries, containing words, expressions and rules. For the English-French datasets (Europarl and EMEA), MT contamination was generated using SMT systems based on Portage (Larkin et al., 2010), the NRC’s phrase-based SMT technology. English-French and French-English systems were trained using a very large corpus of Canadian government data harvested from the Web (domain gc.ca), containing over 500M words in each language. Phrase extractio"
2020.coling-main.576,P14-2048,0,0.04731,"Missing"
2020.coling-main.576,P13-1157,0,0.0293958,"be seen in most examples of Figure 2. This suggests that alignment features in the bilingual task could be useful. T- MOP explicitly captures alignment information, but does not seem to make good use of it. We were otherwise impressed 6560 Figure 1: Cumulative accuracy in the bilingual task calculated over the number of target sentences produced by the XLM engine, sorted by their number of tokens. by the overall quality of the MT, and rapidly realized how difficult it would be for human annotators to achieve a decent level of performance on this task. This is in line with the observations of Arase and Zhou (2013), who report lower performances for humans than for machines at detecting translations produced by statistical phrase-based MT. To better understand the type of information our classifiers base their decisions on, we inspected cases where our classifiers predominantly classified the human translations as such20 , and the machine translation counterpart is predominantly recognized as a machine translation. For 32 such cases randomly selected, we manually produced minimal pairs (3 on average), that is, as small as possible variants of the automatic translation, to see at which point the classifi"
2020.coling-main.576,Q19-1038,0,0.0462084,"Missing"
2020.coling-main.576,W15-5202,0,0.0189346,"e-translated material of our training corpus: two left-to-right models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio) adapted from Barbu (2015), some based on IBM models computed by MGIZA++ (Gao and Vogel, 2008), as well We used the fairseq-interactive module of the fairseq-py toolkit10 . Very specific rules such as replace(’ ;’,’;’) or replace(’https :’,’https:’). 13 Larger vocabularies do not yield notable performance differences. 11 12 6556 as some features based on multilingual word embeddings, using the method proposed by Søgaard et al. (2015). While in T- MOP, those features are aggregated in an unsupervised way (that is, with rules), we instead pass them to a random forest classifier trained specifically to distinguish human f"
2020.coling-main.576,W08-0509,0,0.012046,"ght models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio) adapted from Barbu (2015), some based on IBM models computed by MGIZA++ (Gao and Vogel, 2008), as well We used the fairseq-interactive module of the fairseq-py toolkit10 . Very specific rules such as replace(’ ;’,’;’) or replace(’https :’,’https:’). 13 Larger vocabularies do not yield notable performance differences. 11 12 6556 as some features based on multilingual word embeddings, using the method proposed by Søgaard et al. (2015). While in T- MOP, those features are aggregated in an unsupervised way (that is, with rules), we instead pass them to a random forest classifier trained specifically to distinguish human from machine translations. Because of the nature of the feature set,"
2020.coling-main.576,C18-1122,0,0.036047,"Missing"
2020.coling-main.576,P13-2121,0,0.0150216,"k most frequent character n-grams in the MT output of our training material, with n ranging from 2 to 7.13 Each sentence is then encoded by the frequency of the terms in this vocabulary, thus leading to a large sparse representation which is passed to a classifier. In the bilingual task, we also consider the top 30k n-grams of the source-language version of the training corpus, leading to representations of 60k dimensions. KEN LM As a point of comparison, in the monolingual task, we experimented with features extracted from four {3, 4}-gram word language models trained with the kenLM package (Heafield et al., 2013) on the machine-translated material of our training corpus: two left-to-right models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio)"
2020.coling-main.576,P16-4009,0,0.025357,"Missing"
2020.coling-main.576,P07-2045,0,0.00571854,"orpus OSCAR (Ortiz Su´arez et al., 2019). Unlike RoBERTa, CamemBERT uses sentence piece tokenization (Kudo and Richardson, 2018) and performs whole word masking, which has been shown to be preferable (Joshi et al., 2019). The architecture of the base model is a multi-layer bidirectional transformer (Devlin et al., 2018b; Vaswani et al., 2017) with 12 transformer blocks of hidden size 768 and 12 self attention heads. FlauBERT (Le et al., 2019) The base model we used is trained on 71GB of publicly available French data and the data was pre-processed and tokenized using a basic French tokenizer (Koehn et al., 2007). The model was trained with the MLM training objective. XLM-RO BERTA (Ruder et al., 2019) is a multilingual language model, trained on 100 different languages. It is an extended version of XLM (see Section 4.1). mBERT (Devlin et al., 2018b) is very similar to the original BERT model with 12 layers of bidirectional transformers, but released as a single language model trained on 104 separate languages from Wikipedia pages, with a shared word piece vocabulary. The model does not use any marker for input language and the pre-trained model is not made to extract translation pairs to have similar"
2020.coling-main.576,D18-2012,0,0.0244325,"l classification task, on all test sets. X, F, D, and GT refer to the XLM, FairSeq, DeepL, and Google translation engines, respectively. Underlined scores are produced by classifiers trained with XLM material; FairSeq material has been used otherwise. CamemBERT (Martin et al., 2019) is based on the RoBERTa (Liu et al., 2019) architecture (which is basically a BERT model with improved hyper-parameters for robust performance) and is trained on 138GB of plain French text taken from multilingual corpus OSCAR (Ortiz Su´arez et al., 2019). Unlike RoBERTa, CamemBERT uses sentence piece tokenization (Kudo and Richardson, 2018) and performs whole word masking, which has been shown to be preferable (Joshi et al., 2019). The architecture of the base model is a multi-layer bidirectional transformer (Devlin et al., 2018b; Vaswani et al., 2017) with 12 transformer blocks of hidden size 768 and 12 self attention heads. FlauBERT (Le et al., 2019) The base model we used is trained on 71GB of publicly available French data and the data was pre-processed and tokenized using a basic French tokenizer (Koehn et al., 2007). The model was trained with the MLM training objective. XLM-RO BERTA (Ruder et al., 2019) is a multilingual"
2020.coling-main.576,Y15-2041,0,0.0614647,"Missing"
2020.coling-main.576,W18-6301,0,0.0139195,"he TM are into French, we focus on this language direction. Our goal is to build classifiers that determine if a translation is human or machine-made. For this, we need training data that contains both types of translations. We create such data by machine translating a subset of 530k sentence pairs, randomly sampled from the TM. These machine translations are performed using two different neural MT systems, themselves trained using a distinct subset of 5.8M sentence pairs, also randomly sampled from the TM.2 These two MT systems, one based on XLM (Lample and Conneau, 2019) and one on FairSeq (Ott et al., 2018), are detailed in Section 4. Thus, two distinct classifier training sets are created, one from each MT system: each contains 530k human translations and 530k machine translations, totalling 1.06M examples. 1 2 http://www.statmt.org/europarl All sampling in the TM was done in such a way as to ensure comparable representations of each domain. 6554 We proceed similarly to produce test sets to evaluate the performance of our classifiers: we randomly sample 10k sentence pairs from the TM, machine translate the English versions into French using our XLM and FairSeq MT systems, thus creating two test"
2020.coling-main.576,P19-4007,0,0.0374539,"Missing"
2020.coling-main.576,P16-1162,0,0.0382988,"NMT systems using English-French texts from the TM. We provide the details of this process here. 4.1 Cross-lingual Language Model (XLM) In Lample and Conneau (2019), the authors propose three models: two unsupervised ones that do not use sentence pairs in translation relation, and a supervised one that does. We focus on the third model, called the Translation Language Modeling (TLM) which tackles cross-lingual pre-training in a way similar to the BERT model (Devlin et al., 2018a) with notable differences. First, XLM is based on a shared source-target vocabulary using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We used the 60k BPE vocabulary which comes with the pre-trained language model.7 Second, XLM is trained to predict both source and target masked words, leveraging both source and target contexts, encouraging the model to align the source and target representations. Third, XLM stores the ID for the language and the token order (i.e., positional encoding) in both languages which builds a relationship between related tokens in the two languages. During training and when translating, we use a beam search of width 6 and a length penalty of 1. XLM is implemented in PyTorch8 and supports distribute"
2020.coling-main.576,P15-1165,0,0.0361127,"Missing"
2021.mtsummit-research.9,W19-5206,0,0.0179028,"s matter, we turn to the question of tagging translation direction (with a special “&lt;translationese>” tag at the start of the source sentence for source-Translationese sentences), to see if this will Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 108 enable the Mixed data systems to make better use of all available information. Tagging has been shown to be effective in multilingual (Johnson et al., 2017; Rikters et al., 2018) and multidomain (Kobus et al., 2017) systems, as well as when using backtranslated data (Caswell et al., 2019). All of these systems for Q2 make use of the full 13.7M line training set. Mixed Tagged Mixed Tagged Mixed+mixdev BLEU ↑ 43.0 ±0.6 43.0 ±0.6 43.1 ±0.6 EN→FR chrF ↑ Human ↓ 0.652 1.81 0.653 1.72 0.653 1.78 BLEU 52.0 ±0.7 52.6 ±0.7 52.9 ±0.7 FR→EN chrF 0.715 0.720 0.722 Human 1.85 1.67 1.75 Table 2: BLEU and chrF scores for training with Mixed data, untagged and tagged (the latter with Authentic source validation or Mixed validation). We indicate if a system is tagged through the addition of “Tagged” in the system name, while untagged systems are unmarked. The untagged systems here are the same"
2021.mtsummit-research.9,W18-1820,0,0.0230404,"tic-FR data. We preprocess the data using open-source normalization and tokenization scripts from PortageTextProcessing.6 Specifically, we applied clean-utf8-text.pl (removing control characters, standardization, etc.), followed by fix-slashes.pl (heuristically adding whitespace around slashes), and tokenization with utokenize.pl -noss -lang=$lang. We then train joint 32k byte-pair encoding (BPE) subword vocabularies on the training data (Sennrich et al., 2016),7 and apply them to train, development, and test. 3 Models We build Transformer (Vaswani et al., 2017) models using Sockeye-1.18.115 (Hieber et al., 2018), with 6 layers, 8 attention heads, network size of 512 units, and feedforward size of 2048 units. We have changed the default gradient clipping type to absolute, used source-target soft3 https://www.ourcommons.ca/ 4 This includes both boilerplate text and full sentences. read in the corpus chronologically, maintaining counts of Authentic-EN and FR and sampling from a Gaussian to determine whether to keep or discard incoming original-EN sentences to maintain similar counts. 6 https://github.com/nrc-cnrc/PortageTextProcessing 7 https://github.com/rsennrich/subword-nmt 5 We Proceedings of the 18"
2021.mtsummit-research.9,kobus-etal-2017-domain,0,0.0187181,"d through automatic and human metrics that the translation direction does matter, we turn to the question of tagging translation direction (with a special “&lt;translationese>” tag at the start of the source sentence for source-Translationese sentences), to see if this will Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 108 enable the Mixed data systems to make better use of all available information. Tagging has been shown to be effective in multilingual (Johnson et al., 2017; Rikters et al., 2018) and multidomain (Kobus et al., 2017) systems, as well as when using backtranslated data (Caswell et al., 2019). All of these systems for Q2 make use of the full 13.7M line training set. Mixed Tagged Mixed Tagged Mixed+mixdev BLEU ↑ 43.0 ±0.6 43.0 ±0.6 43.1 ±0.6 EN→FR chrF ↑ Human ↓ 0.652 1.81 0.653 1.72 0.653 1.78 BLEU 52.0 ±0.7 52.6 ±0.7 52.9 ±0.7 FR→EN chrF 0.715 0.720 0.722 Human 1.85 1.67 1.75 Table 2: BLEU and chrF scores for training with Mixed data, untagged and tagged (the latter with Authentic source validation or Mixed validation). We indicate if a system is tagged through the addition of “Tagged” in the system name, w"
2021.mtsummit-research.9,W04-3250,0,0.630786,"Missing"
2021.mtsummit-research.9,2009.mtsummit-papers.9,0,0.0972195,"data imbalance in terms of translation direction, we find that tagging the translation direction of training data can close the performance gap. We perform a human evaluation that differs slightly from the automatic metrics, but nevertheless confirms that for this French–English dataset that is known to contain high-quality translations, authentic or tagged mixed source improves over translationese source for training. 1 Introduction Prior work in statistical machine translation (SMT) highlighted potential benefits of making use of information about the translation direction of training data (Kurokawa et al., 2009). When text is translated, there is an authentic source (the language in which the text was originally produced), and its translation, which in contrast can be described as translationese. Thus when considering translation direction in machine translation, training data can be described as consisting of authentic source, translationese source, or a mix.1 Backtranslated data produced by machine translation may be thought of as an extreme case of translationese source (Marie et al., 2020), but because the quality and types of errors that occur in machine translation are quite different from thos"
2021.mtsummit-research.9,1999.mtsummit-1.50,0,0.161931,"Missing"
2021.mtsummit-research.9,2020.acl-main.532,0,0.0317103,"lighted potential benefits of making use of information about the translation direction of training data (Kurokawa et al., 2009). When text is translated, there is an authentic source (the language in which the text was originally produced), and its translation, which in contrast can be described as translationese. Thus when considering translation direction in machine translation, training data can be described as consisting of authentic source, translationese source, or a mix.1 Backtranslated data produced by machine translation may be thought of as an extreme case of translationese source (Marie et al., 2020), but because the quality and types of errors that occur in machine translation are quite different from those that occur in human translation, it is worth examining translation direction of human translation separately from MT-based data augmentation. In Figure 1 we show a fairly dramatic example of the kinds of translation quality differences that can occur when building MT systems using authentic source as opposed to translationese source. Recent work in neural machine translation (NMT) has revisited this issue, motivating the automatic detection of (human) translationese by showing improve"
2021.mtsummit-research.9,2020.acl-main.448,0,0.0863168,"Missing"
2021.mtsummit-research.9,P02-1040,0,0.109636,"Missing"
2021.mtsummit-research.9,W15-3049,0,0.0271911,"Missing"
2021.mtsummit-research.9,W18-6319,0,0.0503689,"Missing"
2021.mtsummit-research.9,L18-1595,0,0.0532347,"Missing"
2021.mtsummit-research.9,2020.acl-main.691,0,0.364482,"ed to translationese source. Recent work in neural machine translation (NMT) has revisited this issue, motivating the automatic detection of (human) translationese by showing improved performance on several metrics when training translation direction matches the testing translation direction (Sominsky and Wintner, 2019), examining domain and backtranslation along with the translation direction of test sets (Bogoychev and Sennrich, 2019), and evaluating the treatment of predicted translation direction as separate languages in a multilingual-style NMT system through human and automatic metrics (Riley et al., 2020). 1 For the purposes of this paper, we will set aside the situation where both sides of the text consist of translationese, translated from one or more other pivot languages. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 103 Source Reference MT (Authentic Src.) MT (Translationese Src.) Les producteurs de fromage au Qu´ebec sont des fleurons dont on est fiers. We are proud of our exceptional Quebec cheese producers. We are proud of the success of cheese producers in Quebec. The cheese producers in Quebec are proud"
2021.mtsummit-research.9,R19-1130,0,0.0177979,"hat occur in human translation, it is worth examining translation direction of human translation separately from MT-based data augmentation. In Figure 1 we show a fairly dramatic example of the kinds of translation quality differences that can occur when building MT systems using authentic source as opposed to translationese source. Recent work in neural machine translation (NMT) has revisited this issue, motivating the automatic detection of (human) translationese by showing improved performance on several metrics when training translation direction matches the testing translation direction (Sominsky and Wintner, 2019), examining domain and backtranslation along with the translation direction of test sets (Bogoychev and Sennrich, 2019), and evaluating the treatment of predicted translation direction as separate languages in a multilingual-style NMT system through human and automatic metrics (Riley et al., 2020). 1 For the purposes of this paper, we will set aside the situation where both sides of the text consist of translationese, translated from one or more other pivot languages. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page"
C98-1113,1990.tc-1.1,0,0.0526616,"Missing"
C98-1113,C92-2079,0,0.0350145,"Missing"
C98-1113,P91-1023,0,0.0677609,"975 = 1515 + 36*20 = (excluding = 975 0.64 = 1 5 Systems tested Six systems were tested, two of which having been submitted by the RALI. 714 l : t A L I / J a c a l This system uses as a first step a program that reduces the search space only to those sentence pairs that axe potentially interesting (Simard and Plamondon, 1996). The underlying principle is the automatic detection of isolated cognates (i.e. for which no other similar word exists in a window of given size). Once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (Gale and Church, 1991). RALI/Salign The second method proposed by RALI is based on a dynamic programming scheme which uses a score function derived from a translation model similar to that of (Brown et ai., 1990). The search space is reduced to a beam of fixed width around the diagonal (which would represent the alignment if the two texts were perfectly synchronized). L O R I A The strategy adopted in this system differs from that of the other systems since sentence alignment is performed after the preliminaxy alignment of larger units (whenever possible, using maxk-up), such as paragraphs and divisions, on the bas"
C98-1113,1996.amta-1.4,0,0.0386261,"last few years, there has been a growing interest in parallel text alignment techniques. These techniques a t t e m p t to map various textual units to their translation and have proven useful for a wide range of applications and tools. A simple example of such a tool is probably the TrunsSearch bilingual concordancing system (Isabelle et al., 1993), which allows a user to query a large archive of existing translations in order to find ready-made solutions to specific translation problems. Such a tool has proved extremely useful not only for translators, but also fbr bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). More sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilingual lexical resources (Melamed, 1996; Klavans 711 Jean V6ronis LPL, Univ. de Provence 29, Av. R. Schuman F-13621 Aix-en-Provence Cedex 1 veronis@univ-aix.fr azld Tzoukermann, 1995), the automatic verification of translations (Macklovitch, 1995), the automatic dictation of translations (Brousseau et al., 1995) and even interactive machine translation (Foster et al., 1997). Enthusiasm for this relatively new"
C98-1113,1995.mtsummit-1.36,0,0.0486228,"Missing"
C98-1113,1996.amta-1.13,0,0.0481671,"d tools. A simple example of such a tool is probably the TrunsSearch bilingual concordancing system (Isabelle et al., 1993), which allows a user to query a large archive of existing translations in order to find ready-made solutions to specific translation problems. Such a tool has proved extremely useful not only for translators, but also fbr bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). More sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilingual lexical resources (Melamed, 1996; Klavans 711 Jean V6ronis LPL, Univ. de Provence 29, Av. R. Schuman F-13621 Aix-en-Provence Cedex 1 veronis@univ-aix.fr azld Tzoukermann, 1995), the automatic verification of translations (Macklovitch, 1995), the automatic dictation of translations (Brousseau et al., 1995) and even interactive machine translation (Foster et al., 1997). Enthusiasm for this relatively new field was sparked early on by the apparent demonstration that very simple techniques could yield almost perfect results. For instance, to produce sentence alignments, Brown et al. (1991) and Gale and Church (1991) both propose"
C98-1113,P97-1039,0,0.144085,"Missing"
C98-1113,1996.amta-1.14,1,0.897358,"lt in a dramatic loss of efficiency. The truth is that, while text alignment is mostly an easy problem, especially when considered at the sentence level, there are situations where even humans have a hard time making the right decision. In fact, it could be argued that, ultimately, text alignment is no easier tha~l the more general problem of natural laalguage understanding. In addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment (Brown et al., 1991; Gale and Church, 1991; Debili, 1992; Kay and RSscheisen, 1993; Simard et al., 1992; Simard and Plamondon, 1996). Alignment at the word and term level, which is extremely useful for applications such as lexical resource extraction, is still a largely unexplored research area(Melamed, 1997). In order to live up to the expectations of the various application fields, alignment technology will therefore have to improve substantially. As was the case with several other language processing techniques (such as information retrieval, document understanding or speech recognition), it is likely that a systematic evaluation will enable such improvements. However, before the ARCADE project started, no formal evalua"
C98-1113,1992.tmi-1.7,1,0.91092,"Missing"
C98-1113,1993.tmi-1.17,0,\N,Missing
C98-1113,A94-1006,0,\N,Missing
C98-1113,J90-2002,0,\N,Missing
C98-1113,P91-1022,0,\N,Missing
H05-1095,J93-2003,0,0.00752012,"asured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing so we expect to improve translation quality by better accounting for additional linguistic phenome"
H05-1095,P05-1033,0,0.0570595,"ned training corpus. Our approach is based on a direct approximation of the posterior probability Pr (tI1 |sJ1 ), using a loglinear model: Pr (tI1 |sJ1 ) M X Our model currently relies on seven feature functions, which we describe here. (1) 1 1 = exp ZsJ • The compositional bi-phrase feature function hcomp : this is introduced to compensate for ! λm hm (tI1 , sJ1 ) m=1 1 In such a model, the contribution of each feature function hm is determined by the corresponding model parameter λm ; ZsJ denotes a normalization 1 constant. This type of model is now quite widely 757 Recent work from Chiang (Chiang, 2005) addresses similar concerns to those motivating our work by introducing a Synchronous CFG for bi-phrases. If on one hand SCFGs allow to better control the order of the material inserted in the gaps, on the other gap size does not seem to be taken into account, and phrase dovetailing such as the one involving “do want” and “not anymore” in Fig. 2 is disallowed. hbp ’s strong tendency to overestimate the probability of rare bi-phrases; it is computed as in equation (2), except that bi-phrase probabilities are computed based on individual word translation probabilities, somewhat as in IBM mod"
H05-1095,P01-1030,1,0.79189,"uent source phrases, as most phrases have less than 20 translations. 6.3 Experiments The parameters of the model were optimized independantly for each bi-phrase library. In all cases, we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the NIST and BLEU scores against one reference translation. As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al., 2001)5 . Table 2 describes the various libraries that were used for our experiments, and the results obtained for each. System/library ReWrite A1 A2 -g0 A2 -g3 B 1 -g0 B1 B 2 -g0 B 2 -g3 B 1 -g1 B 1 -g2 B 1 -g3 B 1 -g4 bi-phrases 238 K 642 K 4.1 M 193 K 267 K 499 K 3.3 M 206 K 213 K 218 K 222 K NIST 6.6838 6.6695 6.7675 6.7068 6.7898 6.9172 6.7290 6.9707 6.8979 6.9406 6.9546 6.9527 BLEU 0.3324 0.3310 0.3363 0.3283 0.3369 0.3407 0.3391 0.3552 0.3441 0.3454 0.3518 0.3423 Table 2: Bi-phrase libraries and results The top part of the table presents the results for the A libraries. As can be seen, librar"
H05-1095,P04-1064,1,0.936967,"iterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are called cepts. For example, in Figure 1, these cepts are represented by the circles numbered 1, 2 and 3; each cept thus connects word tokens in the source and the target, regardless of position or contiguity. These cepts naturally constitute bi-phrases, and can be used directly to produce a biphrase library. Obviously, the"
H05-1095,W02-1018,0,0.0165656,"lso presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequ"
H05-1095,P00-1056,0,0.0383638,"equivalents. While the first of these filters typically eliminates a large number of entries, the second only affects the most frequent source phrases, as most phrases have less than 20 translations. 6.3 Experiments The parameters of the model were optimized independantly for each bi-phrase library. In all cases, we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the NIST and BLEU scores against one reference translation. As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al., 2001)5 . Table 2 describes the various libraries that were used for our experiments, and the results obtained for each. System/library ReWrite A1 A2 -g0 A2 -g3 B 1 -g0 B1 B 2 -g0 B 2 -g3 B 1 -g1 B 1 -g2 B 1 -g3 B 1 -g4 bi-phrases 238 K 642 K 4.1 M 193 K 267 K 499 K 3.3 M 206 K 213 K 218 K 222 K NIST 6.6838 6.6695 6.7675 6.7068 6.7898 6.9172 6.7290 6.9707 6.8979 6.9406 6.9546 6.9527 BLEU 0.3324 0.3310 0.3363 0.3283 0.3369 0.3407 0.3391 0.3552 0.3441 0.3454 0.3518 0.3"
H05-1095,J03-1002,0,0.00889681,"d on the first “free” position in the target language sentence, i.e. either the leftmost gap, or the right end of the sequence. Figure 2 illustrates this process with an example. To produce translations, our approach therefore relies on a collection of bi-phrases, what we call a bi-phrase library. Such a library is constructed from a corpus of existing translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, su"
H05-1095,J04-4002,0,0.0387242,"ting translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are called cepts. For examp"
H05-1095,W99-0604,0,0.0809757,"slation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possib"
H05-1095,P03-1021,0,0.0338002,"guities they contain. 4 Say we have a set of source-language sentences S. For a given value of λ, we can compute the set of corresponding target-language translations T . Given a set of reference (“gold-standard”) translations R for S and a function E(T, R) which measures the “error” in T relative to R, then we can formulate the parameter estimation problem as2 : Parameter Estimation The values of the λ parameters of the log-linear model can be set so as to optimize a given criterion. For instance, one can maximize the likelyhood of some set of training sentences. Instead, and as suggested by Och (2003), we chose to maximize directly the quality of the translations produced by the system, as measured with a machine translation evaluation metric. 758 As pointed out by Och, one notable difficulty with this approach is that, because the computation of T is based on an argmax operation (see eq. 1), it is not continuous with regard to λ, and standard gradientdescent methods cannot be used to solve the optimization. Och proposes two workarounds to this problem: the first one relies on a direct optimization method derived from Powell’s algorithm; the second introduces a smoothed (continuous) versio"
H05-1095,P02-1040,0,0.111177,"to λ, and standard gradientdescent methods cannot be used to solve the optimization. Och proposes two workarounds to this problem: the first one relies on a direct optimization method derived from Powell’s algorithm; the second introduces a smoothed (continuous) version of the error function E(T, R) and then relies on a gradient-based optimization method. We have opted for this last approach. Och shows how to implement it when the error function can be computed as the sum of errors on individual sentences. Unfortunately, this is not the case for such widely used MT evaluation metrics as BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We show here how it can be done for NIST; a similar derivation is possible for BLEU. The NIST evaluation metric computes a weighted n-gram precision between T and R, multiplied by a factor B(S, T, R) that penalizes short translations. It can be formulated as: B(S, T, R) × N X n=1 P s∈S In (ts , rs ) P s∈S Cn (ts ) (3) where N is the largest n-gram considered (usually N = 4), In (ts , rs ) is a weighted count of common n-grams between the target (ts ) and reference (rs ) translations of sentence s, and Cn (ts ) is the total number of n-grams in ts . To derive a ver"
H05-1095,N03-2036,0,0.095765,"as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing so we expect to improve t"
H05-1095,P02-1039,1,0.503169,"ls such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing"
H05-1095,N03-1017,0,0.183385,"rom a corpus of existing translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are cal"
H05-1095,N04-1033,0,\N,Missing
J03-3003,J90-2002,0,0.200643,"Missing"
J03-3003,P00-1006,0,0.0510111,"Missing"
J03-3003,1998.amta-tutorials.5,0,0.00872054,"ically that can form the basis for an effective CLIR system, and (2) we will compare several ways to embed translation models in an IR system to exploit these corpora for cross-language query expansion. Our experiments will show that these translation tools can result in CLIR of comparable effectiveness to MT systems. This in turn will demonstrate the feasibility of exploiting the Web as a large parallel corpus for the purpose of CLIR. 1.3 Problems in Query Translation Now let us turn to query translation problems. Previous studies on CLIR have identified three problems for query translation (Grefenstette 1998): identifying possible translations, pruning unlikely translations, and weighting the translation words. Finding translations. First of all, whatever translation tool is employed in translating queries has to provide a good coverage of the source and target vocabularies. In a dictionary-based approach to CLIR, we will encounter the same problems that have been faced in MT research: phrases, collocations, idioms, and domain-specific terminology are often translated incorrectly. These classes of expressions require a sophisticated morphological analysis, and furthermore, domain-specific terms ch"
J03-3003,1999.mtsummit-1.50,0,0.0154126,"e. The European parliament documents represent another large parallel corpus in several European languages. However, the availability of this corpus is much more restricted than the Canadian Hansard. The Hong Kong government publishes official documents in both Chinese and English. They form a Chinese-English parallel corpus, but again, its size is much smaller than that of the Canadian Hansard. For many other languages, no large parallel corpora are available for the training of statistical models. LDC has tried to collect additional parallel corpora, resorting at times to manual collection (Ma 1999). Several other research groups (for example, the RALI lab at Universit´e de Montr´eal) have also tried to acquire manually constructed parallel corpora. However, manual collection of large corpora is a tedious task that is timeand resource-consuming. On the other hand, we observe that the increasing usage of different languages on the Web results in more and more bilingual and multilingual sites. Many Web pages are now translated into different languages. The Web contains 5 LDC provides a version containing texts from the mid-1970s through 1988; see http://www.ldc.upenn.edu/. 387 Computatio"
J03-3003,resnik-1998-parallel,0,0.011957,"utomatically, then this would help solve, to some extent, the problem of parallel corpora. PTMiner (for Parallel Text Miner) was built precisely for this purpose. Of course, an automatic mining program is unable to understand the texts it extracts and hence to judge in a totally reliable way whether they are parallel. However, CLIR is quite error-tolerant. As we will show, a noisy parallel corpus can still be very useful for CLIR. 2.1 General Principles of Automatic Mining Parallel Web pages usually are not published in isolation; they are often linked to one another in some way. For example, Resnik (1998) observed that some parallel Web pages are often referenced in the same parent index Web page. In addition, the anchor text of such links usually identifies the language. For example, if a Web page index.html provides links to both English and French versions of a page it references, and the anchor texts of the links are respectively “English version” and “French version,” then the referenced versions are probably parallel pages in English and French. To locate such pages, Resnik first sends a query of the following form to the Web search engine AltaVista, which returns the parent indexing p"
J03-3003,1992.tmi-1.7,1,0.736184,"Missing"
K19-1020,W11-4533,0,0.0508847,"Missing"
K19-1020,S16-1089,0,0.0696589,"Missing"
K19-1020,S17-2001,0,0.0331208,"ation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in each language directly"
K19-1020,S16-1081,0,0.308764,"gual context representation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in ea"
K19-1020,N19-1423,0,0.239773,"vily on existing parallel resources to first build a machine translation (MT) system and translate one of the test sentences into the other language for applying monolingual STS methods (Brychc´ın and Svoboda, 2016). Methods that do not rely explicitly on MT, such as that in Lo et al. (2018), still require parallel resources to build bilingual word representations for evaluating crosslingual lexical semantic similarity. It is clear that there is a circular dependency problem on parallel resources. Massively multilingual context representation models, such as MUSE (Conneau et al., 2017), BERT (Devlin et al., 2019), and XLM (Lample and Conneau, 2019), that are trained in an unsupervised manner with non-parallel data from each We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT – Bidirectional Encoder Representations from Transformers (Devlin et al., 2019). The goal of crosslingual STS is to measure to what degree two segments of text in different languages express the same meaning. Not only is it a key task in crosslingual natural language understanding (XLU), it is also particularly useful for identifying parallel res"
K19-1020,W18-6481,1,0.939811,"ngual STS, neither the direction nor the origin (human or machine) of the translation is taken into account. Furthermore, MTQE also typically considers the fluency and grammaticality of the target text; these aspects are usually not perceived as relevant for crosslingual STS. Many previous crosslingual STS methods rely heavily on existing parallel resources to first build a machine translation (MT) system and translate one of the test sentences into the other language for applying monolingual STS methods (Brychc´ın and Svoboda, 2016). Methods that do not rely explicitly on MT, such as that in Lo et al. (2018), still require parallel resources to build bilingual word representations for evaluating crosslingual lexical semantic similarity. It is clear that there is a circular dependency problem on parallel resources. Massively multilingual context representation models, such as MUSE (Conneau et al., 2017), BERT (Devlin et al., 2019), and XLM (Lample and Conneau, 2019), that are trained in an unsupervised manner with non-parallel data from each We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT – Bidirectional Enco"
K19-1020,E14-1044,0,0.0184522,"rinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in each language directly on the texts under consideration, we rely on precomputed weights from monolingual corpora E and F of the two tested languages. The YiSi metrics are formulated as an F-score: by viewing the source text as a “query” and the target as an “answer”, precision"
K19-1020,W15-1521,0,0.195967,"Missing"
K19-1020,2012.amta-papers.7,0,0.197922,"This is done by means of log-linear model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much"
K19-1020,D19-1632,0,0.059965,"Missing"
K19-1020,S13-2005,0,0.0608354,"Missing"
K19-1020,W18-2709,0,0.0298621,"r model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much Here, we use data from the WMT19 shared ta"
K19-1020,P02-1040,0,0.109123,"used in training the bilingual word embeddings for evaluating crosslingual lexical semantic similarity in YiSi-2. hala, into English.7 Both corpora were crawled from the web, using ParaCrawl (Koehn et al., 2018a). Specifically, the task is to produce a score for each sentence pair in these noisy corpora, reflecting the quality of that pair. The scoring schemes are evaluated by extracting the topscoring sentence pairs from each corpus, then using them to train MT systems; these systems are run on test sets of Wikipedia articles (Guzm´an et al., 2019), and the results are evaluated using BLEU (Papineni et al., 2002). In addition to the noisy corpora, participants are allowed to use a few small sets of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali). The provided data also included much larger monolingual corpora for each of English, Hindi, Nepali and Sinhala. 4.1 WMT19 parallel corpus filtering system 1M-word 5M-word random 1.30 3.01 Zipporah 4.14 4.42 YiSi-2bivec 3.86 3.76 YiSi-2vecmap 4.00 3.76 YiSi-2bert 3.77 3.77 Table 4: Uncased BLEU scores on the official WMT19 PCF dev (“dev-tes"
K19-1020,W18-6453,0,0.115895,"imilarity can be broken. 2 Crosslingual STS metric v(u) = embedding of unit u s(e, f ) = cos(v(e), v(f )) Our crosslingual STS metric is based on YiSi (Lo, 2019). YiSi is a unified adequacy-oriented MT quality evaluation and estimation metric for languages with different levels of available resources. Lo et al. (2018) showed that YiSi-2, the crosslingual MT quality estimation metric, performed almost as well as the “MT + monolingual MT evaluation metric (YiSi-1)” pipeline for identifying parallel sentence pairs from a noisy web-crawled corpus in the Parallel Corpus Filtering task of WMT 2018 (Koehn et al., 2018b). To measure semantic similarity between pairs of segments, YiSi-2 proceeds by finding alignments between the words of these segments that maximize semantic similarity at the lexical level. For evaluating crosslingual lexical semantic similarity, it relies on a crosslingual embedding model, using cosine similarity of the embeddings from the crosslingual lexical representation model. Following the approach of Corley and Mihalcea (2005), these lexical semantic similarities are weighed by lexical specificity using inverse document frequency (IDF) collected from each side of the tested corpus. A"
K19-1020,P19-1493,0,0.0641239,"Missing"
K19-1020,J03-3002,0,0.258421,"Missing"
K19-1020,2014.amta-researchers.6,1,0.888004,"s of log-linear model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much Here, we use"
K19-1020,W19-5358,1,0.824685,"from BERT without finetuning. In an intrinsic crosslingual STS evaluation and extrinsic parallel corpus filtering and human translation error detection tasks, we show that our BERT-based metric achieves performance on par with similar metrics based on supervised or weakly supervised approaches. With the availability of the multilingual context representation models, we show that the deadlock around parallel resources for crosslingual textual similarity can be broken. 2 Crosslingual STS metric v(u) = embedding of unit u s(e, f ) = cos(v(e), v(f )) Our crosslingual STS metric is based on YiSi (Lo, 2019). YiSi is a unified adequacy-oriented MT quality evaluation and estimation metric for languages with different levels of available resources. Lo et al. (2018) showed that YiSi-2, the crosslingual MT quality estimation metric, performed almost as well as the “MT + monolingual MT evaluation metric (YiSi-1)” pipeline for identifying parallel sentence pairs from a noisy web-crawled corpus in the Parallel Corpus Filtering task of WMT 2018 (Koehn et al., 2018b). To measure semantic similarity between pairs of segments, YiSi-2 proceeds by finding alignments between the words of these segments that ma"
K19-1020,W18-6451,0,0.0611751,"Missing"
K19-1020,S16-1102,1,0.901242,"Missing"
K19-1020,W16-2327,0,0.0190502,"filter out pairs which are not proper translations, possibly with some tolerance for pairs of segments that do share partial meaning. In TEED, the data is mostly expected to be high-quality translations; the task is then to identify those pairs that deviate from this norm, even on small details. extract the 1M-word and 5M-word samples from the original test corpora, using the scores of each of our systems in turn. We then trained MT systems using the extracted data: our MT systems are standard phrase-based SMT systems, with components and parameters similar to the GermanEnglish SMT system in Williams et al. (2016). 4.2 Results BLEU scores of the resulting MT systems are shown in Table 4. For comparison, we present the results of random scoring, as well as results obtained by the Zipporah PCF method (Xu and Koehn, 2017). Zipporah combines fluency and adequacy features to score sentence pairs; adequacy features are derived from existing parallel corpora, and the feature combination (logistic regression) is optimized on in-domain parallel data. Therefore, Zipporah can be seen as a fully supervised method. The Zipporah-based MT systems were trained similarly to other systems in the results reported here. A"
K19-1020,D17-1319,0,0.0366912,"k is then to identify those pairs that deviate from this norm, even on small details. extract the 1M-word and 5M-word samples from the original test corpora, using the scores of each of our systems in turn. We then trained MT systems using the extracted data: our MT systems are standard phrase-based SMT systems, with components and parameters similar to the GermanEnglish SMT system in Williams et al. (2016). 4.2 Results BLEU scores of the resulting MT systems are shown in Table 4. For comparison, we present the results of random scoring, as well as results obtained by the Zipporah PCF method (Xu and Koehn, 2017). Zipporah combines fluency and adequacy features to score sentence pairs; adequacy features are derived from existing parallel corpora, and the feature combination (logistic regression) is optimized on in-domain parallel data. Therefore, Zipporah can be seen as a fully supervised method. The Zipporah-based MT systems were trained similarly to other systems in the results reported here. All systems produced with YiSi-2 produce similar results. Interestingly, the MT systems produced with YiSi-2 in the 5M-word condition are not better than those of the 1M-word condition. This is possibly explain"
langlais-simard-2002-merging,macklovitch-russell-2000-whats,0,\N,Missing
langlais-simard-2002-merging,C96-1030,0,\N,Missing
langlais-simard-2002-merging,J93-2003,0,\N,Missing
langlais-simard-2002-merging,W00-0731,0,\N,Missing
langlais-simard-2002-merging,P98-2158,0,\N,Missing
langlais-simard-2002-merging,C98-2153,0,\N,Missing
langlais-simard-2002-merging,1999.mtsummit-1.92,0,\N,Missing
langlais-simard-2002-merging,P01-1050,0,\N,Missing
langlais-simard-2002-merging,2001.mtsummit-papers.60,1,\N,Missing
langlais-simard-2002-merging,macklovitch-etal-2000-transsearch,1,\N,Missing
macklovitch-etal-2000-transsearch,J93-1004,0,\N,Missing
macklovitch-etal-2000-transsearch,1993.tmi-1.17,0,\N,Missing
macklovitch-etal-2000-transsearch,P91-1022,0,\N,Missing
macklovitch-etal-2000-transsearch,J93-1006,0,\N,Missing
N06-1004,P05-1066,0,0.485406,"nal Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These “segment choice” models (SCMs) can be trained on “segmen"
N06-1004,koen-2004-pharaoh,0,0.574965,"al., 2003; Och and Ney, 2004). Distortion in phrase-based MT occurs when the order of phrases in the source-language sentence changes during translation, so the order of corresponding phrases in the target-language translation is different. Some MT systems allow arbiOur model assumes that the source sentence is completely segmented prior to distortion. This simplifying assumption requires generation of hypotheses about the segmentation of the complete source sentence during decoding. The model also assumes that each translation hypothesis grows in a predetermined order. E.g., Koehn’s decoder (Koehn 2004) builds each new hypothesis by adding phrases to it left-to-right (order is deterministic for the target hypothesis). Our model doesn’t require this order of operation – it would support right-to-left or inwards-outwards hypothesis construction – but it does require a predictable order. One can keep track of how segments in the source sentence have been rearranged during decoding for a given hypothesis, using what we call a “distorted source-language hypothesis” (DSH). A similar concept appears in (Collins et al., 2005) (this paper’s preoccupations strongly resemble 25 Proceedings of the Human"
N06-1004,N03-1017,0,0.00873664,"bles consists of all parallel text available for the NIST MT05 Chinese-English evaluation, except the Xinhua corpora and part 3 of LDC's “MultipleTranslation Chinese Corpus” (MTCCp3). The English language model was trained on the same corpora, plus 250M words from Gigaword. The DTbased SCM was trained and tuned on a subset of this same training corpus (above). The dev corpus for optimizing component weights is MTCCp3. The experimental results below were obtained by testing on the evaluation set for MTeval NIST04. Phrase tables were learned from the training corpus using the “diag-and” method (Koehn et al., 2003), and using IBM model 2 to produce initial word alignments (these authors found this worked as well as IBM4). Phrase probabilities were based on unsmoothed relative frequencies. The model used by the decoder was a log-linear combination of a phrase translation model (only in the P(source|target) direction), trigram language model, word penalty (lexical weighting), an optional segmentation model (in the form of a phrase penalty) and distortion model. Weights on the components were assigned using the (Och, 2003) method for max-BLEU training on the development set. The decoder uses a dynamicprogr"
N06-1004,P03-1021,0,0.0267581,"Missing"
N06-1004,P02-1040,0,0.0721342,"se it is the leftmost RS: the “leftmost” predictor. Or, the last phrase in the DSH will be followed by the phrase that originally followed it, [8 9]: the “following” predictor. Or, perhaps positions in the source and target should be close, so since the next DSH position to be filled is 4, phrase [4] should be favoured: the “parallel” predictor. original: [0 1] [2 3] [4] [5] [6] [7] [8 9] DSH: [0 1] [5] [7], RS: [2 3], [4], [6], [8 9] Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it, as measured by a metric like BLEU (Papineni et al., 2002). However, training and 26 Figure 2. Segment choice prediction example Model B will be based on the “leftmost” predictor, giving the leftmost segment in the RS twice the probability of the other segments, and giving the others uniform probabilities. Model C will be based on the “following” predictor, doubling the probability for the segment in the RS whose first word was the closest to the last word in the DSH, and otherwise assigning uniform probabilities. Finally, Model D combines “leftmost” and “following”: where the leftmost and following segments are different, both are assigned double th"
N06-1004,P05-1069,0,0.0636859,"d Kuhn, Denis Yuen, Michel Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that"
N06-1004,N04-4026,0,0.426085,"Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they"
N06-1004,J93-2003,0,\N,Missing
N06-1004,C96-2141,0,\N,Missing
N06-1004,H05-1021,0,\N,Missing
N06-1004,J04-4002,0,\N,Missing
N06-1004,W06-3118,1,\N,Missing
N07-1064,allen-2004-case,0,0.0455551,"Missing"
N07-1064,J93-2003,0,0.0126252,"algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. 511 To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e. source-to-target and target-to-source. In the experiments reported in the next section, the Portage system is used both as a translation and as an APE system. While we can think of a number of modifications to such a system to better adapt it to the post-editing task (some of which are discussed later on), we have done no such modifications to the sy"
N07-1064,2006.eamt-1.27,0,0.223315,"Missing"
N07-1064,W06-1607,0,0.0214531,"e sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. Portage’s model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (˜ s|t˜) are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the loglinear model are set using Och’s minium error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate n-best translation hypotheses, that are representative of the entire decoding search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until"
N07-1064,N03-1017,0,0.0249678,"Missing"
N07-1064,koen-2004-pharaoh,0,0.0105662,"sing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. For languages such as French and English, the first of these phases (tokenization) is mostly a straightforward process; we do not describe it any further here. Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P (t|s). Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. Portage’s model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to t"
N07-1064,W02-1018,0,0.021527,"ime-consuming and expensive, it can only fix a subset of the MT system’s problems. The quality of machine translation (MT) is generally considered insufficient for use in the field without a significant amount of human correction. In the translation world, the term post-editing is often used to refer to the process of manually correcting MT output. While the conventional wisdom is that postediting MT is usually not cost-efficient compared to full human translation, there appear to be situations The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al. (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks. However, the success of this operation cru508 Proceedings of NAACL HLT 2007, pages 508–515, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics cially depends on the amount of training data available. Moreover, the current state of the technology is still insufficient for consistently producing human readable translations. This state of affa"
N07-1064,P03-1021,0,0.0247436,"ombination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (˜ s|t˜) are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the loglinear model are set using Och’s minium error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate n-best translation hypotheses, that are representative of the entire decoding search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. 511 To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder,"
N07-1064,W05-0822,0,0.0179818,"hile the automation of this process can be envisaged in many different ways, the task is not conceptually very different from the translation task itself. Therefore, there doesn’t seem to be any good reason why a machine translation system could not handle the post-editing task. In particular, given such data as described in Section 2.2, the idea of using a statistical MT system for post-editing is appealing. Portage is precisely such a system, which we describe here. Portage is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005). A version of the Portage system is made available by the NRC to Canadian universities for research and education purposes. Like other SMT systems, it learns to translate from existing parallel corpora. The system translates text in three main phases: preprocessing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. For languages such as French and English, the first of these phases (tokenization) is mostly a straightforward process; we do not describe it any further here. Decoding is the central phas"
N07-1064,2006.amta-papers.25,0,0.19614,"Missing"
P98-1117,J90-2002,0,0.0991742,"a program that reduces the search space only to those sentence pairs that are potentially interesting (Simard and Plamondon, 1996). The underlying principle is the automatic detection of isolated cognates (i.e. for which no other similar word exists in a window of given size). Once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (Gale and Church, 1991). RALI/Sallgn The second m e t h o d proposed by RALI is based on a dynamic programming scheme which uses a score function derived from a translation model similar to that of (Brown et al., 1990). T h e search space is reduced to a beam of fixed width around the diagonal (which would represent the alignment if the two texts were perfectly synchronized). L O R I A The strategy adopted in this system differs from that of the other systems since sentence alignment is performed after the preliminary alignment of larger units (whenever possible, using mark-up), such as paragraphs and divisions, on the basis of the SGML structure. A dynamic programming scheme is applied to all alignment levels in successive steps. IRMC This system involves a preliminary, rough word alignment step which uses"
P98-1117,P91-1022,0,0.559327,"Missing"
P98-1117,A94-1006,0,0.0164473,"growing interest in parallel text alignment techniques. These techniques attempt to m a p various textual units to their translation and have proven useful for a wide range of applicatious and tools. A simple example of such a tool is probably the TransSearch bilingual concordancing system (Isabelle et al., 1993), which allows a user to query a large archive of existing translations in order to find ready-made solutions to specific translation problems. Such a tool has proved extremely useful not only for translators, but also for bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). More sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilingual lexical resources (Melamed, 1996; Klavans 711 J e a n Vdronis LPL, Univ. de Provence 29, Av. R. Schuman F-13621 Aix-en-Provence Cedex 1 veronis~univ-aix.fr and Tzoukermann, 1995), the automatic verification of translations (Macklovitch, 1995), the automatic dictation of translations (Brousseau et al., 1995) and even interactive machine translation (Foster et al., 1997). Enthusiasm for this relatively new field was sparked early on by the apparent"
P98-1117,C92-2079,0,0.034314,"Missing"
P98-1117,P91-1023,0,0.0852987,"36*20 = 975 recall = 975/1515 precision = 0.64 = 1 F=0.78 5 Systems tested Six systems were tested, two of which having been submitted by the I:tALI. 714 RALI/Jacal This system uses as a first step a program that reduces the search space only to those sentence pairs that are potentially interesting (Simard and Plamondon, 1996). The underlying principle is the automatic detection of isolated cognates (i.e. for which no other similar word exists in a window of given size). Once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (Gale and Church, 1991). RALI/Sallgn The second m e t h o d proposed by RALI is based on a dynamic programming scheme which uses a score function derived from a translation model similar to that of (Brown et al., 1990). T h e search space is reduced to a beam of fixed width around the diagonal (which would represent the alignment if the two texts were perfectly synchronized). L O R I A The strategy adopted in this system differs from that of the other systems since sentence alignment is performed after the preliminary alignment of larger units (whenever possible, using mark-up), such as paragraphs and divisions, on"
P98-1117,1993.tmi-1.17,0,0.0122396,"e both on the evaluation protocols and metrics, and the algoritbm.q used by the different systems. For the second phase, which is now underway, A R C A D E has been opened to a larger number of teams who will tackle the problem of word-level alignment. 1 Introduction In the last few years, there has been a growing interest in parallel text alignment techniques. These techniques attempt to m a p various textual units to their translation and have proven useful for a wide range of applicatious and tools. A simple example of such a tool is probably the TransSearch bilingual concordancing system (Isabelle et al., 1993), which allows a user to query a large archive of existing translations in order to find ready-made solutions to specific translation problems. Such a tool has proved extremely useful not only for translators, but also for bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). More sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilingual lexical resources (Melamed, 1996; Klavans 711 J e a n Vdronis LPL, Univ. de Provence 29, Av. R. Schuman F-13621 Aix-en-Provence Cedex 1 veroni"
P98-1117,1996.amta-1.4,0,0.038694,"the last few years, there has been a growing interest in parallel text alignment techniques. These techniques attempt to m a p various textual units to their translation and have proven useful for a wide range of applicatious and tools. A simple example of such a tool is probably the TransSearch bilingual concordancing system (Isabelle et al., 1993), which allows a user to query a large archive of existing translations in order to find ready-made solutions to specific translation problems. Such a tool has proved extremely useful not only for translators, but also for bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). More sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilingual lexical resources (Melamed, 1996; Klavans 711 J e a n Vdronis LPL, Univ. de Provence 29, Av. R. Schuman F-13621 Aix-en-Provence Cedex 1 veronis~univ-aix.fr and Tzoukermann, 1995), the automatic verification of translations (Macklovitch, 1995), the automatic dictation of translations (Brousseau et al., 1995) and even interactive machine translation (Foster et al., 1997). Enthusiasm for this relatively ne"
P98-1117,1995.mtsummit-1.36,0,0.0477579,"Missing"
P98-1117,P97-1039,0,0.122178,"Missing"
P98-1117,1996.amta-1.14,1,0.888247,"sult in a dramatic loss of efficiency. The truth is that, while text alignment is mostly an easy problem, especially when considered at the sentence level, there are situations where even humans have a hard time making the right decision. In fact, it could be argued that, ultimately, text alignment is no easier than the more general problem of natural language understanding. In addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment (Brown et al., 1991; Gale and Church, 1991; Debili, 1992; Kay and l~scheisen, 1993; Simard et al., 1992; Simard and Plamondon, 1996). Alignment at the word and term level, which is extremely useful for applications such as lexieal resource extraction, is still a largely unexplored research area(Melamed, 1997). In order to live up to the expectations of the various application fields, alignment technology will therefore have to improve substantially. As was the case with several other language processing techniques (such as information retrieval, document understanding or speech recognition), it is likely that a systematic evaluation will enable such improvements. However, before the ARCADE project started, no forreal evalu"
P98-1117,1992.tmi-1.7,1,0.897757,"Missing"
P98-1117,1996.amta-1.13,0,\N,Missing
S14-2030,P07-1019,0,0.0169033,"andard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance larg"
S14-2030,2010.jec-1.4,0,0.0387737,"Missing"
S14-2030,2005.mtsummit-papers.11,0,0.0142599,"his results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-en train dev 2002k - 157k 2000 2158k 2000 nl-en train dev 1974k 1984 - 1974k 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for all 4 language pairs of the evaluation. As Europarl covers parliamentary proceedings, we added some news and commentary (henceforth ”News”) data provided for the 2013 workshop on Machine Translation shared task (Bojar et al., 2013) for language pairs other than nl-en. In all cases, we extracted from the corpus a tuning (“dev”) set of around 2000 sentence pairs. Statistics for the training data are given in Table 1. The trial and test data each consist of 500 sentences with L1 fragments in L2 context provided by the organizers. As the trial data came from Europarl, we f"
S14-2030,J03-1002,0,0.00547523,"Missing"
S14-2030,2009.mtsummit-papers.14,1,0.823815,"Missing"
S14-2030,N04-4026,0,0.01245,"op on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-e"
S14-2030,N12-1047,0,0.0151326,"anslation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inTable 2 shows that our run1 system already yields high performance on the trial data, while 193 en-de1 en-de2 en-es1 en"
S14-2030,N04-1033,0,0.02597,"ation and reordering models), as well as the decoding and parameter tuning. Translation Models We use a single static phrase table including phrase pairs extracted from the symmetrized HMM word-alignment learned c 2014, The Crown in Right of Canada. 192 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following"
S14-2030,W07-0717,0,0.0205944,"only component of the SMT system that scores how well the translation of the L1 fragment fits in the existing L2 context. We test two different LM configurations. The first of these (run1) uses a single static LM: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adap"
S14-2030,P99-1043,0,0.0910822,"Carpuat National Research Council Canada Multilingual Text Processing 1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada FirstName.LastName@nrc.ca Abstract there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train the SMT engine (Section 3), and our results on the trial and test data, as well as a short error analysis (Section 4). section We describe the system entered by the National Research Council Canada in the SemEval-2014 L2 writing assistant task. Our system relies on a standard Phrase-Based Statistical Machine Transl"
S14-2030,W10-1717,0,\N,Missing
S14-2030,W13-2201,0,\N,Missing
S14-2030,2005.iwslt-1.8,0,\N,Missing
S16-1102,W09-1206,0,0.0837081,"Missing"
S16-1102,N12-1047,0,0.0449274,"feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done b"
S16-1102,W07-0717,0,0.0431433,"(NC) – totaling approximately 110M words in each language. Phrase extraction was done by aligning the corpora at the word level using HMM, IBM2 and IBM4 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely"
S16-1102,P07-1019,0,0.0734543,"age models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done by means of loglinear model features that aim at maximizing n-gram prec"
S16-1102,P14-2124,1,0.849566,"arget English word using a word embeddings model. In our experiments, we used pretrained word2vec (Mikolov et al., 2013) embeddings.2 The resulting crosslingual lexical similarity of the targeted pair of Spanish and English words is the highest similarity between the 5 mapped words and the target English word. We then reconstruct the semantic phrasal similarity by averaging the Englishidf-weighted crosslingual embeddings mapped lexical similarity according to the 1-1 maximal matching alignment of the lexicons in the two phrases. In addition to the flat lexical semantic feature, we use XMEANT (Lo et al., 2014), the crosslingual semantic frame based machine translation evaluation metric, for generating shallow structural semantic features. We use MATE (Bj¨orkelund et al., 2009) for Spanish shallow semantic parsing and SENNA (Collobert et al., 2011) for English shallow semantic parsing. In evaluating machine translation quality, the confusion of semantic roles is a major source of errors due to reordering. However, in evaluating STS, confusion of semantic roles is less frequent while missing information in one of the test fragments is more frequent. This motivates a further simplification of the 12 s"
S16-1102,W15-3056,1,0.877844,"Missing"
S16-1102,N04-4026,0,0.0512051,"Missing"
S16-1102,N04-1033,0,0.0665645,"phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface"
S16-1102,2005.iwslt-1.8,0,\N,Missing
W03-0304,J90-2002,0,0.404222,"Missing"
W03-0304,J93-2003,0,0.035939,"Missing"
W03-0304,P00-1056,0,0.242079,"Missing"
W03-0304,W99-0604,0,0.0924544,"Missing"
W03-0304,J97-3002,0,0.220223,"Missing"
W03-0313,W00-0731,0,0.0238223,"anguage. These documents were mostly collected over the Internet, had the HTML markup removed, were then segmented into paragraphs and sentences, aligned at the sentence level using an implementation of the method described in (Simard et al., 1992), and finally dumped into a document-retrieval system (MG (Witten et al., 1999)). We call this the Hansard TM. To identify SL queries, a distinct document from the Hansard was used, the transcript from a session held in March 2002. The English version of this document was segmented into syntactic chunks, using an implementation of Osborne’s chunker (Osborne, 2000). All sequences of chunks from this text that contained three or more word tokens were then looked up in the Hansard TM. Among the sequences that did match sentences in the TM, 100 were selected at random. These made up the test SL queries. Recursion Level 1 2 3 4 Answers: SL segment [Let us see] [where the government ’s commitment is really at in terms of the farm community] [where the government ’s commitment is really at] [in terms of the farm community] [where] [the government ’s commitment is really at] [the government ’s commitment] [is really at] rq (S) =the government ’s commitment ←→"
W03-0313,C04-1031,0,0.0393011,"Missing"
W03-0313,macklovitch-etal-2000-transsearch,1,0.871203,"Missing"
W03-0313,P01-1050,0,0.0669025,"Missing"
W03-0313,W99-0604,0,\N,Missing
W03-0313,C96-1030,0,\N,Missing
W03-0313,J93-2003,0,\N,Missing
W03-0313,1999.tc-1.11,0,\N,Missing
W03-0313,J97-3002,0,\N,Missing
W06-3118,N06-1004,1,0.829818,"s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SCMs differ from the conventional penalty-based distortion, which always favours less rather than more distortion. We developed a particular kind of SCM based on decision trees (DTs) containing both questions of a positional type (e.g., questions about the distance of a given phrase from the beginning of the source sentence or from the previ"
W06-3118,W05-0822,1,0.831685,"le data set and explore the benefits of a number of recently added features. Section 2 describes the changes that have been made to Portage in the past year that affect the participation in the 2006 shared task. Section 3 outlines the methods employed for this task and extensions of it. In Section 4 the results are summarized in tabular form. Following these, there is a conclusions section that highlights what can be gleaned of value from these results. 2 Portage Because this is the second participation of Portage in such a shared task, a description of the base system can be found elsewhere (Sadat et al, 2005). Briefly, Portage is a research vehicle and development prototype system exploiting the state-of-the-art in statistical machine translation (SMT). It uses a custom Phrase-Table Smoothing Phrase-based SMT relies on conditional distributions p(s|t) and p(t|s) that are derived from the joint frequencies c(s, t) of source/target phrase pairs observed in an aligned parallel corpus. Traditionally, relative-frequency estimation is used to derive conP ditional distributions, ie p(s|t) = c(s, t)/ s c(s, t). However, relative-frequency estimation has the well-known problem of favouring rare events. For"
W06-3118,W05-0800,0,0.0869259,"Missing"
W06-3118,2006.jeptalnrecital-poster.23,1,0.726586,"n to the training resources used in WPT 2005 for the French-English task, i.e. Europarl and Hansard, we used a bilingual dictionary, Le Grand Dictionnaire Terminologique (GDT) 2 to train translation models and the English side of the UN parallel corpus (LDC2004E13) to train an English language model. Integrating terminological lexicons into a statistical machine translation engine is not a straightforward operation, since we cannot expect them to come with attached probabilities. The approach we took consists on viewing all translation candidates of each source term or phrase as equiprobable (Sadat et al, 2006). In total, the data used in this second part of our contribution to WMT 2006 is described as follows: (1) A set of 688,031 sentences in French and English extracted from the Europarl parallel corpus (2) A set of 6,056,014 sentences in French and English extracted from the Hansard parallel corpus, the official record of Canada’s parliamentary debates. (3) A set of 701,709 sentences in French and English extracted from the bilingual dictionary GDT. (4) Language models were trained on the French and English parts of the Europarl and Hansard. We used the provided Europarl corpus while omitting da"
W06-3118,N04-1033,0,0.0248659,"s phrase. The resulting estimates are: cg (s, t) , s cg (s, t) + p(t)n1 pg (s|t) = P P where p(t) = c(t)/ t c(t). The estimates for pg (t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 1995), using the interpolated variant described in (Chen and Goodman., 1998):1 pk (s|t) = c(s, t) − D + D n1+ (∗, t) pk (s) P s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SC"
W07-0724,J93-2003,0,0.00863756,"Missing"
W07-0724,W06-1607,1,0.932977,"d several new decoder and rescoring models. PORTAGE was also used in a joint system developed in cooperation with Systran. The interested reader is referred to (Simard et al., 2007). Throughout this paper, let sJ1 := s1 . . . sJ denote a source sentence of length J, tI1 := t1 . . . tI a target sentence of length I, and s˜ and t˜ phrases in source and target language, respectively. • one or several phrase table(s), which model the translation direction p(˜ s |t˜). They are generated from the training corpus via the “diag-and” method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm."
W07-0724,W06-3118,1,0.847694,"Missing"
W07-0724,D07-1103,1,0.828914,"Missing"
W07-0724,N03-1017,0,0.00913297,"e, a higher-order language model, adapted language models, and several new decoder and rescoring models. PORTAGE was also used in a joint system developed in cooperation with Systran. The interested reader is referred to (Simard et al., 2007). Throughout this paper, let sJ1 := s1 . . . sJ denote a source sentence of length J, tI1 := t1 . . . tI a target sentence of length I, and s˜ and t˜ phrases in source and target language, respectively. • one or several phrase table(s), which model the translation direction p(˜ s |t˜). They are generated from the training corpus via the “diag-and” method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented i"
W07-0724,P03-1021,0,0.0201459,"t˜). They are generated from the training corpus via the “diag-and” method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. 185 Proceedings of the Second Workshop on Statistical Machine Translation, pages 185–188, c Prague, June 2007. 2007 Association for Computational Linguistics After the decoding step, rescoring with additional models is performed. The baseline system generates a 1,000-best list of alternative translations for each source sentence. These lists are rescored with the different models described above, a character penalty, and three different features bas"
W07-0724,W07-0728,1,0.90131,"een made available to Canadian universities and research institutions. It is a state-of-the-art phrase-based SMT system. We will shortly describe its basics in this paper and then highlight the new methods which we incorporated since our participation in the WMT 2006 shared task. These include new scoring methods for phrase pairs, pruning of phrase tables based on significance, a higher-order language model, adapted language models, and several new decoder and rescoring models. PORTAGE was also used in a joint system developed in cooperation with Systran. The interested reader is referred to (Simard et al., 2007). Throughout this paper, let sJ1 := s1 . . . sJ denote a source sentence of length J, tI1 := t1 . . . tI a target sentence of length I, and s˜ and t˜ phrases in source and target language, respectively. • one or several phrase table(s), which model the translation direction p(˜ s |t˜). They are generated from the training corpus via the “diag-and” method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, •"
W07-0724,J07-1003,1,0.84029,", using an IBM1 translation model in the direction p(tI1 |sJ1 ). In the rescoring process, we additionally included several types of posterior probabilities. One is the posterior probability of the sentence length over the N -best list for this source sentence. The others are determined on the level of words, phrases, and n-grams, and then combined into a value for the whole sentence. All posterior probabilities are calculated over the N best list, using the sentence probabilities which the baseline system assigns to the translation hypotheses. For details on the posterior probabilities, see (Ueffing and Ney, 2007; Zens and Ney, 2006). This year, we increased the length of the N -best lists from 1,000 to 5,000. 3.4 Post-processing For truecasing the translation output, we used the model described in (Agbago et al., 2005). This model uses a combination of statistical components, including an n-gram language model, a case mapping model, and a specialized language model for unknown words. The language model is a 5-gram model trained on the WMT 2007 data. The detokenizer which we used is the one provided for WMT 2007. 4 Experimental results We submitted results for six of the translation directions of the"
W07-0724,W06-3110,0,0.03046,"tion model in the direction p(tI1 |sJ1 ). In the rescoring process, we additionally included several types of posterior probabilities. One is the posterior probability of the sentence length over the N -best list for this source sentence. The others are determined on the level of words, phrases, and n-grams, and then combined into a value for the whole sentence. All posterior probabilities are calculated over the N best list, using the sentence probabilities which the baseline system assigns to the translation hypotheses. For details on the posterior probabilities, see (Ueffing and Ney, 2007; Zens and Ney, 2006). This year, we increased the length of the N -best lists from 1,000 to 5,000. 3.4 Post-processing For truecasing the translation output, we used the model described in (Agbago et al., 2005). This model uses a combination of statistical components, including an n-gram language model, a case mapping model, and a specialized language model for unknown words. The language model is a 5-gram model trained on the WMT 2007 data. The detokenizer which we used is the one provided for WMT 2007. 4 Experimental results We submitted results for six of the translation directions of the shared task: French ↔"
W07-0728,W06-1607,1,0.620595,"tails of which can be found in (Ueffing et al., 2007). The main features of this configuration are: • The use of two distinct phrase tables, containing phrase pairs extracted from the Europarl and the News Commentary training corpora respectively. • Multiple phrase-probability feature functions in the log-linear models, including a joint prob1 A version of PORTAGE is made available by the NRC to Canadian universities for research and education purposes. 204 ability estimate, a standard frequency-based conditional probability estimate, and variants thereof based on different smoothing methods (Foster et al., 2006). • A 4-gram language model trained on the combined Europarl and News Commentary targetlanguage corpora. • A 3-gram adapted language model: this is trained on a mini-corpus of test-relevant targetlanguage sentences, extracted from the training material using standard information retrieval techniques. • A 5-gram truecasing model, trained on the combined Europarl and News Commentary target-language corpora. 2.3 Training data Ideally, the training material for the post-editing layer of our system should consist in a corpus of text in two parallel versions: on the one hand, raw machine translation"
W07-0728,W05-0822,1,0.476331,"e did not rely on this feature, and used the system in its basic “out-of-the-box” configuration. 2.2 Statistical Phrase-based Post-Editing The output of the rule-based MT system described above is fed into a post-editing layer that performs domain-specific corrections and adaptation. This operation is conceptually not very different from a “target-to-target” translation; for this task, we used the PORTAGE system, a state-of-the-art statistical phrase-based machine translation system developed at the National Research Council of Canada (NRC). 1 A general description of PORTAGE can be found in (Sadat et al., 2005). For our participation in this shared task, we decided to configure and train the PORTAGE system for post-editing in a manner as much as possible similar to the corresponding translation system, the details of which can be found in (Ueffing et al., 2007). The main features of this configuration are: • The use of two distinct phrase tables, containing phrase pairs extracted from the Europarl and the News Commentary training corpora respectively. • Multiple phrase-probability feature functions in the log-linear models, including a joint prob1 A version of PORTAGE is made available by the NRC to"
W07-0728,N07-1064,1,0.836481,", and as much as 5 BLEU points improvement over the direct SMT approach. This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system. An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation. Experimental results on the test data of the previous campaign are presented. 1 Introduction Simard et al. (2007) have recently shown how a statistical phrase-based machine translation system can be used as an automatic post-editing (APE) layer, on top of a rule-based machine translation system. The motivation for their work is the repetitive nature of the errors typically made by rule-based systems. Given appropriate training material, a statistical MT system can be trained to correct these systematic errors, therefore reducing the post-editing effort. The statistical system views the output of the rule-based system as the source language, and reference human translations as the target language. Because"
W07-0728,W07-0732,0,\N,Missing
W07-0728,W07-0724,1,\N,Missing
W12-3156,N09-1014,0,0.0206576,"Missing"
W12-3156,J93-2003,0,0.0280353,"ce level, ignoring wider document context. Does this hurt the consistency of translated documents? Using a phrase-based SMT system in various data conditions, we show that SMT translates documents remarkably consistently, even without document knowledge. Nevertheless, translation inconsistencies often indicate translation errors. However, unlike in human translation, these errors are rarely due to terminology inconsistency. They are more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sen"
W12-3156,W09-2404,1,0.412746,"cument modeling for SMT (Section 2), we describe our corpora in Section 3 and our general methodology in Section 4. In Section 5, we discuss the results of an automatic analysis of translation consistency, before turning to manual analysis in Section 6. 442 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442–449, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Related work While most SMT systems operate at the sentence level, there is increased interest in modeling document context and consistency in translation. In earlier work (Carpuat, 2009), we investigate whether the “one sense per discourse” heuristic commonly used in word sense disambiguation (Gale et al., 1992) can be useful in translation. We show that “one translation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-Engli"
W12-3156,P12-1098,0,0.0311256,"Missing"
W12-3156,A94-1006,0,0.0773822,"re more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sentence-based SMT on the translation of full documents. From a quality standpoint, it seems safe to assume that translation consistency is as desirable for SMT as for human translations. However, consistency needs to be balanced with other quality requirements. For instance, strict consistency might result in awkward repetitions that make translations less fluent. From a translation modeling standpoint, while typical SMT systems do not"
W12-3156,2010.amta-papers.24,0,0.103598,"Missing"
W12-3156,H92-1045,0,0.247853,"ion 5, we discuss the results of an automatic analysis of translation consistency, before turning to manual analysis in Section 6. 442 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442–449, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Related work While most SMT systems operate at the sentence level, there is increased interest in modeling document context and consistency in translation. In earlier work (Carpuat, 2009), we investigate whether the “one sense per discourse” heuristic commonly used in word sense disambiguation (Gale et al., 1992) can be useful in translation. We show that “one translation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-English news documents from the NIST evaluation. Several recent contributions integrate translation consistency models in SMT using"
W12-3156,D11-1084,0,0.348987,"Missing"
W12-3156,2007.mtsummit-papers.36,0,0.0200872,"inconsistency. They are more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sentence-based SMT on the translation of full documents. From a quality standpoint, it seems safe to assume that translation consistency is as desirable for SMT as for human translations. However, consistency needs to be balanced with other quality requirements. For instance, strict consistency might result in awkward repetitions that make translations less fluent. From a translation modeling standpoint, while ty"
W12-3156,P11-1124,0,0.0269517,"Missing"
W12-3156,W10-2602,0,0.379412,"lation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cache-based adaptive models (Tiedemann, 2010a; Gong et al., 2011), which lets lexical choice in a sentence be informed by translations of previous sentences. However, cache-based models are sensitive to error propagation and can have a negative impact on some data sets (Tiedemann, 2010b). Moreover, this approach blurs the line between consistency and domain modeling. In fact, Gong et al. (2011) reports statistically significant improvements in BLEU only when combining pure consistency caches with topic and similarity caches, which do not enforce consistency but essentially perform domain or topic adaptation. There is also work that indi"
W12-3156,W10-1728,0,0.0696175,"lation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cache-based adaptive models (Tiedemann, 2010a; Gong et al., 2011), which lets lexical choice in a sentence be informed by translations of previous sentences. However, cache-based models are sensitive to error propagation and can have a negative impact on some data sets (Tiedemann, 2010b). Moreover, this approach blurs the line between consistency and domain modeling. In fact, Gong et al. (2011) reports statistically significant improvements in BLEU only when combining pure consistency caches with topic and similarity caches, which do not enforce consistency but essentially perform domain or topic adaptation. There is also work that indi"
W12-3156,N12-1046,0,0.295652,"Missing"
W12-3156,2011.mtsummit-papers.13,0,0.283077,"anslation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-English news documents from the NIST evaluation. Several recent contributions integrate translation consistency models in SMT using a two-pass decoding approach. In phrase-based SMT, Xiao et al. (2011) show that enforcing translation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cac"
W18-6480,W11-2123,0,0.0229985,"disproportionately Parallelism estimation With sentence vectors (§2.1) for the reduced corpus (§2.2) in hand, we set out to estimate the degree of parallelism of sentence pairs. A novel measure of parallelism, based on ratios of squared Mahalanobis distances, performed better on a synthetic dataset than some more obvious measurements, and the single-feature submission based on it was our best unsupervised submission. We also made several other unsupervised measurements: 2 901 https://github.com/aboSamoor/pycld2 1. Perplexity of the German sentence according to a 6-gram KenLM language model3 (Heafield, 2011) not just the one that happened to come first in the original corpus. 3 2. Perplexity of the English sentence according to a 6-gram KenLM language model Mahalanobis ratios for parallelism assessment As mentioned in §2.3, we performed several unsupervised measurements on each sentence pair; of these, the measurement that best predicted paralellism (on synthetic data and on our small 300sentence annotated set) was a novel measurement based on squared Mahalanobis distances. This measurement rests on two insights: 3. The ratio between (1) and (2), to find sentences pairs that contain different amo"
W18-6480,2005.mtsummit-papers.11,0,0.223295,"supervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task—translating the European Medicines Agency corpus (Tiedemann, 2009)—scored among the best systems even in the 10M-word conditions. 1 Introduction and motivation The WMT18 shared task on parallel corpus filtering assumes (but does not require) a supervised learning approach. Given 1. a set of “clean” German-English parallel corpora including past WMT data, Europarl (Koehn, 2005), etc., and 2. a large, potentially “dirty” corpus (i.e., one that may contain non-parallel data, nonlinguistic data, etc.) scraped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We a"
W18-6480,W18-6481,1,0.639364,"onference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervised submission, NRC-yisi-bicov (Lo et al., 2018); it differs primarily in the parallelism estimation component (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began b"
W18-6480,W03-0320,0,0.0898108,"craped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We are thinking in particular of the English-Inuktitut translation pair, which is a long-standing research interest of NRC (e.g. Martin et al., 2003). 900 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervi"
W18-6480,N18-1049,0,0.0495645,"ent (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began by training monolingual sentence embeddings using sent2vec (Pagliardini et al., 2018), on all available monolingual data. This included the monolingual data available in the “clean” parallel training data. That is to say, we did not completely throw out the clean parallel data for this task, we simply used it as two unaligned monolingual corpora. We trained sentence vectors of 10, 50, 100, 300, and 700 dimensions; our final submissions used the 300-dimensional vectors as a compromise between accuracy (lower-dimensional vectors had lower accuracy during sanity-checking) and efficiency (higher-dimensional vectors ended up exceeding our memory capacity in downstream components)."
W18-6481,2012.amta-papers.7,1,0.908512,"allel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108"
W18-6481,W11-2123,0,0.10867,"aining data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry"
W18-6481,J93-2003,0,0.0529985,"placeholder token, before deciding which sentences were duplicates. Sentence pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network j"
W18-6481,E17-3017,0,0.0199669,"selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the feature scores showed the same trend as the results of the sanity check. That is to say, a feature that performed better in the sanity che"
W18-6481,W10-1703,0,0.0648474,"Missing"
W18-6481,D11-1125,0,0.0564437,"log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the fi"
W18-6481,P07-1019,0,0.0250594,"sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 srl measures the semantic similarity with additional frame semantic or"
W18-6481,W12-3102,0,0.0959196,"Missing"
W18-6481,W18-6453,0,0.0549896,". In fact, our best performing system—NRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textua"
W18-6481,W11-2103,0,0.0555991,"Missing"
W18-6481,N03-1017,0,0.0132764,"ce pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source"
W18-6481,N13-1003,0,0.0199856,"word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P"
W18-6481,N12-1047,0,0.0247327,"2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 s"
W18-6481,P14-1129,0,0.126783,"Missing"
W18-6481,W18-6480,1,0.552227,"he larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the crosslingual lexical semantic similarity of the sentence pairs over the weighted count of tokens in the German and English sentences respectively. In this work, we set the n-gram siz"
W18-6481,W17-4767,1,0.789134,"egmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide the inverse-document-frequency weighted sum of the n-gram lexical semantic similarity of the MT output and the English sentence of the target pair by the weighted count of n-grams in the MT output and the English sentences, respectively. In this work, we set the n-gram size to two. Precisely, YiSi-1 is computed as follows: 0 qi,j = ARG j of aligned frame i in MT 1 qi,j = ARG j of aligned frame i in REF #tokens filled in aligned frame i of MT = total #token"
W18-6481,D13-1140,0,0.0158726,"e models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+"
W18-6481,S16-1102,1,0.839606,"Missing"
W18-6481,C96-2141,0,0.302528,"nguage. The target scores of these pairs are proportional to the percentage of tokens offset, deleted or introduced. Lastly, misaligned sentence pairs were added as fluent but non-parallel negative examples. The resulting development set had 11k sentence pairs of positive and synthetic negative examples. Alignment scores The SMT model trained on the “clean” WMT18 news translation task parallel training data for YiSi score computation include several alignment models as components, from which probabilities p(d|e) and p(e|d) were computed. We find the hidden markov model (HMM) alignment models (Vogel et al., 1996) are reliably useful for scoring parallelism of the sentence pairs in the target corpus. Perplexity ratio of input sentences and output sentences The perplexity ratio reflects the different amounts of information contained in each side of the sentence pairs. This is computed by dividing the smaller perplexity score of the two sentences in the target pair by the larger one. Thus, the ratio ranged from 0 to 1, where a larger value represents better parallelism. 911 features baselines random hunalign parallelism YiSi-1 precision YiSi-1 recall YiSi-1 srl (β=1) precision YiSi-1 srl (β=1) recall YiS"
W18-6481,W15-1521,0,0.159484,"Missing"
W18-6481,W16-2327,0,0.0946713,"M words) selections, the redundancy removal had virtually no effect when applied after YiSi scoring. However, on the smaller (10M words) selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the fea"
W18-6481,N18-1049,0,0.039983,"evious feature, the perplexity ratio of the input and output sentences POS tags is computed by dividing the smaller POS perplexity score of the two sentences in the target pair by the larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the cross"
W18-6481,P02-1040,0,0.115557,"on processing only, and includes non-parallel, or even non-linguistic data. It contains 104 million German-English sentence pairs, with 1 billion English tokens and 964 million German tokens before punctuation tokenization. A 10-million-word (10M-word) and a 100-millionword (100M-word) corpus sub-selected by the participating cleanliness scoring system were used to train statistical machine translation (SMT) and neural machine translation (NMT) systems. The success of the participating scoring systems was determined by the quality of the MT output from the four MT systems as measured by BLEU (Papineni et al., 2002) on some in-domain and out-ofdomain evaluation sets. In this paper, we describe the efforts in developing our supervised submissions: the initial filWe present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSi—a novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-millionword evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing system—"
W18-6481,D14-1045,0,0.0312771,"nformation. It uses a more principle way to compute the precision and recall of semantic similarity between the translation output and the reference when comparing to MEANT 2.0. Instead of aggregating the precision and recall at the segmental semantic similarity level, YiSi-1 srl precision is the weighted sum of the segmental semantic precision and the frame semantic precision and similarly, for YiSi-1 srl recall. The frame semantic precision is the weighted sum of the segmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide"
W18-6481,2014.amta-researchers.6,1,0.700032,"pus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108 We also observ"
W98-1504,A88-1019,0,0.213109,"Missing"
W98-1504,P94-1013,0,0.082627,"pairs at positions i + 1 and i + 2. The experimental results reported in El-BE:ze et al. (1994) indicate success levels slightly superior to ours. This may be explained in part by the use of a better language model (their HMM is three-tag, ours is twotag). It must be said, however, that their test-corpus was relatively small (in all, a little over 8000 words), and that the performances varied wildly from text to text, with average distances between errors varying between 100 and 600 words. A method which exploits different sources of information in the candidate selection task is described in Yarowsky (1994b): this system relies on local context (e.g., words within a 2- or 4-word window around the current word), global context (e.g. a 40-word window), part-of-speech of surrounding words, etc. These arc combined within a unifying framework known as decision lists. Vithin this framework, the system bases its decision for each individual candidate selection on the single most reliable piece of evidence. Although the work described in Yarowsky (1994b) does address the problem of l&lt;'rcnch automatic accentuation, it mostly focuses on the Spanish language. Furthermore, the evaluation focuses on specif"
W99-0602,J90-2002,0,0.109324,"R (McEnery et al., 1997), MULTEXT (Ide and V4ronis, 1994) and MULTEXT-EAST (Erjavec and Ide, 1998) projects. Access to this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations? Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993; Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Melamed, 1996), and cross-language information retrieval (Nie et al., 1 Trilingual Alignments There are various ways in which the concept of alignment can be formalized. Here, we choose to view alignments as mathematical relations between linguistic entities: Given two texts, A and B, seen as sets of linguistic units: A = {al,a2,...,am} and B = {bl, b2, ...,bn}, we define a binary alignment XAB as a relation on A tj B: * This research was funded by the Canadian Department of Foreign Affairs and"
W99-0602,P91-1022,0,0.0644624,"nslation alignments appear to be equivalence relations, which means that they generally display the properties of reflexivity, symmetry and transitivity: 2 • reflexivity: Any word or sequence of words aligns with i t s e l f - which is natural, insofar as we extend the notion of &quot;translation&quot;, so as to include the translation from one language to itself... A General M e t h o d for Aligning Multiple Versions of a Text Existing alignment Mgorithms that rely on the optimality principle and dynamic programming to find the best possible sentence alignment, such as those of Gale and Church (1991), Brown et al. (1991), Simard et al. (1992), Chen (1993), Langlais and E1-B~ze (1997), Simard and Plamondon (1998), etc. can be naturally extended to deal with three texts instead of two, or more generally to deal with N texts. While the resolution of the bilingual problem is analogous to finding an optimal path in a rectangular matrix, aligning N texts is analogous to the same problem, this time in a N-dimensional matrix. Normally, these methods produce alignments in the form of parallel segmentations of the texts into equal numbers of segments. These segmentations are such that 1) segmentation points coincide wi"
W99-0602,J93-2003,0,0.0570107,"t p : / / ~ . franeophonie, orE) XAB={(al,bl),(a2,b2),(a2,b3),...} 2 to be (although we can question the interest of a character-level alignment). However, in the experiments described here, we focus on alignment at the level of sentences, this for a number of reasons: First, sentence alignments have so far proven their usefulness in a number of applications, e.g. bilingual lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Dagan and Church, 1994), automatic translation verification (Macklovitch, 1995; Macklovitch, 1996) and the automatic acquisition of knowledge about translation (Brown et al., 1993). Also, the sentence alignment problem has been widely studied, and we could even say that at this point in time, a certain consensus exists regarding how the problem should be approached. On the other hand, not only is the computation of finer-resolution alignments, such as phrase- or word-level alignments, a much more complex operation, it also raises a number of difficult problems related to evaluation (Melamed, 1998), which we wanted to avoid, at least at this point. Finally, we believe that the concepts, methods and results discussed here can be applied just as well to alignments at other"
W99-0602,P93-1002,0,0.0775966,"e relations, which means that they generally display the properties of reflexivity, symmetry and transitivity: 2 • reflexivity: Any word or sequence of words aligns with i t s e l f - which is natural, insofar as we extend the notion of &quot;translation&quot;, so as to include the translation from one language to itself... A General M e t h o d for Aligning Multiple Versions of a Text Existing alignment Mgorithms that rely on the optimality principle and dynamic programming to find the best possible sentence alignment, such as those of Gale and Church (1991), Brown et al. (1991), Simard et al. (1992), Chen (1993), Langlais and E1-B~ze (1997), Simard and Plamondon (1998), etc. can be naturally extended to deal with three texts instead of two, or more generally to deal with N texts. While the resolution of the bilingual problem is analogous to finding an optimal path in a rectangular matrix, aligning N texts is analogous to the same problem, this time in a N-dimensional matrix. Normally, these methods produce alignments in the form of parallel segmentations of the texts into equal numbers of segments. These segmentations are such that 1) segmentation points coincide with sentence boundaries and 2) the k"
W99-0602,A94-1006,0,0.0177957,"(Ide and V4ronis, 1994) and MULTEXT-EAST (Erjavec and Ide, 1998) projects. Access to this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations? Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993; Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Melamed, 1996), and cross-language information retrieval (Nie et al., 1 Trilingual Alignments There are various ways in which the concept of alignment can be formalized. Here, we choose to view alignments as mathematical relations between linguistic entities: Given two texts, A and B, seen as sets of linguistic units: A = {al,a2,...,am} and B = {bl, b2, ...,bn}, we define a binary alignment XAB as a relation on A tj B: * This research was funded by the Canadian Department of Foreign Affairs and International Trade ( h t t p : / / ~"
W99-0602,P91-1023,0,0.255524,"text-translation alignments appear to be equivalence relations, which means that they generally display the properties of reflexivity, symmetry and transitivity: 2 • reflexivity: Any word or sequence of words aligns with i t s e l f - which is natural, insofar as we extend the notion of &quot;translation&quot;, so as to include the translation from one language to itself... A General M e t h o d for Aligning Multiple Versions of a Text Existing alignment Mgorithms that rely on the optimality principle and dynamic programming to find the best possible sentence alignment, such as those of Gale and Church (1991), Brown et al. (1991), Simard et al. (1992), Chen (1993), Langlais and E1-B~ze (1997), Simard and Plamondon (1998), etc. can be naturally extended to deal with three texts instead of two, or more generally to deal with N texts. While the resolution of the bilingual problem is analogous to finding an optimal path in a rectangular matrix, aligning N texts is analogous to the same problem, this time in a N-dimensional matrix. Normally, these methods produce alignments in the form of parallel segmentations of the texts into equal numbers of segments. These segmentations are such that 1) segmentati"
W99-0602,1993.tmi-1.17,0,0.0174755,"eloped within the CRATER (McEnery et al., 1997), MULTEXT (Ide and V4ronis, 1994) and MULTEXT-EAST (Erjavec and Ide, 1998) projects. Access to this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations? Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993; Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Melamed, 1996), and cross-language information retrieval (Nie et al., 1 Trilingual Alignments There are various ways in which the concept of alignment can be formalized. Here, we choose to view alignments as mathematical relations between linguistic entities: Given two texts, A and B, seen as sets of linguistic units: A = {al,a2,...,am} and B = {bl, b2, ...,bn}, we define a binary alignment XAB as a relation on A tj B: * This research was funded by the Canadian Department of"
W99-0602,1996.amta-1.4,0,0.0384503,"ST (Erjavec and Ide, 1998) projects. Access to this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations? Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993; Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Melamed, 1996), and cross-language information retrieval (Nie et al., 1 Trilingual Alignments There are various ways in which the concept of alignment can be formalized. Here, we choose to view alignments as mathematical relations between linguistic entities: Given two texts, A and B, seen as sets of linguistic units: A = {al,a2,...,am} and B = {bl, b2, ...,bn}, we define a binary alignment XAB as a relation on A tj B: * This research was funded by the Canadian Department of Foreign Affairs and International Trade ( h t t p : / / ~ . d f a i t - m a e c i . g c"
W99-0602,1995.mtsummit-1.36,0,0.029988,"( h t t p : / / ~ . d f a i t - m a e c i . g c . c a / ) , via the Agence de la francophonie ( h t t p : / / ~ . franeophonie, orE) XAB={(al,bl),(a2,b2),(a2,b3),...} 2 to be (although we can question the interest of a character-level alignment). However, in the experiments described here, we focus on alignment at the level of sentences, this for a number of reasons: First, sentence alignments have so far proven their usefulness in a number of applications, e.g. bilingual lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Dagan and Church, 1994), automatic translation verification (Macklovitch, 1995; Macklovitch, 1996) and the automatic acquisition of knowledge about translation (Brown et al., 1993). Also, the sentence alignment problem has been widely studied, and we could even say that at this point in time, a certain consensus exists regarding how the problem should be approached. On the other hand, not only is the computation of finer-resolution alignments, such as phrase- or word-level alignments, a much more complex operation, it also raises a number of difficult problems related to evaluation (Melamed, 1998), which we wanted to avoid, at least at this point. Finally, we believe th"
W99-0602,1996.amta-1.13,0,0.0114381,"this type of corpora raises a number of questions: Do they make new applications possible? Can methods developed for handling bilingual texts be applied to multilingual texts? More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations? Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation (Isabelle et al., 1993; Brown et al., 1990), terminology (Dagan and Church, 1994) lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Melamed, 1996), and cross-language information retrieval (Nie et al., 1 Trilingual Alignments There are various ways in which the concept of alignment can be formalized. Here, we choose to view alignments as mathematical relations between linguistic entities: Given two texts, A and B, seen as sets of linguistic units: A = {al,a2,...,am} and B = {bl, b2, ...,bn}, we define a binary alignment XAB as a relation on A tj B: * This research was funded by the Canadian Department of Foreign Affairs and International Trade ( h t t p : / / ~ . d f a i t - m a e c i . g c . c a / ) , via the Agence de la francophonie"
W99-0602,W98-1103,0,0.0300817,"Missing"
W99-0602,J93-1004,0,\N,Missing
W99-0602,C94-1097,0,\N,Missing
W99-0602,J93-1006,0,\N,Missing
