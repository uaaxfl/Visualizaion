2020.acl-main.214,P19-1285,0,0.261233,", sentiment classification, part-of-speech (POS) tagging and similarity modeling(Peters et al., 2018; Devlin et al., 2018). The first two notable contextual representation based models were ELMO (Peters et al., 2018) and GPT (Radford et al., 2018). However, they only captured unidirectional context and therefore, missed more nuanced interactions among words of a sentence. BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) outperforms both ELMO and GPT since it can provide better representation through capturing bi-directional context using Transformers. XLNet(Dai et al., 2019) gives new contextual representations through building an auto-regressive model capable of capturing all possible factorizations of the input. Fine-tuning pretrained mod2360 els for BERT and XLNet has been a key factor in achieving state of the art performance for downstream tasks. Even though previous works have explored using BERT to model multimodal data (Sun et al., 2019), to the best of our knowledge, directly fine-tuning BERT or XLNet for multimodal data has not been explored in previous works. 3 BERT and XLNet To better understand the proposed multimodal framework in this paper, we firs"
2020.acl-main.214,N18-1193,1,0.821899,"Related Works The studies in this paper are related to the following research areas: 2.1 Multimodal Language Analyses Multimodal language analyses is a recent research trend in natural language processing (Zadeh et al., 2018b) that helps us understand language from the modalities of text, vision and acoustic. These analyses have particularly focused on the tasks of sentiment analysis (Poria et al., 2018), emotion recognition (Zadeh et al., 2018d), and personality traits recognition (Park et al., 2014). Works in this area often focus on novel multimodal neural architectures (Pham et al., 2019; Hazarika et al., 2018) and multimodal fusion approaches (Liang et al., 2018; Tsai et al., 2018). Related to content in this paper, we discuss some of the models in this domain including TFN, MARN, MFN, RMFN and MulT. Tensor Fusion Network (TFN) (Zadeh et al., 2017) creates a multi-dimensional tensor to explicitly capture all possible interactions between the three modalities: unimodal, bimodal and trimodal. Multiattention Recurrent Network (MARN) (Zadeh et al., 2018c) uses three separate hybrid LSTM memories that have the ability to propagate the cross-modal interactions. Memory Fusion Network (Zadeh et al., 2018a)"
2020.acl-main.214,D18-1014,1,0.86044,"e following research areas: 2.1 Multimodal Language Analyses Multimodal language analyses is a recent research trend in natural language processing (Zadeh et al., 2018b) that helps us understand language from the modalities of text, vision and acoustic. These analyses have particularly focused on the tasks of sentiment analysis (Poria et al., 2018), emotion recognition (Zadeh et al., 2018d), and personality traits recognition (Park et al., 2014). Works in this area often focus on novel multimodal neural architectures (Pham et al., 2019; Hazarika et al., 2018) and multimodal fusion approaches (Liang et al., 2018; Tsai et al., 2018). Related to content in this paper, we discuss some of the models in this domain including TFN, MARN, MFN, RMFN and MulT. Tensor Fusion Network (TFN) (Zadeh et al., 2017) creates a multi-dimensional tensor to explicitly capture all possible interactions between the three modalities: unimodal, bimodal and trimodal. Multiattention Recurrent Network (MARN) (Zadeh et al., 2018c) uses three separate hybrid LSTM memories that have the ability to propagate the cross-modal interactions. Memory Fusion Network (Zadeh et al., 2018a) synchronizes the information from three separate LST"
2020.acl-main.214,D14-1162,0,0.0973983,"ed interactions among the modalities in a multi-stage manner, giving each stage the ability to focus on a subset of signals. Multimodal Transformer for Unaligned Multimodal Language Sequences (MulT) (Tsai et al., 2019) deploys three Transformers – each for one modality – to capture the interactions with the other two modalities in a selfattentive manner. The information from the three Transformers are aggregated through late-fusion. 2.2 Pre-trained Language Representations Learning word representations from large corpora has been an active research area in NLP community (Mikolov et al., 2013; Pennington et al., 2014). Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) contributed to advancing the state-of-the-art of many NLP tasks. A major setback of these word representations is their non-contextual nature. Recently, contextual language representation models trained on large text corpora have achieved state of the art results on several NLP tasks including question answering, sentiment classification, part-of-speech (POS) tagging and similarity modeling(Peters et al., 2018; Devlin et al., 2018). The first two notable contextual representation based models were ELMO (Peters et al., 2018)"
2020.acl-main.214,N18-1202,0,0.031999,"ord representations from large corpora has been an active research area in NLP community (Mikolov et al., 2013; Pennington et al., 2014). Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) contributed to advancing the state-of-the-art of many NLP tasks. A major setback of these word representations is their non-contextual nature. Recently, contextual language representation models trained on large text corpora have achieved state of the art results on several NLP tasks including question answering, sentiment classification, part-of-speech (POS) tagging and similarity modeling(Peters et al., 2018; Devlin et al., 2018). The first two notable contextual representation based models were ELMO (Peters et al., 2018) and GPT (Radford et al., 2018). However, they only captured unidirectional context and therefore, missed more nuanced interactions among words of a sentence. BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) outperforms both ELMO and GPT since it can provide better representation through capturing bi-directional context using Transformers. XLNet(Dai et al., 2019) gives new contextual representations through building an auto-regressive model cap"
2020.acl-main.214,P19-1656,1,0.855718,"modal and trimodal. Multiattention Recurrent Network (MARN) (Zadeh et al., 2018c) uses three separate hybrid LSTM memories that have the ability to propagate the cross-modal interactions. Memory Fusion Network (Zadeh et al., 2018a) synchronizes the information from three separate LSTMs through a multi-view gated memory. Recurrent Memory Fusion Network (RMFN) (Liang et al., 2018) captures the nuanced interactions among the modalities in a multi-stage manner, giving each stage the ability to focus on a subset of signals. Multimodal Transformer for Unaligned Multimodal Language Sequences (MulT) (Tsai et al., 2019) deploys three Transformers – each for one modality – to capture the interactions with the other two modalities in a selfattentive manner. The information from the three Transformers are aggregated through late-fusion. 2.2 Pre-trained Language Representations Learning word representations from large corpora has been an active research area in NLP community (Mikolov et al., 2013; Pennington et al., 2014). Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) contributed to advancing the state-of-the-art of many NLP tasks. A major setback of these word representations is their non-"
2020.acl-main.214,W18-3300,0,0.16192,"selines as well as language-only finetuning of BERT and XLNet. On the CMUMOSI dataset, MAG-XLNet achieves humanlevel multimodal sentiment analysis performance for the first time in the NLP community. 1 Introduction Human face-to-face communication flows as a seamless integration of language, acoustic, and vision modalities. In ordinary everyday interactions, we utilize all these modalities jointly to convey our * - Equal contribution intentions and emotions. Understanding this faceto-face communication falls within an increasingly growing NLP research area called multimodal language analysis (Zadeh et al., 2018b). The biggest challenge in this area is to efficiently model the three pillars of communication together. This gives artificial intelligence systems the capability to comprehend the multi-sensory information without disregarding nonverbal factors. In many applications such as dialogue systems and virtual reality, this capability is crucial to maintain the high quality of user interaction. The recent success of contextual word representations in NLP is largely credited to new Transformer-based (Vaswani et al., 2017) models such as BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019). Thes"
2020.acl-main.214,P18-1208,1,0.904321,"Missing"
2020.acl-main.488,S18-2005,0,0.0615348,"Missing"
2020.acl-main.488,W19-3823,0,0.0205839,"et al., 2016) to sentences. Related Work: Although there has been some recent work in measuring the presence of bias in sentence representations (May et al., 2019; Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, Zhao et al. (2019), Park et al. (2018), and Garg et al. (2019) are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. Bordia and Bowman (2019) only study word-level language models and also requires re-training. Finally, Kurita et al. (2019) only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT) (Caliskan et al., 2017) metric in a manner similar to May et al. (2019). In this paper, as a compelling step towards generalizing debiasing methods to sentence representations, we capture the various ways in which biasattribute words can be used in natural sentences. This is performed by contextualizing bias-attribute words using a diverse set of sentence templates from various text corpora into bias-attribute sentences. We propose S ENT-D EBIAS, an extension of the H ARD -D EBIAS method (Bolukbasi et"
2020.acl-main.488,W17-1601,0,0.0173014,"as-attribute sentences. We propose S ENT-D EBIAS, an extension of the H ARD -D EBIAS method (Bolukbasi et al., 2016), to debias sentences for both binary1 and multiclass bias attributes spanning gender and religion. Key to our approach is the contextualization step in which bias-attribute words are converted into bias-attribute sentences by using a diverse set 1 Although we recognize that gender is non-binary and there are many important ethical principles in the design, ascription of categories/variables to study participants, and reporting of results in studying gender as a variable in NLP (Larson, 2017), for the purpose of this study, we follow existing research and focus on female and male gendered terms. Binary Gender man, woman he, she father, mother son, daughter Multiclass Religion jewish, christian, muslim torah, bible, quran synagogue, church, mosque rabbi, priest, imam Table 1: Examples of word pairs to estimate the binary gender bias subspace and the 3-class religion bias subspace in our experiments. of sentence templates from text corpora. Our experimental results demonstrate the importance of using a large number of diverse sentence templates when estimating bias subspaces of sent"
2020.acl-main.488,S19-1010,0,0.0510442,"Missing"
2020.acl-main.488,D18-1014,1,0.893927,"Missing"
2020.acl-main.488,2021.ccl-1.108,0,0.112552,"Missing"
2020.acl-main.488,N19-1062,0,0.415041,"reflect and propagate social biases present in training corpora (Lauscher and Glavaˇs, 2019; Caliskan et al., 2017; Swinger et al., 2019; Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019b; Zhao et al., 2017; Barocas and Selbst, 2016) and unfairly discriminate against users, particularly those from disadvantaged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion. More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP (Wu and Dredze, 2019), multimodal learning (Zellers et al., 2019; Sun et al., 2019a), and grounded language learning (Urbanek et al., 2019). As their usage proliferates across various real-world applications (Huang et al., 2019;"
2020.acl-main.488,N19-1063,0,0.437661,"they are used in downstream tasks (Bolukbasi et al., 2016; Manzini et al., 2019). Secondly, sentences display large variety in how they are composed from individual words. This variety is driven by many factors such as topics, individuals, settings, and even differences between spoken and written text. As a result, it is difficult to scale traditional word-level debiasing approaches (which involve bias-attribute words such as man, woman) (Bolukbasi et al., 2016) to sentences. Related Work: Although there has been some recent work in measuring the presence of bias in sentence representations (May et al., 2019; Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, Zhao et al. (2019), Park et al. (2018), and Garg et al. (2019) are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. Bordia and Bowman (2019) only study word-level language models and also requires re-training. Finally, Kurita et al. (2019) only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT) (Caliskan et al., 2017) metric in a manner simila"
2020.acl-main.488,D18-1302,0,0.0426437,"This variety is driven by many factors such as topics, individuals, settings, and even differences between spoken and written text. As a result, it is difficult to scale traditional word-level debiasing approaches (which involve bias-attribute words such as man, woman) (Bolukbasi et al., 2016) to sentences. Related Work: Although there has been some recent work in measuring the presence of bias in sentence representations (May et al., 2019; Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, Zhao et al. (2019), Park et al. (2018), and Garg et al. (2019) are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. Bordia and Bowman (2019) only study word-level language models and also requires re-training. Finally, Kurita et al. (2019) only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT) (Caliskan et al., 2017) metric in a manner similar to May et al. (2019). In this paper, as a compelling step towards generalizing debiasing methods to sentence representations, we capture the various ways in which biasattri"
2020.acl-main.488,D14-1162,0,0.0917252,"d removing social biases from widely adopted sentence representations for fairer NLP. 1 Introduction Machine learning tools for learning from language are increasingly deployed in real-world scenarios such as healthcare (Velupillai et al., 2018), legal systems (Dale, 2019), and computational social science (Bamman et al., 2016). Key to the success of these models are powerful embedding layers which learn continuous representations of input information such as words, sentences, and documents from large amounts of data (Devlin et al., 2019; Mikolov et al., 2013). Although word-level embeddings (Pennington et al., 2014; Mikolov et al., 2013) are highly informative features useful for a variety of tasks in Natural Language Processing (NLP), recent work has shown that word-level embeddings reflect and propagate social biases present in training corpora (Lauscher and Glavaˇs, 2019; Caliskan et al., 2017; Swinger et al., 2019; Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019b; Zhao et al., 2017; Barocas and Selbst, 2016) and unfairly discriminate against users, particularly those from disadvantaged social groups. Fortunately, r"
2020.acl-main.488,N18-1202,0,0.625883,"019; Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019b; Zhao et al., 2017; Barocas and Selbst, 2016) and unfairly discriminate against users, particularly those from disadvantaged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion. More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP (Wu and Dredze, 2019), multimodal learning (Zellers et al., 2019; Sun et al., 2019a), and grounded language learning (Urbanek et al., 2019). As their usage proliferates across various real-world applications (Huang et al., 2019; Alsentzer et al., 2019), it becomes necessary to recognize the role they play in shaping social biases and stereotypes. Debiasing sen"
2020.acl-main.488,P19-1050,0,0.0268389,"Missing"
2020.acl-main.488,D16-1264,0,0.0406965,"e-tuned on two single sentence datasets, Stanford Sentiment Treebank (SST-2) sentiment classification (Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). It is also possible to apply BERT (Devlin et al., 2019) on downstream tasks that involve two sentences. The output sentence pair representation can also be debiased (after fine-tuning and normalization). We test the effect of S ENT-D EBIAS on Question Natural Language Inference (QNLI) (Wang et al., 2018) which converts the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) into a binary classification task. These results are 5506 2 We used uncased BERT-Base throughout all experiments. Test C6: M/F Names, Career/Family BERT BERT post SST-2 BERT post CoLA +0.477 → −0.096 +0.036 → −0.109 −0.009 → +0.149 C6b: M/F Terms, Career/Family +0.108 → −0.437 +0.010 → −0.057 BERT post QNLI ELMo −0.261 → −0.054 −0.380 → −0.298 +0.199 → +0.186 −0.155 → −0.004 −0.345 → −0.327 C7: M/F Terms, Math/Arts +0.253 → +0.194 −0.219 → −0.221 +0.268 → +0.311 C7b: M/F Names, Math/Arts +0.254 → +0.194 +1.153 → −0.755 +0.150 → +0.308 −0.581 → −0.629 C8: M/F Terms, Science/Arts +0.399 → −0.07"
2020.acl-main.488,D13-1170,0,0.00521639,"al., 2018). Note that the pre-trained BERT encoder must be fine-tuned on task-specific data. This implies that the final BERT encoder used during debiasing changes from task to task. To account for these differences, we report two sets of metrics: 1) BERT: simply debiasing the pre-trained BERT encoder, and 2) BERT post task: first fine-tuning BERT and post-processing (i.e. normalization) on a specific task before the final BERT representations are debiased. We apply S ENT-D EBIAS on BERT fine-tuned on two single sentence datasets, Stanford Sentiment Treebank (SST-2) sentiment classification (Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). It is also possible to apply BERT (Devlin et al., 2019) on downstream tasks that involve two sentences. The output sentence pair representation can also be debiased (after fine-tuning and normalization). We test the effect of S ENT-D EBIAS on Question Natural Language Inference (QNLI) (Wang et al., 2018) which converts the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) into a binary classification task. These results are 5506 2 We used uncased BERT-Base throughout al"
2020.acl-main.488,D19-1062,0,0.0173041,"asing these word representations for both binary (Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion. More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP (Wu and Dredze, 2019), multimodal learning (Zellers et al., 2019; Sun et al., 2019a), and grounded language learning (Urbanek et al., 2019). As their usage proliferates across various real-world applications (Huang et al., 2019; Alsentzer et al., 2019), it becomes necessary to recognize the role they play in shaping social biases and stereotypes. Debiasing sentence representations is difficult for two reasons. Firstly, it is usually unfeasible to fully retrain many of the state-of-the-art sentencebased embedding models. In contrast with conventional word-level embeddings such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) which can be retrained on a single machine within a few hours, the best sentence enco"
2020.acl-main.488,W18-5446,0,0.0201493,"ore the final BERT representations are debiased. We apply S ENT-D EBIAS on BERT fine-tuned on two single sentence datasets, Stanford Sentiment Treebank (SST-2) sentiment classification (Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). It is also possible to apply BERT (Devlin et al., 2019) on downstream tasks that involve two sentences. The output sentence pair representation can also be debiased (after fine-tuning and normalization). We test the effect of S ENT-D EBIAS on Question Natural Language Inference (QNLI) (Wang et al., 2018) which converts the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) into a binary classification task. These results are 5506 2 We used uncased BERT-Base throughout all experiments. Test C6: M/F Names, Career/Family BERT BERT post SST-2 BERT post CoLA +0.477 → −0.096 +0.036 → −0.109 −0.009 → +0.149 C6b: M/F Terms, Career/Family +0.108 → −0.437 +0.010 → −0.057 BERT post QNLI ELMo −0.261 → −0.054 −0.380 → −0.298 +0.199 → +0.186 −0.155 → −0.004 −0.345 → −0.327 C7: M/F Terms, Math/Arts +0.253 → +0.194 −0.219 → −0.221 +0.268 → +0.311 C7b: M/F Names, Math/Arts +0.254 → +0.194 +1"
2020.acl-main.488,D19-1077,0,0.0120175,"taged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion. More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP (Wu and Dredze, 2019), multimodal learning (Zellers et al., 2019; Sun et al., 2019a), and grounded language learning (Urbanek et al., 2019). As their usage proliferates across various real-world applications (Huang et al., 2019; Alsentzer et al., 2019), it becomes necessary to recognize the role they play in shaping social biases and stereotypes. Debiasing sentence representations is difficult for two reasons. Firstly, it is usually unfeasible to fully retrain many of the state-of-the-art sentencebased embedding models. In contrast with conventional word-level embeddings such as GloVe (Pennington et al., 2014) and"
2020.acl-main.488,N19-1064,0,0.0329889,"Missing"
2020.acl-main.488,D17-1323,0,0.0533367,"sentences, and documents from large amounts of data (Devlin et al., 2019; Mikolov et al., 2013). Although word-level embeddings (Pennington et al., 2014; Mikolov et al., 2013) are highly informative features useful for a variety of tasks in Natural Language Processing (NLP), recent work has shown that word-level embeddings reflect and propagate social biases present in training corpora (Lauscher and Glavaˇs, 2019; Caliskan et al., 2017; Swinger et al., 2019; Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019b; Zhao et al., 2017; Barocas and Selbst, 2016) and unfairly discriminate against users, particularly those from disadvantaged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion. More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding. When compared to"
2020.acl-main.488,D18-1521,0,0.0333169,"omplexity of natural sentences, it is extremely hard to identify the set of neutral sentences and its complement. Thus, in downstream tasks, we removed bias from all sentences which could possibly harm downstream task performance if the dataset contains a significant number of non-neutral sentences. Finally, a fundamental challenge lies in the fact that these representations are trained without explicit bias control mechanisms on large amounts of naturally occurring text. Given that it becomes infeasible (in standard settings) to completely retrain these large sentence encoders for debiasing (Zhao et al., 2018; Zhang et al., 2018), future work should focus on developing better post-hoc debiasing techniques. In our experiments, we need to re-estimate the bias subspace and perform debiasing whenever the BERT encoder was fine-tuned. It remains to be seen whether there are debiasing methods which are invariant to fine-tuning, or can be efficiently re-estimated as the encoders are fine-tuned. 5 Conclusion This paper investigated the post-hoc removal of social biases from pretrained sentence representations. We proposed the S ENT-D EBIAS method that accurately captures the bias subspace of sentence repre"
2020.acl-main.488,D19-1531,0,0.0167918,"even if the magnitudes of sentence representations are not normalized, the debiased representations are still pointing in directions orthogonal to the bias subspace. Therefore, skipping the equalize step still results in debiased sentence representations as measured by our definition of bias. 3 Experiments We test the effectiveness of S ENT-D EBIAS at removing biases and retaining performance on downstream tasks. All experiments are conducted on English terms and downstream tasks. We acknowledge that biases can manifest differently across different languages, in particular gendered languages (Zhou et al., 2019), and emphasize the need for future extensions in these directions. Experimental details are in the appendix and code is released at https://github.com/pliang279/ sent_debias. 3.1 Evaluating Biases Biases are traditionally measured using the Word Embedding Association Test (WEAT) (Caliskan et al., 2017). WEAT measures bias in word embeddings by comparing two sets of target words to two sets of attribute words. For example, to measure social bias surrounding genders with respect to careers, one could use the target words programmer, engineer, scientist, and nurse, teacher, librarian, and the at"
2020.acl-main.625,D14-1162,0,0.0822502,"Missing"
2020.acl-main.625,N18-1202,0,0.0832822,"Missing"
2020.acl-main.625,D18-1039,0,0.0875703,"sks using short descriptions of the object classes. This leads to better metadata efficiency of our proposed method. 6995 Dynamic Parameter Generation As mentioned before, N3 dynamically generates classification models for designated classes. Dynamic parameter generation has been explored in the context of generating recurrent cells at different time-steps of RNNs (Ha et al., 2016), constructing intermediate linear models for interpretability inside neural networks (Al-Shedivat et al., 2017), and contextual parameter generation for different language pairs in multilingual machine translation (Platanios et al., 2018). As mentioned in Section 2, some zeroshot learning methods can also be viewed as generating classifier parameters (Lei Ba et al., 2015; Elhoseiny et al., 2013). However, many of the previous work directly or indirectly mentions the challenge of memory and computation complexity - after all, the output of the parameter generation model are large matrices that are excessively high dimensional. To tackle this issue, previous work either only generate very simple linear layers (Lei Ba et al., 2015; Elhoseiny et al., 2013; Al-Shedivat et al., 2017), or impose low-rank constraints on the weights to"
2020.acl-main.644,D16-1203,0,0.181898,"of referring expression datasets, including our proposed Refer360° dataset. Refer360° poses a more challenging scenario where the system observes only a partial and dynamic FoV. Refer360° also has includes explicit alignments between intermediate instruction steps and human follower actions which can be used as an auxiliary evaluation metric or source of supervision. Second, unlike other datasets, the target locations in Refer360° are randomly distributed and thus may occur anywhere – not just on predetermined objects. As a result, target locations are less prone to bias (Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016; Cirik et al., 2018b). These random locations lead to more linguistically complex instructions, as shown in our analyses – when instead annotators choose the target location, they are likely to be biased towards locations that are more easily described (e.g. on top of a named object). Table 1 shows a comparison of similar datasets. In the following section, we motivate Refer360° dataset in more detail. 2 Motivation The vision behind Refer360° is to build systems that perform localization of any point in 3D space, bringing us closer to human-like reasoni"
2020.acl-main.644,N18-2123,1,0.725918,"ation from a bathroom. ‘First, face the sink, then find the second drawer in the cabinet to your left. The pills should be inside that drawer behind the toothbrush.” Interpreting instruction sequences in order to locate targets in novel environments is challenging for AI systems (e.g. personal robots and self-driving cars). First, the system needs to ground the instructions into visual perception (Anderson et al., 2018b; Hu et al., 2019). This often requires identification of the mentioned object (Plummer et al., 2015) through physical relationships with surrounding objects (Hu et al., 2017b; Cirik et al., 2018a). Second, since human visual perception has limited field-of-view, instructions are often sequential: First, the correct FoV should be identified before searching for the final target. In many situations, the target location is not visually unique (e.g. in the middle of a plain wall), and several intermediate instructions are required. To study these challenges, we introduce a novel dataset, named Refer360°1 , for the task of localizing a target in 360° scenes given a sequence of instructions. Figure 1 presents an example scenario 1 The annotations, learning simulator, and annotation setup a"
2020.acl-main.644,P19-1655,0,0.0191985,"align with what the instruction describes. Please see Figure 2a to see the correct location of Waldo. Introduction Imagine a scenario in which you are asked to retrieve medication from a bathroom. ‘First, face the sink, then find the second drawer in the cabinet to your left. The pills should be inside that drawer behind the toothbrush.” Interpreting instruction sequences in order to locate targets in novel environments is challenging for AI systems (e.g. personal robots and self-driving cars). First, the system needs to ground the instructions into visual perception (Anderson et al., 2018b; Hu et al., 2019). This often requires identification of the mentioned object (Plummer et al., 2015) through physical relationships with surrounding objects (Hu et al., 2017b; Cirik et al., 2018a). Second, since human visual perception has limited field-of-view, instructions are often sequential: First, the correct FoV should be identified before searching for the final target. In many situations, the target location is not visually unique (e.g. in the middle of a plain wall), and several intermediate instructions are required. To study these challenges, we introduce a novel dataset, named Refer360°1 , for the"
2020.acl-main.644,D14-1086,0,0.0797505,"nging since it is harder to describe a location when we cannot readily refer to it with the name of an object. Refer360° consists of 17,137 instruction sequences with ground-truth actions to complete these instructions in 360° scenes. Refer360° has some unique characteristics which differentiate it from prior work. First, Refer360° allows the scene to be viewed through a partial FoV that can be dynamically changed as instructions are followed. This is in contrast with existing 360° scene-based datasets such as Touchdown-SDR (Chen et al., 2018) and 2D image-based referring expression datasets (Kazemzadeh et al., 2014; Hu et al., 2016; Mao et al., 2016), where the visual input is either fixed, corresponding to a holistic, oracle-like view, or consists of fixed, cardinal FoVs. The partial and dynamic FoV in Refer360° poses new challenges for language grounding (see Figure 2a, 2b, and 2c for an illustrative comparison). For instance, the mentioned objects may not be visible in the current FoV, and language may refer to the FoV itself. Further, since our annotators generate instructions while observing a partial and dynamic FoV, and do so for a follower whose first FoV will be initially located at random, the"
2020.acl-main.644,D18-1287,0,0.044938,"Missing"
2020.emnlp-main.141,C18-1197,0,0.0276354,"; Zadeh et al., 2017). In addition to these purely discriminative approaches, recent work has also explored generative-discriminative methods for learning from multimodal language (Tsai et al., 2019b), learning from noisy or missing modalities (Mai et al., 2019; Liang et al., 2019b; Pham et al., 2019), strong baselines suitable for learning from limited data (Liang et al., 2019a), and interpretable models for language analysis (Karimi, 2018; Zadeh et al., 2018b). Several other lines of work have focuses on building stronger unimodal representations such as language (Kordjamshidi et al., 2017; Beinborn et al., 2018) and speech (Sanabria et al., 2018; Lakomkin et al., 2019; Gu et al., 2019) for multimodal language understanding. 3 CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes) Dataset The CMU-MOSEAS dataset covers 4 languages of Spanish (>500M total speakers globally), Portuguese (>200M speakers globally), German (>200M speakers globally), and French (>200M speakers globally). These languages either have Romance or Germanic roots (Renfrew, 1989). They originate from Europe, which is also the main region for our video acquisition. The languages are also spoken in the American contin"
2020.emnlp-main.141,P19-1455,1,0.871582,"Missing"
2020.emnlp-main.141,W17-1106,0,0.257712,"∶ 28 00 ∶ 78 90 ∶ 23 12 ∶ 00 03 ∶ 00 02 ∶ 28 11 ∶ 00 04 ∶ 11 04 ∶ 39 06 ∶ 30 03 ∶ 50 – – – – – 03 ∶ 20 Table 1: Best viewed zoomed in. Comparison between CMU-MOSEAS and relevant datasets. CMU-MOSEAS presents a unique resource for languages of Spanish, Portuguese, German and French. [l, v, a] denote [language, vision and acoustic] modalities. Duration is in the HH:MM format. and Guzman, 2017) and hotel reviews (MolinaGonz´alez et al., 2014). Polarity classification tasks based on Twitter data have also been collected in Portuguese (Brum and das Grac¸as Volpe Nunes, 2017) (TweetSentBR), German (Cieliebak et al., 2017; Flender and Gips, 2017) (SB10k), and French (Rhouati et al., 2018). Another line of related work aims to predict humor from text in multiple languages (Castro et al., 2016, 2017). Table 1 demonstrates that CMU-MOSEAS is a unique resource for the languages of Spanish, Portuguese, German and French. 2.2 Computational Models of Multimodal Language Studies of multimodal language have particularly focused on the tasks of sentiment analysis (Morency et al., 2011; Yadav et al., 2015), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014). Works in this are"
2020.emnlp-main.141,W17-4306,0,0.0175867,"r methods (Hou et al., 2019; Zadeh et al., 2017). In addition to these purely discriminative approaches, recent work has also explored generative-discriminative methods for learning from multimodal language (Tsai et al., 2019b), learning from noisy or missing modalities (Mai et al., 2019; Liang et al., 2019b; Pham et al., 2019), strong baselines suitable for learning from limited data (Liang et al., 2019a), and interpretable models for language analysis (Karimi, 2018; Zadeh et al., 2018b). Several other lines of work have focuses on building stronger unimodal representations such as language (Kordjamshidi et al., 2017; Beinborn et al., 2018) and speech (Sanabria et al., 2018; Lakomkin et al., 2019; Gu et al., 2019) for multimodal language understanding. 3 CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes) Dataset The CMU-MOSEAS dataset covers 4 languages of Spanish (>500M total speakers globally), Portuguese (>200M speakers globally), German (>200M speakers globally), and French (>200M speakers globally). These languages either have Romance or Germanic roots (Renfrew, 1989). They originate from Europe, which is also the main region for our video acquisition. The languages are also spoke"
2020.emnlp-main.141,N19-1267,1,0.887494,"Missing"
2020.emnlp-main.141,P19-1152,1,0.892623,"Missing"
2020.emnlp-main.141,D18-1014,1,0.837507,"demonstrates that CMU-MOSEAS is a unique resource for the languages of Spanish, Portuguese, German and French. 2.2 Computational Models of Multimodal Language Studies of multimodal language have particularly focused on the tasks of sentiment analysis (Morency et al., 2011; Yadav et al., 2015), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014). Works in this area often focus on novel multimodal neural architectures based on Transformer (Tsai et al., 2019a; Mai et al., 2019; Zadeh et al., 2019) and recurrent fusion approaches (Rahman et al., 2019; Liang et al., 2018; Zadeh et al., 2018a, 2017), as well as learning via statistical techniques such as correlation analysis (Sun et al., 2019) and tensor methods (Hou et al., 2019; Zadeh et al., 2017). In addition to these purely discriminative approaches, recent work has also explored generative-discriminative methods for learning from multimodal language (Tsai et al., 2019b), learning from noisy or missing modalities (Mai et al., 2019; Liang et al., 2019b; Pham et al., 2019), strong baselines suitable for learning from limited data (Liang et al., 2019a), and interpretable models for language analysis (Karimi,"
2020.emnlp-main.141,P11-1015,0,0.081982,"ed dataset of French language, consisting of 9.5 hours of audio, visual, and physiological (electrocardiogram, and electrodermal activity) signals. EmoDB (Burkhardt et al., 2005; Vondra and V´ıch, 2009) is a dataset of emotion recognition in German for speech and acoustic modalities. Aside the aforementioned multimodal datasets, the following are related datasets that use only the text modality. Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data. Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews. Textual annotated Spanish datasets have been collected from Twitter (TASS) (Villena Roma¡n et al., 2013-03; Pla and Hurtado, 2018; Miranda 1802 Dataset CMU-MOSEAS CMU-MOSEI ICT-MMMO CMU-MOSI YouTube MOUD IEMOCAP AMMER UR-FUNNY VAM EmoDB AFEW Mimicry HUMAINE SEWA SEMAINE RECOLA SST Large Movie TASS TweetSentBR SB10k AM-FED Samples 40, 000 23, 453 340 2, 199 300 400 10, 000 288 16, 514 499 800 1, 645 48 50 538 80 46 11, 855 25, 000 3, 413 15, 000 10, 000 242 Speakers 1, 645 1000 200 98 50 101 10 36 1, 741 20 10 330 48 4 408 20 46 – – – – – 242 Moda"
2020.emnlp-main.141,P13-1096,1,0.766595,"t which consists of 151 videos of scripted dialogues between acting participants. POM dataset contains 1, 000 videos annotated for attributes (Park et al., 2014). The language of the dataset is English. ICT-MMMO (W¨ollmer et al., 2013) consists of online social review videos annotated at the video level for sentiment. CMUMOSI (Zadeh et al., 2016b) is a collection of 2199 opinion video clips each annotated with sentiment in the range [−3, 3]. YouTube (Morency et al., 2011) contains videos from the social media web site YouTube that span a wide range of product reviews and opinion videos. MOUD (Perez-Rosas et al., 2013) consists of product review videos in Spanish, annotated for sentiment. AMMER (Cevher et al., 2019) is a German emotion recognition dataset collected from a driver’s interactions with both a virtual agent as well as a co-driver in a simulated driving environment. URFUNNY (Hasan et al., 2019) consists of more than 16000 video samples from TED talks annotated for humor. Vera am Mittag (VAM) (Grimm et al., 2008) corpus consists of recordings from the German TV talk-show “Vera am Mittag”. This audiovisual dataset is labeled for continuous emotions of valence, activation and dominance. RECOLA (Ring"
2020.emnlp-main.141,P19-1050,1,0.756664,"al language. 1 Introduction Humans use a coordinated multimodal signal to communicate with each other. This communication signal is called multimodal language (Perniss, 2018); a complex temporal and idiosyncratic signal which includes the modalities of language, visual and acoustic. On a daily basis across the world, intentions and emotions are conveyed through joint utilization of these three modalities. While English, Chinese, and Spanish languages have resources for computational analysis of multimodal language (focusing on analysis of sentiment, subjectivity, or emotions (Yu et al., 2020; Poria et al., 2019; Zadeh et al., 2018b; Park et al., 2014; W¨ollmer et al., 2013; Poria et al., 2020)), other commonly spoken languages across the globe lag behind. As Artificial Intelligence 1801 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1801–1812, c November 16–20, 2020. 2020 Association for Computational Linguistics (AI) increasingly blends into everyday life across the globe, there is a genuine need for intelligent entities capable of understanding multimodal language in different cultures. The lack of large-scale in-the-wild resources presents a substant"
2020.emnlp-main.141,D13-1170,0,0.00302646,"talk-show “Vera am Mittag”. This audiovisual dataset is labeled for continuous emotions of valence, activation and dominance. RECOLA (Ringeval et al., 2013) is an acted dataset of French language, consisting of 9.5 hours of audio, visual, and physiological (electrocardiogram, and electrodermal activity) signals. EmoDB (Burkhardt et al., 2005; Vondra and V´ıch, 2009) is a dataset of emotion recognition in German for speech and acoustic modalities. Aside the aforementioned multimodal datasets, the following are related datasets that use only the text modality. Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data. Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews. Textual annotated Spanish datasets have been collected from Twitter (TASS) (Villena Roma¡n et al., 2013-03; Pla and Hurtado, 2018; Miranda 1802 Dataset CMU-MOSEAS CMU-MOSEI ICT-MMMO CMU-MOSI YouTube MOUD IEMOCAP AMMER UR-FUNNY VAM EmoDB AFEW Mimicry HUMAINE SEWA SEMAINE RECOLA SST Large Movie TASS TweetSentBR SB10k AM-FED Samples 40, 000 23, 453 340 2, 199 300 400 10, 000 288 16, 514 4"
2020.emnlp-main.141,P19-1656,1,0.917609,"line of related work aims to predict humor from text in multiple languages (Castro et al., 2016, 2017). Table 1 demonstrates that CMU-MOSEAS is a unique resource for the languages of Spanish, Portuguese, German and French. 2.2 Computational Models of Multimodal Language Studies of multimodal language have particularly focused on the tasks of sentiment analysis (Morency et al., 2011; Yadav et al., 2015), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014). Works in this area often focus on novel multimodal neural architectures based on Transformer (Tsai et al., 2019a; Mai et al., 2019; Zadeh et al., 2019) and recurrent fusion approaches (Rahman et al., 2019; Liang et al., 2018; Zadeh et al., 2018a, 2017), as well as learning via statistical techniques such as correlation analysis (Sun et al., 2019) and tensor methods (Hou et al., 2019; Zadeh et al., 2017). In addition to these purely discriminative approaches, recent work has also explored generative-discriminative methods for learning from multimodal language (Tsai et al., 2019b), learning from noisy or missing modalities (Mai et al., 2019; Liang et al., 2019b; Pham et al., 2019), strong baselines suita"
2020.emnlp-main.141,2020.acl-main.343,0,0.126455,"udies in multimodal language. 1 Introduction Humans use a coordinated multimodal signal to communicate with each other. This communication signal is called multimodal language (Perniss, 2018); a complex temporal and idiosyncratic signal which includes the modalities of language, visual and acoustic. On a daily basis across the world, intentions and emotions are conveyed through joint utilization of these three modalities. While English, Chinese, and Spanish languages have resources for computational analysis of multimodal language (focusing on analysis of sentiment, subjectivity, or emotions (Yu et al., 2020; Poria et al., 2019; Zadeh et al., 2018b; Park et al., 2014; W¨ollmer et al., 2013; Poria et al., 2020)), other commonly spoken languages across the globe lag behind. As Artificial Intelligence 1801 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1801–1812, c November 16–20, 2020. 2020 Association for Computational Linguistics (AI) increasingly blends into everyday life across the globe, there is a genuine need for intelligent entities capable of understanding multimodal language in different cultures. The lack of large-scale in-the-wild resources"
2020.emnlp-main.141,D17-1115,1,0.740417,"language have particularly focused on the tasks of sentiment analysis (Morency et al., 2011; Yadav et al., 2015), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014). Works in this area often focus on novel multimodal neural architectures based on Transformer (Tsai et al., 2019a; Mai et al., 2019; Zadeh et al., 2019) and recurrent fusion approaches (Rahman et al., 2019; Liang et al., 2018; Zadeh et al., 2018a, 2017), as well as learning via statistical techniques such as correlation analysis (Sun et al., 2019) and tensor methods (Hou et al., 2019; Zadeh et al., 2017). In addition to these purely discriminative approaches, recent work has also explored generative-discriminative methods for learning from multimodal language (Tsai et al., 2019b), learning from noisy or missing modalities (Mai et al., 2019; Liang et al., 2019b; Pham et al., 2019), strong baselines suitable for learning from limited data (Liang et al., 2019a), and interpretable models for language analysis (Karimi, 2018; Zadeh et al., 2018b). Several other lines of work have focuses on building stronger unimodal representations such as language (Kordjamshidi et al., 2017; Beinborn et al., 2018"
2020.emnlp-main.141,P18-1208,1,0.890159,"Missing"
2020.emnlp-main.143,P18-1209,1,0.819541,", spoken or written words), visual (e.g., body gestures), and acoustic (e.g., voice tones) modalities. It acts as a medium for human communication and has been advanced in areas spanning affect recognition (Busso et al., 2008), media description (Lin et al., 2014), and multimedia information retrieval (Abu-El-Haija et al., 2016). Modeling multimodal sources requires to understand the relative importance of not only each single modality (defined as unimodal explanatory features) but also the interactions (defined as bimodal or trimodal explanatory features) (B¨uchel et al., 1998). Recent work (Liu et al., 2018; Williams et al., 2018; Ortega et al., 2019) proposed methods to fuse information across modalities and ∗ indicates equal contribution. Code is available at https://github.com/martinmamql/ multimodal_routing. Figure 1: An example of Multimodal Routing, where the weights between visual, textual, and visual-textual explanatory features and concepts of emotions (happy and sad) are dynamically adjusted given every input sample. The model associates vision and v-t features to sad concept in the left sample, and v-t and text features to happy concept in the right example, showing local weights inte"
2020.emnlp-main.143,D14-1162,0,0.0941754,"Missing"
2020.emnlp-main.143,W18-3302,0,0.0180373,"n words), visual (e.g., body gestures), and acoustic (e.g., voice tones) modalities. It acts as a medium for human communication and has been advanced in areas spanning affect recognition (Busso et al., 2008), media description (Lin et al., 2014), and multimedia information retrieval (Abu-El-Haija et al., 2016). Modeling multimodal sources requires to understand the relative importance of not only each single modality (defined as unimodal explanatory features) but also the interactions (defined as bimodal or trimodal explanatory features) (B¨uchel et al., 1998). Recent work (Liu et al., 2018; Williams et al., 2018; Ortega et al., 2019) proposed methods to fuse information across modalities and ∗ indicates equal contribution. Code is available at https://github.com/martinmamql/ multimodal_routing. Figure 1: An example of Multimodal Routing, where the weights between visual, textual, and visual-textual explanatory features and concepts of emotions (happy and sad) are dynamically adjusted given every input sample. The model associates vision and v-t features to sad concept in the left sample, and v-t and text features to happy concept in the right example, showing local weights interpretation upon differe"
2020.emnlp-main.143,P18-1208,1,0.942281,"ltimodal Routing. In human multimodal language, such routing dynamically changes weights between modalities and output labels for each sample as shown in Fig. 1. The most significant contribution of Multimodal Routing is its ability to establish local weights dynamically for each input sample between modality features and the labels during training and inference, thus providing local interpretation for each sample. Our experiments focus on two tasks of sentiment analysis and emotion recognition tasks using two benchmark multimodal language datasets, IEMOCAP (Busso et al., 2008) and CMU-MOSEI (Zadeh et al., 2018). We first study how our model compares with the state-of-the-art methods on these tasks. More importantly we provide local interpretation by qualitatively analyzing adjusted local weights for each sample. Then we also analyze the global interpretation using statistical techniques to reveal crucial features for prediction on average. Such interpretation of different resolutions strengthens our understanding of multimodal language learning. 2 Related Work Multimodal language learning is based on the fact that human integrates multiple sources such as acoustic, textual, and visual information to"
2020.findings-emnlp.170,L16-1553,0,0.0284415,"he sake of demonstration. extracted transcripts for audio signals corresponding to 250+ hours of freeform gesture information and 25 speakers. Our experiments study the effectiveness of our proposed method with a focus on precision-coverage trade-off. These quantitative experiments are complimented with important subjective human studies as the englobing judges of the generation quality. bine the modalities ignores multi-scale correlations (Tsai et al., 2019) between speech and language. While publicly datasets of co-speech gestures are available, they are either small (Sadoughi et al., 2015; Tolins et al., 2016; Yoon et al., 2019) or do not contain language information (Ginosar et al., 2019; Joo et al., 2015; Lee et al., 2019), which motivates for a dataset that resolves these shortcomings. 2 Distribution Coverage in Generative Modeling Implicit generative models have seen a lot of progress in the past decade with the introduction of GANs (Goodfellow et al., 2014; Yan and Wang, 2017). Especially two aspects of distribution estimation, (1) conditional generation precision (Zhang et al., 2017; Ginosar et al., 2019; Isola et al., 2017; Mirza and Osindero, 2014) and (2) coverage of the entire underlying"
2020.findings-emnlp.170,P19-1656,1,0.913813,"ortant to have sub-word level alignment between language and acoustics to generate the freeform gestures. In this paper, we study the link between spoken language and free form gestures. As a first contribution, we propose Adversarial Importance Sampled Learning(or AISLe), an approach whose main novelty is to bring adversarial learning and importance sampling together to improve coverage of the generated distribution without compromising on the precision at no extra computational cost. As a second contribution, we introduce the use of neural cross-attention architecture (Vaswani et al., 2017; Tsai et al., 2019) for gesture generation conditioned on spoken language. This idea allows transformer blocks to help with subword alignment between language and acoustic signals. A third contribution is the extension of dataset proposed in Ahuja et al. (2020) with automatically 1884 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1884–1895 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1: A toy representation of data distribution pdata as a histogram. Colours , , represent bins from the mode, heavy tail and long tail of pdata respectively. The color"
2021.naacl-main.171,Q18-1031,0,0.179491,"t al., 2018; Wu et al., 2019), politeness (Madaan et al., 2020), formality (Rao and Tetreault, 2018; Liu et al., 2020; Krishna et al., 2020), writing styles (Jhamtani et al., 2017; Syed et al., 2020; Jin et al., 2020) and some other styles (Kang and Hovy, 2019). However, these only focus on only high-level styles, unlike S TYLE PTB. Computational models for style transfer span statistical NLP methods (Hovy, 1987; Xu et al., 2012), neural generative models (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020), and Retrieve-and-Edit approaches (Li et al., 2018; Hashimoto et al., 2018; Guu et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020). These approaches work for a predefined set of styles but are unable to generalize to compositions of styles. Evaluating style transfer is difficult due to the diversity of plausible transferred sentences. In addition to automatic scores such as BLEU, perplexity, or binary classification accuracy of style transfer (Hu et al., 2017; Lample et al., 2019; He et al., 2020), other automatic metrics (Fu et al., 2018; Mir et al., 2019) and human evaluation are also commonly used (Li et al., 2018; Shen et al., 2017). 3 Fine-Grained Style Constructs As a st"
2021.naacl-main.79,D14-1162,0,0.0895706,"Missing"
2021.naacl-main.79,P17-1081,1,0.822405,"s and temporal edges. For a given node of a particular modality, its interactions with nodes 3 MTAG from different modalities should be considered difIn this section, we describe our proposed frame- ferently. For example, given a Video node, its interwork: Modal Temporal Attention Graph (MTAG) action with an Audio node should be different from for unaligned multimodal language sequences. We that with a Text node. In addition, the temporal describe how we formulate the multimodal data order of the nodes also plays a key role in multiinto a graph G(V, E), and the MTAG fusion op- modal analysis (Poria et al., 2017). For example, eration that operates on G. In essence, our graph a transition from a frown to a smile ( → → ) formulation by design alleviates the need for any may imply a positive sentiment, whereas a tranhard alignments, and combined with MTAG fu- sition from a smile to a frown ( → → ) may imply a negative sentiment. Therefore, interactions sion, allows nodes from one modality to interact 1011 between nodes that appear in different temporal orders should also be considered differently. In GNNs, the edges define how node features are aggregated within a graph. In order to encapsulate the dive"
2021.naacl-main.79,P19-1656,1,0.884963,"anguage, tributed multimodal sequential data. Modalities visual, and acoustic modalities that each uses a do not need to be pre-aligned, nor do they need to different sampling rate. Earlier works assumed follow similar sampling rate. MTAG can capture multimodal sequences are aligned based on word interactions of various types across any number boundaries (Lazaridou et al., 2015; Ngiam et al., of modalities all at once, comparing to previous 2011; Gu et al., 2018; Dumpala et al., 2019; Pham methods that model bi-modal interactions at a time et al., 2019) and applied fusion methods for aligned (Tsai et al., 2019a). At its core, MTAG utilizes sequences. To date, modeling unaligned multian efficient trimodal-temporal graph fusion oper- modal language sequences remains understudied, ation. Coupled with our proposed dynamic prun- except for (Tsai et al., 2019a; Khare et al., 2020; ing technique, MTAG learns a parameter-efficient Zheng et al., 2020), which used cross-modal Transand interpretable graph. In our experiments, we formers to model unaligned multimodal language use two unaligned multimodal emotion recognition sequences. However, the cross-modal Transformer and sentiment analysis benchmarks: IEMO"
2021.naacl-main.79,2020.acl-main.273,0,0.0403911,"rence (Nicolicioiu et al., 2019), and attention (Veliˇckovi´c et al., 2018) to graph. Recently, several heterogeneous GNN methods (Wang et al., 2019a; Wei et al., 2019; Shi et al., 2016) have been proposed. The heterogeneous nodes referred in these works consist of uni-modal views of multiple data generating sources (such as movie metadata node, audience metadata node, etc.), whereas in our case the graph nodes represent multimodal views of a single data generating source (visual, acoustic, textual nodes from a single speaking person). In the NLP domain, multimodal GNN methods (Khademi, 2020; Yin et al., 2020) on tasks such as Visual Question Answering and Machine Translation. However, these settings still differ from ours because they focused on static images and short text which, unlike the multimodal video data in our case, do not exhibit long-term temporal dependencies across modalities. Based on these findings, we discovered there has been little research using graph-based methods for modeling unaligned, multimodal language sequences, which includes video, audio and text. In this paper, we demonstrate our proposed MTAG method can effectively model such unaligned, multimodal sequential data. No"
2021.naacl-main.79,2020.acl-main.683,0,0.0138511,"y number boundaries (Lazaridou et al., 2015; Ngiam et al., of modalities all at once, comparing to previous 2011; Gu et al., 2018; Dumpala et al., 2019; Pham methods that model bi-modal interactions at a time et al., 2019) and applied fusion methods for aligned (Tsai et al., 2019a). At its core, MTAG utilizes sequences. To date, modeling unaligned multian efficient trimodal-temporal graph fusion oper- modal language sequences remains understudied, ation. Coupled with our proposed dynamic prun- except for (Tsai et al., 2019a; Khare et al., 2020; ing technique, MTAG learns a parameter-efficient Zheng et al., 2020), which used cross-modal Transand interpretable graph. In our experiments, we formers to model unaligned multimodal language use two unaligned multimodal emotion recognition sequences. However, the cross-modal Transformer and sentiment analysis benchmarks: IEMOCAP module is a bi-modal operation that only account (Busso et al., 2008) and CMU-MOSI (Zadeh et al., for two modalities’ input at a time. In Tsai et al. 2016). The proposed MTAG model achieves state- (2019a), the authors used multiple cross-modal of-the-art performance with far fewer parameters. Transformers and applies late fusion to o"
C08-1106,W99-0606,0,0.0124845,"der generative probabilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synt"
C08-1106,W02-1001,0,0.218668,"Missing"
C08-1106,N01-1025,0,0.218911,"cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conventionally labeled by ‘BO"
C08-1106,W02-2018,0,0.0112059,"g previous studies on shallow parsing, our experiments are performed on the CoNLL 2000 LDCRF for Shallow Parsing We implemented LDCRFs in C++, and optimized the system to cope with large scale problems, in which the feature dimension is beyond millions. We employ similar predicate sets defined in Sha & Pereira (2003). We follow them in using predicates that depend on words as well as POS tags in the neighborhood of a given position, taking into account only those 417,835 features which occur at least once in the training data. The features are listed in Table 3. As for numerical optimization (Malouf 2002; Wallach 2002), we performed gradient decent with the Limited-Memory BFGS (L-BFGS) optimization technique (Nocedal & Wright 1999). L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle large-scale problems in an efficient manner. We implemented an L-BFGS optimizer in C++ by modifying the OWLQN package (Andrew & Gao 2007) developed by Galen Andrew. In our experiments, storing 10 pairs of previous gradients for the approximation of the function’s inver"
C08-1106,P05-1010,1,0.787072,"as shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the concept of latentdynamics for shallow parsing, showing how hidden states automatically learned by the model present similar characteristics. We"
C08-1106,H05-1124,0,0.502342,"ssifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the conc"
C08-1106,W95-0107,0,0.054769,"Missing"
C08-1106,W96-0213,0,0.150328,"abilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on P"
C08-1106,W00-0726,0,0.0690631,"33 1.00 1.00 0.98 1.00 1.00 1.00 1.00 0.99 0.99 0.88 0.73 0.67 1.00 1.00 0.62 0.94 0.93 0.92 0.97 0.94 0.92 Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi , hi−2 hi−1 hi } POS Features: {ti−1 , ti , ti+1 , ti−2 ti−1 , ti−1 ti , ti ti+1 , ti+1 ti+2 , ti−2 ti−1 ti , ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi , hi−2 hi−1 hi } Table 3: Feature templates used in the experiments. wi is the current word; ti is current POS tag; and hi is the current hidden state (for the case of latent models) or the current label (for the case of conventional models). data set (Sang & Buchholz 2000; Ramshow & Marcus 1995). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. The standard evaluation metrics for this task are precision p (the fraction of output chunks matching the reference chunks), recall r (the fraction of reference chunks returned), and the Fmeasure given by F = 2pr/(p + r). 6.1 Table 2: Latent-dynamics learned automatically by the LDCRF model. This table shows the top three words and their gold-standard POS tags for each hidden states. lar roles in modeling the dynamics in shallow parsing. Further, the singular proper nouns and t"
C08-1106,N03-1028,0,0.245876,"s the non-recursive cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conve"
C08-1106,A00-2007,0,\N,Missing
C10-1097,P08-1024,0,0.0294485,"al model based on pause duration and trigram part-of-speech frequency. The model was constructed by identifying, from the HCRC Map Task Corpus (Anderson et al., 1991), trigrams ending with a backchannel. Fujie et al. (2004) used Hidden Markov Models to perform head nod recognition. In their paper, they combined head gesture detection with prosodic low-level features from the same person to determine strongly positive, weak positive and negative responses to yes/no type utterances. In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al., 2008). One of the most relevant works is that of Eisenstein and Davis (2007), which presents a latent conditional model for fusion of multiple modalities (speech and gestures). One of the key difference of our work is that we are explicitly modeling the micro dynamics and temporal relationship between modalities. 3 Multimodal Prediction Models Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal dis861 plays and signals. A topic of central interest in modeling such behaviors is the patterning of interlocuto"
C10-1097,P08-1097,0,0.0223883,"onfirming the importance of combining different types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In pa"
C10-1097,P07-1045,0,0.0663785,"Missing"
C10-1097,H94-1020,0,0.112976,"tions, we included all individual words (i.e., unigrams) spoken by the speaker during the interactions. S YNTACTIC STRUCTURE Finally, we attempt to capture syntactic information that may provide relevant cues by extracting four types of features from a syntactic dependency structure corresponding to the utterance. The syntactic structure is produced automatically using a CRF partof-speech (POS) tagger and a data-driven left-toright shift-reduce dependency parser (Sagae and Tsujii, 2007), both trained on POS tags and dependency trees extracted from the Switchboard section of the Penn Treebank (Marcus et al., 1994), converted to dependency trees using the Penn2Malt tool1 . The four syntactic features are: 864 • Part-of-speech tags for each word (e.g. noun, verb, etc.), taken from the output of the POS tagger • Grammatical function for each word (e.g. subject, object, etc.), taken directly from the dependency labels produced by the parser • Part-of-speech of the syntactic head of each word, taken from the dependency links produced by the parser • Distance and direction from each word to its syntactic head, computed from the dependency links produced by the parser 1 http://w3.msi.vxu.se/ nivre/research/Pe"
C10-1097,D09-1140,0,0.0252282,"timodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts"
C10-1097,D09-1118,0,0.0577146,"Missing"
C10-1097,P03-1070,0,0.0283658,"eech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts and this paper addresses the problem of how to predict and generate this important class of dyadic nonverbal behavior. Several researchers have developed models to predict when backchannel should happen. In general, these results are difficult to compare as they utilize different corpora and present varying evaluation metrics. Ward"
C10-1097,P07-2031,0,0.0199849,"g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts and this paper addresses the problem of how to predict and generate this important class of dyadic nonverbal behavior. Several researchers have developed models to predict when backchannel should happen. In general, these results are difficult to compare as they utilize different corpora and present varying evaluation metrics. Ward and Tsukahara (2000)"
C10-1097,P07-1101,0,0.0238199,"types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and p"
C10-1097,W07-1903,0,0.0623161,"Missing"
C10-1097,D07-1111,1,0.815075,"me studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Using the transcriptions, we included all individual words (i.e., unigrams) spoken by the speaker during the interactions. S YNTACTIC STRUCTURE Finally, we attempt to capture syntactic information that may provide relevant cues by extracting four types of features from a syntactic dependency structure corresponding to the utterance. The syntactic structure is produced automatically using a CRF partof-speech (POS) tagger and a data-driven left-toright shift-reduce dependency parser (Sagae and Tsujii, 2007), both trained on POS tags and dependency trees extracted from the Switchboard section of the Penn Treebank (Marcus et al., 1994), converted to dependency trees using the Penn2Malt tool1 . The four syntactic features are: 864 • Part-of-speech tags for each word (e.g. noun, verb, etc.), taken from the output of the POS tagger • Grammatical function for each word (e.g. subject, object, etc.), taken directly from the dependency labels produced by the parser • Part-of-speech of the syntactic head of each word, taken from the dependency links produced by the parser • Distance and direction from eac"
C10-1097,P05-1003,0,0.12531,"sture class corresponds to a state label. (See Figure 3a). M ULTIMODAL CLASSIFIERS ( EARLY FUSION ) Our second baseline consists of two models: CRF and LDCRF (Morency et al., 2007). To train these models, we concatenate all multimodal features (lexical, syntactic, prosodic and visual) in one input vector. Graphical representation of these baseline models are given in Figure 3. CRF M IXTURE OF EXPERTS To show the importance of latent variable in our LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). The training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the last graph of Figure 3. 5.4 Methodology We performed held-out testing by randomly selecting a subset of 11 interactions (out of 47) for the test set. The training set contains the remaining 36 dyadic interactions. All models in this paper were evaluated with the same training and test sets. Validation of all model parameters (regularization term and number of hidden states) was performed using a 3-fold cross-validation strategy on the training set. The regularization term was valida"
C10-1097,E06-1022,0,0.0674659,"Missing"
C10-1097,W98-0319,0,0.100375,"processing: (1) temporal synchrony/asynchrony between modalities, (2) micro dynamics and (3) integration of different levels of interpretation. We present an empirical evaluation on nonverbal feedback prediction (e.g., head nod) confirming the importance of combining different types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues"
D16-1185,P15-1153,0,0.0302229,"Missing"
D16-1185,E14-1075,0,0.0325415,"ogues. In Section 6 and 7, we present our experimental results and analyses, and finally conclude our work in Section 8. 2 Related Work In this section, we discuss three related research topics. Text summarization is an relevant task that aims to create a summary that retains the most important points of the original document. Then we discuss the 3 http://multicomp.cs.cmu.edu 1798 evaluation metrics of text summarization. Finally, we discuss the video description which is complementary to our work. Generic Text Summarization Alogrithms Text summarization is widely explored in the news domain (Hong and Nenkova, 2014; McKeown, 2005). Generally, there are two approaches: extractive and abstractive summarization. Extractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD (Wasson, 1998) was pioneering work. It selected leading text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news documents. He et al. (2012) assumed that summarization should consist of sentences that could best reconstruct the original document. They modeled relationship among sente"
D16-1185,N03-1020,0,0.172458,"ments. − TREM: Our TREM model proposed in Section 4 extracts sentences that can both summarize the current episode and prompt the next episode with two contingency factors. − TREM w/o SR: The TREM model without the sparse reconstruction factor proposed in Section 4.2.2. − TREM w/o CC: The TREM model without the concept coverage factor proposed in Section 4.2.1. − TREM w/o SR&CC: The summarization-only TREM model without contingency factors. In the 1803 Methodology Using TVRecap, we measure the quality of generated sentences following the standard metrics in the summarization community, ROUGE (Lin and Hovy, 2003). For the purpose of evaluation, we defined a development and a test set, by randomly selecting 18 adjacent pairs of episodes from all seasons. These episodes were selected to have at least two recap description sentences. The remaining 70 episodes were only used during the learning process of W. After tuning hyper-parameters on development set, we report the comparison results on the test set. 7 7.1 Results and Discussion Overall Results Table 2 shows our experimental results comparing TREM and baseline models using descriptions. In general, contingency-based methods (TREM, TREM w/o SR and TR"
D16-1185,N10-1134,0,0.163543,"active summarization. Extractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD (Wasson, 1998) was pioneering work. It selected leading text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news documents. He et al. (2012) assumed that summarization should consist of sentences that could best reconstruct the original document. They modeled relationship among sentences by forming an optimization problem. Moreover, Sipos et al. (2012) and Lin and Bilmes (2010) studied multi-document summarization using coverage-based methods. Among them, Lin and Bilmes (2010) proposed to approximate the optimal solution of a class of functions by exploiting submodularity. Abstractive summarization automatically create new sentences. For example, compared with the sentence-level analysis in extractive summarization, Bing et al. (2015) explored fine-grained syntactic units, i.e. noun/verb phrases, to represent concepts in input documents. The informative phrases were then used to generate sentences. In this paper, we generalize the idea of text summarization to text"
D16-1185,P11-1052,0,0.0272368,"hat best capture the contingency between Di and Di+1 . The reconstruction contingency factor can be defined as: X Mr (Ri , Di+1 ) = max r |W∗ d. (8) d∈Di+1 4.3 r∈Ri Optimization In this section, we describe our approach to optimize the main objective function expressed in Equations 1 and 7. Finding an efficient algorithm to optimize a set function like Equation 1 is often challenging. However, it can be easily shown that the objective function of Equation 1 is submodular, since all its components S(Ri , Di ), Ms (Ri , Di+1 ) and Mr (Ri , Di+1 ) are submodular with respect to Ri . According to Lin and Bilmes (2011), there exists a simple greedy algorithm for monotonic submodular function maximization where the solution is guaranteed to be close to the real optimum. Specifii cally, if we denote Rgreedy as the approximation optimized by greedy algorithm and Ri∗ as the best posi sible solution, then F(Rgreedy ) ≥ (1 − 1e ) · F(Ri∗ ), where F(·) is the objective function of Equation 1 and e ≈ 2.718 denotes the natural constant. The greedy approach is shown in Algorithm 1. Algorithm 1 Text Recap Extraction Input: Vectorized sentence representations {Di }E , parameters λ , λ , θ, γ, budget K, s r i=1 optimal"
D16-1185,U05-1002,0,0.0452108,"7, we present our experimental results and analyses, and finally conclude our work in Section 8. 2 Related Work In this section, we discuss three related research topics. Text summarization is an relevant task that aims to create a summary that retains the most important points of the original document. Then we discuss the 3 http://multicomp.cs.cmu.edu 1798 evaluation metrics of text summarization. Finally, we discuss the video description which is complementary to our work. Generic Text Summarization Alogrithms Text summarization is widely explored in the news domain (Hong and Nenkova, 2014; McKeown, 2005). Generally, there are two approaches: extractive and abstractive summarization. Extractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD (Wasson, 1998) was pioneering work. It selected leading text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news documents. He et al. (2012) assumed that summarization should consist of sentences that could best reconstruct the original document. They modeled relationship among sentences by forming"
D16-1185,N15-1046,0,0.017454,"(2015) explored fine-grained syntactic units, i.e. noun/verb phrases, to represent concepts in input documents. The informative phrases were then used to generate sentences. In this paper, we generalize the idea of text summarization to text recap extraction. Instead of summarizing a given document or collection, our model emphasizes plot contingency with the next episode. Summarization Applications Summarization techniques are not restricted to informative resources (e.g. news), applications in broader areas are gaining attention (Apar´ıcio et al., 2016). As the prevailance of online forums, Misra et al. (2015) developed tools to recognize arguments from opinionated conversations, and group them across discussions. In entertainment industry, Sang and Xu (2010) proposed a character-based movie summarization approach by incorporating scripts into movie analysis. Moreover, recent applications include multimedia artifact generation (Figueiredo et al., 2015), music summarization (Raposo et al., 2015) and customer satisfaction analysis (Roy et al., 2016). Video Description Generating video descriptions is a task that studies automatic generation of natural language that describes events happening in video"
D16-1185,P98-2222,0,0.20954,"summary that retains the most important points of the original document. Then we discuss the 3 http://multicomp.cs.cmu.edu 1798 evaluation metrics of text summarization. Finally, we discuss the video description which is complementary to our work. Generic Text Summarization Alogrithms Text summarization is widely explored in the news domain (Hong and Nenkova, 2014; McKeown, 2005). Generally, there are two approaches: extractive and abstractive summarization. Extractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD (Wasson, 1998) was pioneering work. It selected leading text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news documents. He et al. (2012) assumed that summarization should consist of sentences that could best reconstruct the original document. They modeled relationship among sentences by forming an optimization problem. Moreover, Sipos et al. (2012) and Lin and Bilmes (2010) studied multi-document summarization using coverage-based methods. Among them, Lin and Bilmes (2010) proposed to approximate the optimal solution of"
D16-1185,C98-2217,0,\N,Missing
D17-1115,C16-1251,1,0.606276,"r multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches. 2 Related Work Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MM"
D17-1115,P15-1162,0,0.0109673,"Missing"
D17-1115,P14-1062,0,0.0613356,"et (performances of the original pre-trained models are shown in parenthesis in Table 3) and compare them to our language only TFNlanguage . RNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure. DAN (Iyyer et al., 2015) The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences. DynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences. CMK-L, SAL-CNN-L and SVM-MD-L are multimodal models from section using only language modality 5.1. Results in Table 3 show that our model using only language modality outperforms state-of-theart approaches for the CMU-MOSI dataset. While previous models are well-studied and suitable models for sentiment analysis in written language, they underperform in modeling the sentiment in spoken language. We suspect that this underperfo"
D17-1115,D14-1162,0,0.11163,"2 h3 .. see .. .. . zl .. . .. hTl LSTM .. . 128 ReLU 128 ReLU Figure 3: Spoken Language Embedding Subnetwork (Ul ) encodings are usable by the rest of the pipeline by simply focusing on relevant parts using the nonlinear affine transformation of time-dependent embeddings which can act as a dimension reducing attention mechanism. To formally define our proposed Spoken Language Embedding Subnetwork (Ul ), let l = {l1 , l2 , l3 , . . . , lTl ; lt ∈ R300 }, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014). A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations hl = {h1 , h2 , h3 , . . . , hTl ; ht ∈ R128 } for words according to the following LSTM formulation.     i sigmoid    f  sigmoid  =  Wl Xt Wle  o  sigmoid d ht−1 m tanh ct = f ct−1 + i m ht = o ⊗ tanh(ct ) hl = [h1 ; h2 ; h3 ; . . . ; hTl ] hl is a matrix of language representations formed from concatenation of h1 , h2 , h3 , . . . hTl . hl is then used as input to a fully-connected network that generates language embedding zl :"
D17-1115,P13-1096,1,0.78677,"Missing"
D17-1115,D15-1303,1,0.925435,"d Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis. 1 Figure 1: Unimodal, bimodal and trimodal interaction in multimodal sentiment analysis. language, visual and acoustic behaviors that change the perception of the expressed sentiment. Introduction Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice). This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.). The central challenge in multimodal sent"
D17-1115,D13-1170,0,0.00557656,"sis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches. 2 Related Work Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (W¨ollmer et al., 2013), YouTube (Morency et al., 2011), and MOUD (P´erez-Rosas et al., 2013), however CMUMOSI is the only Englis"
D17-1115,W16-2346,0,0.0526902,"Missing"
D17-1115,J11-2001,0,0.00665678,"rms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches. 2 Related Work Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b)"
D17-1115,P15-1150,0,0.0140376,"ks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches. 2 Related Work Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (W¨ollmer et al., 2013), YouTube (Morency et al., 2011), and MOUD (P´erez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels. The newest mul"
D17-1115,P17-1142,1,0.87509,"Missing"
D17-1115,D12-1122,0,0.0161197,"tics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches. 2 Related Work Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015). Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (W¨ollmer et al., 2013), YouTube (Morency et al.,"
D18-1014,P17-1047,0,0.0276486,"or the sequence of facial muscle activations for the presentation of a frown. Cross-modal interactions refer to interactions between modalities. For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed occurrence of a laughter after the end of a sentence. Modeling these interactions lie at the heart of human multimodal language analysis and has recently become a centric research direction in multimodal natural language processing (Liu et al., 2018; Pham et al., 2018; Chen et al., 2017), multimodal speech recognition (Sun et al., 2016; Gupta et al., 2017; Harwath and Glass, 2017; Kamper et al., 2017), as well as multimodal machine learning (Tsai et al., 2018; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). Recent advances in cognitive neuroscience have demonstrated the existence of multistage aggregation across human cortical networks and functions (Taylor et al., 2015), particularly during the integration of multisensory information (Parisi et al., 2017). At later stages of cognitive processing, higher level semantic meaning is extracted from phrases, facial expressions, and tone of voice, eventually leading to the formation of higher level crossmodal conce"
D18-1014,P15-1162,0,0.0556641,"Missing"
D18-1014,P18-1209,1,0.72226,"31 - November 4, 2018. 2018 Association for Computational Linguistics to the generative grammar of a language (Chomsky, 1957) or the sequence of facial muscle activations for the presentation of a frown. Cross-modal interactions refer to interactions between modalities. For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed occurrence of a laughter after the end of a sentence. Modeling these interactions lie at the heart of human multimodal language analysis and has recently become a centric research direction in multimodal natural language processing (Liu et al., 2018; Pham et al., 2018; Chen et al., 2017), multimodal speech recognition (Sun et al., 2016; Gupta et al., 2017; Harwath and Glass, 2017; Kamper et al., 2017), as well as multimodal machine learning (Tsai et al., 2018; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). Recent advances in cognitive neuroscience have demonstrated the existence of multistage aggregation across human cortical networks and functions (Taylor et al., 2015), particularly during the integration of multisensory information (Parisi et al., 2017). At later stages of cognitive processing, higher level semantic meaning i"
D18-1014,P16-1101,0,0.0165765,"a et al., 2016). These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics (Xu et al., 2013). Multimodal Temporal Graphical Models: The application of graphical models in sequence modeling has been an important research problem. Hidden Markov Models (HMMs) (Baum and Petrie, 1966), Conditional Random Fields (CRFs) (Lafferty et al., 2001), and Hidden Conditional Random Fields (HCRFs) (Quattoni et al., 2007) were shown to work well on modeling sequential data from the language (Misawa et al., 2017; Ma and Hovy, 2016; Huang et al., 2015) and acoustic (Yuan and Liberman, 2008) modalities. These temporal graphical models have also been extended for modeling multimodal data. Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song 151 et al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013). Multimodal Tempor"
D18-1014,W17-4114,0,0.0128264,"ng et al., 2016; Poria et al., 2016). These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics (Xu et al., 2013). Multimodal Temporal Graphical Models: The application of graphical models in sequence modeling has been an important research problem. Hidden Markov Models (HMMs) (Baum and Petrie, 1966), Conditional Random Fields (CRFs) (Lafferty et al., 2001), and Hidden Conditional Random Fields (HCRFs) (Quattoni et al., 2007) were shown to work well on modeling sequential data from the language (Misawa et al., 2017; Ma and Hovy, 2016; Huang et al., 2015) and acoustic (Yuan and Liberman, 2008) modalities. These temporal graphical models have also been extended for modeling multimodal data. Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song 151 et al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013)"
D18-1014,D13-1170,0,0.00400566,"odel data from multiple views (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song 151 et al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013). Multimodal Temporal Neural Networks: More recently, with the advent of deep learning, Recurrent Neural Networks (Elman, 1990; Jain and Medsker, 1999) have been used extensively for language and speech based sequence modeling (Zilly et al., 2016; Soltau et al., 2016), sentiment analysis (Socher et al., 2013; dos Santos and Gatti, 2014; Glorot et al., 2011; Cambria, 2016), and emotion recognition (Han et al., 2014; Bertero et al., 2016; Lakomkin et al., 2018). Long-short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997a) have also been extended for multimodal settings (Rajagopalan et al., 2016) and by learning binary gating mechanisms to remove noisy modalities (Chen et al., 2017). Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions. These use Bayesian ranking algorithms (Herbrich et al., 2007) to model both person-independent and perso"
D18-1014,D14-1162,0,0.0829154,"mmunicative behaviors. POM (Park et al., 2014) contains 903 movie review videos each annotated for 12 speaker traits: confident (con), passionate (pas), voice pleasant (voi), credible (cre), vivid (viv), expertise (exp), reserved (res), trusting (tru), relaxed (rel), thorough (tho), nervous (ner), persuasive (per) and humorous (hum). (13) m∈M where � denotes vector concatenation. E can then be used as a multimodal representation for supervised or unsupervised analysis of multimodal language. It summarizes all modeled intra-modal 154 4.2 Multimodal Features and Alignment GloVe word embeddings (Pennington et al., 2014), Facet (iMotions, 2017) and COVAREP (Degottex et al., 2014) are extracted for the language, visual and acoustic modalities respectively 1 . Forced alignment is performed using P2FA (Yuan and Liberman, 2008) to obtain the exact utterance times 1 Details on feature extraction are in supplementary. Dataset Task Metric SOTA3 SOTA2 SOTA1 RMFN SOT A A2 ↑ 76.5◇ 77.1§ 77.4� 78.4 ↑ 1.0 F1 ↑ 74.5† 77.0§ 77.3� 78.0 ↑ 0.7 CMU-MOSI Sentiment A7 ↑ 33.2# 34.1� 34.7§ 38.3 ↑ 3.6 MAE ↓ 0.968§ 0.965� 0.955◇ 0.922 ↓ 0.033 Corr ↑ 0.622♭ 0.625§ 0.632� 0.681 ↑ 0.049 Table 1: Sentiment prediction results on CMUMOSI."
D18-1014,W18-3308,1,0.926268,"2018. 2018 Association for Computational Linguistics to the generative grammar of a language (Chomsky, 1957) or the sequence of facial muscle activations for the presentation of a frown. Cross-modal interactions refer to interactions between modalities. For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed occurrence of a laughter after the end of a sentence. Modeling these interactions lie at the heart of human multimodal language analysis and has recently become a centric research direction in multimodal natural language processing (Liu et al., 2018; Pham et al., 2018; Chen et al., 2017), multimodal speech recognition (Sun et al., 2016; Gupta et al., 2017; Harwath and Glass, 2017; Kamper et al., 2017), as well as multimodal machine learning (Tsai et al., 2018; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). Recent advances in cognitive neuroscience have demonstrated the existence of multistage aggregation across human cortical networks and functions (Taylor et al., 2015), particularly during the integration of multisensory information (Parisi et al., 2017). At later stages of cognitive processing, higher level semantic meaning is extracted from ph"
D18-1014,D15-1303,0,0.0350802,"n IEMOCAP. EFLSTM concatenates the multimodal inputs and uses that as input to a single LSTM (Hochreiter and Schmidhuber, 1997b). We also implement the Stacked, (EF-SLSTM) (Graves et al., 2013) Bidirectional (EF-BLSTM) (Schuster and Paliwal, 1997) and Stacked Bidirectional (EF-SBLSTM) LSTMs. For descriptions of the remaining baselines, we refer the reader to EF-HCRF (Quattoni et al., 2007), EF/MV-LDHCRF (Morency et al., 2007), MV-HCRF (Song et al., 2012), EF/MVHSSHCRF (Song et al., 2013), MV-LSTM (Rajagopalan et al., 2016), DF (Nojavanasghari et al., 2016), SAL-CNN (Wang et al., 2016), C-MKL (Poria et al., 2015), THMM (Morency et al., 2011), SVM (Cortes and Vapnik, 1995; Park et al., 2014) and RF (Breiman, 2001). 4.4 Table 2: Emotion recognition results on IEMOCAP. Best results are highlighted in bold and SOT A shows improvement over previous SOTA. Symbols denote baseline model which achieves the reported performance: MFN: �, MARN: §, BC-LSTM: ●, TFN: †, MV-LSTM: #, EF-LSTM: ♭, SVM: ×. The RMFN outperforms the current SOTA across evaluation metrics except SOT A entries in gray. Improvements are highlighted in green. of each word. We obtain the aligned video and audio features by computing the expecta"
D18-1014,P17-1081,1,0.941469,"related to human multimodal language: sentiment analysis, emotion recognition, and speaker traits recognition across three public multimodal datasets. RMFN achieves state-of-the-art performance in all three tasks. Through a comprehensive set of ablation experiments and visualizations, we demonstrate the advantages of explicitly defining multiple recursive stages for multimodal fusion. 2 Related Work Previous approaches in human multimodal language modeling can be categorized as follows: Non-temporal Models: These models simplify the problem by using feature-summarizing temporal observations (Poria et al., 2017). Each modality is represented by averaging temporal information through time, as shown for language-based sentiment analysis (Iyyer et al., 2015; Chen et al., 2016) and multimodal sentiment analysis (Abburi et al., 2016; Nojavanasghari et al., 2016; Zadeh et al., 2016; Morency et al., 2011). Conventional supervised learning methods are utilized to discover intra-modal and cross-modal interactions without specific model design (Wang et al., 2016; Poria et al., 2016). These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-mo"
D18-1014,C14-1008,0,0.024754,"iews (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song 151 et al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013). Multimodal Temporal Neural Networks: More recently, with the advent of deep learning, Recurrent Neural Networks (Elman, 1990; Jain and Medsker, 1999) have been used extensively for language and speech based sequence modeling (Zilly et al., 2016; Soltau et al., 2016), sentiment analysis (Socher et al., 2013; dos Santos and Gatti, 2014; Glorot et al., 2011; Cambria, 2016), and emotion recognition (Han et al., 2014; Bertero et al., 2016; Lakomkin et al., 2018). Long-short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997a) have also been extended for multimodal settings (Rajagopalan et al., 2016) and by learning binary gating mechanisms to remove noisy modalities (Chen et al., 2017). Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions. These use Bayesian ranking algorithms (Herbrich et al., 2007) to model both person-independent and person-dependent features (Liang"
D18-1014,D17-1115,1,0.831196,"values over each word utterance time interval (Tsai et al., 2018). 4.3 Baseline Models We compare to the following models for multimodal machine learning: MFN (Zadeh et al., 2018a) synchronizes multimodal sequences using a multi-view gated memory. It is the current state of the art on CMU-MOSI and POM. MARN (Zadeh et al., 2018b) models intra-modal and cross-modal interactions using multiple attention coefficients and hybrid LSTM memory components. GMELSTM(A) (Chen et al., 2017) learns binary gating mechanisms to remove noisy modalities that are contradictory or redundant for prediction. TFN (Zadeh et al., 2017) models unimodal, bimodal and trimodal interactions using tensor products. 155 Evaluation Metrics For classification, we report accuracy Ac where c denotes the number of classes and F1 score. For regression, we report Mean Absolute Error MAE and Pearson’s correlation r. For MAE lower values indicate stronger performance. For all remaining metrics, higher values indicate stronger performance. 5 Results and Discussion 5.1 Performance on Multimodal Language Results on CMU-MOSI, IEMOCAP and POM are presented in Tables 1, 2 and 3 respectively2 . We achieve state-of-the-art or competitive results fo"
D19-1211,L16-1079,0,0.289647,"257 {t,a,v} type joke pun political speech tv show speech #spk 1192 &lt;50 1741 Table 1: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number, positive, negative, modalities and speaker respectively. among active areas of research in both natural language processing and affective computing. Notable datasets in this area include “16000 OneLiners” (Mihalcea and Strapparava, 2005), “Pun of the Day” (Yang et al., 2015), “PTT Jokes” (Chen and Soo, 2018), “Ted Laughter” (Chen and Lee, 2017), and “Big Bang Theory” (Bertero et al., 2016). The above datasets have studied humor from different perspectives. For example, “16000 One-Liner” and “Pun of the Day” focus on joke detection (binary task), while “Ted Laughter” focuses on punchline detection (whether or not punchline triggers laughter). Similar to “Ted Laughter”, UR-FUNNY focuses on punchline detection. Furthermore, punchline is accompanied by context sentences to properly model the build up of humor. Unlike previous datasets where negative samples were drawn from a different domain, UR-FUNNY uses a challenging negative sampling case where samples are drawn from the same v"
D19-1211,W17-5009,0,0.569738,"{t} 2423 {t} 2551 {t} 4726 {t} 24981 {t,a} 8257 {t,a,v} type joke pun political speech tv show speech #spk 1192 &lt;50 1741 Table 1: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number, positive, negative, modalities and speaker respectively. among active areas of research in both natural language processing and affective computing. Notable datasets in this area include “16000 OneLiners” (Mihalcea and Strapparava, 2005), “Pun of the Day” (Yang et al., 2015), “PTT Jokes” (Chen and Soo, 2018), “Ted Laughter” (Chen and Lee, 2017), and “Big Bang Theory” (Bertero et al., 2016). The above datasets have studied humor from different perspectives. For example, “16000 One-Liner” and “Pun of the Day” focus on joke detection (binary task), while “Ted Laughter” focuses on punchline detection (whether or not punchline triggers laughter). Similar to “Ted Laughter”, UR-FUNNY focuses on punchline detection. Furthermore, punchline is accompanied by context sentences to properly model the build up of humor. Unlike previous datasets where negative samples were drawn from a different domain, UR-FUNNY uses a challenging negative samplin"
D19-1211,N18-2018,0,0.283157,"1425 4726 18691 8257 #Neg Mod 16000 {t} 2423 {t} 2551 {t} 4726 {t} 24981 {t,a} 8257 {t,a,v} type joke pun political speech tv show speech #spk 1192 &lt;50 1741 Table 1: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number, positive, negative, modalities and speaker respectively. among active areas of research in both natural language processing and affective computing. Notable datasets in this area include “16000 OneLiners” (Mihalcea and Strapparava, 2005), “Pun of the Day” (Yang et al., 2015), “PTT Jokes” (Chen and Soo, 2018), “Ted Laughter” (Chen and Lee, 2017), and “Big Bang Theory” (Bertero et al., 2016). The above datasets have studied humor from different perspectives. For example, “16000 One-Liner” and “Pun of the Day” focus on joke detection (binary task), while “Ted Laughter” focuses on punchline detection (whether or not punchline triggers laughter). Similar to “Ted Laughter”, UR-FUNNY focuses on punchline detection. Furthermore, punchline is accompanied by context sentences to properly model the build up of humor. Unlike previous datasets where negative samples were drawn from a different domain, UR-FUNN"
D19-1211,N18-1193,1,0.845704,"mor, mostly limited to adding simple audio features (Rakov and Rosenberg, 2013; Bertero et al., 2016) or the description of cartoon images (Shahaf et al., 2015). Furthermore, these attempts have been restricted to certain topics and domains (such as “Big Bang Theory” TV show 2047 (Bertero et al., 2016)). Multimodal Language Analysis: Studying natural language from modalities of text, visual and acoustic is a recent research trend in natural language processing (Zadeh et al., 2018b). Notable works in this area present novel multimodal neural architectures (Wang et al., 2019; Pham et al., 2019; Hazarika et al., 2018; Poria et al., 2017; Zadeh et al., 2017), multimodal fusion approaches (Liang et al., 2018; Tsai et al., 2018; Liu et al., 2018; Zadeh et al., 2018a; Barezi et al., 2018) as well as resources (Poria et al., 2018a; Zadeh et al., 2018c, 2016; Park et al., 2014; Rosas et al., 2013; W¨ollmer et al., 2013). Multimodal language datasets mostly target multimodal sentiment analysis (Poria et al., 2018b), emotion recognition (Zadeh et al., 2018c; Busso et al., 2008), and personality traits recognition (Park et al., 2014). URFUNNY dataset is similar to the above datasets in diversity (speakers and topi"
D19-1211,D18-1014,1,0.849508,", 2016) or the description of cartoon images (Shahaf et al., 2015). Furthermore, these attempts have been restricted to certain topics and domains (such as “Big Bang Theory” TV show 2047 (Bertero et al., 2016)). Multimodal Language Analysis: Studying natural language from modalities of text, visual and acoustic is a recent research trend in natural language processing (Zadeh et al., 2018b). Notable works in this area present novel multimodal neural architectures (Wang et al., 2019; Pham et al., 2019; Hazarika et al., 2018; Poria et al., 2017; Zadeh et al., 2017), multimodal fusion approaches (Liang et al., 2018; Tsai et al., 2018; Liu et al., 2018; Zadeh et al., 2018a; Barezi et al., 2018) as well as resources (Poria et al., 2018a; Zadeh et al., 2018c, 2016; Park et al., 2014; Rosas et al., 2013; W¨ollmer et al., 2013). Multimodal language datasets mostly target multimodal sentiment analysis (Poria et al., 2018b), emotion recognition (Zadeh et al., 2018c; Busso et al., 2008), and personality traits recognition (Park et al., 2014). URFUNNY dataset is similar to the above datasets in diversity (speakers and topics) and size, with the main task of humor detection. Beyond the scope of multimodal languag"
D19-1211,P18-1209,1,0.87395,"mages (Shahaf et al., 2015). Furthermore, these attempts have been restricted to certain topics and domains (such as “Big Bang Theory” TV show 2047 (Bertero et al., 2016)). Multimodal Language Analysis: Studying natural language from modalities of text, visual and acoustic is a recent research trend in natural language processing (Zadeh et al., 2018b). Notable works in this area present novel multimodal neural architectures (Wang et al., 2019; Pham et al., 2019; Hazarika et al., 2018; Poria et al., 2017; Zadeh et al., 2017), multimodal fusion approaches (Liang et al., 2018; Tsai et al., 2018; Liu et al., 2018; Zadeh et al., 2018a; Barezi et al., 2018) as well as resources (Poria et al., 2018a; Zadeh et al., 2018c, 2016; Park et al., 2014; Rosas et al., 2013; W¨ollmer et al., 2013). Multimodal language datasets mostly target multimodal sentiment analysis (Poria et al., 2018b), emotion recognition (Zadeh et al., 2018c; Busso et al., 2008), and personality traits recognition (Park et al., 2014). URFUNNY dataset is similar to the above datasets in diversity (speakers and topics) and size, with the main task of humor detection. Beyond the scope of multimodal language analysis, the dataset and studies i"
D19-1211,H05-1067,0,0.23644,"iners Pun of the Day PTT Jokes Ted Laughter Big Bang Theory UR-FUNNY #Pos 16000 2423 1425 4726 18691 8257 #Neg Mod 16000 {t} 2423 {t} 2551 {t} 4726 {t} 24981 {t,a} 8257 {t,a,v} type joke pun political speech tv show speech #spk 1192 &lt;50 1741 Table 1: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number, positive, negative, modalities and speaker respectively. among active areas of research in both natural language processing and affective computing. Notable datasets in this area include “16000 OneLiners” (Mihalcea and Strapparava, 2005), “Pun of the Day” (Yang et al., 2015), “PTT Jokes” (Chen and Soo, 2018), “Ted Laughter” (Chen and Lee, 2017), and “Big Bang Theory” (Bertero et al., 2016). The above datasets have studied humor from different perspectives. For example, “16000 One-Liner” and “Pun of the Day” focus on joke detection (binary task), while “Ted Laughter” focuses on punchline detection (whether or not punchline triggers laughter). Similar to “Ted Laughter”, UR-FUNNY focuses on punchline detection. Furthermore, punchline is accompanied by context sentences to properly model the build up of humor. Unlike previous dat"
D19-1211,D14-1162,0,0.0834143,"an, 1997). Rigid and nonrigid facial shape parameters are also extracted (Baltruˇsaitis et al., 2016). We observed that the camera angle and position changes frequently during TED presentations. However, for the majority of time, the camera stays focused on the presenter. Due to the volatile camera work, the only consistently available source of visual information was the speaker’s face. UR-FUNNY dataset is publicly available for download alongside all the extracted features. Extracted Features For each modality, the extracted features are as follows: 4 Language (text): Glove word embeddings (Pennington et al., 2014) are used as pre-trained word vectors for the text features. P2FA forced alignment model (Yuan and Liberman, 2008) is used to align the text and audio on phoneme level. From the force alignment, we extract the timing annotations of context and punchline on In this section, we first outline the problem formulation for performing binary multimodal humor detection on UR-FUNNY dataset. We then proceed to study the UR-FUNNY dataset through the lens of a contextualized extension of Memory Fusion Network (MFN) (Zadeh et al., 2018a) - a state-of-the-art model in multimodal language. 2050 Multimodal Hu"
D19-1211,D15-1284,0,0.138492,"g Theory UR-FUNNY #Pos 16000 2423 1425 4726 18691 8257 #Neg Mod 16000 {t} 2423 {t} 2551 {t} 4726 {t} 24981 {t,a} 8257 {t,a,v} type joke pun political speech tv show speech #spk 1192 &lt;50 1741 Table 1: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘#’,‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote number, positive, negative, modalities and speaker respectively. among active areas of research in both natural language processing and affective computing. Notable datasets in this area include “16000 OneLiners” (Mihalcea and Strapparava, 2005), “Pun of the Day” (Yang et al., 2015), “PTT Jokes” (Chen and Soo, 2018), “Ted Laughter” (Chen and Lee, 2017), and “Big Bang Theory” (Bertero et al., 2016). The above datasets have studied humor from different perspectives. For example, “16000 One-Liner” and “Pun of the Day” focus on joke detection (binary task), while “Ted Laughter” focuses on punchline detection (whether or not punchline triggers laughter). Similar to “Ted Laughter”, UR-FUNNY focuses on punchline detection. Furthermore, punchline is accompanied by context sentences to properly model the build up of humor. Unlike previous datasets where negative samples were draw"
D19-1211,W18-3300,0,0.345941,"for a punchline in advance. There is a gradual build up in the story with a sudden twist using a punchline (Ramachandran, 1998). Some punchlines when viewed in isolation (as illustrated in Figure 1) may not appear funny. The humor stems from the prior build up, cross-referencing multiple sources, and its delivery. Therefore, a full understanding of humor requires analyzing the context of the punchline. Understanding the unique dependencies across modalities and its impact on humor require knowledge from multimodal language; a recent research trend in the field of natural language processing (Zadeh et al., 2018b). Studies in this area aim to explain natural language from three modalities of text, visual and acoustic. In this paper, alongside computational descriptors for text, gestures such as smile or vocal properties such as loudness are measured and put together in a multimodal framework to define humor recognition as a multimodal task. The main contribution of this paper to the NLP community is introducing the first multimodal language (including text, visual and acoustic modalities) dataset of humor detection named “URFUNNY”. This dataset opens the door to understanding and modeling humor in a"
D19-1211,P18-1208,1,0.91905,"Missing"
D19-1211,D17-1115,1,\N,Missing
D19-1443,P19-1285,1,0.04926,"el with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction. 1 Introduction Transformer (Vaswani et al., 2017) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017), language understanding (Devlin et al., 2018), sequence prediction (Dai et al., 2019), image generation (Child et al., 2019), video activity classification (Wang et al., 2018), music generation (Huang et al., 2018a), and multimodal sentiment analysis (Tsai et al., 2019a). Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence. At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full Transformer model as exemplified with neural machin"
D19-1443,N19-4009,0,0.124308,"inputs. Formally, a sequence x = [x1 , x2 , ⋯, xT ] defines each element as xi = (fi , ti ) with fi ∈ F being the nontemporal feature at time i and ti ∈ T as an temporal feature (or we called it positional embedding). Note that fi can be the word representation (in neural machine translation (Vaswani et al., 2017)), a pixel in a frame (in video activity recognition (Wang et al., 2018)), or a music unit (in music generation (Huang et al., 2018b)). ti can be a mixture of sine and cosine functions (Vaswani et al., 2017) or parameters that can be learned during back-propagation (Dai et al., 2019; Ott et al., 2019). The feature vector are defined over a joint space X ∶= (F × T ). The resulting permutationinvariant set is: Sx = {x1 , x2 , ⋯, xT } = {(f1 , t1 ), (f2 , t2 ), ⋯, (fT , tT )}. Followed the definition by Vaswani et al. (2017), we use queries(q)/keys(k)/values(v) to represent the inputs for the attention. To be 4345 more precise, x{q/k/v} is used for denoting a query/key/value data in the query/key/value sequence x{q/k/v} (x{q/k/v} ∈ Sx{ q/k/v} ) with Sx{ q/k/v} being its set representation. We note that the input sequences are the same (xq = xk ) for self-attention and are different (xq from d"
D19-1443,N18-2074,0,0.0756503,"ce with the same length as the decoded sequence. It should be noted that some applications has only the decoder self-attention such as sequence prediction (Dai et al., 2019). In all cases, the Transformer’s attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is order-agnostic to the permutation in the input sequence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018; Dai et al., 2019)). The 1 The generated sequence can be regarded as a translated sequence (i.e., translating from the encoded sequence), where each generated token depends on all tokens in the encoded sequence. 4344 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4344–4353, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics above observation inspires us to connect Transformer’s attention to kernel learning (Scholkopf and Smola, 2001): the"
D19-1443,P17-1138,0,0.0651075,"he main components of Transformer’s attention, enabling a better understanding of this mechanism: recent variants of Transformers (Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019; Lee et al., 2018; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components. Among all the components, we argue that the most important one is the construction of the kernel function. We empirically study multiple kernel forms and the ways to integrate positional embedding in neural machine translation (NMT) using IWSLT’14 German-English (De-En) dataset (Edunov et al., 2017) and sequence prediction (SP) using WikiText-103 dataset (Merity et al., 2016). 2 Attention This section aims at providing an understanding of attention in Transformer via the lens of kernel. The inspiration for connecting the kernel (Scholkopf and Smola, 2001) and attention instantiates from the observation: both operations concurrently processes all inputs and calculate the similarity between the inputs. We first introduce the background (i.e., the original formulation) of attention and then provide a new reformulation within the class of kernel smoothers (Wasserman, 2006). Next, we show tha"
D19-1443,P19-1656,1,0.369683,". 1 Introduction Transformer (Vaswani et al., 2017) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017), language understanding (Devlin et al., 2018), sequence prediction (Dai et al., 2019), image generation (Child et al., 2019), video activity classification (Wang et al., 2018), music generation (Huang et al., 2018a), and multimodal sentiment analysis (Tsai et al., 2019a). Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence. At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full Transformer model as exemplified with neural machine translation application (Vaswani et al., 2017): 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded"
gratch-etal-2014-distress,W13-4032,1,\N,Missing
gratch-etal-2014-distress,brugman-russel-2004-annotating,0,\N,Missing
L16-1078,scherer-etal-2012-audiovisual,1,0.919937,"processing, virtual humans) have been deployed in many types of social skills training applications, from job interview training (Hoque et al., 2013; Ben Youssef et al., 2015) to intercultural skills training (Lane et al., 2013) or public speaking skills training (Damian et al., 2015; Chollet et al., 2015). Moreover, virtual audiences have been used for supporting people suffering from severe public speaking anxiety (North et al., 1998; Pertaub et al., 2002). Finally, recent works have proposed to automatically assess the public speaking ability of politicians (Rosenberg and Hirschberg, 2005; Scherer et al., 2012; Brilman and Scherer, 2015) or job applicants (Nguyen et al., 2013). The implementation of such technologies often require the use of multimodal corpora, either as data for training the models that will recognize multimodal behaviors (e.g. smiles, gestures) or higher level variables (e.g. emotions of the user, performance of a speaker), or for building the repertoire of behaviors of a virtual character. In this paper, we present and share a multimodal corpus of public speaking presentations that we collected while studying the potential of interactive virtual audiences for public speaking ski"
L16-1078,sloetjes-wittenburg-2008-annotation,0,0.0340773,"enberg and Hirschberg, 2005) and rated on 7-point Likert scales - applied more to the pre- or posttraining presentation, allowing us to evaluate whether the participants’ skill improved or not after training4 : 1. Eye Contact 6. Confidence Level 2. Body Posture 7. Stage Usage 3. Flow of Speech 8. Avoids pause fillers 4. Gesture Usage 9. Presentation Structure 5. Intonation 10. Overall Performance 2.4.3. Objective Measures To complement the expert ratings, two annotators manually marked periods of eye contact with the audience and the occurrence of pause fillers using the annotation tool ELAN (Sloetjes and Wittenburg, 2008). We observed high inter-rater agreement for a randomly selected subset of four videos that both annotators assessed: Krippendorff α for eye contact is α = 0.751 and pause fillers α = 0.957 respectively (α computed on a frame-wise basis at 30 Hz). 2.4.4. Automatic Acoustic Behavior Assessment We used the freely available COVAREP toolbox, a collaborative speech analysis repository (Degottex et al., 2014), to automatically extract audio features. COVAREP provides an extensive selection of open-source robust and tested speech processing algorithms enabling comparative and cooperative research wit"
N18-1193,P14-1062,0,0.0557824,"features of all utterances in the conversations. The dyadic conversations are present in the form of videos. Each utterance of a particular conversation is thus a small segment of the full video. For each utterance, we extract features for the modes: audio, visual and text. The process of feature extraction for each mode is described below. 4.1.1 Textual Features Extraction We extract features from the transcript of an utterance video using convolutional neural networks (CNNs). CNNs are effective in learning high level abstract representations of sentences from constituting words or n-grams (Kalchbrenner et al., 2014). To get our sentence representation, we use a simple CNN with one convolutional layer followed by max-pooling (Kim, 2014; Poria et al., 2016). Specifically, the convolution layer consists filters of sizes 3, 4 and 5 with 50 feature maps each. Max-pooling is employed on these feature maps with a pooling window of size 2. Finally, a fully connected layer is used with 100 neurons. The activations of this layer form our sentence representation tu . 4.1.2 Audio Feature Extraction To extract audio features we use openSMILE (Eyben et al., 2010). It is an open-source software which provides high dime"
N18-1193,D14-1181,0,0.0366111,"ar conversation is thus a small segment of the full video. For each utterance, we extract features for the modes: audio, visual and text. The process of feature extraction for each mode is described below. 4.1.1 Textual Features Extraction We extract features from the transcript of an utterance video using convolutional neural networks (CNNs). CNNs are effective in learning high level abstract representations of sentences from constituting words or n-grams (Kalchbrenner et al., 2014). To get our sentence representation, we use a simple CNN with one convolutional layer followed by max-pooling (Kim, 2014; Poria et al., 2016). Specifically, the convolution layer consists filters of sizes 3, 4 and 5 with 50 feature maps each. Max-pooling is employed on these feature maps with a pooling window of size 2. Finally, a fully connected layer is used with 100 neurons. The activations of this layer form our sentence representation tu . 4.1.2 Audio Feature Extraction To extract audio features we use openSMILE (Eyben et al., 2010). It is an open-source software which provides high dimensional audio vectors. These vectors comprise of features like loudness, Mel-spectra, MFCC, pitch, etc. Audio features pl"
N18-1193,L16-1075,0,0.0945675,"breathes or pauses) of such conversational videos. Emotional dynamics in a conversation is known to be driven by two prime factors: self and interspeaker emotional influence (Morris and Keltner, 2000; Liu and Maitlis, 2014). Self-influence relates to the concept of emotional inertia, i.e., the degree to which a person’s feelings carry over from one moment to another (Koval and Kuppens, 2012). Inter-speaker emotional influence is another trait where the other person acts as an influencer in the speaker’s emotional state. Conversely, speakers also tend to mirror emotions of their counterparts (Navarretta et al., 2016). Figure 1 provides an example from the dataset showing the presence of these two traits in a dialogue. Existing works in the literature do not capitalize on these two factors. Context-free systems infer emotions based only on the current utterance in the conversation (Bertero et al., 2016). Whereas, state-of-the-art context-based networks like Poria et al., 2017b, use long short-term memory (LSTM) networks to model speaker-based context that suffers from incapability of long-range summarization and unweighted influence from context, leading to model bias. Our proposed CMN incorporates these f"
N18-1193,P17-1081,1,0.593015,"Missing"
N18-1193,C16-1151,1,0.842072,"tion is thus a small segment of the full video. For each utterance, we extract features for the modes: audio, visual and text. The process of feature extraction for each mode is described below. 4.1.1 Textual Features Extraction We extract features from the transcript of an utterance video using convolutional neural networks (CNNs). CNNs are effective in learning high level abstract representations of sentences from constituting words or n-grams (Kalchbrenner et al., 2014). To get our sentence representation, we use a simple CNN with one convolutional layer followed by max-pooling (Kim, 2014; Poria et al., 2016). Specifically, the convolution layer consists filters of sizes 3, 4 and 5 with 50 feature maps each. Max-pooling is employed on these feature maps with a pooling window of size 2. Finally, a fully connected layer is used with 100 neurons. The activations of this layer form our sentence representation tu . 4.1.2 Audio Feature Extraction To extract audio features we use openSMILE (Eyben et al., 2010). It is an open-source software which provides high dimensional audio vectors. These vectors comprise of features like loudness, Mel-spectra, MFCC, pitch, etc. Audio features play a significant role"
N18-1193,D17-1115,1,0.831336,"are decided using hyperparameter tuning (see Section 5). For the input utterance, the activations of this layer form the video representation vu . Fusion: We perform feature level fusion to map the individual modalities to a joint space. This is done through a simple feature concatenation. Thus, the extracted features tu , au and vu are joined to form the utterance representation u = [tu ; au ; vu ] of dimension din = 300. This multimodal representation is generated for all utterances in a conversation. Literature consists of numerous fusion techniques for multimodal data (Atrey et al., 2010; Zadeh et al., 2017; Poria et al., 2017c). Exploring these on CMN, however, is beyond the scope of this paper and left as a future work. 4.2 Conversational Memory Network For classifying the emotion of an utterance ui , its corresponding histories (hista and histb ) are taken. Each history histλ contains the preceding K utterances by person Pλ (see Section 3). Here, both ui and utterances in the histories are represented using their multimodal feature vectors of dimension Rdin (Figure 2). The histories are first modeled into memory cells using GRUs. This provides the memories with context information summarized"
N18-2123,D16-1203,0,0.103231,"y a human (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2016; Rohrbach et al., 2016; Yu et al., 2016; Nagaraja et al., 2016; Hu et al., 2017). Recent work on RER has sought to make progress by introducing models that are better capable of reasoning about linguistic structure (Hu et al., 2017; Nagaraja et al., 2016) – however, since most of the state-of-the-arts systems involve complex neural parameterizations, what these models actually learn has been difficult to interpret. This is concerning because several post-hoc analyses of related tasks (Zhou et al., 2015; Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016) have revealed that some positive results are actually driven by superficial biases in datasets or shallow correlations without deeper visual or linguistic understanding. Evidently, it is hard to be completely sure if a model is performing well for the right reasons. To increase our understanding of how RER systems function, we present several analyses inspired by approaches that probe systems with perturbed inputs (Jia and Liang, 2017) and employ simple models to exploit and reveal biases in datasets (Chen et al., 2016a). First, we investigate whether"
N18-2123,P16-1223,0,0.069549,"Missing"
N18-2123,N18-2017,0,0.0221213,"network (Andreas et al., 2016) where the computation graph is defined in terms of a constituency parse of the input referring expression. Previous studies on other tasks have found that state-of-the-art systems may be successful for reasons different than originally assumed. For example, Chen et al. (2016b) show that a simple logistic regression baseline with carefully defined features can achieve competitive results for reading comprehension on CNN/Daily Mail datasets (Hermann et al., 2015), indicating that more sophisticated models may be learning realtively simple correlations. Similarly, Gururangan et al. (2018) reveal bias in a dataset for semantic inference by demonstrating a simple model that achieves competitive results without looking at the premise. 3 Analysis Methodology To perform our analysis, we take two state-of-theart systems CNN+LSTM-MIL (Nagaraja et al., 2016) and CMN (Hu et al., 2017) and train them from scratch with perturbed referring expressions. We note that the perturbation experiments explained in next subsections are performed on all train and test instances. All experiments are done on the standard train/test splits for the Google-Ref dataset (Mao et al., 2016). Systems are eva"
N18-2123,D17-1215,0,0.0298631,"erpret. This is concerning because several post-hoc analyses of related tasks (Zhou et al., 2015; Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016) have revealed that some positive results are actually driven by superficial biases in datasets or shallow correlations without deeper visual or linguistic understanding. Evidently, it is hard to be completely sure if a model is performing well for the right reasons. To increase our understanding of how RER systems function, we present several analyses inspired by approaches that probe systems with perturbed inputs (Jia and Liang, 2017) and employ simple models to exploit and reveal biases in datasets (Chen et al., 2016a). First, we investigate whether systems that were designed to incorporate linguistic structure actually require it and make use of it. To test this, we perform perturbation experiments on the input referring expressions. Surprisingly, we find that models are robust to shuffling the word order and limiting the word categories to nouns and adjectives. Second, we attempt to reveal shallower correlations that systems might instead be leveraging to do well on this task. We build two simple systems called Neural S"
N18-2123,D14-1086,0,0.346215,"xpressions where various aspects of linguistic structure are obscured. We perform three types of analyses: the first one studying syntactic structure (Section 3.2), the second focusing on the importance of word categories (Section 3.3), and the final one analyzing potential biases in the dataset (Section 3.4). 2 3.1 Related Work Referring expression recognition and generation is a well studied problem in intelligent user interfaces (Chai et al., 2004), human-robot interaction (Fang et al., 2012; Chai et al., 2014; Williams et al., 2016), and situated dialogue (Kennington and Schlangen, 2017). Kazemzadeh et al. (2014) and Mao et al. (2016) introduce two benchmark datasets for referring expression recognition. Several models that leverage linguistic structure have been proposed. Nagaraja et al. (2016) propose a model where target and supporting objects (i.e. objects that are mentioned in order to disambiguate the target object) are identified and scored jointly. The resulting model is able to localize supporting objects without direct supervision. Hu et al. (2017) introduce a compositional approach for the RER task. They assume that the referring expression can be decomposed into a triplet consisting of the"
N18-2123,N16-1030,0,0.0435666,"for which the target object is contained in the model’s top-k predictions. We provide further details of our experimental methodology in Section 4.1. 3.2 Syntactic Analysis by Permuting Word Order In English, word order is important for correctly understanding the syntactic structure of a sentence. Both models we analyze use Recurrent Neural Networks (RNN) (Elman, 1990) with Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997). Previous studies have shown that reccurrent architectures can perform well on tasks where word order and syntax are important: for example, tagging (Lample et al., 2016), parsing (Sutskever et al., 2014), and machine translation (Bahdanau et al., 2014). We seek to determine whether recurrent models for RER depend on syntactic structure. Premise 1: Randomly permuting the word order of an English referring expression will obscure its syntactic structure. We train CMN and CNN+LSTM-MIL with shuffled referring expressions as input and evaluate their performance. Analysis by Perturbation In this section, we would like to analyze how the state-of-the-art referring expression recognition systems utilize linguistic structure. We conduct 782 Model CMN LSTM+CNN-MIL No P"
N18-2123,D14-1162,0,0.080959,"onsists of around 26K images with 104K annotations. We use their Ground-Truth evaluation setup where the ground truth bounding box annotations from MSCOCO (Lin et al., 2014) are provided to the system as a part of the input. We used the split provided by Nagaraja et al. (2016) where splits have disjoint sets of images. We use precision@k for evaluating the performance of models. Implementation Details. To train our models, we used stochastic gradient descent for 6 epochs with an initial learning rate of 0.01 and multiplied by 0.4 after each epoch. Word embeddings were initialized using GloVe (Pennington et al., 2014) and finetuned during training. We extracted features for bounding boxes using the fc7 layer output of Faster-RCNN VGG-16 network (Ren et al., 2015) pre-trained on MSCOCO dataset (Lin et al., 2014). Hyperparameters such as hidden layer size of LSTM networks were picked based on the best Baseline Models. We compare Neural Sieves to the state-of-the-art models from the literature. LSTM + CNN - MIL Nagaraja et al. (2016) score target object-context object pairs using LSTMs for processing the referring expression and CNN features for bounding boxes. The pair with the highest score is predicted as"
N19-1267,N15-1027,0,0.0175895,"imodal utterance. The likelihood of a segment s given the embedding ms is therefore P[s∣ms ] = P[w∣ms ]αw (P[v∣ms ])αv P[a∣ms ]αa = ∏ P[w∣ms ]αw ∏ P[v∣ms ]αv ∏ P[a∣ms ]αa . w∈w v∈v a∈a (3) Choice of Likelihood Functions: As suggested by Arora et al. (2017), given ms , we model the probability of a word w using Equation (2). In order to analytically solve for ms , a lemma is introduced by Arora et al. (2016, 2017) which states that the partition function Zms is concentrated around some constant Z (for all ms ). This lemma is also known as the “self-normalizing” phenomenon of log-linear models (Andreas and Klein, 2015; Andreas et al., 2015). We use the same assumption and treat Zmst ≈ Z for all ms . Unlike discrete text tokens, the visual features are continuous. We assume that the visual features are generated from an isotropic Gaussian distribution. In section 5.1, we visually analyze the distribution of the features for real world datasets and show that these likelihood modeling assumptions are indeed justified. The Gaussian distribution is parametrized by simple linear transformations Wvµ , Wvσ ∈ R∣v∣×∣ms ∣ and bµv , bσv ∈ R∣v∣ : v∣ms ∼ N (µv , σv2 ), µv = Wvµ ms σv = diag (exp (Wvσ ms (4) + bµv , (5)"
N19-1267,Q16-1028,0,0.0998549,"tterance embedding. Each unimodal factor is modeled using the simple form of a likelihood function obtained via a linear transformation of the utterance embedding. We show that, under some assumptions, maximum likelihood estimation for the utterance embedding can be derived in closed form and is equivalent to taking a weighted average of the language, visual and acoustic features. ms for each segment that summarizes information present in the multimodal utterance. 3.2 probability of a word at time t is given by P[wt ∣cs ] = αp(wt ) + (1 − α) Background Our model is related to the work done by Arora et al. (2016) and Arora et al. (2017). In the following, we first provide a brief review of their method. Given a sentence, Arora et al. (2016) aims to learn a sentence embedding cs . They do so by assuming that the probability of observing a word wt at time t is given by a log-linear word production model (Mnih and Hinton, 2007) with respect to cs : P[wt ∣cs ] = exp (⟨vwt , cs ⟩) , Z cs (1) where cs is the sentence embedding (context), vwt represents the word vector associated with word wt and Zcs = ∑w∈V exp (⟨vw , cs ⟩) is a normalizing constant over all words in the vocabulary. Given this posterior prob"
N19-1267,W17-3203,0,0.0174682,"ulrajani et al., 2017) led to better performing multimodal representations. In our paper, we present a new perspective on learning multimodal utterance embeddings by assuming a conditional factorization over the language, visual and acoustic features. Our simple but strong baseline models offer an alternative approach that is extremely fast and competitive on both supervised and semi-supervised prediction tasks. 2.3 Strong Baseline Models A recent trend in NLP research has been geared towards building simple but strong baselines (Arora et al., 2017; Shen et al., 2018; Wieting and Kiela, 2019; Denkowski and Neubig, 2017). The effectiveness of these baselines indicate that complicated network components are not always required. For example, Arora et al. (2017) constructed sentence embeddings from weighted combinations of word embeddings which requires no trainable parameters yet generalizes well to down-stream tasks. Shen et al. (2018) proposed parameter-free pooling operations on word embeddings for document classification, text sequence matching, and text tagging. Wieting and Kiela (2019) discovered that random sentence encoders achieve competitive performance as compared to larger models that involve expens"
N19-1267,P15-1162,0,0.0832308,"Missing"
N19-1267,P02-1048,0,0.355996,"Missing"
N19-1267,W18-3304,0,0.0616425,"Missing"
N19-1267,D18-1014,1,0.907508,"rmation (Baltruˇsaitis et al., 2017). This challenging yet crucial research area has real-world applications in robotics (Montalvo et al., 2017; Noda et al., 2014), dialogue systems (Johnston et al., 2002; Rudnicky, 2005), intelligent tutoring systems (Mao and Li, 2012; Banda and Robinson, 2011; Pham and Wang, 2018), and healthcare diagnosis (Wentzel and van der Geest, 2016; Lisetti et al., 2003; Sonntag, 2017). Recent progress on multimodal representation learning has investigated various neural models that utilize one or more of attention, memory and recurrent components (Yang et al., 2017; Liang et al., 2018). There has also been a general trend of building more complicated models for improved performance. Human language is a rich multimodal signal consisting of spoken words, facial expressions, body gestures, and vocal intonations. Learning representations for these spoken utterances is a complex research problem due to the presence of multiple heterogeneous sources of information. Recent advances in multimodal learning have followed the general trend of building more complex models that utilize various attention, memory and recurrent components. In this paper, we propose two simple but strong ba"
N19-1267,D14-1162,0,0.101156,"Missing"
N19-1267,N18-1202,0,0.0487347,"rong baselines. 2.1 Language-Based Sentence Embeddings Sentence embeddings are crucial for down-stream tasks such as document classification, opinion analysis, and machine translation. With the advent of deep neural networks, multiple network designs such as Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986), LongShort Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997), Temporal Convolutional Networks (Bai et al., 2018), and the Transformer (Vaswani et al., 2017) have been proposed and achieve superior performance. However, more training data is required for larger models (Peters et al., 2018). In light of this challenge, researchers have started to leverage unsupervised training objectives to learn sentence embedding which showed state-of-the-art performance across multiple tasks (Devlin et al., 2018). In our paper, we go beyond unimodal language-based sentence embeddings and consider multimodal spoken utterances where additional information from the nonverbal behaviors is crucial to infer speaker intent. 2.2 Multimodal Utterance Embeddings Learning multimodal utterance embeddings brings a new level of complexity as it requires modeling both intra-modal and inter-modal interaction"
N19-1267,P18-1041,0,0.0203707,"and using attention to weight modalities (Gulrajani et al., 2017) led to better performing multimodal representations. In our paper, we present a new perspective on learning multimodal utterance embeddings by assuming a conditional factorization over the language, visual and acoustic features. Our simple but strong baseline models offer an alternative approach that is extremely fast and competitive on both supervised and semi-supervised prediction tasks. 2.3 Strong Baseline Models A recent trend in NLP research has been geared towards building simple but strong baselines (Arora et al., 2017; Shen et al., 2018; Wieting and Kiela, 2019; Denkowski and Neubig, 2017). The effectiveness of these baselines indicate that complicated network components are not always required. For example, Arora et al. (2017) constructed sentence embeddings from weighted combinations of word embeddings which requires no trainable parameters yet generalizes well to down-stream tasks. Shen et al. (2018) proposed parameter-free pooling operations on word embeddings for document classification, text sequence matching, and text tagging. Wieting and Kiela (2019) discovered that random sentence encoders achieve competitive perfor"
N19-1267,D17-1115,1,0.856882,"erance embedding. We derive a coordinate-ascent style algorithm (Wright, 2015) to learn the optimal multimodal embeddings under our model. We show that, under some assumptions, maximum likelihood estimation for the utterance embedding can be derived in closed form and is equivalent to computing a weighted average of the language, visual and acoustic features. Only a few linear transformation parameters need to be learned. In order to capture bimodal and trimodal representations, our second baseline extends the first one by assuming a factorization into unimodal, bimodal, and trimodal factors (Zadeh et al., 2017). To summarize, our simple baselines 1) consist primarily of linear functions, 2) have few parameters, and 3) can be approximately solved in a closed form solution. As a result, they demonstrate simplicity and 2599 Proceedings of NAACL-HLT 2019, pages 2599–2609 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics efficiency during learning and inference. We perform a set of experiments across two tasks and datasets spanning multimodal personality traits recognition (Park et al., 2014) and multimodal sentiment analysis (Zadeh et al., 2016). Our propose"
P11-2058,C10-1097,1,0.887949,"ntroduction In many real life scenarios, it is hard to collect the actual labels for training, because it is expensive or the labeling is subjective. To address this issue, a new direction of research appeared in the last decade, taking full advantage of the ”wisdom of crowds” (Surowiecki, 2004). In simple words, wisdom of crowds enables parallel acquisition of opinions from multiple annotators/experts. In this paper, we propose a new method to fuse wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fusion (Ozkan et al., 2010). In our Wisdom-LMDE model, a discriminative expert is trained for each crowd member. The key advantage of our computational model is that it can automatically discover the prototypical patterns of experts and learn the dynamic between these patterns. An overview of our approach is depicted in Figure 1. 335 We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determi"
P11-2058,D07-1111,0,0.0298428,"oposed. As motivated earlier, we focus our experiments on predicting listener backchannel since it is a well-suited task where variability exists among listeners. 4.1 Multimodal Speaker Features The speaker videos were transcribed and annotated to extract the following features: Lexical: Some studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Therefore, we use all the words (i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech,"
P11-2058,P05-1003,0,0.0242268,"ine model, we use consensus labels to train a CRF model, which are constructed by a similar approach presented in (Huang et al., 2010). The consensus threshold is set to 3 (at least 3 listeners agree to give feedback at a point) so that it contains approximately the same number of head nods as the actual listener. See Figure 1 for a graphical representation of CRF model. CRF Mixture of Experts To show the importance of latent variable in our Wisdom-LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the Figure 1. Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the ”Actual Listeners” (AL) from the RAPPORT dataset. Multimodal LMDE In this baseline model, we compare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-spe"
P11-2058,D08-1027,0,0.0822745,"Missing"
P13-1096,H05-1073,0,0.0105201,"on we provide a brief overview of related work in text-based sentiment analysis, as well as audio-visual emotion analysis. 2.1 Text-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more l"
P13-1096,N12-1073,0,0.0127022,"processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two pol"
P13-1096,E06-2031,0,0.0557996,"Missing"
P13-1096,P07-1056,0,0.0412454,"1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being sai"
P13-1096,P08-1041,0,0.0103924,"vity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opi"
P13-1096,P11-2099,0,0.0596938,"ext-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corp"
P13-1096,P12-1043,0,0.0135778,"u and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly"
P13-1096,P11-1015,0,0.157096,"ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addition of speech and visual modalities to text ana"
P13-1096,P12-1060,0,0.0247281,"nguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at uttera"
P13-1096,P07-1123,1,0.29988,"11), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by"
P13-1096,esuli-sebastiani-2006-sentiwordnet,0,0.0378227,"ular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addi"
P13-1096,D12-1034,0,0.0150462,"Missing"
P13-1096,P97-1023,0,0.057328,"ve been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitz"
P13-1096,P04-1035,0,0.0155733,"t analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical state"
P13-1096,D08-1049,0,0.0192736,"esents our multimodal sen973 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics timent analysis approach, including details about our linguistic, acoustic, and visual features. Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6. 2 such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. Related Work In this section we provide a brief overview of related work in text-based"
P13-1096,W06-0607,0,0.0283006,"t annotations. Section 4 presents our multimodal sen973 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics timent analysis approach, including details about our linguistic, acoustic, and visual features. Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6. 2 such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. Related Work In this section we provide a brief overview of"
P13-1096,S07-1013,1,0.787519,"tically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognit"
P13-1096,J11-2001,0,0.00592148,"pressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mih"
P13-1096,P02-1053,0,0.0366184,"er of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 200"
P13-1096,P09-1027,0,0.0107132,"and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling t"
P13-1096,D12-1122,0,0.0128718,"in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed,"
P17-1059,P07-1063,0,0.010097,"erature on affective language generation has not focused sufficiently on customizable state-of-the-art neural network techniques to generate emotional text, nor have they quantitatively evaluated their models on multiple emotionally colored corpora. Mahamood and Reiter (2011) use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare. While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text. Mairesse and Walker (2007) developed PERSONAGE, a system for dialogue generation conditioned on extraversion dimensions. They trained regression models on ground truth judge’s selections to automatically determine which of the sentences selected by their model exhibit appropriate extroversion attributes. In Keshtkar and Inkpen (2011), the authors use heuristics and rule-based approaches Motivated by these advances in neural language modeling and affective analysis of text, in this paper we propose a model for representation and generation of emotional text, which we call the Affect-LM. Our model is trained on conversat"
P17-1059,cieri-etal-2004-fisher,0,0.0146374,"rom P (wi |w1 , w2 , ..., wi−1 , e; β) for i ∈ {M + 1, M + 2, ..., M + N }. 4 Speech Corpora The Fisher English Training Speech Corpus is the main corpus used for training the proposed model, in addition to which we have chosen three emotionally colored conversational corpora. A brief description of each corpus is given below, and in Table 1, we report relevant statistics, such as the total number of words, along with the fraction of emotionally colored words (those belonging to the LIWC affective word categories) in each corpus. Fisher English Training Speech Parts 1 & 2: The Fisher dataset (Cieri et al., 2004) consists of speech from telephonic conversations of 10 minutes each, along with their associated transcripts. Each conversation is between two strangers who are requested to speak on a randomly selected topic from a set. Examples of conversation topics are Minimum Wage, Time Travel and Comedy. Distress Assessment Interview Corpus (DAIC): The DAIC corpus introduced by Gratch (2014) consists of 70+ hours of dyadic interviews between a human subject and a virtual human, where the virtual human asks questions designed to diagnose symptoms of psychological distress in the subject such as depressio"
P17-1059,gratch-etal-2014-distress,1,0.760608,"Missing"
P17-1059,W12-2502,0,0.0654493,"Missing"
P17-1059,W11-2803,0,0.0360921,"are fed to an LSTM language model to generate image captions. Kiros et al. (2014) used an LBL model (Log-Bilinear language model) for two applications - image retrieval given sentence queries, and image captioning. Lower perplexity was achieved on text conditioned on images rather than language models trained only on text. In contrast, previous literature on affective language generation has not focused sufficiently on customizable state-of-the-art neural network techniques to generate emotional text, nor have they quantitatively evaluated their models on multiple emotionally colored corpora. Mahamood and Reiter (2011) use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare. While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text. Mairesse and Walker (2007) developed PERSONAGE, a system for dialogue generation conditioned on extraversion dimensions. They trained regression models on ground truth judge’s selections to automatically determine which of the sentences selected by their model exhibit appropr"
P17-1081,C16-1251,1,0.728585,"mental results and discussion are shown in Section 4; finally, Section 5 concludes the paper. Related Work The opportunity to capture people’s opinions has raised growing interest both within the scientific community, for the new research challenges, and in the business world, due to the remarkable benefits to be had from financial market prediction. Text-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based approaches (Cambria et al., 2017a). While the use of knowledge bases was initially more popular for the identification of polarity in text (Cambria et al., 2016; Poria et al., 2016c), sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Socher et al., 2013; Oneto et al., 2016). In 1974, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, a"
P17-1081,C16-1017,1,0.133829,"Missing"
P17-1081,P13-1096,1,0.653714,"Missing"
P17-1081,D15-1303,1,0.861595,"al., 2017a). An utterance (Olson, 1977) is a unit of speech bound by breathes or pauses. Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video). In particular, utterance-level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his/her speech. Recently, a number of approaches to multimodal sentiment analysis, producing interesting results, have been proposed (P´erez-Rosas et al., 2013; Wollmer et al., 2013; Poria et al., 2015). However, there are major issues that remain unaddressed. Not considering the relation and dependencies among the utterances is one of such issues. State-of-the-art approaches in this area treat utterances independently and ignore the order of utterances in a video (Cambria et al., 2017b). Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based"
P17-1081,C16-1151,1,0.475899,"cussion are shown in Section 4; finally, Section 5 concludes the paper. Related Work The opportunity to capture people’s opinions has raised growing interest both within the scientific community, for the new research challenges, and in the business world, due to the remarkable benefits to be had from financial market prediction. Text-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based approaches (Cambria et al., 2017a). While the use of knowledge bases was initially more popular for the identification of polarity in text (Cambria et al., 2016; Poria et al., 2016c), sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Socher et al., 2013; Oneto et al., 2016). In 1974, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, and duration. As for"
P17-1081,P16-2034,0,0.0112835,"Missing"
P17-1081,D13-1170,0,0.0050966,"within the scientific community, for the new research challenges, and in the business world, due to the remarkable benefits to be had from financial market prediction. Text-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based approaches (Cambria et al., 2017a). While the use of knowledge bases was initially more popular for the identification of polarity in text (Cambria et al., 2016; Poria et al., 2016c), sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Socher et al., 2013; Oneto et al., 2016). In 1974, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, and duration. As for fusing audio and visual modalities for emotion recognition, two of the early works were (De Silva et al., 1997) and (Chen et al., 1998). Both works showed that a bi"
P17-1142,D14-1162,0,0.0833386,"Missing"
P17-1142,W16-2346,0,0.084038,"Missing"
P18-1208,W14-3103,0,0.064385,"Missing"
P18-1208,N18-1193,1,0.883168,"Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competitive performance compared to the current state of the art. 1 Introduction Theories of language origin identify the combination of language and nonverbal behaviors (vision and acoustic modality) as the prime form of communication utilized by humans throughout evolution (M¨uller, 1866). In natural language processing, this form of language is regarded as human multimodal language. Modeling multimodal language has recently become a centric research direction in both NLP and multimodal machine learning (Hazarika et al., 2018; Zadeh et al., 2018a; Poria et al., 2017a; Baltruˇsaitis et al., 2017; Chen et al., 2017). Studies strive to model the dual dynamics of multimodal language: intra-modal dynamics (dynamics within each modality) and cross-modal dynamics (dynamics across different modalities). However, from a resource perspective, previous multimodal language datasets have severe shortcomings in the following aspects: Diversity in the training samples: The diversity in training samples is crucial for comprehensive multimodal language studies due to the complexity of the underlying distribution. This complexity i"
P18-1208,P15-1162,0,0.0328841,"Missing"
P18-1208,P14-1062,0,0.0232654,"ter and Schmidhuber, 1997; Graves et al., 2013; Schuster and Paliwal, 1997). In case of unimodal models EF-LSTM refers to a single LSTM. We also compare to the following baseline models: † BC-LSTM (Poria et al., 2017b), ♣ C-MKL (Poria et al., 2016), ♭ DF (Nojavanasghari et al., 2016), ♡ SVM (Cortes and Vapnik, 1995; Zadeh et al., 2016b; Perez-Rosas et al., 2013; Park et al., 2014), ● RF (Breiman, 2001), THMM (Morency et al., 2011), SAL-CNN (Wang et al., 2016), 3DCNN (Ji et al., 2013). For language only baseline models: ∪ CNN-LSTM (Zhou et al., 2015), RNTN (Socher et al., 2013), ×: DynamicCNN (Kalchbrenner et al., 2014), ⊳ DAN (Iyyer et al., 2015), ≀ DHN (Srivastava et al., 2015), ⊲ RHN (Zilly et al., 2016). For acoustic only baseline models: AdieuNet (Trigeorgis et al., 2016), SERLSTM (Lim et al., 2016). 3 CMU-MOSEI Dataset Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. We introduce a novel dataset for multimodal sentiment and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI). In the following subsections, we first explain the details of the CMU-MOSEI data acquisition, followed by details of annotation and feat"
P18-1208,P11-1015,0,0.0574707,"sed, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. 2.1.2 Language Datasets Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data. While SST has larger pool of annotations, we only consider the root level annotations for comparison. Cornell Movie Review (Pang et al., 2002) is a collection of 2000 moviereview documents and sentences labeled with respect to their overall sentiment polarity or subjective rating. Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews. Sanders Tweets Sentiment (STS) consists of 5513 hand-classified tweets each classified with respect to one of four topics of Microsoft, Apple, Twitter, and Google. 2.1.3 Visual and Acoustic Datasets The Vera am Mittag (VAM) corpus consists of 12 hours of recordings of the German TV talk2237 show “Vera am Mittag” (Grimm et al., 2008). This audio-visual data is labeled for continuous-valued scale for three emotion primitives: valence, activation and dominance. VAM-Audio and VAMFaces are subsets that contain on acoustic and visual inputs respectivel"
P18-1208,P14-5010,0,0.00463157,"al number of words in sentences Total of unique words in sentences Total number of words appearing at least 10 times in the dataset Total number of words appearing at least 20 times in the dataset Total number of words appearing at least 50 times in the dataset more detailed analysis such as exact percentages and number of videos per topic are available in the supplementary material sentences using punctuation markers manually provided by transcripts. Due to the high quality of the transcripts, using punctuation markers showed better sentence quality than using the Stanford CoreNLP tokenizer (Manning et al., 2014). This was verified on a set of 20 random videos by two experts. After tokenization, a set of 23,453 sentences were chosen as the final sentences in the dataset. This was achieved by restricting each identity to contribute at least 10 and at most 50 sentences to the dataset. Table 2 shows high-level summary statistics of the CMU-MOSEI dataset. 3.2 Annotation Annotation of CMU-MOSEI follows closely the annotation of CMU-MOSI (Zadeh et al., 2016a) and Stanford Sentiment Treebank (Socher et al., 2013). Each sentence is annotated for sentiment on a [-3,3] Likert scale of: [−3: highly negative, −2"
P18-1208,W02-1011,0,0.0196872,"umns indicate presence of sentiment and emotion labels. TL denotes the total number of video hours. segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. 2.1.2 Language Datasets Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data. While SST has larger pool of annotations, we only consider the root level annotations for comparison. Cornell Movie Review (Pang et al., 2002) is a collection of 2000 moviereview documents and sentences labeled with respect to their overall sentiment polarity or subjective rating. Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews. Sanders Tweets Sentiment (STS) consists of 5513 hand-classified tweets each classified with respect to one of four topics of Microsoft, Apple, Twitter, and Google. 2.1.3 Visual and Acoustic Datasets The Vera am Mittag (VAM) corpus consists of 12 hours of recordings of the German TV talk2237 show “Vera am Mittag” (Grimm et al., 2008). This audio-visual data is labe"
P18-1208,D14-1162,0,0.0834849,"ent in CMU-MOSI. The emotion histogram shows different prevalence for different emotions. The most common category is happiness with more than 12,000 positive sample points. The least prevalent emotion is fear with almost 1900 positive sample points which is an acceptable number for machine learning studies. 3.3 Extracted Features Data points in CMU-MOSEI come in video format with one speaker in front of the camera. The extracted features for each modality are as follows (for other benchmarks we extract the same features): Language: All videos have manual transcription. Glove word embeddings (Pennington et al., 2014) were used to extract word vectors from transcripts. Words and audio are aligned at phoneme level using P2FA forced alignment model (Yuan and Liberman, 2008). Following this, the visual and acoustic modalities are aligned to the words by interpolation. Since the utterance duration of words in English is usually short, this interpolation does not lead to substantial information loss. Visual: Frames are extracted from the full videos at 30Hz. The bounding box of the face is extracted using the MTCNN face detection algorithm (Zhang et al., 2016). We extract facial action units through Facial Acti"
P18-1208,P13-1096,1,0.887226,"ion recognition. The following datasets include a combination of language, visual and acoustic modalities as their input data. 2.1.1 Multimodal Datasets CMU-MOSI (Zadeh et al., 2016b) is a collection of 2199 opinion video clips each annotated with sentiment in the range [-3,3]. CMU-MOSEI is the next generation of CMU-MOSI. The ICT-MMMO (W¨ollmer et al., 2013) consists of online social review videos annotated at the video level for sentiment. YouTube (Morency et al., 2011) contains videos from the social media web site YouTube that span a wide range of product reviews and opinion videos. MOUD (Perez-Rosas et al., 2013) consists of product review videos in Spanish. Each video consists of multiple segments labeled to display positive, negative or neutral sentiment. IEMOCAP (Busso et al., 2008) consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each 1 following creative commons license allows for personal unrestricted use and redistribution of the videos 2 https://github.com/A2Zadeh/CMUMultimodalDataSDK #S 23,453 2,199 340 300 400 11,855 2,000 25,000 5,513 10,000 23 499 1,867 50 46 538 80 1,645 242 48 600 # Sp 1,000 98 200 50 101 – – – – 10 4"
P18-1208,P17-1081,1,0.689412,"s, DFG is highly interpretable and achieves competitive performance compared to the current state of the art. 1 Introduction Theories of language origin identify the combination of language and nonverbal behaviors (vision and acoustic modality) as the prime form of communication utilized by humans throughout evolution (M¨uller, 1866). In natural language processing, this form of language is regarded as human multimodal language. Modeling multimodal language has recently become a centric research direction in both NLP and multimodal machine learning (Hazarika et al., 2018; Zadeh et al., 2018a; Poria et al., 2017a; Baltruˇsaitis et al., 2017; Chen et al., 2017). Studies strive to model the dual dynamics of multimodal language: intra-modal dynamics (dynamics within each modality) and cross-modal dynamics (dynamics across different modalities). However, from a resource perspective, previous multimodal language datasets have severe shortcomings in the following aspects: Diversity in the training samples: The diversity in training samples is crucial for comprehensive multimodal language studies due to the complexity of the underlying distribution. This complexity is rooted in variability of intra-modal an"
P18-1208,D13-1170,0,0.161799,"taset with previous sentiment analysis and emotion recognition datasets. #S denotes the number of annotated data points. #Sp is the number of distinct speakers. Mod indicates the subset of modalities present from {(l)anguage, (v)ision, (a)udio}. Sent and Emo columns indicate presence of sentiment and emotion labels. TL denotes the total number of video hours. segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. 2.1.2 Language Datasets Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data. While SST has larger pool of annotations, we only consider the root level annotations for comparison. Cornell Movie Review (Pang et al., 2002) is a collection of 2000 moviereview documents and sentences labeled with respect to their overall sentiment polarity or subjective rating. Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews. Sanders Tweets Sentiment (STS) consists of 5513 hand-classified tweets each classified with respect to one"
P18-1208,P17-1142,1,0.762351,"uT . This output is subsequently connected to a classification or regression layer for final prediction (for sentiment and emotion recognition). 5 Experiments and Discussion In our experiments, we seek to evaluate how modalities interact during multimodal fusion by studying the efficacies of DFG through time. Table 3 shows the results on CMU-MOSEI. Accuracy is reported as Ax where x is the number of sentiment classes as well as F1 measure. For regression we report MAE and correlation (r). For emotion recognition due to the natural imbalances across various emotions, we use weighted accuracy (Tong et al., 2017) and F1 measure. Graph-MFN shows superior performance in sentiment analysis and competitive performance in emotion recognition. Therefore, DFG is both an effective and interpretable model for multimodal fusion. To better understand the internal fusion mechanism between modalities, we visualize the behavior of the learned DFG efficacies in Figure 5 for various cases (deep red denotes high efficacy and deep blue denotes low efficacy). Multimodal Fusion has a Volatile Nature: The first observation is that the structure of the DFG is changing case by case and for each case over time. As a result,"
P18-1208,D17-1115,1,0.925088,"approaches are listed as follows and indicated with a symbol for reference in the Experiments and Discussion section (Section 5). # MFN: (Memory Fusion Network) (Zadeh et al., 2018a) synchronizes multimodal sequences using a multi-view gated memory that stores intraview and cross-view interactions through time. ∎ MARN: (Multi-attention Recurrent Network) (Zadeh et al., 2018b) models intra-modal and multiple cross-modal interactions by assigning multiple attention coefficients. Intra-modal and cross-modal interactions are stored in a hybrid LSTM memory component. ∗ TFN (Tensor Fusion Network) (Zadeh et al., 2017) models inter and intra modal interactions by creating a multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions. ◇ MV-LSTM (Multi-View LSTM) (Rajagopalan et al., 2016) is a recurrent model that designates regions inside a LSTM to different views of the data. § EF-LSTM (Early Fusion LSTM) concatenates the inputs from different modalities at each time-step and uses that as the input to a single LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2013; Schuster and Paliwal, 1997). In case of unimodal models EF-LSTM refers to a single LSTM. We also compare to the fol"
P18-1209,D16-1044,0,0.628822,"ency et al., 2011) ∗ equal contributions as well as speaker trait analysis and media description (Park et al., 2014a) have seen a great boost in performance with developments in multimodal research. However, a core research challenge yet to be solved in this domain is multimodal fusion. The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions. In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity. Some of the recent attempts (Fukui et al., 2016), (Zadeh et al., 2017) at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance. Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations. This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset. In this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on perf"
P18-1209,P14-1130,0,0.0245308,"have shown a great success. However, such methods suffer from exponentially increasing computational complexity, as the outer product over multiple modalities results in extremely high dimensional tensor representations. For unimodal data, the method of low-rank tensor approximation has been used in a variety of applications to implement more efficient tensor operations. Razenshteyn et al. (2016) proposes a modified weighted version of low-rank approximation, and Koch and Lubich (2010) applies the method towards temporally dependent data to obtain lowrank approximations. As for applications, Lei et al. (2014) proposes a low-rank tensor technique for dependency parsing while Wang and Ahuja (2008) uses the method of low-rank approximation applied directly on multidimensional image data (Datumas-is representation) to enhance computer vision applications. Hu et al. (2017) proposes a low-rank tensor-based fusion framework to improve the face recognition performance using the fusion of facial attribute information. However, none of these previous work aims to apply low-rank tensor techniques for multimodal fusion. Our Low-rank Multimodal Fusion method provides a much more efficient method to compute ten"
P18-1209,D14-1162,0,0.0902629,"g set are present in the test sets. Table 1 illustrates the data splits for all datasets in detail. 4.2 Features Each dataset consists of three modalities, namely language, visual, and acoustic modalities. To reach the same time alignment across modalities, we perform word alignment using P2FA (Yuan and Liberman, 2008) which allows us to align the three modalities at the word granularity. We calculate the visual and acoustic features by taking the average of their feature values over the word time interval (Chen et al., 2017). Language We use pre-trained 300-dimensional Glove word embeddings (Pennington et al., 2014) to encode a sequence of transcribed words into a sequence of word vectors. Visual The library Facet1 is used to extract a set of visual features for each frame (sampled at 30Hz) including 20 facial action units, 68 facial landmarks, head pose, gaze tracking and HOG features (Zhu et al., 2006). Acoustic We use COVAREP acoustic analysis framework (Degottex et al., 2014) to extract a set of low-level acoustic features, including 12 Mel frequency cepstral coefficients (MFCCs), pitch, voiced/unvoiced segmentation, glottal source, peak slope, and maxima dispersion quotient features. 4.3 Model Archi"
P18-1209,P13-1096,1,0.866122,"Missing"
P18-1209,D17-1115,1,0.739783,"qual contributions as well as speaker trait analysis and media description (Park et al., 2014a) have seen a great boost in performance with developments in multimodal research. However, a core research challenge yet to be solved in this domain is multimodal fusion. The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions. In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity. Some of the recent attempts (Fukui et al., 2016), (Zadeh et al., 2017) at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance. Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations. This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset. In this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance. The overall a"
P19-1152,P18-1207,0,0.0313079,"in mind: 1) What is the effect of various levels of imperfect data on tensor rank in T2FN? 2) Does T2FN with rank regularization perform well on prediction with imperfect data? We answer these questions in §4.2 and §4.3 respectively. 4.1 Datasets We experiment with real video data consisting of humans expressing their opinions using a combination of language and nonverbal behaviors. We use the CMU-MOSI dataset which contains 2199 videos annotated for sentiment in the range [−3, +3] (Zadeh et al., 2016). CMU-MOSI and related multimodal language datasets have been studied in the NLP community (Gu et al., 2018; Liu et al., 2018; Liang et al., 2018) from fully supervised settings but not from the perspective of supervised learning with imperfect data. We 4.2 Rank Analysis We first study the effect of imperfect data on the rank of tensor M. We introduce the following types of noises parametrized by noise level = [0.0, 0.1, ..., 1.0]. Higher noise levels implies more imperfection: 1) clean: no imperfection, 2) random drop: each entry is dropped independently with probability p ∈ noise level, and 3) structured drop: independently for each modality, each time step is chosen with probability p ∈ noise le"
P19-1152,P14-1130,0,0.0327131,"r results back up our intuitions that imperfect data increases tensor rank. Finally, we show that our model achieves good results across various levels of imperfection. 2 Related Work Tensor Methods: Tensor representations have been used for learning discriminative representations in unimodal and multimodal tasks. Tensors are powerful because they can capture important higher order interactions across time, feature dimensions, and multiple modalities (Kossaifi et al., 2017). For unimodal tasks, tensors have been used for part-of-speech tagging (Srikumar and Manning, 2014), dependency parsing (Lei et al., 2014), word segmentation (Pei et al., 2014), question answering (Qiu and Huang, 2015), and machine translation (Setiawan et al., 2015). For multimodal tasks, Huang et al. (2017) used tensor products between images and text features for image captioning. A similar approach was proposed to learn representations across text, visual, and acoustic features to infer speaker sentiment (Liu et al., 2018; Zadeh et al., 2017). Other applications include multimodal machine translation (Delbrouck and Dupont, 2017), audio-visual speech recognition (Zhang et al., 2017), and video semantic analysis (Wu et al., 20"
P19-1152,D18-1014,1,0.841433,"various levels of imperfect data on tensor rank in T2FN? 2) Does T2FN with rank regularization perform well on prediction with imperfect data? We answer these questions in §4.2 and §4.3 respectively. 4.1 Datasets We experiment with real video data consisting of humans expressing their opinions using a combination of language and nonverbal behaviors. We use the CMU-MOSI dataset which contains 2199 videos annotated for sentiment in the range [−3, +3] (Zadeh et al., 2016). CMU-MOSI and related multimodal language datasets have been studied in the NLP community (Gu et al., 2018; Liu et al., 2018; Liang et al., 2018) from fully supervised settings but not from the perspective of supervised learning with imperfect data. We 4.2 Rank Analysis We first study the effect of imperfect data on the rank of tensor M. We introduce the following types of noises parametrized by noise level = [0.0, 0.1, ..., 1.0]. Higher noise levels implies more imperfection: 1) clean: no imperfection, 2) random drop: each entry is dropped independently with probability p ∈ noise level, and 3) structured drop: independently for each modality, each time step is chosen with probability p ∈ noise level. If a time step is chosen, all feat"
P19-1152,W12-3701,0,0.02474,"bySN+vJerHerY/ZaMHKd/bJH1ifP4Lml48=&lt;/latexit&gt; Figure 1: Clean multimodal time series data (in shades of green) exhibits correlations across time and across modalities, leading to redundancy in low rank tensor representations. On the other hand, the presence of imperfect entries (in gray, blue, and red) breaks these correlations and leads to higher rank tensors. In these scenarios, we use tensor rank regularization to learn tensors that more accurately represent the true correlations and latent structures in multimodal data. Introduction ∗ imperfect entries facial behaviors, and body postures (Mihalcea, 2012; Rossiter, 2011). However, as much as more modalities are required for improved performance, we now face a challenge of imperfect data where data might be 1) incomplete due to mismatched modalities or sensor failure, or 2) corrupted with random or structured noise. As a result, an important research question involves learning robust representations from imperfect multimodal data. Recent research in both unimodal and multimodal learning has investigated the use of tensors for representation learning (Anandkumar et al., 2014). Given representations h1 , ..., hM from M modalities, the order-M ou"
P19-1152,P14-1028,0,0.0233093,"imperfect data increases tensor rank. Finally, we show that our model achieves good results across various levels of imperfection. 2 Related Work Tensor Methods: Tensor representations have been used for learning discriminative representations in unimodal and multimodal tasks. Tensors are powerful because they can capture important higher order interactions across time, feature dimensions, and multiple modalities (Kossaifi et al., 2017). For unimodal tasks, tensors have been used for part-of-speech tagging (Srikumar and Manning, 2014), dependency parsing (Lei et al., 2014), word segmentation (Pei et al., 2014), question answering (Qiu and Huang, 2015), and machine translation (Setiawan et al., 2015). For multimodal tasks, Huang et al. (2017) used tensor products between images and text features for image captioning. A similar approach was proposed to learn representations across text, visual, and acoustic features to infer speaker sentiment (Liu et al., 2018; Zadeh et al., 2017). Other applications include multimodal machine translation (Delbrouck and Dupont, 2017), audio-visual speech recognition (Zhang et al., 2017), and video semantic analysis (Wu et al., 2009; Gao et al., 2009). Imperfect Data:"
P19-1152,D14-1162,0,0.0821067,"ncy leads to low-rank tensor representations. 3) rclean &lt; rimperf ect &lt; rnoisy : If the data is imperfect, the presence of noise or incomplete values breaks these natural correlations and leads to higher rank tensor representations. These intuitions are also backed up by several experimental results which are presented in §4.2. 3.4 Tensor Rank Regularization Given our intuitions above, it would then seem natural to augment the discriminative objective function with a term to minimize the rank of M. 1571 use 52 segments for training, 10 for validation and 31 for testing. GloVe word embeddings (Pennington et al., 2014), Facet (iMotions, 2017), and COVAREP (Degottex et al., 2014) features are r r M i i ∥X ∥∗ = inf {∑ ∣λi ∣ ∶ X = ∑ λi ( ⊗ wm ) , ∥wm ∥ = 1, r ∈ N} . extracted for the language, visual and acoustic m=1 i=1 i=1 modalities respectively. Forced alignment is per(3) formed using P2FA (Yuan and Liberman, 2008) to When M = 2, this reduces to the matrix nuclear align visual and acoustic features to each word, renorm (sum of singular values). However, comsulting in a multimodal sequence. Our data splits, puting the rank of a tensor or its nuclear norm is features, alignment, and preprocessing steps are N"
P19-1152,P15-1004,0,0.0163657,"sults across various levels of imperfection. 2 Related Work Tensor Methods: Tensor representations have been used for learning discriminative representations in unimodal and multimodal tasks. Tensors are powerful because they can capture important higher order interactions across time, feature dimensions, and multiple modalities (Kossaifi et al., 2017). For unimodal tasks, tensors have been used for part-of-speech tagging (Srikumar and Manning, 2014), dependency parsing (Lei et al., 2014), word segmentation (Pei et al., 2014), question answering (Qiu and Huang, 2015), and machine translation (Setiawan et al., 2015). For multimodal tasks, Huang et al. (2017) used tensor products between images and text features for image captioning. A similar approach was proposed to learn representations across text, visual, and acoustic features to infer speaker sentiment (Liu et al., 2018; Zadeh et al., 2017). Other applications include multimodal machine translation (Delbrouck and Dupont, 2017), audio-visual speech recognition (Zhang et al., 2017), and video semantic analysis (Wu et al., 2009; Gao et al., 2009). Imperfect Data: In order to account for imperfect data, several works have proposed generative approaches"
P19-1656,P18-1008,0,0.0222121,"has the same length as Qα (i.e., Tα ), but is meanwhile represented in the feature √ space of Vβ . Specifically, the scaled (by dk ) softmax in Equation (1) computes a score matrix softmax (·) ∈ RTα ×Tβ , whose (i, j)-th entry measures the attention given by the i-th time step of modality α to the j-th time step of modality β. Hence, the i-th time step of Yα is a weighted summary of Vβ , with the weight determined by i-th row in softmax(·). We call Equation (1) a singlehead crossmodal attention, which is illustrated in Figure 3(a). Following prior works on transformers (Vaswani et al., 2017; Chen et al., 2018; Devlin et al., 2018; Dai et al., 2018), we add a residual connection to the crossmodal attention computation. Then, another positionwise feed-forward sublayer is injected to complete a crossmodal attention block (see Figure 3(b)). Each crossmodal attention block adapts directly [0] from the low-level feature sequence (i.e., Zβ in Figure 3(b)) and does not rely on self-attention, which makes it different from the NMT encoderdecoder architecture (Vaswani et al., 2017; Shaw et al., 2018) (i.e., taking intermediate-level features). We argue that performing adaptation 6560 ✓ ◆ Q↵ K &gt; softmax p V"
P19-1656,P18-1207,0,0.0206527,"imodal representations from static domains such as image and textual attributes (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012), human language contains time-series and thus requires fusing time-varying signals (Liang et al., 2018; Tsai et al., 2019). Earlier work used early fusion approach to concatenate input features from different modalities (Lazaridou et al., 2015; Ngiam et al., 2011) and showed improved performance as compared to learning from a single modality. More recently, more advanced models were proposed to learn representations of human multimodal language. For example, Gu et al. (2018) used hierarchical attention strategies to learn multimodal representations, Wang et al. (2019) adjusted the word representations using accompanying non-verbal behaviors, Pham et al. (2019) learned robust multimodal representations using a cyclic translation objective, and Dumpala et al. (2019) explored cross-modal autoencoders for audio-visual alignment. These previous approaches relied on the assumption that multimodal language sequences are already aligned in the resolution of words and considered only short-term multimodal interactions. In contrast, our proposed method requires no alignmen"
P19-1656,D18-1014,1,0.807831,"er suggests that the crossmodal attention used by MulT is capable of capturing correlated signals across asynchronous modalities. 2 Related Works Human Multimodal Language Analysis. Prior work for analyzing human multimodal language lies in the domain of inferring representations from multimodal sequences spanning language, vision, and acoustic modalities. Unlike learning multimodal representations from static domains such as image and textual attributes (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012), human language contains time-series and thus requires fusing time-varying signals (Liang et al., 2018; Tsai et al., 2019). Earlier work used early fusion approach to concatenate input features from different modalities (Lazaridou et al., 2015; Ngiam et al., 2011) and showed improved performance as compared to learning from a single modality. More recently, more advanced models were proposed to learn representations of human multimodal language. For example, Gu et al. (2018) used hierarchical attention strategies to learn multimodal representations, Wang et al. (2019) adjusted the word representations using accompanying non-verbal behaviors, Pham et al. (2019) learned robust multimodal represe"
P19-1656,P14-5010,0,0.00371493,"ghts Audio-to-Language (‘spectacle’) Attention Weights Figure 1: Example video clip from movie reviews. [Top]: Illustration of word-level alignment where video and audio features are averaged across the time interval of each spoken word. [Bottom] Illustration of crossmodal attention weights between text (“spectacle”) and vision/audio. Human language possesses not only spoken words but also nonverbal behaviors from vision (facial attributes) and acoustic (tone of voice) modalities (Gibson et al., 1994). This rich information provides us the benefit of understanding human behaviors and intents (Manning et al., 2014). Nevertheless, the heterogeneities across modalities often increase the difficulty of analyzing human language. For example, the receptors for audio and vision streams may vary with variable receiving frequency, and hence we may not obtain optimal mapping between them. A frowning face may relate to a pessimistically word spoken in the past. *equal contribution. … Language It’s huge sort Introduction ∗ [ ][ ][ ][ ] That is to say, multimodal language sequences often exhibit “unaligned” nature and require inferring long term dependencies across modalities, which raises a question on performing"
P19-1656,D16-1244,0,0.0867997,"Missing"
P19-1656,D14-1162,0,0.085685,"F-LSTM CTC + RAVEN (Wang et al., 2019) CTC + MCTN (Pham et al., 2019) 46.3 48.8 45.5 48.2 76.1 77.5 75.4 79.3 75.9 78.2 75.7 79.7 0.680 0.624 0.664 0.631 0.585 0.656 0.599 0.645 MulT (ours) 50.7 81.6 81.6 0.591 0.694 Table 2: Results for multimodal sentiment analysis on (relatively large scale) CMU-MOSEI with aligned and non-aligned multimodal sequences. 4.1 Datasets and Evaluation Metrics Each task consists of a word-aligned (processed in the same way as in prior works) and an unaligned version. For both versions, the multimodal features are extracted from the textual (GloVe word embeddings (Pennington et al., 2014)), visual (Facet (iMotions, 2017)), and acoustic (COVAREP (Degottex et al., 2014)) data modalities. A more detailed introduction to the features is included in Appendix. For the word-aligned version, following (Zadeh et al., 2018a; Tsai et al., 2019; Pham et al., 2019), we first use P2FA (Yuan and Liberman, 2008) to obtain the aligned timesteps (segmented w.r.t. words) for audio and vision streams, and we then perform averaging on the audio and vision features within these time ranges. All sequences in the word-aligned case have length 50. The process remains the same across all the datasets."
P19-1656,P17-1081,1,0.931047,"the crossmodal interactions at the scale of the entire utterances. This module latently adapts streams from one modality to another (e.g., vision → language) by repeated reinforcing one modality’s features with those from the other modalities, re6558 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6558–6569 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics gardless of the need for alignment. In comparison, one common way of tackling unaligned multimodal sequence is by forced word-aligning before training (Poria et al., 2017; Zadeh et al., 2018a,b; Tsai et al., 2019; Pham et al., 2019; Gu et al., 2018): manually preprocess the visual and acoustic features by aligning them to the resolution of words. These approaches would then model the multimodal interactions on the (already) aligned time steps and thus do not directly consider long-range crossmodal contingencies of the original features. We note that such wordalignment not only requires feature engineering that involves domain knowledge; but in practice, it may also not always be feasible, as it entails extra meta-information about the datasets (e.g., the exact"
P19-1656,N18-2074,0,0.0216755,"attention, which is illustrated in Figure 3(a). Following prior works on transformers (Vaswani et al., 2017; Chen et al., 2018; Devlin et al., 2018; Dai et al., 2018), we add a residual connection to the crossmodal attention computation. Then, another positionwise feed-forward sublayer is injected to complete a crossmodal attention block (see Figure 3(b)). Each crossmodal attention block adapts directly [0] from the low-level feature sequence (i.e., Zβ in Figure 3(b)) and does not rely on self-attention, which makes it different from the NMT encoderdecoder architecture (Vaswani et al., 2017; Shaw et al., 2018) (i.e., taking intermediate-level features). We argue that performing adaptation 6560 ✓ ◆ Q↵ K &gt; softmax p V 2 RT↵ ⇥dv dk CM !↵ (X↵ , X Z [D] !↵ Z [i] !↵ Addition Positionwise ) Feed-forward Block i ( ! ↵) Crossmodal Transformer ( ! ↵) LayerNorm softmax Q↵ 2 R T↵ ⇥dk ✓ Q↵ K &gt; p dk ◆ Addition K 2R T ⇥dk V 2R CM [i 1] [0] !↵ (Z !↵ , Z ) Q↵ K WQ↵ X↵ 2 RT↵ ⇥d↵ WK X 2 RT WV LayerNorm ⇥d↵ Z V LayerNorm [i 1] !↵ Z↵[0] Modality ↵ ⇥D Layers Multi-head T ⇥dv Z [0] Layer 0 Modality (a) Crossmodal attention CMβ→α (Xα , Xβ ) between sequences Xα , Xβ from distinct modalities. (b) A crossmodal transformer i"
P19-1656,W18-3305,0,0.0231748,"Missing"
P19-1656,D18-1548,0,0.0481527,"Missing"
P19-1656,D18-1458,0,0.0334043,"Missing"
P19-1656,D16-1138,0,0.0190645,"tween elements of multiple modalities are purely based on attention. In other words, MulT does not handle modality non-alignment by (simply) aligning them; instead, the crossmodal attention encourages the model to directly attend to elements in other modalities where strong signals or relevant information is present. As a result, MulT can capture long-range crossmodal contingencies in a way that conventional alignment could not easily reveal. Classical crossmodal alignment, on the other hand, can be expressed as a special (step diagonal) crossmodal attention matrix (i.e., monotonic attention (Yu et al., 2016)). We illustrate their differences in Figure 4. 4 Experiments In this section, we empirically evaluate the Multimodal Transformer (MulT) on three datasets that are frequently used to benchmark human multimodal affection recognition in prior works (Pham et al., 2019; Tsai et al., 2019; Liang et al., 2018). Our goal is to compare MulT with prior competitive approaches on both word-aligned (by word, which almost all prior works employ) and unaligned (which is more challenging, and which MulT is generically designed for) multimodal language sequences. 6562 Metric h Acch 7 Acc2 F1h MAE` Corrh (Word"
P19-1656,P18-1208,1,0.811581,"Missing"
P19-1656,N15-1016,0,\N,Missing
W12-1616,P07-1033,0,0.108891,"Missing"
W12-1616,W08-0125,0,0.0164307,"teractions with the same participants. From these archives, a small subset of spoken utterances can be efficiently annotated. As we will later show in our experiments, even a small number of annotated utterances can make a significant difference. Introduction By representing a higher level intention of utterances during human conversation, dialogue act labels are being used to enrich the information provided by spoken words (Stolcke et al., 2000). Dialogue act recognition is a preliminary step towards deep dialogue understanding. It plays a key role in the design of dialogue systems. Besides, Fernandez et al. (2008) find certain dialogue acts are important cues for detecting decisions in Multi-party dialogue. In 1 This paper is an extended version of a poster presented at SemDial 2011, with new experiments and deeper analysis. In this paper, we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers’ data. By treating each speaker as one domain, we point out the connection between training speaker specific dialogue act classifier and supervised domain adaptation problem. We analyze idiosyncrac"
W12-1616,D09-1130,0,0.224481,"Missing"
W12-1616,P07-1034,0,0.0787334,"Missing"
W12-1616,D09-1035,0,0.258101,"Missing"
W12-1616,W96-0213,0,0.481684,"Missing"
W12-1616,J00-3003,0,0.780629,"ning of generic classifiers extremely challenging. Luckily, in many applications such as face-to-face meetings or tele-immersion, we have access to archives of previous interactions with the same participants. From these archives, a small subset of spoken utterances can be efficiently annotated. As we will later show in our experiments, even a small number of annotated utterances can make a significant difference. Introduction By representing a higher level intention of utterances during human conversation, dialogue act labels are being used to enrich the information provided by spoken words (Stolcke et al., 2000). Dialogue act recognition is a preliminary step towards deep dialogue understanding. It plays a key role in the design of dialogue systems. Besides, Fernandez et al. (2008) find certain dialogue acts are important cues for detecting decisions in Multi-party dialogue. In 1 This paper is an extended version of a poster presented at SemDial 2011, with new experiments and deeper analysis. In this paper, we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers’ data. By treating each"
W12-1616,W04-2319,0,\N,Missing
W12-1616,W10-2607,0,\N,Missing
W12-1616,W08-0100,0,\N,Missing
W13-4032,baccianella-etal-2010-sentiwordnet,0,0.0040804,"tuting a single turn. While this simple scheme does not provide a detailed treatment of relevant phenomena such as overlapping speech, backchannels, and the interactive process of negotiating the turn in dialogue (Yang and Heeman, 2010), it provides a conceptually simple model for the definition of features for aggregate statistical analysis. 4.2 Valence features for user speech Features (e)(g) are meant to explore the idea that distressed users might use more negative or less positive vocabulary than non-distressed subjects. As an exploratory approach to this topic, we used SentiWordNet 3.0 (Baccianella and Sebastiani, 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users in our study. The dictionary contains approximately 117,000 entries. In general, each word w may appear in multiple entries, corresponding to different parts of speech and word senses. To assign a single valence score v(w) to each word in the dictionary, in our features we compute the average score across all parts of speech and word senses: Context-independent feature analysis We begin by analyzing a set of shallow features which we describe as context-independent, as they apply to user speech segments ind"
W13-4032,D12-1004,0,0.0145618,"3–202, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics Heeman et al. (2010) observed differences in children with autism in how long they pause before speaking and in their use of fillers, acknowledgments, and discourse markers. Some of these features are similar to those studied here, but looked at children communicating with clinicians rather than a virtual human dialogue system. Recent work on machine classification has demonstrated the ability to discriminate between schizophrenic patients and healthy controls based on transcriptions of spoken narratives (Hong et al., 2012), and to predict patient adherence to medical treatment from word-level features of dialogue transcripts (Howes et al., 2012). Automatic speech recognition and word alignment has also been shown to give good results in scoring narrative recall tests for identification of cognitive impairment (Prud’hommeaux and Roark, 2011; Lehr et al., 2012). Figure 1: Ellie. communicative behavior of patients with specific psychological disorders such as depression. In this section, we briefly summarize some closely related work. Most work has observed the behavior of patients in human-human interactions, suc"
W13-4032,W12-1610,0,0.0311906,"rences in children with autism in how long they pause before speaking and in their use of fillers, acknowledgments, and discourse markers. Some of these features are similar to those studied here, but looked at children communicating with clinicians rather than a virtual human dialogue system. Recent work on machine classification has demonstrated the ability to discriminate between schizophrenic patients and healthy controls based on transcriptions of spoken narratives (Hong et al., 2012), and to predict patient adherence to medical treatment from word-level features of dialogue transcripts (Howes et al., 2012). Automatic speech recognition and word alignment has also been shown to give good results in scoring narrative recall tests for identification of cognitive impairment (Prud’hommeaux and Roark, 2011; Lehr et al., 2012). Figure 1: Ellie. communicative behavior of patients with specific psychological disorders such as depression. In this section, we briefly summarize some closely related work. Most work has observed the behavior of patients in human-human interactions, such as clinical interviews and doctor-patient interactions. PTSD is generally less well studied than depression. Examples of th"
W13-4032,J11-2001,0,0.0104219,"context-independent, as they apply to user speech segments independently of what the system has recently said. Most of these are features that apply to many or all user speech segments. We describe our context-independent features in Section 4.2.1, and present our results for these features in Section 4.2.2. 4.2.1 v(w) = P e∈E(w) PosScoree (w) |E(w)| − NegScoree (w) where E(w) is the set of entries for the word w, PosScoree (w) is the positive score for w in entry e, and NegScoree (w) is the negative score for w in entry e. This is similar to the “averaging across senses” method described in Taboada et al. (2011). We use several different measures of the valence of each speech segment with transcript t = hw1 , ..., wn i. We compute the min, mean, and max valence of each transcript: Context-independent features We summarize our context-independent features in Figure 2. Speaking rate and onset times Based on previous clinical observations related to slowed speech and increased onset time for depressed individuals (Section 2), we defined features for speaking rate and onset time of user speech segments. We quantify the speaking rate of a user speech segment hs, e, ti, where t = hw1 , ..., wN i, as N/(e −"
W13-4032,W10-4346,0,\N,Missing
W13-4032,wittenburg-etal-2006-elan,0,\N,Missing
W14-4334,baccianella-etal-2010-sentiwordnet,0,0.00639605,"understanding of user speech. SimSensei Kiosk currently uses 4 statistically trained utterance classifiers to capture different aspects of user utterance meaning. The first NLU classifier identifies generic dialogue act types, including statements, yes-no questions, wh-questions, yes and no answers, and several others. This classifier is trained using the Switchboard DAMSL corpus (Jurafsky et al., 1997) using a maximum entropy model. The second NLU classifier assigns positive, negative, or neutral valence to utterances, in order to guide Ellie’s expression of empathy. We use SentiWordNet 3.0 (Baccianella et al., 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users (as recognized by the ASR); the valence assigned to an utterance is based primarily on the mean valence scores of Opening Rapport Building Phase What are some things you really like about LA? (top level question) I love the weather, I love the palm trees, I love the beaches, there’s a lot to do here. Ellie Diagnostic Phase Have you noticed any changes in your behavior or thoughts lately? (top level question) User Yes. Ellie Can you tell me about that? (continuation prompt) User I’m having a lot more nightma"
W14-4334,W13-4032,1,0.849274,"ss conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD) (DeVault et al., 2014). SimSensei Kiosk has two main functions – a virtual human called Ellie (pictured in Figure 1), who converses with a user in a spoken, semi-structured interview, and a multimodal perception system which analyzes the user’s behavior in real time to identify indicators of psychological distress. The system has been designed and developed over two years using a series of face-toface, Wizard-of-Oz, and automated system studies involving more than 350 human participants (Scherer et al., 2013; DeVault et al., 2013; DeVault et al., 2014). Agent design has been guided by two overarching goals: (1) the agent should make 254 Proceedings of the SIGDIAL 2014 Conference, pages 254–256, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics gine. The perception system analyzes audio and video in real time to identify features such as head position, gaze direction, smile intensity, and voice quality. DeVault et al. (2014) provides details on all the agent’s modules. 2 2.1 Ellie User Overview of Dialogue Processing ASR and NLU components Unlike many task-oriented dialogue domains"
W14-5908,W13-4032,1,0.893149,"eviews). The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative. Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness. For instance, a speaker’s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content. Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasi"
W14-5908,W02-1011,0,0.0147775,"Missing"
