2020.aacl-main.10,W13-3512,0,0.0524236,"2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distribution. There have also been some previous res"
2020.aacl-main.10,Q17-1010,0,0.0942011,"020. 2020 Association for Computational Linguistics 2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distrib"
2020.aacl-main.10,D17-1309,0,0.0484502,"Missing"
2020.aacl-main.10,2012.eamt-1.60,0,0.0426019,"n PTB Wiki2 58.80 66.00 58.39 57.85 65.48 63.93 Table 1: Perplexity results on PTB and Wiki2. 4.3 Low-resource NMT We applied our method to the standard long-short term memory networks (LSTMs) based sequenceto-sequence (seq2seq) model on two datasets: German-English (de-en) with 153K sentence pairs 3 https://github.com/facebookresearch/fastText When we set the minimum count of word occurrence to 1, the standard CBOW does not perform well. 5 https://github.com/salesforce/awd-lstm-lm 4 82 from IWSLT 2014 (Cettolo et al., 2014), EnglishVietnamese (en-vi) with 133K sentence pairs from IWSLT 2015 (Cettolo et al., 2012). The detailed data statistics of two low-resource NMT datasets is in Table 2. We used the opennmt-py toolkit6 with a 2-layer bidirectional LSTM with hidden size of 500 and set the training epoch to 30. The word embedding size is set to 500 and the batch size is 64. We trained the seq2seq models by the SGD optimizer with start learning rate being 1.0, which will be decayed by 0.5 if perplexity does not decrease on the validation set. Other hyper-parameters were kept default. We also include some published results based on LSTM-based seq2seq models to gauge the result of our baseline. As shown"
2020.aacl-main.10,D14-1113,0,0.0195351,"1 We used ClusterCat (https://github.com/jonsafari/clustercat) as the implementation. 2 https://github.com/yukunfeng/cluster-cbow 80 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 80–86 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window s"
2020.aacl-main.10,2014.iwslt-evaluation.1,0,0.0372576,"Missing"
2020.aacl-main.10,D14-1162,0,0.105114,"Missing"
2020.aacl-main.10,C14-1015,0,0.0246946,"hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distribution. There have also been some previous researches that utiliz"
2020.aacl-main.10,N16-1139,0,0.0392281,"Missing"
2020.aacl-main.10,D11-1141,0,0.129519,"Missing"
2020.aacl-main.10,Q18-1032,0,0.054332,"Missing"
2020.aacl-main.10,P17-1184,0,0.0188732,"rameters of our standard LSTM model on language modeling task. en-vi 133,317 1,268 1,553 54,169 25,615 5 seq2seq with attention (Luong and Manning, 2015) AC+LL (Bahdanau et al., 2017) NPMT (Huang et al., 2018) Our seq2seq with attention CBOW Our ReIn+ReOut 5.1 Targeted Perplexity Results To show the gain for frequent and infrequent words, we measured the perplexity for frequent and infrequent words in the test data separately. Specifically, we calculated the perplexity of the next word, when an infrequent word is given as the current word. A similar analysis on language models can be found in Vania and Lopez (2017). Our analysis do not contain new words in the test dataset. The results are shown in Table 7. As we see, ReIn+ReOut is more effective than CBOW in learning both the embeddings of frequent and infrequent words, as we explained in Sec. 3.2.1. en-vi 23.3 27.69 28.16 28.24 28.67 Table 3: BLEU scores on two low-resource MT datasets. NPMT in Huang et al. (2018) used a neural phrase-based machine translation model and AC+LL in Bahdanau et al. (2017) used a one-layer GRU encoder and decoder with attention. 4.4 Analysis In this section, we analyse ReIn+ReOut on the basis of LM experiments with en and"
2020.aacl-main.10,D14-1108,0,0.0292458,"ost well-known methods for obtaining word embeddings is based on Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013a) and there have been many research efforts to extend it. In this paper, we focus on incorporating word clusters into CBOW model. Each word cluster consists of words that function similarly. By aggregating such words, we can alleviate data sparsity, even though each of those words is infrequent. In the past few years, word clusters have been applied to various tasks, such as named-entity recognition (Ritter et al., 2011), machine translation (Wuebker et al., 2013) and parsing (Kong et al., 2014). Many word clustering algorithms can be applied to a raw corpus with different languages and help us obtain word clusters easily without additional language resources. In our method, we keep only very frequent words and replace the other words with their clusters for both input and output words in the CBOW model. We evaluate our cluster-incorporated word embeddings2 on downstream tasks, in which finetuning of word embeddings is involved. The evaluation for frequent words, for which our method also works well, on word similarity tasks can be found in appendix A. For the downstream tasks, we ch"
2020.aacl-main.10,D13-1138,0,0.0297482,"semantic information. One of the most well-known methods for obtaining word embeddings is based on Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013a) and there have been many research efforts to extend it. In this paper, we focus on incorporating word clusters into CBOW model. Each word cluster consists of words that function similarly. By aggregating such words, we can alleviate data sparsity, even though each of those words is infrequent. In the past few years, word clusters have been applied to various tasks, such as named-entity recognition (Ritter et al., 2011), machine translation (Wuebker et al., 2013) and parsing (Kong et al., 2014). Many word clustering algorithms can be applied to a raw corpus with different languages and help us obtain word clusters easily without additional language resources. In our method, we keep only very frequent words and replace the other words with their clusters for both input and output words in the CBOW model. We evaluate our cluster-incorporated word embeddings2 on downstream tasks, in which finetuning of word embeddings is involved. The evaluation for frequent words, for which our method also works well, on word similarity tasks can be found in appendix A."
2020.aacl-main.10,2015.iwslt-evaluation.11,0,0.0184906,"Missing"
2020.acl-main.309,N16-1024,0,0.0303711,"tion task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples, and we argue this allows us to evaluate the true capacity of the current architectures. In our experiments (Section 4), we show that our margin loss achieves higher syntactic performance than their multi-task learning. Another relevant work on the capacity of LSTMLMs is Kuncoro et al. (2019), which shows that by distilling from syntactic LMs (Dyer et al., 2016), LSTM-LMs can improve their robustness on various agreement phenomena. We show that our LMs with the margin loss outperform theirs in most of the aspects, further strengthening the argument about a stronger capacity of LSTM-LMs. The latter part of this paper is a detailed analysis of the trained models and introduced losses. Our second question is about the true limitation of LSTM-LMs: are there still any syntactic constructions that the models cannot handle robustly even with our direct learning signals? This question can be seen as a fine-grained one raised by Enguehard et al. (2017) with a"
2020.acl-main.309,K17-1003,0,0.348138,"bustness on the target syntactic constructions? Regarding this point, we find that adding additional token-level loss trying to guarantee a margin between log-probabilities for the correct and incorrect words (e.g., log p(laughs|h) and log p(laugh|h) for (1a)) is superior to the alternatives. On the test set of Marvin and Linzen (2018), we show that LSTM language models (LSTM-LMs) trained by this loss reach near perfect level on most syntactic constructions for which we create negative examples, with only a slight increase of perplexity about 1.0 point. Past work conceptually similar to us is Enguehard et al. (2017), which, while not directly exploiting negative examples, trains an LM with additional explicit supervision signals to the evaluation task. They hypothesize that LSTMs do have enough capacity to acquire robust syntactic abilities but the learning signals given by the raw text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples,"
2020.acl-main.309,N18-1108,0,0.280443,"te the models’ syntactic robustness by a different task. 2.1 Syntactic evaluation task As introduced in Section 1, the task for a model is to assign a higher probability to the grammatical sentence over the ungrammatical one, given a pair of minimally different sentences at a critical position affecting the grammaticality. For example, (1a) and (1b) only differ at a final verb form, and to assign a higher probability to (1a), models need 3376 to be aware of the agreement dependency between author and laughs over an RC. Marvin and Linzen (2018) test set While initial work (Linzen et al., 2016; Gulordava et al., 2018) has collected test examples from naturally occurring sentences, this approach suffers from the coverage issue, as syntactically challenging examples are relatively rare. We use the test set compiled by Marvin and Linzen (2018), which consists of synthetic examples (in English) created by a fixed vocabulary and a grammar. This approach allows us to collect varieties of sentences with complex structures. The test set is divided by the syntactic constructions appearing in each example. Many constructions are different types of subject-verb agreement, including local agreement on different senten"
2020.acl-main.309,N01-1021,0,0.134486,"ll any syntactic constructions that the models cannot handle robustly even with our direct learning signals? This question can be seen as a fine-grained one raised by Enguehard et al. (2017) with a stronger tool and improved evaluation metric. Among tested constructions, we find that syntactic agreement across an object relative clause (RC) is challenging. To inspect whether this is due to the architectural limitation, we train another LM on a dataset, on which we unnaturally augment sentences involving object RCs. Since it is known that object RCs are relatively rare compared to subject RCs (Hale, 2001), frequency may be the main reason for the lower performance. Interestingly, even when increasing the number of sentences with an object RC by eight times (more than twice of sentences with a subject RC), the accuracy does not reach the same level as agreement across a subject RC. This result suggests an inherent difficulty in tracking a syntactic state across an object RC for sequential neural architectures. We finally provide an ablation study to understand the encoded linguistic knowledge in the models learned with the help of our method. We experiment under reduced supervision at two diffe"
2020.acl-main.309,D18-1150,0,0.0745688,"Missing"
2020.acl-main.309,P18-1027,0,0.0543623,"Missing"
2020.acl-main.309,P18-1249,0,0.0496477,"Missing"
2020.acl-main.309,P18-1132,0,0.16024,"Missing"
2020.acl-main.309,P19-1337,0,0.44358,"text are weak, and show that multi-task learning with a binary classification task to predict the upcoming verb form (singular or plural) helps models aware of the target syntax (subject-verb agreement). Our experiments basically confirm and strengthen this argument, with even stronger learning signals from negative examples, and we argue this allows us to evaluate the true capacity of the current architectures. In our experiments (Section 4), we show that our margin loss achieves higher syntactic performance than their multi-task learning. Another relevant work on the capacity of LSTMLMs is Kuncoro et al. (2019), which shows that by distilling from syntactic LMs (Dyer et al., 2016), LSTM-LMs can improve their robustness on various agreement phenomena. We show that our LMs with the margin loss outperform theirs in most of the aspects, further strengthening the argument about a stronger capacity of LSTM-LMs. The latter part of this paper is a detailed analysis of the trained models and introduced losses. Our second question is about the true limitation of LSTM-LMs: are there still any syntactic constructions that the models cannot handle robustly even with our direct learning signals? This question can"
2020.acl-main.309,Q16-1037,0,0.20272,"ults for RNN language models (RNN-LMs) trained only with raw text are overall negative; prior work has reported low performance on the challenging test cases (Marvin and Linzen, 2018) even with the massive size of the data and model (van Schijndel et al., 2019), or argue the necessity of an architectural change to track the syntactic structure explicitly (Wilcox et al., 2019b; Kuncoro et al., 2018). Here the task is to evaluate whether a model assigns a higher likelihood on a grammatically correct sentence (1a) over an incorrect sentence (1b) that is minimally different from the original one (Linzen et al., 2016). We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreeme"
2020.acl-main.309,D18-1151,0,0.258249,"eeting of the Association for Computational Linguistics, pages 3375–3385 c July 5 - 10, 2020. 2020 Association for Computational Linguistics The first research question we pursue is about this latter point: what is a better method to utilize negative examples that help LMs to acquire robustness on the target syntactic constructions? Regarding this point, we find that adding additional token-level loss trying to guarantee a margin between log-probabilities for the correct and incorrect words (e.g., log p(laughs|h) and log p(laugh|h) for (1a)) is superior to the alternatives. On the test set of Marvin and Linzen (2018), we show that LSTM language models (LSTM-LMs) trained by this loss reach near perfect level on most syntactic constructions for which we create negative examples, with only a slight increase of perplexity about 1.0 point. Past work conceptually similar to us is Enguehard et al. (2017), which, while not directly exploiting negative examples, trains an LM with additional explicit supervision signals to the evaluation task. They hypothesize that LSTMs do have enough capacity to acquire robust syntactic abilities but the learning signals given by the raw text are weak, and show that multi-task le"
2020.acl-main.309,N19-1356,0,0.124066,"Missing"
2020.acl-main.309,D19-1592,0,0.0827344,"Missing"
2020.acl-main.309,N03-1033,0,0.0306675,"particular for our margin losses, such negative effects are very small. 4 Experiments on Additional Losses We first see the overall performance of baseline LSTM-LMs as well as the effects of additional losses. Throughout the experiments, for each setting, we train five models from different random seeds and report the average score and standard deviation. The code is available at https://github.com/aistairc/lm syntax negative. Naive LSTM-LM performs well The main accuracy comparison across target constructions for different settings is presented in Table 1. We first 7 We use Stanford tagger (Toutanova et al., 2003) to find the present verbs. We change the number of verbs tagged by VBZ or VBP using inflect.py (https://pypi.org/project/inflect/). 3379 LSTM-LM M&L18 Ours Additional margin loss (δ = 10) Additional loss (α = 1000, β = 1) Sentence-level Binary-pred. Token-level Distilled Unlike. K19 AGREEMENT: Simple In a sent. complement Short VP coordination Long VP coordination Across a PP Across a SRC Across an ORC Across an ORC (no that) In an ORC In an ORC (no that) 94.0 99.0 90.0 61.0 57.0 56.0 50.0 52.0 84.0 71.0 98.1 (±1.3) 96.1 (±2.0) 93.6 (±3.0) 82.2 (±3.4) 92.6 (±1.4) 91.5 (±3.4) 84.5 (±3.1) 75.7"
2020.acl-main.309,N03-1000,0,0.156075,"Missing"
2020.acl-main.309,N19-1334,0,0.157351,"ge Models Hiroshi Noji Artificial Intelligence Research Center AIST, Tokyo, Japan hiroshi.noji@aist.go.jp Abstract robust enough to deal with syntactically challenging constructions such as long-distance subjectverb agreement. So far, the results for RNN language models (RNN-LMs) trained only with raw text are overall negative; prior work has reported low performance on the challenging test cases (Marvin and Linzen, 2018) even with the massive size of the data and model (van Schijndel et al., 2019), or argue the necessity of an architectural change to track the syntactic structure explicitly (Wilcox et al., 2019b; Kuncoro et al., 2018). Here the task is to evaluate whether a model assigns a higher likelihood on a grammatically correct sentence (1a) over an incorrect sentence (1b) that is minimally different from the original one (Linzen et al., 2016). We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this w"
2020.coling-main.192,W15-4319,0,0.0322076,"Missing"
2020.coling-main.192,N19-1423,0,0.0145435,"Missing"
2020.coling-main.192,W19-4427,0,0.0559945,"Missing"
2020.coling-main.192,D12-1039,0,0.0298724,"important for a human to understand non-standard words, current neural network-based text normalization methods do not consider this information. We assume that text normalization more intuitive to humans is possible by explicitly considering such features in a neural network-based method. Based on this assumption, in this work, we propose neural text normalization models that leverage both string and sound similarities. Experimental results show that our proposed models outperformed a baseline and achieved state-of-the-art results in the text normalization track on WNUT-2015. 2 Related Work Han et al. (2012) proposed a ranking-based text normalization method that incorporates the matching degree of surrounding word n-grams to the target word and the edit distance from existing words. Li and Liu (2012) proposed a text normalization method leveraging phonetic information to translate nonstandard words into standard ones. Ansari et al. (2017) proposed an automatic optimization-based nearest neighbor matching approach leveraging string and phonetic similarity. Jin (2015) achieved the best This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/license"
2020.coling-main.192,W15-4313,0,0.0257767,"s outperformed a baseline and achieved state-of-the-art results in the text normalization track on WNUT-2015. 2 Related Work Han et al. (2012) proposed a ranking-based text normalization method that incorporates the matching degree of surrounding word n-grams to the target word and the edit distance from existing words. Li and Liu (2012) proposed a text normalization method leveraging phonetic information to translate nonstandard words into standard ones. Ansari et al. (2017) proposed an automatic optimization-based nearest neighbor matching approach leveraging string and phonetic similarity. Jin (2015) achieved the best This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2126 Proceedings of the 28th International Conference on Computational Linguistics, pages 2126–2131 Barcelona, Spain (Online), December 8-13, 2020 【 Architecture of Deep Levenshtein 】 【 Architecture of Seq2Seq 】 Normalized edit distance Attention Encoder Cosine similarity Decoder you LSTM LSTM LSTM LSTM Embedding layer Embedding layer … + Word: x coming Token + + … you Character string Word: y Figure 1: How to introduce the char"
2020.coling-main.192,N19-2024,0,0.0189753,"ore Af ter yes → AS Bef ore Af ter By using Double Metaphone, similar to Deep Levenshtein, Deep Metaphone can predict the sound similarity between two words. Therefore, we can use the vector that captures the feature of the sound. 3.3 Incorporating new features to Seq2Seq In this study, we use a bi-directional LSTM for the encoder and decoder. Lourentzou et al. (2019) incoras an input to the encoder. We further incorporate the character porated only token embeddings etoken xt feature cleven , obtained from Deep Levenshtein, and the sound feature cphone , obtained from Deep Metaxt xt 2 phone. Mansfield et al. (2019) empirically tested addition, concatenation, and multi-layer perceptron to combine new features with token embeddings. They reported that concatenation outperforms the are and cphone other two methods. Therefore, we choose concatenation. Our new feature vectors cleven xt xt incorporated into the Seq2Seq encoder as follows: henc = Encoder([etoken ; cleven ; cphone ; henc t xt xt xt t−1 ]). 4 (2) Experiments 4.1 Experimental settings We used WNUT-2015 Shared Task2 (Baldwin et al., 2015),3 which is a task of normalizing social media texts, for our evaluation. The official dataset consists of 4,91"
2020.coling-main.192,P18-1186,0,0.0145464,"Mani et al. (2020) used Seq2Seq in automatic speech recognition error correction, a task similar to text normalization. Taking these trends into account, in this work, we propose a neural text normalization model that leverages both string and sound similarity. 3 Methodology Figure 1 shows an overview of our proposed method. In this study, we perform text normalization based on the method of Lourentzou et al. (2019), which incorporates token embeddings as an input to the encoder. Our method expands their work by utilizing features related to character strings and sounds. 3.1 Deep Levenshtein Moon et al. (2018) proposed Deep Levenshtein, a network that captures the feature of character strings based on the Levenshtein edit distance (Levenshtein, 1966) in order to correct any character fluctuations of named entities in a text. In this study, we incorporate this mechanism into the Seq2Seq model to make it more robust to a broken text. Deep Levenshtein is a neural network that takes two words x and y as an input and then outputs hidden representations for the character strings of the words. Two words x and y are fed into the word embedding layer, and then we obtain word embeddings ex and ey , respectiv"
2020.coling-main.192,N18-1202,0,0.0554979,"Missing"
2020.coling-main.192,D19-3011,0,0.0241981,"characters delete characters randomly python → pyhon 3. Replace characters replace characters randomly python → pyhtno 4. Extend characters extend words ending with {u, y, s, r} beer → beerrrr 5. Extend short vowels stretch short vowels {a, i, u, e, o} cat → caaat 6. Delete symbol delete apostrophe I’m → Im 7. Misplaced sign insert apostrophe in different position don’t → do’nt 8. Typo replace with another character that is at a near position on a keyboard hello → jello 9. Convert to another token convert to completely different token python → ruby Table 1: Noise generator. 3.2 Deep Metaphone Raghuvanshi et al. (2019) revealed that relying only on surface text similarities cannot capture phonetic differences between words. Furthermore, Han et al. (2013) showed that sound-related features are effective in text normalization. In this work, we propose Deep Metaphone to capture sound features for text normalization by learning phonetic edit distance. Deep Metaphone has the same network structure as Deep Levenshtein. The difference between them is the training data they use. Deep Metaphone learns the phonetic edit distance, which is the edit distance of the strings obtained from Double Metaphone (Philips, 2000)"
2020.coling-main.192,W16-2209,0,0.0272688,"model were set to the same as , of LSTM and cphone in (Lourentzou et al., 2019), where it was 100. The sizes of hidden layers, cleven xt xt for Deep Levenshtein and Deep Metaphone were tuned from {10, 20, 30, 40, 50} on the validation data, randomly extracted 100 sentences from the training data. When only Deep Levenshtein was used, 20 was selected. When only Deep Metaphone was used, 50 was selected. When both were used, 10 was selected for each of them. The reason why we set the range of smaller values from 10 to 50 compared with the size of token embeddings, 100, is based on the finding in (Sennrich and Haddow, 2016). They in our case, should be smaller reported that the size of the secondary embeddings, i.e., cleven and cphone xt xt token than that of the primary embeddings, i.e., ext in our case. 4.2 Compared models In the experiments, we compared a baseline model and our models, which are listed below. Two-stage Seq2Seq (baseline): A model proposed by Lourentzou et al. (2019). We reimplemented this model and will report its performance in addition to the scores reported in their paper. Two-stage Seq2Seq + LS (Levenshtein): A model where we add character string features to the input of the encoder in th"
2020.coling-main.213,D16-1162,0,0.255693,"2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules"
2020.coling-main.213,D19-1310,0,0.0828,"loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the u"
2020.coling-main.213,D18-1150,0,0.124047,"t. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent studies show the effectiveness of learning methods that exploit explicit negative examples in language modeling. Huang et al. (2018) introduced a margin loss to penalize sentences in a beam, assuming that the generated sentences are imperfect. Noji and Takamura (2020) used synthesized ungrammatical sentences in addition to the originals to improve the syntactic ability of language models. ∗ The ﬁrst and second authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2352 Proceedings of the 28th International Conference on Computational Linguistics, pages 2352–2362 Barcelona, Spain (Online),"
2020.coling-main.213,D16-1128,0,0.151849,"grading the ﬂuency, 2) the choice of the loss function is an important factor because the performances of different metrics depend on the types of loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. S"
2020.coling-main.213,2020.acl-main.309,1,0.923847,"es except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent studies show the effectiveness of learning methods that exploit explicit negative examples in language modeling. Huang et al. (2018) introduced a margin loss to penalize sentences in a beam, assuming that the generated sentences are imperfect. Noji and Takamura (2020) used synthesized ungrammatical sentences in addition to the originals to improve the syntactic ability of language models. ∗ The ﬁrst and second authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2352 Proceedings of the 28th International Conference on Computational Linguistics, pages 2352–2362 Barcelona, Spain (Online), December 8-13, 2020 ・ ・ ・ ・ ・・・ ・・ ・ Stock Prices[yen] 19,600 19,500 19,400 19,300 Previous day Today Time Gold: Nikkei suddenly drops…"
2020.coling-main.213,P02-1040,0,0.107919,"583 2,592 12.77 Test 1,951 1,615 2,634 12.69 Table 2: The statistics of the dataset. every epoch, then selected the best model on validation dataset. We used Adam (Kingma and Ba, 2015) optimizer with the initial learning rate 0.001. Each index was converted to a 32-dimensional vector. The dimensions for the hidden layer in the encoder and the decoder were set to 256. We used 128 for the dimensions of word embeddigns. We report the averaged values of three trials with different random seeds for automatic evaluation. 3.3 Automatic Evaluation Since we aim to improve correctness, only using BLEU (Papineni et al., 2002) is not sufﬁcient. It is ideal to evaluate the effect of the use of contrastive pairs from various perspectives. We propose four metrics to capture how well models exploit contrastive examples and generate crucial terms. 3.3.1 Accuracy We expect the trained model should correctly distinguish the difference between reference sentences and their contrastive sentences i.e., assign a higher probability to the reference sentences than its contrastive sentences as a direct effect of the learning with the losses that take into account contrastive examples. Therefore, following the work by Sennrich et"
2020.coling-main.213,P19-1195,0,0.0898084,"r because the performances of different metrics depend on the types of loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translat"
2020.coling-main.213,P17-1099,0,0.332476,"nerate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent"
2020.coling-main.213,E17-2060,0,0.111184,"7; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of los"
2020.coling-main.28,W05-0909,0,0.13943,"an input snippet of source code and with a hierarchical copy mechanism. Our models outperformed the existing methods in terms of our modified F1 and accuracy. Our proposed copy mechanism is applicable to tree-structured inputs such as discourse structures, cooking recipes, and social network services. Moreover, replacing the most frequent subword seems to be useful in tasks where the vocabulary is relatively small. There remain two major issues to address. The first is the need for better evaluation metrics. We believe that this task requires a metric that can accept synonyms such as METEOR (Banerjee and Lavie, 2005). However, some words that are considered synonymous in WordNet8 are used differently in the context of source code. For example, increment is an operation that increases the value of a variable by 1 in source code. It cannot be replaced with a word such as increase, even if they are synonymous with each other. Therefore, we need an evaluation metric that takes into account the subtle difference between synonyms. The second is to consider context in source code. Our approach generates function names only from the information inside the function. However, the behavior of other functions and the"
2020.coling-main.28,N18-2097,0,0.0501046,"Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in this paper. Code2seq first converts an input snippet of s"
2020.coling-main.28,P16-1154,0,0.160865,"Expr IfStmt Object Variable Declaration Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in"
2020.coling-main.28,P16-1014,0,0.16206,"pr Variable Declaration Expr IfStmt Object Variable Declaration Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model"
2020.coling-main.28,P17-1019,0,0.0171408,"(2019) used a hierarchical attention network for function name generation. In this model, the important information of the lower layer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot"
2020.coling-main.28,P18-1013,0,0.0454148,"tion Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in this paper. Code2seq first converts"
2020.coling-main.28,P16-1195,0,0.133134,"whose elem.key is the same as target key. However, the function name would be inappropriate as it implies that the function returns the value of the object. Such a function name adversely affects readability and sometimes causes bugs, especially in collaborative environments. A proper function name such as indexOfTarget in this case, instead of getTargetValue, can help programmers understand the code efficiently and avoid possible bugs (Takang et al., 1996; Binkley et al., 2013). Automatically generating such function names has been studied as a generation task in natural language processing (Iyer et al., 2016). Figure 1: Example of a function and its inappropriate name. (Java) Recently, various neural network-based approaches have been proposed to solve this problem by generating a function name from given source code (Allamanis et al., 2016; Alon et al., 2018; Fernandes et al., 2018). In these approaches, a function name is treated as a sequence of subwords (get, Target and Value in Figure 1). Since these approaches heavily rely on a subword-based predefined dictionary to generate a function name, it is difficult to generate a function name containing low-frequency or unknown subwords. To solve th"
2020.coling-main.28,W04-3250,0,0.0647436,"Missing"
2020.coling-main.28,W04-1013,0,0.0164829,"positons just before an uppercase character follows lowercase characters because programmers generally use camel case when writing code with Java. Long variable names were truncated to have at most 6 subwords. We used only the paths that had less than 9 subwords. We used TensorFlow to implement our models. We used F1 as an evaluation metric, following Alon et al. (2019a), and added accuracy as another. Furthermore, to correctly evaluate outputs with repeating tokens, we also used modified-F1 (F1**), calculated with the modified unigram precision of Papineni et al. (2002) and unigram recall of Lin (2004). F1** can prevent the models that repeatedly output subwords in the Gold function name from unreasonably obtaining high scores. We calculated the above metrics on the basis of the number of subwords. The accuracy measure was defined to be the number of correctly generated function names divided by the total number of test instances. Here, we supposed an output is correct only if it is completely the same as the gold function name, while we calculated the other metrics by counting the overlap of subwords between generated function names and gold function names. We trained and evaluated each mo"
2020.coling-main.28,P17-2045,0,0.0133674,"ble Our model Table 3: Outputs for the top box of Figure 5 5 Output contains morpheme has data is morpheme single analysis morpheme morpheme morpheme name morpheme has morpheme Table 4: Outputs for the bottom box of Figure 5 Related Work There have been a lot of research efforts on tasks where source code is the input. Hindle et al. (2012) and Babii et al. (2019) constructed language models for the source code. Raychev et al. (2015) proposed a method for outputting variable names in the source code. Iyer et al. (2016) proposed a model to summarize the behavior of functions in the source code. Loyola et al. (2017) proposed a method for generating descriptions of source code changes. 323 While these studies focus on source code as an input, their outputs are not function names. As a method for representing source code, Allamanis et al. (2015b) converted a snippet of the source code into AST and proposed a method for generating a short description of the behavior of the snippet. Allamanis et al. (2018) later proposed a method for detecting inappropriate variable names using AST. We also used AST to represent the input snippet of source code while many other researches treat the source code as a sequence"
2020.coling-main.28,D15-1166,0,0.386541,"nning and end of the sequence, respectively. 2.3 Decoder with Attention The decoder inherits the averaged vector of all possible paths between the leaf nodes in the AST as an initial state s0 . To prevent the computational space from becoming too large, the maximum number of paths is set to 200; if there are more than 200 paths, 200 paths are randomly selected. At each time step t, the decoder calculates the current hidden state st = LST M (st−1 , yt−1 ), where yt−1 is the embedding of the predicted subword in the previous time step. By using st , the decoder calculates the attention weights (Luong et al., 2015) on the paths, each of which connects two leaf nodes. The weight on the r-th path is defined as follows: exp(dTa tanh(Wa [st ; qr ])) , Σr′ exp(dTa tanh(Wa [st ; qr′ ])) X δiT sof tmax(Wl [Σr atr qr ; st ]), pvoc (w) = atr = (1) (2) i:w=wi where qr is the vector representation of the r-th path in the encoder, Wa is a weight matrix for the linear transformation, and da is a parameter vector. Finally, the output layer calculates the label distribution at time step t as pvoc (w), where Wl is a weight matrix. δi is a one-hot vector, where only the i-th element is 1, and the others are 0. wi is the"
2020.coling-main.28,K16-1028,0,0.0172367,"model, the important information of the lower layer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot be directly applied to our setting because each input is assumed to be a set of paths in"
2020.coling-main.28,P02-1040,0,0.106701,"urce code into a sequence of subwords at the positons just before an uppercase character follows lowercase characters because programmers generally use camel case when writing code with Java. Long variable names were truncated to have at most 6 subwords. We used only the paths that had less than 9 subwords. We used TensorFlow to implement our models. We used F1 as an evaluation metric, following Alon et al. (2019a), and added accuracy as another. Furthermore, to correctly evaluate outputs with repeating tokens, we also used modified-F1 (F1**), calculated with the modified unigram precision of Papineni et al. (2002) and unigram recall of Lin (2004). F1** can prevent the models that repeatedly output subwords in the Gold function name from unreasonably obtaining high scores. We calculated the above metrics on the basis of the number of subwords. The accuracy measure was defined to be the number of correctly generated function names divided by the total number of test instances. Here, we supposed an output is correct only if it is completely the same as the gold function name, while we calculated the other metrics by counting the overlap of subwords between generated function names and gold function names."
2020.coling-main.28,P17-1099,0,0.15993,"+ wc′ T gt . (9) Experiments 4.1 Experimental Settings We evaluated our approaches on the following two datasets: Java-small and Java-large.4 Java-small consists of 691,974 functions for training, 23,844 for development, and 57,088 for testing. Java-large 4 https://github.com/tech-Srl/code2seq#datasets 319 consists of 15,344,512 functions for training, 320,866 for development, and 417,003 for testing. The models for comparison are as follows: • Code2seq We described the model in Section 2. We reran the code5 of Alon et al. (2019a). • Copy This is a 2-layer LSTM-based pointer-generator model (See et al., 2017). We experimented with OpenNMT-py6 with the copy attn option. • Pointer This is a variant of our hierarchical copy mechanism. Following the decoder of Fernandes et al. (2018),7 this model only points to tokens (Vinyals et al., 2015) and does not generate any tokens. This model was prepared to verify the report of Fernandes et al. (2018) that a pointer-network works effectively and yields higher F1 scores than code2seq on the function naming task. For training all the models, we used momentum-SGD (Qian, 1999) as an optimizer. The batch size was set to 256, and the dimension of subword embedding"
2020.coling-main.28,D18-1424,0,0.0140142,"yer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot be directly applied to our setting because each input is assumed to be a set of paths in AST. Fernandes et al. (2018) proposed a meth"
2020.coling-main.465,C16-1236,0,0.0187955,"suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al."
2020.coling-main.465,D13-1160,0,0.284366,"nion of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestion"
2020.coling-main.465,P13-1042,0,0.0875409,"st and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be rob"
2020.coling-main.465,P16-1076,0,0.0200008,"ess of existing systems using different datasets. Our analysis, including shifting of training and test datasets and training on a union of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates"
2020.coling-main.465,N19-1423,0,0.0276074,"ble. FreebaseQA can be seen as an attempt toward this goal, but we found that this dataset has several issues. Another direction is to invent a model or a learning mechanism that can generalize robustly from biased datasets. Our data union can be seen as a simple approach toward this end, but we found that current models do not exploit useful information beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical importance toward robust question answering not specific to a single dataset. Acknowledgements This paper is based on"
2020.coling-main.465,D17-1215,0,0.0334288,"itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 po"
2020.coling-main.465,N19-1028,0,0.126841,"gued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the train"
2020.coling-main.465,P17-1147,0,0.0188656,"edi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from TriviaQA (Joshi et al., 2017) and then filtered by heuristics to collect factoid questions answerable on Freebase. Although the authors argue that their procedure reliably eliminates non-factoid questions, we find several problems in this dataset, which we describe in Section 4.2. 2.1 Preprocessing Apart from the difference in construction methods, the four datasets additionally differ based on (1) whether they contain non-factoid questions and (2) the assumed subset of Freebase. Because we aim 1 https://github.com/ad-freiburg/aqqu Cai and Yates (2013) only mention that questions are written by two native English speakers"
2020.coling-main.465,P19-1334,0,0.0224764,"uate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than that on WebQuestions."
2020.coling-main.465,N18-2047,0,0.0758464,". Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that ar"
2020.coling-main.465,C18-1198,0,0.0236031,"o this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than th"
2020.coling-main.465,D19-1432,0,0.0167117,"is to improve the dataset quality. We need to create a dataset, that is real and challenging, while still being scalable. FreebaseQA can be seen as an attempt toward this goal, but we found that this dataset has several issues. Another direction is to invent a model or a learning mechanism that can generalize robustly from biased datasets. Our data union can be seen as a simple approach toward this end, but we found that current models do not exploit useful information beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical"
2020.coling-main.465,D19-1005,0,0.0237469,"nformation beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical importance toward robust question answering not specific to a single dataset. Acknowledgements This paper is based on results obtained from projects JPNP20006 and JPNP15009, commissioned by the New Energy and Industrial Technology Development Organization (NEDO), and also with the support of RIKEN–AIST Joint Research Fund (Feasibility study). For experiments, computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of A"
2020.coling-main.465,D18-1051,0,0.423246,"in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset a"
2020.coling-main.465,Q16-1010,0,0.0522411,"Missing"
2020.coling-main.465,2020.acl-main.442,0,0.0151219,"f four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than that on WebQuestions. Although the simplicity"
2020.coling-main.465,N18-1059,0,0.0156238,"ents, containing over 100,000 questions answerable by a single fact. Contrary to WebQuestions, each question in this dataset is created from a sampled fact in Freebase, which is then verbalized and paraphrased by a crowd worker. Possibly due to this procedure starting from a KB fact, we find that, as in Free917, this dataset also tends to verbalize a predicate with directly related terms, such as “What type of music . . .?” for music.artist.genre.2 This approach eases the collection of a lot of data and is popular in data creation for semantic parsing (Wang et al., 2015; Trivedi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from TriviaQA (Joshi et al., 2017) and then filtered by h"
2020.coling-main.465,P19-1485,0,0.0793845,"a careful comparison and manual analysis using the standardized datasets. Given our analysis, we suggest two possible future directions. One is to invent a clever novel data creation method that would be scalable while avoiding bias as much as possible. In this respect, we point out that a recent attempt by FreebaseQA (Jiang et al., 2019) is not successful, and that significant bias still exists. Another is to exploit useful information from the large dataset of SimpleQuestions in a better way. In the last analysis, we demonstrate that a simple approach of training on a union of the datasets (Talmor and Berant, 2019) is not satisfactory toward this end, calling for a more sophisticated method of exploiting useful features across datasets effectively. 2 Datasets We use four QA datasets over a knowledge base (KB) as our target datasets. These datasets were selected because they share a common KB (Freebase), and a large portion of each dataset comprises of factoid questions, which are the main focus of this paper. A factoid question asks a single fact, or a triple (subject, predicate, object) on a KB, where the object corresponds to the answer. For example, “Which country is Albert Bolender from?” correspond"
2020.coling-main.465,D17-1307,0,0.0191326,"itself is not available. 5 https://github.com/castorini/BuboQA 5323 architectures (Bordes et al., 2015; Yin et al., 2016). Specifically, for entity linking, a trained LSTM first detects the entity spans, which are then heuristically mapped to the candidate KB entities and scored with the Levenshtein distance to the canonical entity label. Relation prediction is performed independently by another classifier on top of a different LSTM. Finally, the best combination of (ˆ e, rˆ) is found according to a weighted sum of these two module scores.6 This is an extension of an even simpler baseline of Ture and Jojic (2017), and a similar approach is employed in Petrochuk and Zettlemoyer (2018). Note that this system treats relation prediction as classification among the predicates appearing in the training data. This means that it cannot solve zero-shot relation prediction, which occurs to some extent especially in the dataset transfer experiment (Section 4.3). On the other hand, the other three systems theoretically can handle them, as described in the following. Hierarchical Residual BiLSTM (HR-BiLSTM) (Yu et al., 2017) On this system (and the next, KBQA-Adapter), relation prediction is performed differently,"
2020.coling-main.465,P15-1129,0,0.013098,"his is the largest dataset in our experiments, containing over 100,000 questions answerable by a single fact. Contrary to WebQuestions, each question in this dataset is created from a sampled fact in Freebase, which is then verbalized and paraphrased by a crowd worker. Possibly due to this procedure starting from a KB fact, we find that, as in Free917, this dataset also tends to verbalize a predicate with directly related terms, such as “What type of music . . .?” for music.artist.genre.2 This approach eases the collection of a lot of data and is popular in data creation for semantic parsing (Wang et al., 2015; Trivedi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from Triv"
2020.coling-main.465,P19-1616,0,0.420991,"vedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though,"
2020.coling-main.465,P15-1128,0,0.029467,"iction are modeled with simple classifiers. Despite its simplicity, this approach outperforms several more complex 3 The reason for the decrease in the first step for FreebaseQA is that it contains two-hop questions involving a mediator node in Freebase, which we exclude from the target. 4 When searching for open software, we often found that many systems along with a paper are not self-contained; in particular, they often are missing an entity linking module. This is especially the case for systems targeting WebQustions, for which many systems rely on the outputs of the entity linker used in Yih et al. (2015) and found in https: //github.com/scottyih/STAGG, while the entity linker itself is not available. 5 https://github.com/castorini/BuboQA 5323 architectures (Bordes et al., 2015; Yin et al., 2016). Specifically, for entity linking, a trained LSTM first detects the entity spans, which are then heuristically mapped to the candidate KB entities and scored with the Levenshtein distance to the canonical entity label. Relation prediction is performed independently by another classifier on top of a different LSTM. Finally, the best combination of (ˆ e, rˆ) is found according to a weighted sum of these"
2020.coling-main.465,P16-2033,0,0.261633,"ataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user q"
2020.coling-main.465,C16-1164,0,0.0491652,"Missing"
2020.coling-main.465,P17-1053,0,0.0602595,"et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowled"
2020.coling-main.507,W99-0201,0,0.34152,"iability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene,"
2020.coling-main.507,2020.lrec-1.6,0,0.0167226,"label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et a"
2020.coling-main.507,chaimongkol-etal-2014-corpus,0,0.0133183,"hat incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; R"
2020.coling-main.507,L16-1323,0,0.371655,"ference on Computational Linguistics, pages 5760–5773 Barcelona, Spain (Online), December 8-13, 2020 General Label Type Discourse New (DN) Discourse Old (DO) Non-Referring (NR) Property (PR) Description The mention is a new entity in the text The mention refers to an entity which has already been introduced The mention refers to no actual entity (e.g., it in expletive constructions) The mention refers to a property of an entity (e.g., the most durable light is a property of the bulb) Figure 1: An example (adapted from the Phrase Table 1: Four general label types in a coreferDetectives Corpus (Chamberlain et al., 2016)) of ence resolution labelling task (Chamberlain et al., crowd-sourced coreference annotation. 2016). mention, by taking into account the annotation complexity of the mention and annotators’ reliability. The first challenge in proposing the encoder is how to incorporate mention context information. We explore the use of contextual embeddings for this purpose. The second challenge is how to effectively model annotators’ behaviour in terms of their quality of annotation. Modelling annotator reliability is helpful in detecting unreliable annotators and in facilitating appropriate task allocation"
2020.coling-main.507,N19-1423,0,0.024144,"erent mentions that appear in the set of mentions classified as DO(·) or PR(·)3 labels. Each annotator’s label is weighted by the product of the annotator’s category and the overall/per-instance reliability. 4 Experiments Dataset: We evaluate our method on the real-world dataset from (Chamberlain et al., 2016), which includes both crowd labels produced by 280 crowd workers and expert labels for 5,654 mentions (3,277 DNs, 2,192 DOs, 136 PRs and 49 NRs). Mention Contextual Embedding: We compare the use of two pre-trained embeddings from ELMo4 (Peters et al., 2018) and BERT (bert-base-uncased)5 (Devlin et al., 2019). When using BERT, we represent each token by using the BERT model outputs from the last four hidden layers, which is the same setting as used in (Peters et al., 2018). Learning: We use the Adam (Kingma and Ba, 2015) optimiser (α = 0.001, β1 = 0.9, β2 = 0.999). λ1 , λ2 and λ3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs. 2 The details are discussed in Section 6.3. For example, DO(mention1) or PR(mention1) indicates that this annotator labels the current mention as referring to another mention mention1. 4 Original(5.5B): https://allennlp.org/elmo 5 We"
2020.coling-main.507,doddington-etal-2004-automatic,0,0.0186799,"plexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008;"
2020.coling-main.507,K15-1020,0,0.0135229,", 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model"
2020.coling-main.507,garcia-gamallo-2014-multilingual,0,0.023439,"ntal results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al.,"
2020.coling-main.507,L16-1021,0,0.0166079,"context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018;"
2020.coling-main.507,N15-1117,0,0.0207658,"d complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al"
2020.coling-main.507,guillou-etal-2014-parcor,0,0.0243891,"ct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt e"
2020.coling-main.507,M98-1029,0,0.0589758,"n about context, annotator reliability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification"
2020.coling-main.507,N13-1132,0,0.0385466,"; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Late"
2020.coling-main.507,P14-2062,0,0.13373,"al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, th"
2020.coling-main.507,D15-1261,0,0.414246,"rs were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the corefer"
2020.coling-main.507,R11-1036,0,0.0602846,"Missing"
2020.coling-main.507,W17-2314,1,0.821671,"crowd-sourced coreference annotation. 2016). mention, by taking into account the annotation complexity of the mention and annotators’ reliability. The first challenge in proposing the encoder is how to incorporate mention context information. We explore the use of contextual embeddings for this purpose. The second challenge is how to effectively model annotators’ behaviour in terms of their quality of annotation. Modelling annotator reliability is helpful in detecting unreliable annotators and in facilitating appropriate task allocation (Donmez and Carbonell, 2008; Donmez and Carbonell, 2010; Li et al., 2017). Modelling only per-category reliability may not be sufficient to characterise annotators’ behaviour patterns for a given annotation task. The original encoder from (Yin et al., 2017) already estimates the per-category reliability. We additionally model overall and per-instance reliability. In addition, we also model the instance complexity. In the second subtask, i.e. coreference chain inference, based on the predicted general classes in the first subtask, we predict each mention’s target (i.e., its referent entity which is usually another mention in the text). If a mention is classified as"
2020.coling-main.507,N19-1295,1,0.703608,"7; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015)"
2020.coling-main.507,P19-3010,0,0.0652096,"6) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a straightforward manner, because coreference labels are m"
2020.coling-main.507,H05-1004,0,0.075939,"and made her pick them up again, and clean and brighten them. Table 4: Instances with lowest and highest complexities as estimated by our model. Mentions are highlighted in bold. We then run the entire autoencoder training by optimising the objective function in Equation (11) until either 300 iterations are reached, or the objective function stops improving. Evaluation: The baselines are: majority voting and the state-of-the-art method, Mention-Pair Annotation model (Paun et al., 2018). Four metrics are used for evaluation, MUC (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL Score (Pradhan et al., 2011). 5 Results From Table 3, we observe that our method achieved better performance than baselines.6 We also report the performance using different settings. Without Context and ELMo/BERT denote the models that do not use context, or which use context, respectively. In terms of reliability, each annotator’s PerCategory Reliability is modelled in our model by default. Per-Category + Overall / Per-Instance Reliability indicates that the model supplements per-category reliability with modelling of annotator overall or per-instance reliability. As shown in Tabl"
2020.coling-main.507,P17-1028,0,0.0605923,"e reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a"
2020.coling-main.507,P18-1019,0,0.0667627,"l. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a straightforward ma"
2020.coling-main.507,D18-1218,0,0.400688,"notators have to determine an appropriate referent mention for some mentions. Because the performance of supervised learning models is highly dependent on the quality of training data, the aggregation of these noisy labels, (i.e., the process of determining the label that is most likely to be correct) is important to obtain a high-quality training corpus. Although label aggregation is a well-studied topic, most existing studies of natural language labelling tasks have only focused on aggregating classification or sequence labels. To the best of our knowledge, there is only one previous study (Paun et al., 2018) that has investigated how to aggregate crowd-sourced coreference labels. In this paper, we propose a 2-step framework in which the aggregation task is broken down into two subtasks, i.e., mention classification and coreference chain inference. In the mention classification subtask, our model predicts the general category of a mention as shown in Table 1. Our model is based on the autoencoder proposed in (Yin et al., 2017), but with significant extensions. Our encoder is a classifier which takes as its input the crowd labels for each mention, together with the mention’s context information. Th"
2020.coling-main.507,N18-1202,0,0.0129429,"DO or PR, we only aggregate those crowd-sourced referent mentions that appear in the set of mentions classified as DO(·) or PR(·)3 labels. Each annotator’s label is weighted by the product of the annotator’s category and the overall/per-instance reliability. 4 Experiments Dataset: We evaluate our method on the real-world dataset from (Chamberlain et al., 2016), which includes both crowd labels produced by 280 crowd workers and expert labels for 5,654 mentions (3,277 DNs, 2,192 DOs, 136 PRs and 49 NRs). Mention Contextual Embedding: We compare the use of two pre-trained embeddings from ELMo4 (Peters et al., 2018) and BERT (bert-base-uncased)5 (Devlin et al., 2019). When using BERT, we represent each token by using the BERT model outputs from the last four hidden layers, which is the same setting as used in (Peters et al., 2018). Learning: We use the Adam (Kingma and Ba, 2015) optimiser (α = 0.001, β1 = 0.9, β2 = 0.999). λ1 , λ2 and λ3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs. 2 The details are discussed in Section 6.3. For example, DO(mention1) or PR(mention1) indicates that this annotator labels the current mention as referring to another mention mention"
2020.coling-main.507,W11-1901,0,0.0983738,"Missing"
2020.coling-main.507,W12-4501,0,0.0391908,"ed into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010;"
2020.coling-main.507,D08-1027,0,0.561537,"Missing"
2020.coling-main.507,M95-1002,0,0.378523,"e how information about context, annotator reliability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused"
2020.coling-main.507,M95-1005,0,0.601386,"eadows. ’Pull off my boots,’ and then he threw them in her face, and made her pick them up again, and clean and brighten them. Table 4: Instances with lowest and highest complexities as estimated by our model. Mentions are highlighted in bold. We then run the entire autoencoder training by optimising the objective function in Equation (11) until either 300 iterations are reached, or the objective function stops improving. Evaluation: The baselines are: majority voting and the state-of-the-art method, Mention-Pair Annotation model (Paun et al., 2018). Four metrics are used for evaluation, MUC (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL Score (Pradhan et al., 2011). 5 Results From Table 3, we observe that our method achieved better performance than baselines.6 We also report the performance using different settings. Without Context and ELMo/BERT denote the models that do not use context, or which use context, respectively. In terms of reliability, each annotator’s PerCategory Reliability is modelled in our model by default. Per-Category + Overall / Per-Instance Reliability indicates that the model supplements per-category reliability with modelling of annotator"
2020.coling-main.507,Q18-1042,0,0.0205613,"e accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or seque"
2020.inlg-1.21,D10-1049,0,0.05664,"Missing"
2020.inlg-1.21,W13-2127,0,0.0411847,"Missing"
2020.inlg-1.21,E17-1060,0,0.0340918,"Missing"
2020.inlg-1.21,J05-1002,0,0.162505,"Missing"
2020.inlg-1.21,P16-1154,0,0.224027,"del. To switch the copy mode and noncopy mode, we add a special token “<PRICE>”, which is inserted before every numerical value in the training data and indicates that the next token is a price value. Utilizing “<PRICE>”, we define each conditional probability of generating target word ? ? at time ? as: 3.2 where ? gen (? ? |·) and ? copy (? ? |·) are obtained by the generation mode and the copy mode, respectively. This method is inspired by Pointer-generator network introduced by See et al. (2017). These two probabilities are defined as: Decoder with Copy Mechanism We adapt a copy mechanism (Gu et al., 2016) in our decoder to generate numerical values by attending to the input. Recall that in the current task, the values in the output text usually do not appear in the input data (Section 2.2); rather, they can be obtained by applying an arithmetic operation to the certain value in the data. We generate a numerical value by an extension of a copy mechanism, wherein a value is generated by applying one of the operations in Table 1 to a data point ? ?,1 ˜ , which is the first value (latest price) of x ?˜ . Denoting an arithmetic operation as ? ∈ {? ? 1 , · · · , ? ? 12 }, the value is identified by"
2020.inlg-1.21,P82-1020,0,0.677071,"Missing"
2020.inlg-1.21,N12-1093,0,0.0483852,"Missing"
2020.inlg-1.21,P83-1022,0,0.625173,"Missing"
2020.inlg-1.21,D16-1128,0,0.0381251,"Missing"
2020.inlg-1.21,C18-1089,0,0.0730793,"Missing"
2020.inlg-1.21,P09-1011,0,0.108843,"Missing"
2020.inlg-1.21,N16-1086,0,0.0177022,"er the movement expression in the comment matches the movement with the latest stock price (X) or not (×). have been increased providing opportunities to treat various types of large-scale data, so that they are interested in automatically learning a correspondence relationship from data to text and generating a description of this relationship. Therefore, recent works have focused on generating text from data with neural networks, that can solve the above sub-tasks in one through. Especially the models, which utilize an encoder-decoder model (Sutskever et al., 2014) have proven to be useful (Mei et al., 2016; Lebret et al., 2016). While text generation by neural network can describe the text fluently, they do not describe the exact entities or numbers. Therefore, a copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which provides a way to directly copy words from the input, has been utilized. By these neural networks, the works such as conditional language generation based on tables (Yang However, Joulin and Mikolov (2015) and Neelakantan et al. (2016) indicate that current neural models have difficulties in learning arithmetic operations such as addition and comparisons by neural program in"
2020.inlg-1.21,D18-1422,0,0.0151064,"as been utilized. By these neural networks, the works such as conditional language generation based on tables (Yang However, Joulin and Mikolov (2015) and Neelakantan et al. (2016) indicate that current neural models have difficulties in learning arithmetic operations such as addition and comparisons by neural program inductions. Thus, there have been some methods to prepare the numerical values with arithmetic operations in advance. Murakami et al. (2017) post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after generation. Nie et al. (2018) utilizes information from pre-computed operations on raw data to consider incorporating the facts that can be inferred from the input data to guide the generation process. Our model prepares numerical values with defined arithmetic operations as Murakami et al. (2017) for copy and that copy target is guided by encoded input. 155 6 Conclusion In this paper, we have proposed an encoder-decoder model with multi-timestep data and a copy mechanism for generating the market comment from data with the noisy alignments. Both BLEU scores and our proposal evaluation showed the accuracy of sentence gene"
2020.inlg-1.21,P17-1099,0,0.0385523,"values in the text with this copy mechanism. To do this, we exclude numerical values from the vocabulary of the model. To switch the copy mode and noncopy mode, we add a special token “<PRICE>”, which is inserted before every numerical value in the training data and indicates that the next token is a price value. Utilizing “<PRICE>”, we define each conditional probability of generating target word ? ? at time ? as: 3.2 where ? gen (? ? |·) and ? copy (? ? |·) are obtained by the generation mode and the copy mode, respectively. This method is inspired by Pointer-generator network introduced by See et al. (2017). These two probabilities are defined as: Decoder with Copy Mechanism We adapt a copy mechanism (Gu et al., 2016) in our decoder to generate numerical values by attending to the input. Recall that in the current task, the values in the output text usually do not appear in the input data (Section 2.2); rather, they can be obtained by applying an arithmetic operation to the certain value in the data. We generate a numerical value by an extension of a copy mechanism, wherein a value is generated by applying one of the operations in Table 1 to a data point ? ?,1 ˜ , which is the first value (lates"
2020.inlg-1.21,P02-1040,0,0.107462,"m vectors, we set ? = 7 for xlong and ? = 62 for x ?˜ , following Murakami et al. (2017), changing the range of ? by setting ? ∈ [0, 6]. The embedding sizes of a word, five-minute tag f ?˜ , article-tag a, and time tag t are 128, 80, 64, and 64, respectively. We trained the models for 150 epochs with the mini-batch size of 100, using Adam (Kingma and Ba, 2015) optimizer with the initial learning rate 1 × 10−4 , and saved the parameters every epoch, selecting the model with the highest BLEU score on the validation dataset. 4.3 Evaluation Metrics We conduct two types of evaluation: one is BLEU (Papineni et al., 2002) to measure the matching degree between the market comments written by humans as references and the output comments generated by the models, and the other is a new evaluation metric that we created. The new metric uses the matching between the market price movement in the data and the movement expressions in ???? the comments. Using (x? , w? , w??? ?? ), which are the ?-th sample of the input data, the market comment written by humans, and the output comment generated by the models, we define the following 3https://hosted.datascope.reuters.com/ DataScope/ (Latest ? move > 0) (Latest ? move < 0"
2020.inlg-1.21,D17-1197,0,0.0345052,"Missing"
2020.inlg-1.21,P19-1195,0,0.0371637,"Missing"
2020.inlg-1.21,W18-6557,0,0.0142069,"s to treat large-scale data. movement on three points (the closing prices of the Hence, there is an increasing demand to automati- last two days and the latest price). This is valid for cally generate a text from large and complex data. In the prices at (I) and (II), but does not hold at (III) recent studies, neural network-based models have because the latest price (17039.22 yen) is lower than achieved significant progress on the data-to-text the last closing price (17041.45 yen). In addition, which is a text generation task from input data the expression “gains 88 yen” is only valid at the (Puzikov and Gurevych, 2018; Liu et al., 2018; Iso opening time (I) and is not valid at (II). To deal with et al., 2019). these inconsistencies, the models have to be aware One important issue in constructing a dataset of these possible mismatch of data and text due for data-to-text is to obtain the correct alignment to the delay, but a simple encoder-decoder-based 148 Proceedings of The 13th International Conference on Natural Language Generation, pages 148–157, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics Delivery time Price movement Comment (bold text: movement expression) ("
2020.webnlg-1.18,W05-0909,0,0.0141841,"EOR CHRF++ TER BERT P BERT R BERT F1 BLEURT • Data Coverage: this metric assesses how much information from the data has been covered in the text. challenge_2020/ 7 https://www.nltk.org/_modules/nltk/ translate/bleu_score.html 163 Baseline 37.56 0.370 0.357 0.584 0.510 0.944 0.936 0.940 0.44 Table 4: Results on Unseen Entity of Our Approach on RDF-to-Text of WebNLG 2020 (English) Baseline Evaluation Settings: There are two evaluation settings: 1) Automatic Evaluation and 2) Human Evaluation. In Automatic Evaluation, the evaluation metrics are BLEU (Papineni et al., 2002), BLEU NLTK7 , METEOR (Banerjee and Lavie, 2005), CHRF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERT Precision (BERT P) (Zhang et al., 2019) , BERT Recall (BERT R) (Zhang et al., 2019), BERT F1 (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). In Human Evaluation, the evaluation metrics are: Our Approach No Select Select 43.82 43.07 0.436 0.430 0.379 0.376 0.618 0.618 0.472 0.456 0.947 0.949 0.944 0.945 0.945 0.946 0.50 0.54 Our Approach No Select Select 50.74 49.40 0.504 0.490 0.405 0.398 0.672 0.669 0.410 0.410 0.958 0.961 0.956 0.958 0.957 0.959 0.61 0.64 Baseline 40.22 0.393 0.384 0.648 0.476 0.949 0.950 0.949 0.55 • Relevanc"
2020.webnlg-1.18,2020.webnlg-1.7,0,0.0265484,"Missing"
2020.webnlg-1.18,N19-1423,0,0.0229687,"entation of the RDF graph in the low dimensional vector space. Plan Score is to compute the score for the given triple order corre161 Figure 4: The overview design of Plan Selection. sponding to the RDF graph by using Linearization Representation and RDF Graph Representation. Linearization Representation is to project the word sequence that linearizes from one order of triples into a low dimensional vector space. In Linear Representation, we learn the representation by feeding the word sequence into the contextualized language representation. Recently, contextualized language representations (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Lewis et al., 2019) gain wide attention from the NLP community. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is one of popular pre-trained models based on the transformer architecture (Vaswani et al., 2017). Due to its performance, it has been used in many NLP tasks. We, therefore, utilize BERT to learn Linearization representations. Given the RDF graph G and the plan p, we can generate the linearization form as the sequence L = w1 w2 w3 ...wn by mapping the plan to sequence as demonstrated in the example of F"
2020.webnlg-1.18,P18-1151,0,0.0158953,"ighest score in the automatic evaluation (Gardent et al., 2017). In this approach, 1 An example is from https://webnlg-challenge. loria.fr Figure 1: Illustration of the WebNLG 2020 task on RDF-to-text (English). the delexicalization, where entities are replaced by placeholders, with N-gram search and the linearization using DBpedia type enrichment are used together with the standard encoder-decoder with attention model. Due to the simple linearization, the relationship between entities as the graph in triples might lessen. To preserve such information, a graph-based triple encoder (GTR-LSTM) (Distiawan et al., 2018), is introduced. Later, Moryossef et al. (2019) argued that splitting the generation process into plan selection and text realization could help generate the description text that is faithful to the triples. PlanEnc uses a graph convolution network-based model to predict the order of the triples and then an LSTM with attention and the copy mechanism is employed to generate the text corresponding to the order of triples (Zhao et al., 2020). Recently, Kale (2020) investigated Textto-Text Transfer Transformer (T5) model for the data-to-text task. With transfer learning ability, the T5 model is th"
2020.webnlg-1.18,W18-6521,0,0.0198279,"n {n.kertkeidkachorn, takamura.hiroya}@aist.go.jp Abstract We report our system description for the RDFto-Text task in English on the WebNLG 2020 Challenge. Our approach consists of two parts: 1) RDF-to-Text Generation Pipeline and 2) Plan Selection. RDF-to-Text Generation Pipeline is built on the state-of-the-art pretraining model, while Plan Selection helps decide the proper plan into the pipeline. 1 Introduction Natural language generation from data, or data-totext, aims to generate natural language text that describes the input data such as tables and graphs. WebNLG (Gardent et al., 2017; Ferreira et al., 2018) is one of the data-to-text tasks, where the given input data is a set of triples (a tree or a graph). A triple consists of entities and a relationship between them in the form of (subject, predicate, object) describing one fact. For example, (Donald Trump, birthplace, New York) describes the facts “Donald Trump was born in New York.” In the WebNLG task, up to 7 triples are given as the input data. Figure 1 presents an instance of the WebNLG dataset1 , where 3 triples are given as the RDF input graph and the output text is the natural language description describing the fact from triples. Gene"
2020.webnlg-1.18,W17-3518,0,0.047708,"Tokyo, 135-0064, Japan {n.kertkeidkachorn, takamura.hiroya}@aist.go.jp Abstract We report our system description for the RDFto-Text task in English on the WebNLG 2020 Challenge. Our approach consists of two parts: 1) RDF-to-Text Generation Pipeline and 2) Plan Selection. RDF-to-Text Generation Pipeline is built on the state-of-the-art pretraining model, while Plan Selection helps decide the proper plan into the pipeline. 1 Introduction Natural language generation from data, or data-totext, aims to generate natural language text that describes the input data such as tables and graphs. WebNLG (Gardent et al., 2017; Ferreira et al., 2018) is one of the data-to-text tasks, where the given input data is a set of triples (a tree or a graph). A triple consists of entities and a relationship between them in the form of (subject, predicate, object) describing one fact. For example, (Donald Trump, birthplace, New York) describes the facts “Donald Trump was born in New York.” In the WebNLG task, up to 7 triples are given as the input data. Figure 1 presents an instance of the WebNLG dataset1 , where 3 triples are given as the RDF input graph and the output text is the natural language description describing the"
2020.webnlg-1.18,P17-1019,0,0.0175947,"or a graph). A triple consists of entities and a relationship between them in the form of (subject, predicate, object) describing one fact. For example, (Donald Trump, birthplace, New York) describes the facts “Donald Trump was born in New York.” In the WebNLG task, up to 7 triples are given as the input data. Figure 1 presents an instance of the WebNLG dataset1 , where 3 triples are given as the RDF input graph and the output text is the natural language description describing the fact from triples. Generating text from triples is useful for many applications such as question and answering (He et al., 2017), where a set of triples retrieved as an answer can be organized and translated into natural language text. Many approaches have been proposed to deal with the WebNLG task. Neural Machine Translation (NMT) approach is one of the popular methods for this task (Gardent et al., 2017). In WebNLG 2017, a neural machine translation-based approach, achieved the highest score in the automatic evaluation (Gardent et al., 2017). In this approach, 1 An example is from https://webnlg-challenge. loria.fr Figure 1: Illustration of the WebNLG 2020 task on RDF-to-text (English). the delexicalization, where en"
2020.webnlg-1.18,2020.inlg-1.14,0,0.0592771,"ween entities as the graph in triples might lessen. To preserve such information, a graph-based triple encoder (GTR-LSTM) (Distiawan et al., 2018), is introduced. Later, Moryossef et al. (2019) argued that splitting the generation process into plan selection and text realization could help generate the description text that is faithful to the triples. PlanEnc uses a graph convolution network-based model to predict the order of the triples and then an LSTM with attention and the copy mechanism is employed to generate the text corresponding to the order of triples (Zhao et al., 2020). Recently, Kale (2020) investigated Textto-Text Transfer Transformer (T5) model for the data-to-text task. With transfer learning ability, the T5 model is the current state-of-the-art for RDF-totext generation. To further investigate the T5 model from the study (Kale, 2020), we conduct experiments with the T5 model on WebNLG 2020 challenge 2 . Based 2 https://webnlg-challenge.loria.fr/ challenge_2020/ 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 159–166, 2020 Association for Computational Linguistics Attribution 4.0 I"
2020.webnlg-1.18,2020.acl-main.703,0,0.0970956,"Missing"
2020.webnlg-1.18,N19-1236,0,0.0392287,"Missing"
2020.webnlg-1.18,P02-1040,0,0.108129,"563 0.945 0.942 0.943 0.41 BLEU BLEU NLTK METEOR CHRF++ TER BERT P BERT R BERT F1 BLEURT • Data Coverage: this metric assesses how much information from the data has been covered in the text. challenge_2020/ 7 https://www.nltk.org/_modules/nltk/ translate/bleu_score.html 163 Baseline 37.56 0.370 0.357 0.584 0.510 0.944 0.936 0.940 0.44 Table 4: Results on Unseen Entity of Our Approach on RDF-to-Text of WebNLG 2020 (English) Baseline Evaluation Settings: There are two evaluation settings: 1) Automatic Evaluation and 2) Human Evaluation. In Automatic Evaluation, the evaluation metrics are BLEU (Papineni et al., 2002), BLEU NLTK7 , METEOR (Banerjee and Lavie, 2005), CHRF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERT Precision (BERT P) (Zhang et al., 2019) , BERT Recall (BERT R) (Zhang et al., 2019), BERT F1 (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). In Human Evaluation, the evaluation metrics are: Our Approach No Select Select 43.82 43.07 0.436 0.430 0.379 0.376 0.618 0.618 0.472 0.456 0.947 0.949 0.944 0.945 0.945 0.946 0.50 0.54 Our Approach No Select Select 50.74 49.40 0.504 0.490 0.405 0.398 0.672 0.669 0.410 0.410 0.958 0.961 0.956 0.958 0.957 0.959 0.61 0.64 Baseline 40.22 0.393 0.3"
2020.webnlg-1.18,N18-1202,0,0.00805659,"raph in the low dimensional vector space. Plan Score is to compute the score for the given triple order corre161 Figure 4: The overview design of Plan Selection. sponding to the RDF graph by using Linearization Representation and RDF Graph Representation. Linearization Representation is to project the word sequence that linearizes from one order of triples into a low dimensional vector space. In Linear Representation, we learn the representation by feeding the word sequence into the contextualized language representation. Recently, contextualized language representations (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Lewis et al., 2019) gain wide attention from the NLP community. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is one of popular pre-trained models based on the transformer architecture (Vaswani et al., 2017). Due to its performance, it has been used in many NLP tasks. We, therefore, utilize BERT to learn Linearization representations. Given the RDF graph G and the plan p, we can generate the linearization form as the sequence L = w1 w2 w3 ...wn by mapping the plan to sequence as demonstrated in the example of Figure 3. After obtain"
2020.webnlg-1.18,W17-4770,0,0.0660673,"Missing"
2020.webnlg-1.18,2020.acl-main.704,0,0.0554291,"Missing"
2020.webnlg-1.18,2006.amta-papers.25,0,0.0476007,"ge: this metric assesses how much information from the data has been covered in the text. challenge_2020/ 7 https://www.nltk.org/_modules/nltk/ translate/bleu_score.html 163 Baseline 37.56 0.370 0.357 0.584 0.510 0.944 0.936 0.940 0.44 Table 4: Results on Unseen Entity of Our Approach on RDF-to-Text of WebNLG 2020 (English) Baseline Evaluation Settings: There are two evaluation settings: 1) Automatic Evaluation and 2) Human Evaluation. In Automatic Evaluation, the evaluation metrics are BLEU (Papineni et al., 2002), BLEU NLTK7 , METEOR (Banerjee and Lavie, 2005), CHRF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERT Precision (BERT P) (Zhang et al., 2019) , BERT Recall (BERT R) (Zhang et al., 2019), BERT F1 (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). In Human Evaluation, the evaluation metrics are: Our Approach No Select Select 43.82 43.07 0.436 0.430 0.379 0.376 0.618 0.618 0.472 0.456 0.947 0.949 0.944 0.945 0.945 0.946 0.50 0.54 Our Approach No Select Select 50.74 49.40 0.504 0.490 0.405 0.398 0.672 0.669 0.410 0.410 0.958 0.961 0.956 0.958 0.957 0.959 0.61 0.64 Baseline 40.22 0.393 0.384 0.648 0.476 0.949 0.950 0.949 0.55 • Relevance: this metric assesses whether the text contains an"
2020.webnlg-1.18,2020.acl-main.224,0,0.179826,"rization, the relationship between entities as the graph in triples might lessen. To preserve such information, a graph-based triple encoder (GTR-LSTM) (Distiawan et al., 2018), is introduced. Later, Moryossef et al. (2019) argued that splitting the generation process into plan selection and text realization could help generate the description text that is faithful to the triples. PlanEnc uses a graph convolution network-based model to predict the order of the triples and then an LSTM with attention and the copy mechanism is employed to generate the text corresponding to the order of triples (Zhao et al., 2020). Recently, Kale (2020) investigated Textto-Text Transfer Transformer (T5) model for the data-to-text task. With transfer learning ability, the T5 model is the current state-of-the-art for RDF-totext generation. To further investigate the T5 model from the study (Kale, 2020), we conduct experiments with the T5 model on WebNLG 2020 challenge 2 . Based 2 https://webnlg-challenge.loria.fr/ challenge_2020/ 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 159–166, 2020 Association for Computational Lingui"
2020.wnut-1.38,D19-1371,0,0.0512799,"Missing"
2020.wnut-1.38,E99-1043,0,0.157853,"Missing"
2020.wnut-1.38,N19-1423,0,0.0304642,"Missing"
2020.wnut-1.38,P16-2011,0,0.043839,"Missing"
2020.wnut-1.38,D17-1018,0,0.0219261,"sentence S has n words and the i-th word, represented by Si , is split into sub-words. This layer assigns a vector vi,j to the j-th sub-word of the i-th word. It also produces the representation vS as a local context for the sentence S, which corresponds to the embedding of [CLS] token. 3.2 Entity Recognition layer We build mention detection layer, a.k.a named entity recognition (NER) on top of the BERT. This layer assigns entity or trigger types to overlapping text spans, or word sequences, in a sentence. We firstly generate mention candidates based on the same idea as the span-based model (Lee et al., 2017; Sohrab and Miwa, 2018; Sohrab et al., 2019a), in which all continuous word sequences are generated given a maximum span length Lx . Since BERT layer works only on sub-words, we choose the embedding of the first sub-word vi,1 as word embedding vi of i-th word. The representation xb,e ∈ Rdx for the span from the b-th word to the e-th word in a sentence is calculated from the 291 Role Classiﬁcation Layer Acts-0n Site Measure Role Exhaustive Layer for Trigger-Arg. Representation NER Classiﬁcation Layer Action Reagent Amount Location NER Exhaustive Layer for Span Representation Weigthed Avg. BERT"
2020.wnut-1.38,P16-1105,1,0.609901,"r wet-lab protocol (Tabassum et al., 2020) shared task1 is an open challenge that allows participants to use any methodology and knowledge sources for the wet lab protocols that specify the steps in performing a lab procedure. The task aims at two sub-tasks in wet lab protocols domain: named entity recognition (NER), and relation recognition or extraction (RE). In NER, the task is to detect mentions and classify them into entity types or no entity. NER has drawn considerable attentions as the first step towards many natural language processing (NLP) applications including relation extraction (Miwa and Bansal, 2016), event extraction (Feng et al., 1 http://noisy-text.github.io/2020/ wlp-task.html 2 Related Work Most NER work focus on flat entities. Lample et al. (2016) proposed a LSTM-CRF (conditional ran290 Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 290–298 c Online, Nov 19, 2020. 2020 Association for Computational Linguistics dom fields) model and this has been widely used and extended for the flat NER, e.g., Akbik et al. (2018). In recent studies of neural network based flat NER, Gungor et al. (2018, 2019) have shown that morphological analysis"
2020.wnut-1.38,D18-1309,1,0.887327,"Missing"
2020.wnut-1.38,D19-5708,1,0.745813,"tional Institute of Advanced Industrial Science and Technology (AIST), 2-4-7 Aomi, Koto-ku, Tokyo, 135-0064, Japan ‡ Toyota Technological Institute, Japan {sohrab.mohammad, khoa.duong, takamura.hiroya}@aist.go.jp, makoto-miwa@toyota-ti.ac.jp Abstract 2016), and co-reference resolution (Fragkou, 2017). In contrast, relation extraction (RE) is a task to identify relation types between known or predicted entity mentions in a sentence. In this paper, we present a BERT-based neural exhaustive approach that addresses both NER and RE tasks. We employ a neural exhaustive model (Sohrab and Miwa, 2018; Sohrab et al., 2019b) for NER and the extended model that addresses RE task. The model detects flat and nested entities by reasoning over all the spans within a specified maximum span length. Unlike the existing models that rely on token-level labels, our model directly employs an entity type as the label of a span. The spans with the representations are classified into their entity types or non-entity. With the mentions predicted by the NER module, we then feed the detected or known mentions to the RE layer that enumerates all trigger-argument pairs as trigger-trigger or trigger-entity pairs and assigns a role"
2020.wnut-1.38,C18-1177,0,0.0481316,"Missing"
2020.wnut-1.38,N18-1131,1,0.729762,"Missing"
2020.wnut-1.38,N18-2016,0,0.273777,"Missing"
2020.wnut-1.38,N16-1030,0,0.059416,"Missing"
2020.wnut-1.38,N19-1308,0,0.0727792,"Missing"
2021.acl-long.115,W05-0909,0,0.0496049,"the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted ta"
2021.acl-long.115,2020.acl-main.708,0,0.26128,"ata-to-text generation studies have shown significant improvement in generating faithful text aligned with data sources. A copy mechanism has been widely explored to improve faithfulness in various ways. Wiseman et al. (2017) used joint probabilities to let models choose between copying records from data sources or generating from a vocabulary. Puduppully et al. (2019) improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representati"
2021.acl-long.115,2020.acl-main.18,0,0.352787,"ata-to-text generation studies have shown significant improvement in generating faithful text aligned with data sources. A copy mechanism has been widely explored to improve faithfulness in various ways. Wiseman et al. (2017) used joint probabilities to let models choose between copying records from data sources or generating from a vocabulary. Puduppully et al. (2019) improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representati"
2021.acl-long.115,N19-1423,0,0.00607741,"ources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted table contents for table sources. 6.2 Implementation Details We trained a pointer-generator model using the Adagrad optimizer with a batch size of 8 and a learning rate of 0.15. For fine-tuning the GPT2 model, the Adam optimizer set weight decay to 3 × 10−5 . Following Raffel"
2021.acl-long.115,P19-1483,0,0.0174021,"sing different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted table contents for table sources. 6.2 Implementation Details We trained a pointer-generator model using the Adagrad optimizer with a batch size of 8 and a learning rate of 0.15. For fine-tuning the GPT2 model, the Adam optimizer set weight decay to 3 × 10−5 . Following Raffel et al. (2020), the T5 model was fine-tuned with a constant learning rate of 0.001. We trained all models for a maximum of ten epochs with early stopping based on the loss score on"
2021.acl-long.115,2020.acl-main.210,0,0.0224537,"ntain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation using open-domain tables with unknown schemas. Different from our target text for generation, wh"
2021.acl-long.115,2020.emnlp-main.527,0,0.175008,"improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representations in pre-trained models. 1451 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1451–1465 August 1–6, 2021. ©2021 Association for Computational Linguistics • We propose a copy mechanism for pre-trained models, that uses general placeholders covering tabl"
2021.acl-long.115,2020.inlg-1.14,0,0.0483836,"improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representations in pre-trained models. 1451 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1451–1465 August 1–6, 2021. ©2021 Association for Computational Linguistics • We propose a copy mechanism for pre-trained models, that uses general placeholders covering tabl"
2021.acl-long.115,D16-1128,0,0.0607728,"Missing"
2021.acl-long.115,P09-1011,0,0.0194762,"numbers in their captions as keywords for the collection. An example of a table and its description is shown in Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operati"
2021.acl-long.115,W04-1013,0,0.0378679,"lated placeholders from the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table"
2021.acl-long.115,D18-1422,0,0.0175222,"loring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation using open-domain tables with unknown schemas. Different from our target text for generation, which consists of several sentences in a paragraph, they proposed a task of generating only one sentence from selected table contents. 3 Numerical Table-to-Text Dataset We created numericNLG, a new table-to-text dataset focusing on a text generation task with numerical reasoning. We collected"
2021.acl-long.115,P02-1040,0,0.110473,"records of TOP in Step 1, the related placeholders from the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of tab"
2021.acl-long.115,2020.emnlp-main.89,0,0.0318933,"ample of a table and its description is shown in Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (20"
2021.acl-long.115,P15-1142,0,0.0327304,"Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation"
2021.acl-long.115,P19-1195,0,0.0463536,"Missing"
2021.acl-long.115,P17-1099,0,0.338191,"like ours, we also include a template-based generator and a pointer-generator network as baselines. 5.1 Non-pre-trained Models Template-based Generator We design a domain-specific template-based generator covering two types of sentences in producing table descriptions: table referring sentences and data description sentences. Since our task focuses on numerical-reasoning descriptions, we define templatized sentences using maximum records in table TOP : <table id&gt; shows <caption&gt;. we can see that <hmax &gt; outperforms other <thmax &gt; with <valmax &gt; of <mmax &gt;. Pointer-Generator Pointer-generator (See et al., 2017) is a sequence-to-sequence model with attention and a copy mechanism. This model copes with the out-of-vocabulary problem in data-to-text generation by jointly copying from source texts and generating from a vocabulary. 5.2 Pre-trained Models Fine-tuned GPT2 GPT2 (Radford et al., 2019) is a pre-trained language model with a decoder-only transformer architecture. We fine-tuned the GPT2 model by using table representation PT as a prefix of our input. Specifically, we fed the concatenation of table representation PT and table description Y to the model and generated Y . In the inference phase, we"
2021.acl-long.115,2020.acl-main.101,0,0.0243932,"ing mathematical operations only on targeted cells to limit the calculation. In this study, we cover maximum, minimum, and difference operations. Examples of a preprocessed table, data table, and pre-executed operation table are shown in Figure 2. Linearized Table Supporting transfer learning of pre-trained transformers to our table-to-text generation task, we prepare a linearized table PT as an input representation so that it similar to the representation that encoder has seen during pre-training. T is converted to a flat string PT = w1 , ..., w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |. In this study, we adopt the template-based input representation, introduced by Kale and Rastogi (2020a), to handle representation bias between a structured data T and a natural language utterance PT , where PT is generated using a manually defined template. We propose not only covering data table TD in the template but also injecting the pre-executed numerical operations of table T through TOP to guide numerical-reasoningbased text generation. We consider four different methods3 fo"
2021.eacl-main.125,D10-1049,0,0.029214,"r, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry"
2021.eacl-main.125,N07-1021,0,0.272096,", and observations, has become a mainstream tool for supporting today’s weather forecasts around the world. Weather forecasters obtain numerical outputs from the simulation models and use their scientific knowledge and historical data to come up with forecast comments such as “sunny and sometimes cloudy”. However, writing local or personalized weather comments for end users is labor intensive and requires a solid knowledge of meteorology. Therefore, the task of generating weather-forecast comments has traditionally been addressed in the field of data-to-text generation (Goldberg et al., 1994; Belz, 2007). In this paper, we focus on the task of generating weather-forecast comments from meteorological 1 https://github.com/titech-nlp/ pinpoint-weather Air pressure simulations. While previous studies have mainly focused on database records and tables (Sripada et al., 2004; Liang et al., 2009), which are modified results by experts based on their local knowledge (Reiter et al., 2005), we use raw simulation results of NWP models as inputs for text generation. This is closer to the real-world scenarios, in which meteorological specialists describe weather comments by interpreting such numerical data"
2021.eacl-main.125,D19-1299,0,0.0245625,"Missing"
2021.eacl-main.125,P18-1240,0,0.0160848,"apanese weather comments from simulation results of NWP models and meteorological observation data. The results of both automatic and human evaluations indicate that our model improves the informativeness of generated comments compared with baselines. 2 Related Work Data-to-text generation, which is the task of automatically producing descriptions from nonlinguistic data (Gatt and Krahmer, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as loc"
2021.eacl-main.125,A92-1007,0,0.609101,"cting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts"
2021.eacl-main.125,C86-1132,0,0.265527,"), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from"
2021.eacl-main.125,P83-1022,0,0.307405,"ments compared with baselines. 2 Related Work Data-to-text generation, which is the task of automatically producing descriptions from nonlinguistic data (Gatt and Krahmer, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). Fo"
2021.eacl-main.125,D16-1128,0,0.0426945,"Missing"
2021.eacl-main.125,P09-1011,0,0.18772,"y and sometimes cloudy”. However, writing local or personalized weather comments for end users is labor intensive and requires a solid knowledge of meteorology. Therefore, the task of generating weather-forecast comments has traditionally been addressed in the field of data-to-text generation (Goldberg et al., 1994; Belz, 2007). In this paper, we focus on the task of generating weather-forecast comments from meteorological 1 https://github.com/titech-nlp/ pinpoint-weather Air pressure simulations. While previous studies have mainly focused on database records and tables (Sripada et al., 2004; Liang et al., 2009), which are modified results by experts based on their local knowledge (Reiter et al., 2005), we use raw simulation results of NWP models as inputs for text generation. This is closer to the real-world scenarios, in which meteorological specialists describe weather comments by interpreting such numerical data. We believe it will be more helpful for less experienced forecasters. There has been little research on generating descriptions from a sequence of raw numerical data even in the data-to-text generation (Gatt and Krahmer, 2018). We illustrate the three characteristic problems of weather-co"
2021.eacl-main.125,W04-1013,0,0.0183021,"dings were both 512. We set the dimension size of the hidden veca tors for the meta-data hm i and observation data hi to 64. The model was trained using the Adam optimizer (Kingma and Ba, 2015). We applied an early stopping strategy with a minimum number of 25 epochs. We stopped training if there was no improvement in validation loss for three consecutive epochs. Evaluation Metrics For the automatic evaluation, since reference texts written by meteorological experts generally mention important information such as sunny and rain, we used BLEU-47 (Papineni et al., 2002) and ROUGE-18 (F1 score) (Lin, 2004) to see whether generated texts properly mention the important information as reference texts do. However, since these metrics based on word overlapping rely on the reference texts, they cannot be used to assess the correctness of the generated texts if their expressions are different from the reference texts. Thus, we also calculated precision, recall and F1 scores of weather labels, which are extracted from the generated texts, to see how they properly describe important information in comparison with those of the reference texts. For the human evaluation, we asked five participants to give"
2021.eacl-main.125,P02-1040,0,0.110503,"n states of our model and size of the word embeddings were both 512. We set the dimension size of the hidden veca tors for the meta-data hm i and observation data hi to 64. The model was trained using the Adam optimizer (Kingma and Ba, 2015). We applied an early stopping strategy with a minimum number of 25 epochs. We stopped training if there was no improvement in validation loss for three consecutive epochs. Evaluation Metrics For the automatic evaluation, since reference texts written by meteorological experts generally mention important information such as sunny and rain, we used BLEU-47 (Papineni et al., 2002) and ROUGE-18 (F1 score) (Lin, 2004) to see whether generated texts properly mention the important information as reference texts do. However, since these metrics based on word overlapping rely on the reference texts, they cannot be used to assess the correctness of the generated texts if their expressions are different from the reference texts. Thus, we also calculated precision, recall and F1 scores of weather labels, which are extracted from the generated texts, to see how they properly describe important information in comparison with those of the reference texts. For the human evaluation,"
2021.eacl-main.125,Q18-1013,0,0.0546871,"Missing"
2021.eacl-main.125,P19-1197,0,0.0817262,"; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts by implicitly introducing a content-matching constraint (Wang et al., 2020), explicitly specifying the content in the table (Ma et al., 2019) or incorporating copy mechanism (Lebret et al., 2016). Nonetheless, the techniques proposed in the table-to-text task are not directly applicable to datasets consisting of raw numerical data, such as simulation results of NWP models, and texts since they rely on task-specific architectures such as the copy mechanism copying words from tables. In addition, Puduppully et al. (2019) proposed a method for generating summaries of basketball games by using the correspondence between entities in text and input tabular data extracted using the information-extraction method (Wiseman et al., 2017). How"
2021.eacl-main.125,N16-1086,0,0.0190113,"dely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selecti"
2021.eacl-main.125,2020.acl-main.101,0,0.198938,"ond and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts by implicitly introducing a content-matching constraint (Wang et al., 2020), explicitly specifying the content in the table (Ma et al., 2019) or incorporating copy mechanism (Lebret et al., 2016). Nonetheless, the techniques proposed in the table-to-text task are not directly applicable to datasets consisting of raw numerical data, such as simulation results of NWP models, and texts since they rely on task-specific architectures such as the copy mechanism copying words from tables. In addition, Puduppully et al. (2019) proposed a method for generating summaries of basketball games by using the correspondence between entities in text and input tabular data extracted u"
2021.eacl-main.267,D19-1371,0,0.0140209,"scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-ty"
2021.eacl-main.267,N19-1423,0,0.0171912,"bles was done by Milosevic et al. (2016) to automatically detect table structures from XML tables. Our pointer-generator-based model in the metrictype generation scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework f"
2021.eacl-main.267,2020.acl-main.116,0,0.0165334,"twork deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-types by searching a set of possible tokens in the dictionary. Focusing on numerical tables, Nourbakhsh et al. (2020) extracted metri"
2021.eacl-main.267,P19-1513,0,0.0223378,"eneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-types by searching a set of possible tokens in the dictionary. Focusing on numerical tables, Nourbakhsh et al. (2020) extracted metric-types in earning reports by using similarity scores between the corresponding non-numeric text for the leftmost cells and stored metric-types. The work closest to ours is the one by Hou et al. (2019), who used tables from the experimental result section, combined with the title and abstract as document representations to extract triples of tasks, 1 Dataset is nlp/metrictable available on https://github.com/titech3.1 Metric-Type Identification for Numerical Tables Datasets We automatically extracted tables from the PDF files of scientific papers in the computational linguistics domain using PDFMiner and Tabula as extraction tools and filtered only numerical tables related to experimental results using the keywords evaluation, result, comparison, and performance. We used papers from the ACL"
2021.eacl-main.267,P18-1031,0,0.0163825,"UGE-1. 5.3 Implementation Details We implemented our models using the AllenNLP library (Gardner et al., 2018). In our pointergenerator-based model, we used pre-trained word embeddings for initialization and two-layer BiLSTMs with 256 hidden sizes in both the caption and header-level encoders. We used dropout (Srivastava and Hovy, 2014) with the probability p = 0.1. For optimization in the training phase, we used Adam as the optimizer with a batch size of 10 and a learning rate of 3 × 10−3 and 3 × 10−5 in pointer-generator-based and BERT-based, respectively, with a slanted triangular schedule (Howard and Ruder, 2018). We trained the model for a maximum of 20 epochs with early stopping on the validation set (patience of 10) and set α to 0.5. We used the original BERT and the domain-specific SciBERT uncased model to fine-tune our BERTbased model. 6 6.1 Results Experimental Results Model comparison The performances of the proposed and baseline models are shown in Table 2. We can see that the Pointer-Generator Supervised-Attention model initialized by Glove embeddings outperformed the baseline in predicting metric-type location. The accuracy of this model in the metric-type generation part mostly scored bette"
2021.eacl-main.267,C16-1291,0,0.0186997,"from a table caption and generating word wm from the metric-type vocabulary, where pcopy ∈ [0, 1]. We use a softmax function to compute the probability distribution over the metric-type vocabulary: Pvocab (wm ) = softmax(Ccapt ). (7) Then, we obtain the following probability distribution over the extended vocabulary: n X P (wm ) = pcopy acapti + i:wi =(wm ) (1 − pcopy )Pvocab (wm ), (8) where i is the index of metric-type tokens in the vocabulary. Learning objective For training, we exploit the negative log-likelihood objective as the loss function. In addition, we adopt supervised attention (Liu et al., 2016) for jointly supervising the row and column header-level attention to obtain the metrictype header-level. We combine all loss functions in the location classification and token generation model, and define α as the weight as follows: X zhloc log phlocc + L = −((1 − α)( c u+v X (4) which includes the probabilities of the metric-types located in row headers (prh ), located in column headers (pch ), or not located in the headers (pcapt ), where prh + pch + pcapt = 1. whleveli = [arhk prh ; achl pch ], Metric-type generation gates In our pointergenerator network, we use the sigmoid layer to obtain"
2021.eacl-main.267,D19-1387,0,0.0136001,"r-level Encoder Crh Cch Col Header-level Encoder ???? ???? ???? Metric-type Header-location Gate Metric-type Header-level Gate Metric-type header-loc capt/row/col capt/row/col capt/row/col Metric-type header-level weight 0.4 0.2 0.2 0.2 0.4 0.2 0.2 0.2 0.3 0.2 0.1 0.5 ???? Figure 4: Architecture of proposed pointer-generator-based model to identify metric-types in tables. types of input text, pairs of question and answer, a [CLS] token is appended before question tokens, and [SEP] tokens are placed after question and after answer tokens, to separate the question and answer segments. Following Liu and Lapata (2019), we customize these preprocessing schemes by inserting [CLS] before each segment and inserting [SEP] after each segment. We divide our inputs into several segments: caption, row header level 1 to u, and column header level 1 to v. The input text after preprocessing is denoted as a sequence of tokens X = (x1 , x2 , · · ·, xn ). There are three kinds of embedding assigned to each xi : token embeddings representing the meaning of each token, segmentation embeddings indicating the segment boundaries of a sequence of tokens, and position embeddings covering token position within the sequences. Sin"
2021.eacl-main.267,D15-1166,0,0.0135079,"pe header-location outputs. In the generation scheme, we adopt the pointer-generator network to take into account captions as source texts and the metric-type vocabulary in the metric-type generation gate. The architecture of our model is shown in Figure 4. Header encoder We use the vector representation of each header-level by averaging the vectors of all header name tokens in the same level. Given Erhk and Echl as the averages of the initial vector representations of the row and column header-level vectors, respectively, we use the BiLSTM encoder with the dot attention mechanism proposed by Luong et al. (2015) to obtain the representations of the row and column header-levels and select the last hidden state of the last level combined with the weighted hidden states as header-level contexts, as follows: Crh = [Crhu ; u X arhk Crhk ], (2) k=1 Cch = [Cchv ; v X achl Cchl ]. (3) l=1 Caption encoder As with the headers, we use the BiLSTM encoder with attention acapti to compute the context vector of caption Ccapt . Metric-type header-location gates We feed the concatenation of the row and column header contexts to the softmax layer to obtain the metric-type header-location probability: phloc = softmax(["
2021.eacl-main.267,P17-1099,0,0.0120712,"et, and metric for leaderboard construction. In our study, we represent the tables in more generic ways, preventing the original table structure in the multi-level headers form. We intend to retain the ability of a table to cover complex categorization in the headers and efficiently present all values. A previous study that also explored multi-dimensional tables was done by Milosevic et al. (2016) to automatically detect table structures from XML tables. Our pointer-generator-based model in the metrictype generation scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1."
2021.eacl-main.267,P14-1060,0,0.0265468,"Predicted LRow LCol Gen 0 0 0 LCol 0 80 6 Gen 0 3 46 Table 6: Confusion matrix of Fine-tuned SciBERT prediction. where d is the number of w ˆm whose characters are all found in wm in the same order. For example, the predicted token RG1 is regarded as correct when the reference token is ROUGE-1. 5.3 Implementation Details We implemented our models using the AllenNLP library (Gardner et al., 2018). In our pointergenerator-based model, we used pre-trained word embeddings for initialization and two-layer BiLSTMs with 256 hidden sizes in both the caption and header-level encoders. We used dropout (Srivastava and Hovy, 2014) with the probability p = 0.1. For optimization in the training phase, we used Adam as the optimizer with a batch size of 10 and a learning rate of 3 × 10−3 and 3 × 10−5 in pointer-generator-based and BERT-based, respectively, with a slanted triangular schedule (Howard and Ruder, 2018). We trained the model for a maximum of 20 epochs with early stopping on the validation set (patience of 10) and set α to 0.5. We used the original BERT and the domain-specific SciBERT uncased model to fine-tune our BERTbased model. 6 6.1 Results Experimental Results Model comparison The performances of the propo"
2021.eacl-main.296,N19-1423,0,0.0321136,"2 (Merity et al., 2016), extracted from Wikipedia articles, as our external data. As shown in Algorithm 1, data loader loads one batch of negative samples, i.e., sentences from WikiText2, which are labeled with 0. Encoder For encoding the text input, i.e., φ(x, W ), we used a Bidirectional LSTM with attention (Hochreiter and Schmidhuber, 1997; Xu et al., 2015), with the number of hidden units being 150. For the pre-trained word embeddings, we experimented with GloVe Vectors (Pennington et al., 2014) and set the dimension to 300. In our experiments, we did not adopt the widely used BERT model (Devlin et al., 2019), as Ruff et al. (2019) showed that BERT model did not improve the performance. Settings As for the optimization of parameters, Adam (Kingma and Ba, 2014) with a base learning rate of 0.001 was used for 50 epochs. The batch sizes were set to 32 and 64 for Reuters and Newsgroups, respectively. For the initialization of mSVDD model, we employed two operation steps. In the absence of negative samples, mSVDD was 5 http://qwone.com/json/20Newsgroups http://daviddlewis.com/resources/testcollections/ reuters21578/ 6 first pre-trained on target samples by using an AutoEncoder with two objectives: 1) w"
2021.eacl-main.296,D14-1162,0,0.0870117,"s often adopted as the training dataset (Mikolov et al., 2013). So we also chose one publicly available corpus WikiText-2 (Merity et al., 2016), extracted from Wikipedia articles, as our external data. As shown in Algorithm 1, data loader loads one batch of negative samples, i.e., sentences from WikiText2, which are labeled with 0. Encoder For encoding the text input, i.e., φ(x, W ), we used a Bidirectional LSTM with attention (Hochreiter and Schmidhuber, 1997; Xu et al., 2015), with the number of hidden units being 150. For the pre-trained word embeddings, we experimented with GloVe Vectors (Pennington et al., 2014) and set the dimension to 300. In our experiments, we did not adopt the widely used BERT model (Devlin et al., 2019), as Ruff et al. (2019) showed that BERT model did not improve the performance. Settings As for the optimization of parameters, Adam (Kingma and Ba, 2014) with a base learning rate of 0.001 was used for 50 epochs. The batch sizes were set to 32 and 64 for Reuters and Newsgroups, respectively. For the initialization of mSVDD model, we employed two operation steps. In the absence of negative samples, mSVDD was 5 http://qwone.com/json/20Newsgroups http://daviddlewis.com/resources/te"
2021.eacl-main.296,P19-1398,0,0.335048,"compact representation for the description of target data. The compact representation could be a set of prototypes or subspaces obtained by optimizing a reconstruction error on the target training data. Regarding the features for representing text in OCC, document-to-word co-occurrence matrices or hand-crafted features have been commonly used in most of the previous work (Manevitz and Yousef, 2001, 2007; Kumaraswamy et al., 2015). Pretrained vectors have been popular for many NLP tasks (Mikolov et al., 2013; Bengio et al., 2003). The recent context vector data description (CVDD), proposed by Ruff et al. (2019), fully uses word embedding knowledge and a neural network structure to process one-class classification problems. Ruff et al. (2018) introduced deep support vector data description (deep SVDD), a fully unsupervised method for deep one-class classification for image data. Deep SVDD learns to extract the common factors of target training samples with a neural network to minimize the radius of a hypersphere that encloses the network representations of the data. The learned hypersphere, with a center c and a neural feature transformer φ(x), can be an end-to-end 3378 Proceedings of the 16th Confer"
2021.findings-emnlp.128,2020.acl-main.207,0,0.0435812,"Missing"
2021.findings-emnlp.128,W14-3348,0,0.0105475,"in OFiD and RAG. It receives inputs (i.e., x/a and C) and outputs the targets (i.e., t˜). For training all the baseline models, we used the AdamW (Loshchilov and Hutter, 2018) optimizer. The learning rate was initialized at 4e-5 and got a linear schedule with warm-up at the first 10,000 iterations. We finetune the models in all tasks for 10 epochs with the same random seed, record the evaluation of each epoch and report the best results. We run the experiments using 4 Nvidia A100 GPU with a batch size of 4. 5.3 Evaluation metrics We use automatic metrics BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and a neural-based metric MoverScore (Zhao et al., 2019). As automatic scores remain tricky for correctly evaluating the text quality, we conduct human evaluation. As this task requires professional knowledge in computer science, we hire five annoRetrieval-Augmented generator (RAG- tators with a degree in computer science (2 Master sequence). RAG-sequence is used in the students, 2 PhD students, and 1 Postdoc). We test context-aware paragraph generation task. It the performance in terms of Fluency, Faithfulness, considers the to-be-retrieved passages (300 words) Entailment and Overall. Fluenc"
2021.findings-emnlp.128,N19-1423,0,0.0193461,"Missing"
2021.findings-emnlp.128,2021.eacl-main.33,0,0.0248592,"019) and PubMed2 consist of 25K, 90K and 2.6M papers, contributing to computational linguistics, computer science and biomedical, respectively. However, these datasets do not contain the citations, equations and paper structures due to the limitation of the PDF parser. Recently, unarXive (Saier and Färber, 2020) and S2ORC (Lo et al., 2020) parses 1.5M papers from their source (i.e., LATEX), providing the possibility to deal with different types of objects (e.g., tables, figures and more). On the other hand, task-specific datasets are tailored for specific tasks, such as paraphrase generation (Dong et al., 2021), summarization (Lu et al., 2020) and table-to-text (Moosavi et al., 2021). Most of them are built upon the corpus-level datasets and add task-specific features for different tasks. In this paper, we propose SciXGen, a corpuslevel dataset, which parses the body text more precisely to retain more information from the papers. Thus, task-specific datasets can be easily obtained for different tasks in context-aware text generation. 2.2 Text generation in scientific domain Text generation in the scientific domain has achieved progress in several ways. Wang et al. (2019) generates the paper abstract"
2021.findings-emnlp.128,2020.acl-main.703,0,0.0805637,"Missing"
2021.findings-emnlp.128,2020.acl-main.447,0,0.39687,"te texts/paragraphs remains challenging. As has not been well explored in literature. a case study shown in Table 1, generating plausible table descriptions always requires not only tabular For conducting experiments on context-aware data itself as the input, but also numerous refer- text generation, a well-developed dataset with comences to the external information (e.g., body text) plete contextual information is required. However, as the context. To this end, we promote a new task existing corpora (Radev et al., 2013; Clement et al., of context-aware text generation (i.e., generating 2019; Lo et al., 2020; Saier and Färber, 2020) are text given a context), a new branch of text genera- not applicable in our task. Radev et al. (2013) tion research in the scientific domain. This task can and Clement et al. (2019) directly extract data from be straightforwardly extended to several specific PDF, failing in capturing the paper structure and requirements where we, in this paper, investigate other objects (e.g., citations). Their datasets thus 1483 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1483–1492 November 7–11, 2021. ©2021 Association for Computational Linguistics"
2021.findings-emnlp.128,P19-1191,0,0.0476882,"Missing"
2021.findings-emnlp.128,D19-1053,0,0.0211378,"s the targets (i.e., t˜). For training all the baseline models, we used the AdamW (Loshchilov and Hutter, 2018) optimizer. The learning rate was initialized at 4e-5 and got a linear schedule with warm-up at the first 10,000 iterations. We finetune the models in all tasks for 10 epochs with the same random seed, record the evaluation of each epoch and report the best results. We run the experiments using 4 Nvidia A100 GPU with a batch size of 4. 5.3 Evaluation metrics We use automatic metrics BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and a neural-based metric MoverScore (Zhao et al., 2019). As automatic scores remain tricky for correctly evaluating the text quality, we conduct human evaluation. As this task requires professional knowledge in computer science, we hire five annoRetrieval-Augmented generator (RAG- tators with a degree in computer science (2 Master sequence). RAG-sequence is used in the students, 2 PhD students, and 1 Postdoc). We test context-aware paragraph generation task. It the performance in terms of Fluency, Faithfulness, considers the to-be-retrieved passages (300 words) Entailment and Overall. Fluency evaluates the lanindependently, generating an output se"
2021.findings-emnlp.128,2020.emnlp-main.648,0,0.443095,"neration (table-to-text) task. Highlighted texts in red denote the factual incorrectness (hallucination), and texts in blue indicate the fact that can be referred from the context. We can see that tables in the scientific domain contain terms and abbreviations that are mentioned in its body text (i.e. tr and te). With the help of the context, the generated description becomes more plausible. Text generation in the scientific domain has been increasingly received attention recently due to its wide range of applications such as summa- context-aware description generation (i.e., generatrization (Lu et al., 2020), paragraph genera- ing description for paper objects such as tables and tion (Wang et al., 2019) and table description gener- figures, given the body text as context), and contextaware paragraph generation (i.e., generating a paraation (Moosavi et al., 2021). Though recent works graph given cited papers as context). Therefore, have brought breakthroughs (Lu et al., 2020; Wang context-aware text generation yields helpful tools et al., 2019; Moosavi et al., 2021), how to faithfully to generate scientific papers automatically, yet it generate texts/paragraphs remains challenging. As has not been"
2021.findings-emnlp.128,P02-1040,0,0.109422,"lso utilize LED as the generator in OFiD and RAG. It receives inputs (i.e., x/a and C) and outputs the targets (i.e., t˜). For training all the baseline models, we used the AdamW (Loshchilov and Hutter, 2018) optimizer. The learning rate was initialized at 4e-5 and got a linear schedule with warm-up at the first 10,000 iterations. We finetune the models in all tasks for 10 epochs with the same random seed, record the evaluation of each epoch and report the best results. We run the experiments using 4 Nvidia A100 GPU with a batch size of 4. 5.3 Evaluation metrics We use automatic metrics BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and a neural-based metric MoverScore (Zhao et al., 2019). As automatic scores remain tricky for correctly evaluating the text quality, we conduct human evaluation. As this task requires professional knowledge in computer science, we hire five annoRetrieval-Augmented generator (RAG- tators with a degree in computer science (2 Master sequence). RAG-sequence is used in the students, 2 PhD students, and 1 Postdoc). We test context-aware paragraph generation task. It the performance in terms of Fluency, Faithfulness, considers the to-be-retrieved passages (300 w"
2021.findings-emnlp.128,2020.emnlp-main.89,0,0.0347188,"Missing"
2021.inlg-1.11,2021.eacl-main.125,1,0.751031,"g (Kim and Choi, 2020) and data-totext (Taniguchi et al., 2019). Various methods for encoding video frames have been actively studied (Dosovitskiy et al., 2021); commentaries often include comments that focus on the positional relation between cars, which requires a more ﬁne-grained understanding of video frames. The performance of current vision encoders still needs to be evaluated. Data-totext is the task of converting structured data into natural language, which has been applied to the domain of ﬁnance (Murakami et al., 2017; Aoki et al., 2018, 2021; Uehara et al., 2020), weather forecast (Murakami et al., 2021), a summary of sports matches (Puduppully and Lapata, 2021; Iso et al., 2019) and live sports commentary (Taniguchi et al., 2019). The inputs used for existing studies are time-sequence numerical data (Murakami et al., 2017), tables (Puduppully and Lapata, 2021; Gardent et al., 2017) or simulated images (Murakami et al., 2021). These models focus on neural networkbased approaches; however, data-to-text tasks have been studied for a long time (see a survey paper (Gatt and Krahmer, 2018) for details). 3 Dataset We describe the procedure used to create our dataset. We then show its statistics and"
2021.inlg-1.11,P17-1017,0,0.0201372,"tary in real-time, we need to solve at least two tasks: timing identiﬁcation and utterance generation tasks. However, existing studies focus on the latter, where the timings are given, for example, minute-by-minute updates (Kubo et al., 2013). Unlike baseball, the timing identiﬁcation task for race commentary is not trivial because a race cannot be segmented simply. Datasets play important roles in studies on generation. Existing datasets for generation tasks contain data in a single modality, such as, videos (Zhou et al., 2018; Krishna et al.) or structured data (Puduppully and Lapata, 2021; Gardent et al., 2017). We propose a new largescale dataset that contains transcribed commentaries aligned with videos and structured numerical data. Our setting can be considered as a combination of two different research topics: video captioning (Kim and Choi, 2020) and data-totext (Taniguchi et al., 2019). Various methods for encoding video frames have been actively studied (Dosovitskiy et al., 2021); commentaries often include comments that focus on the positional relation between cars, which requires a more ﬁne-grained understanding of video frames. The performance of current vision encoders still needs to be"
2021.inlg-1.11,P02-1040,0,0.119124,"en state of the LSTM in the decoder side is 230, which is the sum of the size of the encoded images, textual information and structured data. The size of the character embeddings in the decoder is set to 100. We use separate vocabularies for the textual input and the target text. We use Adam (Kingma and Ba, 2015) with several initial learning rates ranging from 10−3 to 10−5 for optimizing parameters. We continue the training iterations until the loss in the validation dataset does not decrease for 10 epochs. We conduct the utterance generation experiments for the gold timestamps. We use BLEU (Papineni et al., 2002) to evaluate the baseline models for this task. The scores are shown in Table 5. The model based only on telemetry data worked well. Adding textual information improved BLEU score if the learning rate is set to lower values i.e., 10−4 or 10−5 . However, we obtained a very low BLEU score when we used only vision-based input. Adding vision information to struct+text model degraded the score if the learning rate is set to 10−3 or 10−4 . Even with a smaller learning rate, 10−5 , vision information did not signiﬁcantly improve the performance. 5.2 Timing Identiﬁcation 6 Discussion the average gap b"
2021.inlg-1.11,P82-1020,0,0.691425,"Missing"
2021.inlg-1.42,P08-1090,0,0.0824098,"eehan, 1976; Riedl and Young, 2010) creates a high-level abstraction or a blueprint 377 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 377–386, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics that encourages the generator to focus on the flow of a story, similar to making an outline before writing. The planned elements are referred to as events in several papers. However, the detailed definition of events varies. For instance, an event can be represented as a verb argument pair (e.g., (admits, subj)) (Chambers and Jurafsky, 2008): tuple of subject, verb, object and modifier or “wildcard” (e.g., (PERSON0, correspond-36.1, empty, PERSON1)) (Martin et al., 2018; Ammanabrolu et al., 2020) or reconstructed verb phrase (e.g., decide(go)) (Peng and Roth, 2016). In this paper, we follow Peng and Roth (2016) to represent an event with verb phrases. Figure 2: Overview of our approach. In the preprocessing step, we cluster the stories into K topics and build an event graph for each topic. In the planning step, an event graph selection module selects an event graph based on the input. Then, a related event graph is retrieved. The"
2021.inlg-1.42,N19-1423,0,0.00579073,"er diagnosed with the flu. (2) The man was coughing a lot. He was sick. He felt sick. He tried to rest for an hour. The man felt better ! (3) The man was coughing a lot. He was sick. He felt sick. He couldn’t eat anything. He starved himself. Table 6: Results on story cloze test. From the results, events planned by our event graphs and mutually exclusive sets have positive effects on this task. different methods into the SCT. The accuracy of SCT reflects the quality of the event features. The event feature is learned by a mask language model (MLM) (i.e., the BERT model with fewer parameters) (Devlin et al., 2019). If the training event sequence is more logical and reasonable, the feature learned by MLM would better fit the SCT. To prove that our event graph and mutually exclusive relation can help us to generate reasonable event sequences, we compared the features generated by the MLM model with different training data: (1) Origin: Event sequences extracted from ROCStories Corpora. (2) RandomWalk: Random walk on the event graphs and sample training data. (3) GraphPlan: Using our planning method to generate training data. Note that the input-event coherence score is excluded in the score function becau"
2021.inlg-1.42,P18-1082,0,0.0147446,"f a story before its generation. Before the emergence of neural-based models, Reiter and Dale (1997) and Riedl (2010) attempted to use hand crafted rules to arrange actions into character sequences. Recently, with the help of neural sequence-to-sequence models, Xu et al. (2018) proposed to generate multiple key phrases and expand them into a complete story. A built-in key phrases generation module is used in their model architecture. In contrast to Xu et al. 378 (2018), some works have explicitly plan a sequence of events (Martin et al., 2018; Ammanabrolu et al., 2020; Tambwekar et al., 2019; Fan et al., 2018; Rashkin et al., 2020; Ammanabrolu et al., 2021), keywords (Yao et al., 2019; Ippolito et al., 2019; Goldfarb-Tarrant et al., 2020) or actions (Fan et al., 2019) before generating the story based on the planned items. All of these planning models rely on a language model for planning, without following an external structure of events, which results in degraded performance (Holtzman et al., 2019). Compared with these works, the main contribution of this paper is to propose a planning method based on automatically created event graphs. Instead of a language model, we use score-based beam search"
2021.inlg-1.42,P19-1254,0,0.0425286,"Missing"
2021.inlg-1.42,2020.emnlp-main.351,0,0.0430913,"attempted to use hand crafted rules to arrange actions into character sequences. Recently, with the help of neural sequence-to-sequence models, Xu et al. (2018) proposed to generate multiple key phrases and expand them into a complete story. A built-in key phrases generation module is used in their model architecture. In contrast to Xu et al. 378 (2018), some works have explicitly plan a sequence of events (Martin et al., 2018; Ammanabrolu et al., 2020; Tambwekar et al., 2019; Fan et al., 2018; Rashkin et al., 2020; Ammanabrolu et al., 2021), keywords (Yao et al., 2019; Ippolito et al., 2019; Goldfarb-Tarrant et al., 2020) or actions (Fan et al., 2019) before generating the story based on the planned items. All of these planning models rely on a language model for planning, without following an external structure of events, which results in degraded performance (Holtzman et al., 2019). Compared with these works, the main contribution of this paper is to propose a planning method based on automatically created event graphs. Instead of a language model, we use score-based beam search to generate a sequence of events by walking on the graph. Graph-based Story Planning An event graph is a variant of a plot graph wh"
2021.inlg-1.42,N19-4016,0,0.0279168,"6) to represent an event with verb phrases. Figure 2: Overview of our approach. In the preprocessing step, we cluster the stories into K topics and build an event graph for each topic. In the planning step, an event graph selection module selects an event graph based on the input. Then, a related event graph is retrieved. The event planning model generates a sequence of events. Finally, based on the input and planned events, a story generation module generates the story. The dashed line denotes mutually exclusive events that are difficult to coexist in the same storyline. Existing approaches (Goldfarb-Tarrant et al., 2019; Martin et al., 2018; Ammanabrolu et al., 2020) regard event generation as an abstracted case of story generation. In other words, they treat each event as one token and use a sequence-to-sequence model to plan the events. Our preliminary experiments show that repetition and logical inconsistency problems occur in the event sequence; further, the same problems occur in the generated stories. Figure 1 shows an example using sequence-tosequence event planning. Both events and stories are repeated and illogical. In this study, instead of a sequence-to-sequence model for event planning, we propos"
2021.inlg-1.42,W19-2405,0,0.0201577,"1997) and Riedl (2010) attempted to use hand crafted rules to arrange actions into character sequences. Recently, with the help of neural sequence-to-sequence models, Xu et al. (2018) proposed to generate multiple key phrases and expand them into a complete story. A built-in key phrases generation module is used in their model architecture. In contrast to Xu et al. 378 (2018), some works have explicitly plan a sequence of events (Martin et al., 2018; Ammanabrolu et al., 2020; Tambwekar et al., 2019; Fan et al., 2018; Rashkin et al., 2020; Ammanabrolu et al., 2021), keywords (Yao et al., 2019; Ippolito et al., 2019; Goldfarb-Tarrant et al., 2020) or actions (Fan et al., 2019) before generating the story based on the planned items. All of these planning models rely on a language model for planning, without following an external structure of events, which results in degraded performance (Holtzman et al., 2019). Compared with these works, the main contribution of this paper is to propose a planning method based on automatically created event graphs. Instead of a language model, we use score-based beam search to generate a sequence of events by walking on the graph. Graph-based Story Planning An event graph"
2021.inlg-1.42,P19-1479,0,0.0209986,"cture of events, which results in degraded performance (Holtzman et al., 2019). Compared with these works, the main contribution of this paper is to propose a planning method based on automatically created event graphs. Instead of a language model, we use score-based beam search to generate a sequence of events by walking on the graph. Graph-based Story Planning An event graph is a variant of a plot graph whose nodes represent events. Previous research has made progress on generating stories from plot graphs (Weyhrauch, 1997; Chen et al., 2009; Regneri et al., 2010; McIntyre and Lapata, 2010; Li et al., 2019). Li et al. (2013) proposed a plot graph on story generation tasks, which is relevant to our work. They crowdsourced the story corpus and manually created plot nodes and edges in the graph. In their graph, mutually exclusive events are not allowed to be present in the same story. In this work, both the event graphs and mutually exclusive sets are automatically generated. We further propose an event planning method that considers the relations between events and various inputs (i.e., title or image). 3 Event Graph Construction As a preprocessing step, we first extract events automatically from"
2021.inlg-1.42,P10-1158,0,0.228482,"ogical inconsistency problems occur in the event sequence; further, the same problems occur in the generated stories. Figure 1 shows an example using sequence-tosequence event planning. Both events and stories are repeated and illogical. In this study, instead of a sequence-to-sequence model for event planning, we propose a planning method, GraphPlan. To plan the event, GraphPlan walks on a topic-specific event graph with a beam search. Event graphs have been adopted for story generation even before the emergence of neuralbased models (Weyhrauch, 1997; Chen et al., 2009; Regneri et al., 2010; McIntyre and Lapata, 2010; Li et al., 2013). An event graph represents the logical flow of events based on the facts presented in a corpus. We can walk on a learned event graph and produce a reasonable event sequence. We follow the graph setting in Li et al. (2013), wherein each graph is composed of event nodes, functions and a set of mutually exclusive events. To generate a story, we first identify the topic based on the input (e.g., title or image), and subsequently, retrieve a related event graph. We then plan the events by running a beam search with a score function that considers event-event coherence and input-e"
2021.inlg-1.42,P16-1028,0,0.161365,"©2021 Association for Computational Linguistics that encourages the generator to focus on the flow of a story, similar to making an outline before writing. The planned elements are referred to as events in several papers. However, the detailed definition of events varies. For instance, an event can be represented as a verb argument pair (e.g., (admits, subj)) (Chambers and Jurafsky, 2008): tuple of subject, verb, object and modifier or “wildcard” (e.g., (PERSON0, correspond-36.1, empty, PERSON1)) (Martin et al., 2018; Ammanabrolu et al., 2020) or reconstructed verb phrase (e.g., decide(go)) (Peng and Roth, 2016). In this paper, we follow Peng and Roth (2016) to represent an event with verb phrases. Figure 2: Overview of our approach. In the preprocessing step, we cluster the stories into K topics and build an event graph for each topic. In the planning step, an event graph selection module selects an event graph based on the input. Then, a related event graph is retrieved. The event planning model generates a sequence of events. Finally, based on the input and planned events, a story generation module generates the story. The dashed line denotes mutually exclusive events that are difficult to coexist"
2021.inlg-1.42,2020.emnlp-main.349,0,0.0308706,"Missing"
2021.inlg-1.42,P10-1100,0,0.229555,"that repetition and logical inconsistency problems occur in the event sequence; further, the same problems occur in the generated stories. Figure 1 shows an example using sequence-tosequence event planning. Both events and stories are repeated and illogical. In this study, instead of a sequence-to-sequence model for event planning, we propose a planning method, GraphPlan. To plan the event, GraphPlan walks on a topic-specific event graph with a beam search. Event graphs have been adopted for story generation even before the emergence of neuralbased models (Weyhrauch, 1997; Chen et al., 2009; Regneri et al., 2010; McIntyre and Lapata, 2010; Li et al., 2013). An event graph represents the logical flow of events based on the facts presented in a corpus. We can walk on a learned event graph and produce a reasonable event sequence. We follow the graph setting in Li et al. (2013), wherein each graph is composed of event nodes, functions and a set of mutually exclusive events. To generate a story, we first identify the topic based on the input (e.g., title or image), and subsequently, retrieve a related event graph. We then plan the events by running a beam search with a score function that considers event-"
2021.inlg-1.42,D18-1462,0,0.16178,"be trained to capture the word-prediction distribution from the training data, it has two serious drawbacks when applied to generating stories: 1) A conditional language model (i.e., the decoder) tends to assign high probabilities to generic, repetitive words, especially when beam search is applied in the decoding phase (Holtzman et al., 2019); and 2) sequence-to-sequence models often fail to produce logically correct stories. Recently, there has been significant interest in decomposing story generation into two phases: Planning and generation (Yao et al., 2019; GoldfarbTarrant et al., 2019; Xu et al., 2018; Fan et al., 2019). Planning (Meehan, 1976; Riedl and Young, 2010) creates a high-level abstraction or a blueprint 377 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 377–386, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics that encourages the generator to focus on the flow of a story, similar to making an outline before writing. The planned elements are referred to as events in several papers. However, the detailed definition of events varies. For instance, an event can be represented as a verb argume"
2021.naacl-industry.32,Q17-1010,0,0.0053715,"2019). We treat the predicted QS as rQ (x, y), the reward of the ad-text’s quality. To train a classifier to predict QS from the given x and y, we prepare an ad dataset from our ad-text databases. This dataset consists of a title, description, and score that represent the quality of each ad. The QS ranges from 1 to 10, where a lower score corresponds to lower ad quality and a higher score indicates higher ad quality. We develop a simple regression model to predict the QS from the title and description. In the model, the title and description are joined and encoded into embeddings by fastText (Bojanowski et al., 2017) and max-pooling of simple word embedding (SWEM) (Shen et al., 2018). After the encoding, we use the gradient boosting regression tree (GBRT) (Ke et al., 2017) to predict the QS. We developed a simple model because that the predicted quality score is used as a reward in RL, which requires a large computational time; therefore, efforts should be made to shorten it. 3 3.1 Experimental Settings Dataset In this dataset, we used the meta-description as the content of the LP. In addition to the ad-texts, we prepared Japanese Wikipedia articles2 to pretrain the language model. The fine-tuning of LM w"
2021.naacl-industry.32,P18-1041,0,0.0132816,"quality. To train a classifier to predict QS from the given x and y, we prepare an ad dataset from our ad-text databases. This dataset consists of a title, description, and score that represent the quality of each ad. The QS ranges from 1 to 10, where a lower score corresponds to lower ad quality and a higher score indicates higher ad quality. We develop a simple regression model to predict the QS from the title and description. In the model, the title and description are joined and encoded into embeddings by fastText (Bojanowski et al., 2017) and max-pooling of simple word embedding (SWEM) (Shen et al., 2018). After the encoding, we use the gradient boosting regression tree (GBRT) (Ke et al., 2017) to predict the QS. We developed a simple model because that the predicted quality score is used as a reward in RL, which requires a large computational time; therefore, efforts should be made to shorten it. 3 3.1 Experimental Settings Dataset In this dataset, we used the meta-description as the content of the LP. In addition to the ad-texts, we prepared Japanese Wikipedia articles2 to pretrain the language model. The fine-tuning of LM was performed on the ad-text dataset. All texts in this dataset were"
2021.naacl-industry.32,D17-1062,0,0.0165433,"iew of proposed ad-text generation method 2.2.1 Fluency 2.2.2 Fluency is an essential factor in generating natural language texts. In addition, the length limitation of the text must be considered, as space for advertising is limited. If the ad-text is truncated owing to space limitations, its fluency is significantly degraded. To address these problems, our fluency reward consists of two types of scores as follows: rF = sLM (y) + sLen (y), (6) where sLM (y) is a grammatical score and sLen (y) scores the fidelity of |y |to the given desired length. We use the function described in Eq. (10) of Zhang and Lapata (2017) as the first score sLM (y). The second score, sLen (y), measures the appropriateness of the length of the generated text. The length of the generated text must not exceed the length limit. However, to maintain informativeness, it should not be significantly shorter than the limit. We incorporate these factors into sLen . Let ytitle be the title part of y, ydesc be the description part of y, Ctitle be the length limit of the title part, Cdesc be the length limit of the description part, and sl be a score function for each part of the generated text. The score sLen is calculated as follows: sLe"
C12-1060,N09-1014,0,0.0141782,"cheme of removing the largest eigenvalue component also improves the linearized model, which is almost equivalent to the label propagation (Zhu and Ghahramani, 2002). 978 Although we focus on a specific problem of constructing the polarity lexicon in this paper, the developed methodology can be employed for general purposes of assessing influences of a few representative nodes in a network via local communications. In fact, network-based semisupervised models are used in a number of tasks in natural language processing, such as word sense disambiguation (Yu et al., 2011), machine translation (Alexandrescu and Kirchhoff, 2009), query classification (Hu et al., 2009; Li et al., 2008). 2 Related Work There has been much related work on building polarity lexicons. One of the earliest studies was done by Hatzivassiloglou and McKeown (1997), who focused on conjoined adjectives in the Wall Street Journal corpus (Marcus et al., 1993). They deduced the polarity of adjectives by using pairs of adjectives appearing with a conjunction in the corpus. For example, pairs of adjectives joined with an “and” tend to have the same semantic orientation (e.g., “simple and well-received”) while those joined with a “but” tend to have th"
C12-1060,D11-1052,0,0.0154516,"osed by Rao and Ravichandran (2009) increases robustness against network disturbances. It treats polarity detection as a semi-supervised label propagation problem in a graph in which higher order correlations of network topology other than the shortest paths are involved. They showed that the label propagation algorithm (Zhu and Ghahramani, 2002) leads to a significant improvement in the accuracy of polarity detection for WordNet-based networks compared to various known heuristics. The label propagation is used for word polarity extraction also in the recent literature (Speriosu et al., 2011; Brody and Diakopoulos, 2011). The linearized model in Section 5.3 can also be interpreted as a graph kernel, which is used in natural language processing (e.g., Komachi et al. (2008)). 979 3 Ising spin model 3.1 Overview of Ising spin model For later analysis, we briefly summarize the basic notation and techniques of Ising spin systems. In general, the Ising spin model is composed of N binary variables termed (Ising) spins: S = S1 , S2 , . . . , SN , where Si ∈ {+1, −1} (i = 1, 2, . . . , N ), for which energy function E(S, β ) = −β X i&gt; j Ji j Si S j − N X hi Si (1) i=1 is defined. Here, Ji j represents the efficacy of"
C12-1060,D09-1062,0,0.0656664,"orientation. For example, we usually receive positive impressions for words such as “good”, “excellent” and “enjoyable”, while “bad”, “poor” and “boring” sound negative. Such word-specific orientation of impression is termed polarity (or semantic orientation). A polarity lexicon is a list of words and phrases that are labeled by their polarity, and is an important resource in extracting semantic information from natural language data. Accordingly, the construction of such lists under various conditions has been a major focus in sentiment analysis research (Hatzivassiloglou and McKeown, 1997; Choi and Cardie, 2009). Many construction methods have been developed so far (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Turney and Littman, 2003; Velikovich et al., 2010; Kamps et al., 2004; Rao and Ravichandran, 2009). Among those methods for polarity lexicon construction, the method proposed by Takamura et al. (2005) is distinctive in terms of emphasizing the utility of probabilities. In their method, the construction of a polarity lexicon is mapped to the Ising spin model of magnetism at a finite temperature. This mapping, in conjunction with the formalism of equilibrium statistical mechanics, y"
C12-1060,P97-1023,0,0.535303,"ing sentences possesses its specific orientation. For example, we usually receive positive impressions for words such as “good”, “excellent” and “enjoyable”, while “bad”, “poor” and “boring” sound negative. Such word-specific orientation of impression is termed polarity (or semantic orientation). A polarity lexicon is a list of words and phrases that are labeled by their polarity, and is an important resource in extracting semantic information from natural language data. Accordingly, the construction of such lists under various conditions has been a major focus in sentiment analysis research (Hatzivassiloglou and McKeown, 1997; Choi and Cardie, 2009). Many construction methods have been developed so far (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Turney and Littman, 2003; Velikovich et al., 2010; Kamps et al., 2004; Rao and Ravichandran, 2009). Among those methods for polarity lexicon construction, the method proposed by Takamura et al. (2005) is distinctive in terms of emphasizing the utility of probabilities. In their method, the construction of a polarity lexicon is mapped to the Ising spin model of magnetism at a finite temperature. This mapping, in conjunction with the formalism of equilibrium"
C12-1060,D07-1115,0,0.0168089,"tructed by connecting each pair of synonymous words provided by WordNet (Fellbaum, 1998) in which the shortest paths to two seed words, “good” and “bad,” are used to obtain the polarity of a word. This method is attractive in terms of computational cost, but the shortest paths are sensitive to local disturbances in the network topology. Similarly, Velikovich et al. (2010) developed a method that aggregates a huge amount of unlabeled corpus data from the Web and constructs a lexical network. They used a graph propagation algorithm, in which the weighted shortest paths from seed words are used. Kaji and Kitsuregawa (2007) proposed a method for constructing a Japanese polarity lexicon from Web data. They collected positive (negative) sentences from the Web using structural clues such as HTML tags and then extracted polar phrases from them. The method proposed by Rao and Ravichandran (2009) increases robustness against network disturbances. It treats polarity detection as a semi-supervised label propagation problem in a graph in which higher order correlations of network topology other than the shortest paths are involved. They showed that the label propagation algorithm (Zhu and Ghahramani, 2002) leads to a sig"
C12-1060,kamps-etal-2004-using,0,0.0780479,"Missing"
C12-1060,D08-1106,0,0.0304662,"m in a graph in which higher order correlations of network topology other than the shortest paths are involved. They showed that the label propagation algorithm (Zhu and Ghahramani, 2002) leads to a significant improvement in the accuracy of polarity detection for WordNet-based networks compared to various known heuristics. The label propagation is used for word polarity extraction also in the recent literature (Speriosu et al., 2011; Brody and Diakopoulos, 2011). The linearized model in Section 5.3 can also be interpreted as a graph kernel, which is used in natural language processing (e.g., Komachi et al. (2008)). 979 3 Ising spin model 3.1 Overview of Ising spin model For later analysis, we briefly summarize the basic notation and techniques of Ising spin systems. In general, the Ising spin model is composed of N binary variables termed (Ising) spins: S = S1 , S2 , . . . , SN , where Si ∈ {+1, −1} (i = 1, 2, . . . , N ), for which energy function E(S, β ) = −β X i&gt; j Ji j Si S j − N X hi Si (1) i=1 is defined. Here, Ji j represents the efficacy of interactions between two spins, Si and S j , and hi stands for the external field added to Si , and β is called the inverse temperature. The most fundame"
C12-1060,J93-2004,0,0.0400803,"rposes of assessing influences of a few representative nodes in a network via local communications. In fact, network-based semisupervised models are used in a number of tasks in natural language processing, such as word sense disambiguation (Yu et al., 2011), machine translation (Alexandrescu and Kirchhoff, 2009), query classification (Hu et al., 2009; Li et al., 2008). 2 Related Work There has been much related work on building polarity lexicons. One of the earliest studies was done by Hatzivassiloglou and McKeown (1997), who focused on conjoined adjectives in the Wall Street Journal corpus (Marcus et al., 1993). They deduced the polarity of adjectives by using pairs of adjectives appearing with a conjunction in the corpus. For example, pairs of adjectives joined with an “and” tend to have the same semantic orientation (e.g., “simple and well-received”) while those joined with a “but” tend to have the opposite semantic orientation (e.g., “simplistic but well-received”). Because of the limited applicability of this method, only adjectives can be entried in a polarity lexicon. Taking a corpus-based approach, Turney and Littman (2003) built a polarity lexicon by using two algorithms. They used a query s"
C12-1060,E09-1077,0,0.265077,"ression is termed polarity (or semantic orientation). A polarity lexicon is a list of words and phrases that are labeled by their polarity, and is an important resource in extracting semantic information from natural language data. Accordingly, the construction of such lists under various conditions has been a major focus in sentiment analysis research (Hatzivassiloglou and McKeown, 1997; Choi and Cardie, 2009). Many construction methods have been developed so far (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Turney and Littman, 2003; Velikovich et al., 2010; Kamps et al., 2004; Rao and Ravichandran, 2009). Among those methods for polarity lexicon construction, the method proposed by Takamura et al. (2005) is distinctive in terms of emphasizing the utility of probabilities. In their method, the construction of a polarity lexicon is mapped to the Ising spin model of magnetism at a finite temperature. This mapping, in conjunction with the formalism of equilibrium statistical mechanics, yields a probabilistic model for assigning a polarity to each word. The optimal assignment of the polarities is determined by approximately evaluating the averages of the spin variables. Its experimental applicatio"
C12-1060,W11-2207,0,0.0141409,"m them. The method proposed by Rao and Ravichandran (2009) increases robustness against network disturbances. It treats polarity detection as a semi-supervised label propagation problem in a graph in which higher order correlations of network topology other than the shortest paths are involved. They showed that the label propagation algorithm (Zhu and Ghahramani, 2002) leads to a significant improvement in the accuracy of polarity detection for WordNet-based networks compared to various known heuristics. The label propagation is used for word polarity extraction also in the recent literature (Speriosu et al., 2011; Brody and Diakopoulos, 2011). The linearized model in Section 5.3 can also be interpreted as a graph kernel, which is used in natural language processing (e.g., Komachi et al. (2008)). 979 3 Ising spin model 3.1 Overview of Ising spin model For later analysis, we briefly summarize the basic notation and techniques of Ising spin systems. In general, the Ising spin model is composed of N binary variables termed (Ising) spins: S = S1 , S2 , . . . , SN , where Si ∈ {+1, −1} (i = 1, 2, . . . , N ), for which energy function E(S, β ) = −β X i&gt; j Ji j Si S j − N X hi Si (1) i=1 is defined. Here, J"
C12-1060,P05-1017,1,0.927103,"yable”, while “bad”, “poor” and “boring” sound negative. Such word-specific orientation of impression is termed polarity (or semantic orientation). A polarity lexicon is a list of words and phrases that are labeled by their polarity, and is an important resource in extracting semantic information from natural language data. Accordingly, the construction of such lists under various conditions has been a major focus in sentiment analysis research (Hatzivassiloglou and McKeown, 1997; Choi and Cardie, 2009). Many construction methods have been developed so far (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Turney and Littman, 2003; Velikovich et al., 2010; Kamps et al., 2004; Rao and Ravichandran, 2009). Among those methods for polarity lexicon construction, the method proposed by Takamura et al. (2005) is distinctive in terms of emphasizing the utility of probabilities. In their method, the construction of a polarity lexicon is mapped to the Ising spin model of magnetism at a finite temperature. This mapping, in conjunction with the formalism of equilibrium statistical mechanics, yields a probabilistic model for assigning a polarity to each word. The optimal assignment of the polarities is de"
C12-1060,N10-1119,0,0.126382,"gative. Such word-specific orientation of impression is termed polarity (or semantic orientation). A polarity lexicon is a list of words and phrases that are labeled by their polarity, and is an important resource in extracting semantic information from natural language data. Accordingly, the construction of such lists under various conditions has been a major focus in sentiment analysis research (Hatzivassiloglou and McKeown, 1997; Choi and Cardie, 2009). Many construction methods have been developed so far (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Turney and Littman, 2003; Velikovich et al., 2010; Kamps et al., 2004; Rao and Ravichandran, 2009). Among those methods for polarity lexicon construction, the method proposed by Takamura et al. (2005) is distinctive in terms of emphasizing the utility of probabilities. In their method, the construction of a polarity lexicon is mapped to the Ising spin model of magnetism at a finite temperature. This mapping, in conjunction with the formalism of equilibrium statistical mechanics, yields a probabilistic model for assigning a polarity to each word. The optimal assignment of the polarities is determined by approximately evaluating the averages o"
C12-1086,W04-3230,0,0.125963,"Missing"
C12-1086,I08-7018,0,0.0757932,"Missing"
C18-1202,D14-1082,0,0.0209474,"this type of error, the writer might not know the correct word, and thus they should be left as is. Considering this, this type of error is not corrected (and thus it is counted as a unique word type). The rest (the third group) are those that do not exist in the English language. Therefore, they are not considered in calculating TTR and K. Namely, they are not included in the number of distinct words nor in the total number. 3 Method We first applied the following preprocessing steps to the target corpora. We respectively used the sentence splitter and the tokenizer in Stanford Parser 3.5.0 (Chen and Manning, 2014) to split the essays into sentences and then into word tokens. We converted all word tokens into lowercase. After this, we removed those tokens containing no English alphabet letter from the corpora. In addition, we removed spelling errors whose correct spellings were not identified. After this, we calculated TTR and K for the three corpora before and after spelling error correction. We tested two ways of correcting spelling errors as described in Sect. 2. Namely, we corrected all 13 types of errors and also only a part of them. We excluded mistakenly concatenated and split words from spelling"
C18-1202,C16-1198,0,0.038683,"Missing"
C18-1202,W17-5018,0,0.0387668,"Missing"
C18-1274,D14-1179,0,0.0474408,"Missing"
C18-1274,P17-1174,0,0.0276391,". , hTx ) from the word embedding vectors of source-language words x = (x1 , x2 , . . . , xj , . . . , xTx ) and the linguistic feature’s embedding vectors as follows: hj = fenc (hj−1 , (Eword (xj )[ ; |F | k=1 Ek (xjk )])), (9) where Eword is an embedding layer for source-language words, Ek is an embedding layer for the k-th type of linguistic features, and |F |is the number of types of linguistic features (i.e., |F |= 4). Note that the model has not used NE tags as a linguistic feature, and the incorporation of NE tags into NMT is still under exploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rathe"
C18-1274,2005.mtsummit-papers.11,0,0.0683783,"g), and English-to-Romanian (En-Ro) translation tasks to confirm the effectiveness of the proposed model. 3244 Table 1: Statistics on Experimental Data (# of parallel sentences) Training Data Development Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out"
C18-1274,D15-1166,0,0.0609428,"roparl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU. 1 Introduction Neural machine translation (NMT) models based on the encoder-decoder model, also known as the sequence-to-sequence model (Sutskever et al., 2014), have successfully shown their quality translation. Consequently, various NMT models are studied in the field of machine translation. To date, the most successful model is the bi-directional multi-layered encoder-decoder model with long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and an attention mechanism (Luong et al., 2015; Bahdanau et al., 2015), also known as attention-based NMT. LSTM and the attention mechanism are introduced to mitigate the difficulty in handling long sentences in the encoder-decoder model. The conventional attention-based NMT model is known to achieve high translation accuracy in bilingual evaluation understudy (BLEU). However, this model encounters two general problems: (i) it tends to have difficulty in translating words with multiple meanings because their translations have high ambiguity, and (ii) translation of compound words seems difficult because the encoder receives only a word, a"
C18-1274,P11-2093,0,0.017711,"f the proposed model. 3244 Table 1: Statistics on Experimental Data (# of parallel sentences) Training Data Development Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out. In the En-Jp task, we used the development and test data employed in WAT 2017. In the En-R"
C18-1274,W10-3707,0,0.0308894,"xploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rather than a word-by-word translation. Their model dynamically selects a word or phrase to be output at each decoding step. Meanwhile Ishiwatari et al. (2017) and Wang et al. (2017) have incorporated chunks identified by a chunker and a SMT model, respectively, our work focuses on chunk information of NE tags. Note that our proposed model can incorporate general chunk information, such as chunks found by a chunker. We will leave this aspect for future work. 3242 3 NMT Incorporating NE Tags In this section, we propose a new NMT model, incorpora"
C18-1274,P02-1040,0,0.103601,"Missing"
C18-1274,W16-2209,0,0.461987,"vector si is calculated as the weighted average, where a weight is ai , over all the encoder’s hidden states: exp(h¯i · hj ) , ai (j) = ∑Tx exp(h¯i · hj ′ ) j ′ =1 si = Tx ∑ ai (j)hj , (6) (7) j=1 where Tx is the length of the source-language sentence x, and · denotes the inner product. In the decoder, the probability distribution of the output word is calculated on the basis of the context vector si in addition to the decoder’s hidden state h¯i : p(yi |y1:i−1 , c) = sof tmax(proj([h¯i ; si ])), (8) where [;] denotes the concatenation of the two vectors. 2.2 Linguistic Input Features for NMT Sennrich and Haddow (2016) have improved the attention-based NMT model by using the linguistic features of source-language sentences. In particular, the encoder receives the lemma, subword tags, POS tags, and dependency labels of source-language sentences in addition to source-language words. A lemma is the original form of the word. Subword tags express prefix, stem, and suffix. A POS tag is part-of-speech information of a word, such as a noun or a verb. Moreover, a dependency label indicates a syntactic relation between words, such as a head and dependents. The encoder converts each of the linguistic features into it"
C18-1274,P16-1162,0,0.11012,"velopment Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out. In the En-Jp task, we used the development and test data employed in WAT 2017. In the En-Ro task, we used the newsdev-2016 as the development data and randomly sampled 3,000 parallel sentences from the"
C18-1274,D17-1149,0,0.0221206,"s of linguistic features (i.e., |F |= 4). Note that the model has not used NE tags as a linguistic feature, and the incorporation of NE tags into NMT is still under exploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rather than a word-by-word translation. Their model dynamically selects a word or phrase to be output at each decoding step. Meanwhile Ishiwatari et al. (2017) and Wang et al. (2017) have incorporated chunks identified by a chunker and a SMT model, respectively, our work focuses on chunk information of NE tags. Note that our proposed model can incorporate general chunk information,"
D07-1063,W00-1303,0,0.440902,"s, “Dep(1), Dep(2), ..., Dep(m)”, by D, where Dep(i) = j means that bi modifies bj . Given the sequence B of chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent"
D07-1063,W02-2016,0,0.647416,"chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent-child relations between two nodes). The difference between the conventional methods and the proposed method"
D07-1063,J94-4001,0,0.475308,"Missing"
D07-1063,C04-1002,0,0.643831,"probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent-child relations between two nodes). The difference between the conventional methods and the proposed method is that the proposed method determines the modifiees based on the likeliness of ancestor-descendant relations in addition to parent-child relations, while the conventional methods tried to capture characteristics that ca"
D07-1063,C00-2110,0,0.281536,"by B, and a sequence of dependency patterns, “Dep(1), Dep(2), ..., Dep(m)”, by D, where Dep(i) = j means that bi modifies bj . Given the sequence B of chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks"
D10-1081,W04-3236,0,\N,Missing
D10-1081,Y03-1017,0,\N,Missing
D10-1081,W98-1210,0,\N,Missing
D10-1081,C08-1113,0,\N,Missing
D10-1081,H94-1020,0,\N,Missing
D10-1081,P07-2055,0,\N,Missing
D10-1081,P06-1085,0,\N,Missing
D10-1081,N09-1036,0,\N,Missing
D10-1081,P09-1012,0,\N,Missing
D10-1081,P06-2056,0,\N,Missing
D10-1081,W99-0702,0,\N,Missing
D14-1017,P07-2045,0,0.0140636,"Missing"
D14-1017,J93-2003,0,0.0941501,"tion tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual se"
D14-1017,P14-1139,0,0.0127301,"methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we d"
D14-1017,N06-1014,0,0.190871,"ika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each"
D14-1017,N12-1047,0,0.0375542,"Missing"
D14-1017,C04-1032,0,0.023836,"Yokohama, Japan 2 National Institute of Information and Communication Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically diffe"
D14-1017,P11-1043,0,0.0127509,"to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function wo"
D14-1017,J03-1002,0,0.0214356,"K 4.91K 4.57K NTCIR10 Japanese English 2.02M 53.4M 49.4M 114K 183K 2K 73K 67.3K 4.38K 5.04K 8.6K 334K 310K 10.4K 12.7K Figure 1: Precision Recall graph in Hansard French-English Figure 2: Precision Recall graph in KFTT Figure 3: AER in Hansard French-English Figure 4: AER in KFTT 156 Table 2: Results of word alignment evaluation with the heuristics-based method (GDF) method symmetric f2f c2c f2c precision 0.4595 0.4633 0.4606 0.4630 KFTT recall AER 0.5942 48.18 0.5997 47.73 0.5964 48.02 0.5998 47.74 F 0.5182 0.5227 0.5198 0.5226 Table 3: Results of translation evaluation by AER and F-measure (Och and Ney, 2003). Since there exists no distinction for sure-possible alignments in the KFTT data, we use only sure alignment for our evaluation, both for the FrenchEnglish and the Japanese-English tasks. Table 2 summarizes our results. The baseline method is symmetric constraint (Ganchev et al., 2010) shown in Table 2. The numbers in bold and in italics indicate the best score and the second best score, respectively. The differences between f2f,f2c and baseline in KFTT are statistically significant at p &lt; 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the basel"
D14-1017,P02-1040,0,0.0919616,"may be treated as content words, based on the previous work of Setiawan et al. (2007). Experiments on word alignment tasks showed better alignment qualities measured by F-measure and AER on both the Hansard task and KFTT. We also observed large gain in BLEU, 0.2 on average, when compared with the previous posterior regularization method under NTCIR10 task. As our future work, we will investigate more precise methods for deciding function words and content words for better alignment and translation qualities. Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May,"
D14-1017,P08-1112,0,0.0467484,"Missing"
D14-1017,P07-1090,0,0.373945,"t the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we differentiate between content words and function words by frequency in bilingual data, following Setiawan et al. (2007). Experimental results show that the proposed methods achieved better alignment qualities on the French-English Hansard data and the JapaneseEnglish Kyoto free translation task (KFTT) measured by AER and F-measure. In translation evaluations, we achieved statistically significant gains Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agre"
D14-1017,C96-2141,0,0.299019,"ffectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual sentence is aligned by a manyto"
D14-1017,D11-1125,0,0.0492598,"Missing"
D14-1017,N03-1017,0,0.0362618,"ith each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Languag"
D14-1017,E12-1045,0,\N,Missing
D15-1143,N10-1028,0,0.0896146,"nsduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to p"
D15-1143,P09-1088,0,0.107802,"a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model o"
D15-1143,W10-1703,0,0.0126229,"ions, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) w"
D15-1143,J14-1007,0,0.0220988,"Missing"
D15-1143,P11-2031,0,0.0259328,"here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that the gains were achieved with the comparable grammar size. When comparing German-English and Japanese-English language pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+future) achieved further gains over the slice sampling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-Eng"
D15-1143,C10-2021,0,0.0174855,"ing et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the"
D15-1143,P13-1077,0,0.0150821,"lating the detailed balance, its time complexity of O(|f |3 |e|3 ) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let Σ denote a set of terminal symbols in the source langua"
D15-1143,W06-3105,0,0.425911,"ecreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A"
D15-1143,D08-1033,0,0.045527,"Missing"
D15-1143,P10-4002,0,0.0242682,"or those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to s"
D15-1143,D11-1125,0,0.0169852,"65.5k fr-en 1.54M 1.83M 55.6M 65.5k 72.5k 61.9k 70.5k ja-en 1.80M 2.03M 27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better t"
D15-1143,D07-1103,0,0.194885,"basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu,"
D15-1143,N03-1017,0,0.199013,"ilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to score every phrase pair created during decoding with ba"
D15-1143,P07-2045,0,0.0145853,"Missing"
D15-1143,D12-1021,0,0.0497587,"Missing"
D15-1143,D12-1088,0,0.038562,"Missing"
D15-1143,W02-1018,0,0.0414167,"ntly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom"
D15-1143,P11-1064,1,0.480702,"s sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sam"
D15-1143,J03-1002,0,0.0330693,"Missing"
D15-1143,P03-1021,0,0.0510498,"heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at least once in the sample. During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior (f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4 . The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probabil"
D15-1143,C12-1176,0,0.200354,"granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Euro"
D15-1143,D12-1089,0,0.0958848,"a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg"
D15-1143,N13-1038,0,0.0306641,"tion. If the Score(rspi ) is less than usp , we prune the rspi from cube. Similar to Blunsom and Cohn (2010), if the span sp is not in the current derivation, the rules with low probability are pruned acd denotes a rule in cording to Equation (14). Let rsp d with span sp, P (d|u) is calculated by: sp∈d d ) P (rsp ∑ rj ∈rsp P (rj )I(usp &lt; Score(rj )) |φ | p| ∏ [θp ]dpp ∏ |φ (14) P (usp |d) = Beta(usp ; a, 1.0), ∏ pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample ⟨dp , θp"
D15-1143,P02-1040,0,0.0940086,"27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that"
D15-1143,D14-1180,0,0.0144786,"ion. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f |3 |e|3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xia"
D15-1143,P06-1124,0,0.367232,"2: Derivation tree generated from the hierarchical back-off model we reach phrase pairs which are generated without any back-offs. Let a discount parameter be dp , a strength parameter be θp , and a base measure be Gp0 . More formally, the generative process is represented as follows: GX ∼ Prule (dr , θr , Gphrase ), Gphrase ∼ Pphrase (dp , θp , GX ), X → ⟨s/t⟩ ∼ Gphrase , X → ⟨α/β⟩ ∼ GX , (4) where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. Pphrase is composed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation (2): ck − dp · |φpk | , θ p + np (5) where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: θp + dp · |φp | cback + γb · Gb · θp + np cback + cbase + γb ·Prule (dr , θr , Gphrase ) ∏ · Pphrase (dp , θp , GX ), X∈⟨α/β⟩ base: As an alternative to the back-off state, we may rea"
D15-1143,J97-3002,0,0.677402,"007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) m"
D15-1143,J07-2003,0,\N,Missing
D16-1140,D13-1176,0,\N,Missing
D16-1140,D15-1044,0,\N,Missing
D16-1140,P00-1041,0,\N,Missing
D16-1140,W03-0501,0,\N,Missing
D16-1140,W08-1105,0,\N,Missing
D16-1140,C08-1018,0,\N,Missing
D16-1140,N10-1131,0,\N,Missing
D16-1140,W04-1013,0,\N,Missing
D16-1140,W11-1610,0,\N,Missing
D16-1140,D15-1042,0,\N,Missing
D16-1140,D13-1155,0,\N,Missing
D16-1140,W01-0100,0,\N,Missing
D16-1140,W12-3018,0,\N,Missing
D16-1140,P16-1154,0,\N,Missing
D16-1140,P16-1094,0,\N,Missing
D16-1140,D10-1050,0,\N,Missing
D16-1140,P16-1014,0,\N,Missing
D16-1140,N16-1012,0,\N,Missing
D16-1140,N16-1005,0,\N,Missing
D16-1210,C00-1004,0,0.0964455,"ulating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since there exists no distinction for sure-possible alignments in the KFTT and BTEC data sets, we treat all alignments of them as sure alignments. 7 We tried values from 0.1 to 1.0 at an interval of 0.1. 2001 Table 2 shows the results of word alignment evaluations8 , where none denotes that the model has no constraint. In KFTT and BTEC Corpus, itg achieved significant improvement against sym and none on IBM Model 4 (p ≤ 0.05)9 . However, in the Hansard Corpus, itg shows no improvement against sym. This indicates that capturing structural coherence by itg yields a significant benefit to wor"
D16-1210,J93-2003,0,0.0759293,"thods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on"
D16-1210,N12-1047,0,0.0145612,"statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is comparable to sym in KFTT, while itg significantly outperforms sym and is comparable to none in IWSLT 2007. 4 Discussion 4.1 Effects of ITG Constraints on Word Alignment and Translation We discuss the effect of our ITG constraint on word alignment and machine translation. As described in Section 2, the ITG constraint is imposed in the E-step of the EM a"
D16-1210,W07-0403,0,0.0603047,"Missing"
D16-1210,J07-2003,0,0.0575014,"6; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 199"
D16-1210,P11-2031,0,0.0294483,"straint are directly reflected in the machine translation results because the phrase tables are extracted from the posterior probabilities calculated in training steps. Therefore, our ITG constraint has a potential to achieve a large improvement of machine translation performance relative to an improvement of alignment performance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction"
D16-1210,P07-1003,0,0.0288247,"effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that use"
D16-1210,J07-3002,0,0.0327245,"used the first 10K sentence pairs in the training data for the IWSLT 2007 translation task, which were manually annotated with word alignment (Chooi-Ling et al., 2010), as the BTEC Corpus. In translation evaluations, we used the KFTT and Ja-En IWSLT 2007 translation tasks5 . Table 1 shows each corpus size. In each training data set, all words were lowercased and sentences with over 80 words on either side were removed. 3.1 Word Alignment Evaluation We measured the performance of word alignment with AER and F-measure (Och and Ney, 2003). We used only sure alignments for calculating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since ther"
D16-1210,P08-1112,0,0.0536238,"Missing"
D16-1210,N03-1017,0,0.0205063,"onal models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical"
D16-1210,P07-2045,0,0.00624361,"e relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1"
D16-1210,W04-3250,0,0.119987,"e models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five differen"
D16-1210,W13-2263,0,0.0170773,"alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to e"
D16-1210,N06-1014,0,0.0348688,"ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (C"
D16-1210,W13-3523,0,0.058296,"Missing"
D16-1210,C04-1032,0,0.391898,"ro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly"
D16-1210,W03-0301,0,0.16238,"Missing"
D16-1210,2007.iwslt-1.23,0,0.0267202,"sed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is c"
D16-1210,J03-1002,0,0.0550643,".titech.ac.jp Eiichiro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the"
D16-1210,P02-1040,0,0.113232,"cally different language pair such as Ja-En. For example, some function words appear more than once in both a source and target sentence, and they are not symmetrically aligned with each other, especially in regards to the Ja-En language pair. Although the baseline methods tend to be unable to align such long-distance word pairs, the proposed method can correctly catch them because itg can determine the relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap"
D16-1210,C12-1142,0,0.018121,"d for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 both to P (A → ϵ/ej ) and P (A → fi /ϵ). To reduce computational cost, the probabilities of phrasal rules P (A → ⟨Y /Z⟩) and P (A → [Y /Z]) are not trained, which are set to 0.5 following Saers et al. (2012). In addition to the probability of each ITG rule, we must provide a probability to an one-to-many alignment because the two step parsing approach must pre-compute probabilities for all one-to-many alignments in the first step. An one-to-many alignment 2 3 We set n to 30 in our experiments. We set pnull to 10−5 . can be decomposed to a rule A → fi /ej and some A → ϵ/ej rules under the ITG form. We select a set of rules with the highest probability for an one-tomany alignment using Viterbi algorithm, which has a complexity of O(|e|). 2.3 Previous Agreement Constraint This section provides an ov"
D16-1210,takezawa-etal-2002-toward,1,0.572641,"ed as a soft constraint in the posterior regularization framework (Ganchev et al., 2010). In addition, our ITG constraint works also on word alignments that are not covered by ITG parse trees, as a standard symmetric constraint. Hence, the proposed method is robust to ITG parse errors compared to an alignment method that uses an ITG directly in model training (e.g., Zhang and Gildea (2004, 2005)). Word alignment evaluations show that the proposed method achieves significant gains in Fmeasure and alignment error rate (AER) on the KFTT (Neubig, 2011) and the BTEC JapaneseEnglish (Ja-En) corpus (Takezawa et al., 2002). Machine translation evaluations show that our constraint significantly outperforms or is comparable to the baseline symmetric constraint (Ganchev et al., 2010) in BLEU on the KFTT Ja-En and IWSLT 2007 Ja-En corpus (Fordyce, 2007). 2 ITG Constraint in the Posterior Regularization Framework 2.1 Overview The proposed method introduces an ITG constraint into the posterior regularization framework (Ganchev et al., 2010) in model training. The proposed model is trained as follows, where agreement constraints are imposed in the E-step of the EM algorithm1 : E-step: 1. Calculate a source-to-target p"
D16-1210,C96-2141,0,0.571603,"onal models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et"
D16-1210,J97-3002,0,0.668646,"ty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on"
D16-1210,C12-1176,0,0.0172759,"n, we present our ITG parsing method, which uses bracketing ITG (Wu, 1997). The rules of the bracketing ITG are as follows: A → ⟨Y /Z⟩, A → [Y /Z], A → fi /ej , A → fi /ϵ, and A → ϵ/ej , where A, Y , and Z are non-terminal symbols, fi and ej are terminal strings, ϵ is a null symbol, ⟨⟩ denotes the inversion of two phrase positions, and [] denotes the reversion of two phrase positions. In general, a bracketing ITG has O(|f |3 |e|3 ) time complexity for parsing a sentence pair {f , e}, where |f |and |e |are the lengths of f and e. For efficient ITG parsing, we use the two-step parsing approach (Xiao et al., 2012), which has been proposed to induce Synchronous Context Free Grammar (SCFG) using n-best pruning2 with time complexity O(|f |3 ). Because ITG is a kind of SCFG, this method can be adopted for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 bo"
D16-1210,P13-1017,0,0.0173604,"ance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction, because excessively long phrase pairs are extracted by bridging the gaps in wrong alignments or simply no phrase pairs are extracted from wrong gappy alignments. Consequently, the phrase table generated by IBM Model 4+sym tend to be sparse and contain longer phrase pairs than the one generated by IBM Model 4+itg. 5 Conclusi"
D16-1210,C04-1060,0,0.0378121,"y different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regu"
D16-1210,P05-1059,0,0.0437533,"rs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regularization framework (Gan"
D16-1210,2007.iwslt-1.1,0,\N,Missing
D17-1246,C14-1154,0,0.0297851,"ntext or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such"
D17-1246,D10-1124,0,0.0316643,"Missing"
D17-1246,Q16-1003,0,0.0154324,"i-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmoid function for the SGNS IN-IN model and with the cosine similarity for the SVD model, their accuracies were worse than those in Table 3. 2326 6 Conclusion We presented a model that uses context embeddings to distinguish Japanese non-standard usages from standard ones on social medi"
D17-1246,W16-1817,0,0.0246182,"ent 4.1 Methods for Comparative Evaluation Our model has three characteristics: (input and output) word embeddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Cor"
D17-1246,D16-1057,0,0.114458,"eights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Corpus Sentences randomly 336,048 picked from the Web Japanese Wikipedia 1,081,154 Japanese Newspapers 1,204,914 #token 1.1b 6.0b"
D17-1246,D13-1147,0,0.0316887,"s that input embeddings should be used in combination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al"
D17-1246,Q15-1016,0,0.299208,"eddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Corpus Sentences randomly 336,048 picked from the Web Japanese Wikipedia 1,081,154 Japanese Newspapers 1,"
D17-1246,C10-2078,0,0.0244279,"mbination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few res"
D17-1246,P14-1096,0,0.0255034,"nguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmoid function for the SGNS IN-IN model and with the cosine similarity for the SVD model, their accuracies were worse than those in Table 3. 2326 6 Conclusion We presented a model that uses context embeddings to distinguish Japanese non"
D17-1246,D14-1113,0,0.0382013,"arget instances. 4 Experiment 4.1 Methods for Comparative Evaluation Our model has three characteristics: (input and output) word embeddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japan"
D17-1246,E17-2025,0,0.161991,"small for word pairs that do not co-occur in the training corpus. We exploited this tendency for recognizing non-standard usages; if the dot-product between the embeddings of the target word and the context words is small, it should indicate a non-standard usage, on the condition that the embeddings have been learned on a general balanced corpus where words correspond to their standard meanings in most cases. v IN is widely used as a word embedding in many studies, while v OU T has not been in the limelight; only a few researchers have examined the effectiveness of v OU T (Mitra et al., 2016; Press and Wolf, 2017). In recent studies, embeddings v IN are usually used for measuring the similarity between words. However, given the characteristics described in the previous paragraph and SGNS’s equivalence with shifted positive pointwise mutual information (Levy and Goldberg, 2014), if we want to measure to what extent word wt tends to co-occur with wk in the training data, then we should use the simiIN · v OU T , instead of v IN · v IN . larity of vw wk wt wk t In this study, we show the importance of using in a task where we need to see if a word matches its context. v OU T 2324 Figure 1: Overview of our"
D17-1246,P10-1029,0,0.0202946,"schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmo"
D17-1246,kawahara-kurohashi-2006-case,0,0.0220099,"d the ranking in terms of the area under the ROC curve (AUC) (Davis and Goadrich, 2006). 4.3 Results Table 3 shows the AUC for each model.13 First, we examined the impact of the choice of training corpus for obtaining word embeddings. The models with BCCWJ are constantly better than those with other corpora, although BCCWJ is smaller than the others (Table 2). This result suggests that use of a balanced corpus is crucial in our method for this task. 9 The Balanced Corpus of Contemporary Written Japanese (Maekawa et al., 2010). 10 Japanese sentences are collected using the method described in (Kawahara and Kurohashi, 2006). 11 We downloaded Japanese Wikipedia articles in July 2016 from https://dumps.wikimedia.org/jawiki/. 12 We used editions of the Mainichi Shimbun, Nihon Keizai Shimbun, and Yomiuri Shimbun published from 1994 to 2004. corpus BCCWJ Web Wikipedia Newspaper SGNS IN-OUT decay uni .875 .846 .827 .844 .870 .842 .821 .839 SGNS IN-IN decay uni SVD decay uni .846 .817 .824 .825 .821 .771 .739 .770 .837 .807 .805 .810 .813 .765 .732 .764 Table 3: Area under the ROC curve (AUC) in usage classification task for each model. Next, we examined the impact of context embeddings. Table 3 shows that our model (S"
D17-1246,N15-1099,0,0.0224251,"should be used in combination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eis"
D17-1246,Y16-3006,0,0.0297227,"erver increases”. 1 This is interlinear-gloss text representation. POSS, NOM, PRS respectively represent the possessive case, nominative case, and present tense. The third line is the standard translation of the Japanese sentence. The Japanese word “鯖 (saba)” (i.e., mackerel) is used to mean computer server by Japanese computer geeks because saba happens to have a pronunciation that is similar to s¯ab¯a (i.e., computer server). When a word is used in a meaning that is different from its dictionary meaning, we call such a usage non-standard.2 Non-standard usages can be found in many languages (Sboev, 2016). For example, the word “catfish” means a ray-finned fish as in a standard dictionary, but on social media, it can mean a person who pretends to be someone else in order to create a fake identity. Such non-standard usages would be an obstacle to a variety of language processings including machine translation; Google Translate cannot correctly interpret examples such as this. Humans, however, would be able to notice non-standard usages from the inconsistency between the expected word meaning and the context. The purpose of this work is to develop a method for distinguishing non-standard usages"
D19-5708,C18-1177,0,0.177662,"Missing"
D19-5708,N18-1131,1,0.777796,"Missing"
D19-5708,N16-1030,0,0.195418,"Missing"
D19-5708,C18-1139,0,0.0497414,"Missing"
D19-5708,P16-1105,1,0.877267,"Missing"
D19-5708,E99-1043,0,0.128647,"Missing"
D19-5708,D14-1162,0,0.0832,"ist of corresponding CUIs {ci }i=1,...,T . Using the SNOMED-CT database, we first conduct dictionary look-up matching for each mention mi with CUIs’ term names to retrieve an optimal CUI. If the CUI is not found for a mention, we then compute a similarity score using the dotproduct with entity embeddings that supposedly should capture possible related CUIs and select the maximum score to predict the optimal CUI for a mention. We use fixed, continuous, task-specific entity embeddings, namely the pre-trained entity embeddings of Spanish SNOMED-CT KB by extracting all CUIs term name using GloVe (Pennington et al., 2014). For the multi-token term name of a CUI, we simply compute the average embeddings. 4 4.2 Hyper-parameters Word representations We generated task specific word embeddings of Spanish PharmaCoNER corpus by merging the raw text of training, development, and test (including background set) sets using GloVe (Pennington et al., 2014). We set the dimension of word embeddings to 200, the dimension of character embeddings for character encoding to 25, and character embeddings for morphological analysis to 25. Hidden dimensions The hidden states in the LSTMs had 200 dimensions. Each feed forward neural"
D19-5708,D18-1309,1,0.875232,"Missing"
D19-5708,P16-2011,0,0.143735,"Missing"
D19-5708,D19-5701,0,0.0632783,"Missing"
D19-5708,P16-1218,0,0.190418,"apture its semantic type by encoding the whole semantic information of the span. We use the average of all the outputs corresponding to the words in the span for the inside representation. Following the above contextual, boundary, and inside representations, we represent the representation R(i, k)[F,L,A,R,B] (Forward-context, Leftboundary, inside with Average, Right-boundary, and Backward-context) of the span (i, k) as follows:  − → h i−1 ; hi ; 1 k−i+1 h→ − ← − i R(i, k)[F,L,A,R,B] = h i−1 ; hi ; xi ; hk ; h i−1 . (5) Contextual LSTM-Minus-based Span Representations We also try LSTM-Minus (Wang and Chang, 2016) for the boundary representation4 . The left boundary is computed as the representation of the previous word of the span subtracted from the representation of the last word of the current span. Similarly, the right boundary is computed as the representation of the next word of the span subtracted from the representation of the first word of the current span. In contextual LSTMMinus-based span representations of an input sequence, we compute the forward- and backwardcontext of a target span as the same manner that stated to represent the forward- and backwardcontext representations of R(i, k)[F"
D19-5727,N19-1423,0,0.156661,"many cases, a mention and its antecedent are far away, e.g., a mention can occur in the result section of a paper while its antecedent is in the abstract section. To address these problems, we enhance the baseline system in two ways; we propose to filter noisy spans by using syntactic information and increase the number of antecedent candidates to capture such long-distance coreferent pairs. We further boost the system by replacing the underlying Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer with the Bidirectional Encoder Representations from Transformer (BERT) model (Devlin et al., 2019)—a contextualized language model that can efficiently capture context in a wide range of NLP tasks. We have evaluated our system on six common metrics for coreference resolution including B3 , BLANC, CEAFE, CEAFM, LEA, and MUC using the official evaluation script provided by the shared task organizers. By increasing the numThis paper describes our system developed for the coreference resolution task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than other existing corpora because it contains full text articles. We have employed an existing span-based state-of-theart neur"
D19-5727,D17-1018,0,0.107226,"n2 , Makoto Miwa1,3 , Hiroya Takamura1 , Sophia Ananiadou2 1 Artificial Intelligence Research Center (AIRC), National Institute of Advanced Industrial Science and Technology (AIST), Japan 2 National Centre for Text Mining, University of Manchester, United Kingdom 3 Toyota Technological Institute, Japan {long.trieu, khoa.duong, takamura.hiroya}@aist.go.jp, makoto-miwa@toyota-ti.ac.jp, {nhung.nguyen, Sophia.Ananiadou}@manchester.ac.uk Abstract present our approach to address the coreference resolution task in this challenging corpus. We employ the state-of-the-art end-to-end coreference system (Lee et al., 2017) as our baseline. The system generates all continuous sequences of words (or spans) in each sentence as mention candidates, which means the number of candidates increases linearly to the number of sentences. Such candidates may contain a large number of noisy spans, which are spans in a sentence that do not fit any noun phrases according to the corresponding parse tree. Such noisy spans are often wasteful when being included in the list of candidates for the coreference resolution step. Especially for the CRAFT corpus, of which the average number of sentences is more than 300, the number of no"
D19-5727,W11-1811,0,0.0703575,"Missing"
D19-5727,W18-2324,1,0.892144,"Missing"
E06-1026,P97-1023,0,0.684876,"n Technology for affect analysis of texts has recently gained attention in both academic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative). For example, the word “beautiful” is positive, while the word “dirty” is negative. Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005; Kobayashi et al., 2001). One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions. No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words. The purpose of this paper is to propose computational models for phrases with semantic orientations as well as classification methods based on the models. Indeed the semantic orientations of phrases depend on context just as the seman"
E06-1026,J93-1007,0,0.0445408,"incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. The three methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words."
E06-1026,P05-1017,1,0.933574,"ademic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative). For example, the word “beautiful” is positive, while the word “dirty” is negative. Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005; Kobayashi et al., 2001). One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions. No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words. The purpose of this paper is to propose computational models for phrases with semantic orientations as well as classification methods based on the models. Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the most ba"
E06-1026,kamps-etal-2004-using,0,0.0821696,"Missing"
E06-1026,P02-1053,0,0.141721,"res in document classification according to semantic orientation. Pang et al. (2002) used bigrams. Matsumoto et al. (2005) used sequential patterns and tree patterns. Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered. Suzuki et al. (2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words i"
E06-1026,H05-1044,0,0.109176,"method is similar to Turney’s in the sense that cooccurrence with seed words is used. The three methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words. For example, a rule [negative+minus=positive] determines “low (minus) risk (negative)” to be positive. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter, which is almost equivalent to the plus/minus attribute above. They manually created the list of polarity shifters. The method that we propose in this paper is an automatic version of Inui’s or Wilson et al.’s idea, in the sense that the method automatically creates word clusters and their polarity shifters. 3 Latent Variable Models for Semantic Orientations of Phrases As mentioned in the Introduction, the semantic orientation of a phrase is not a mere sum of its component words. If we know that “low risk” is pos"
E06-1026,W02-1011,0,0.0455765,"ours, and also from Torisawa’s, in that a probabilistic model is used for feature extraction. 2.2 Identification of Semantic Orientations The semantic orientation classification of words has been pursued by several researchers (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005). However, no computational model for semantically oriented phrases has been proposed to date although research for a similar purpose has been proposed. Some researchers used sequences of words as features in document classification according to semantic orientation. Pang et al. (2002) used bigrams. Matsumoto et al. (2005) used sequential patterns and tree patterns. Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered. Suzuki et al. (2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classi"
E06-1026,H05-1043,0,0.0543259,"– 67.02 0.73 91.7 81.39 0.60 174.0 81.94 0.64 60.0 76 74 72 70 68 66 64 62 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 beta Figure 4: U-shaped model with standard dataset This kind of ellipsis often occurs in Japanese. 206 Table 2: Contingency table of classification result by the U-shaped model Gold standard positive neutral negative sum U-shaped model positive neutral negative 1856 281 69 202 2021 394 102 321 2335 2160 2623 2798 identified. To tackle these examples, we will need methods for correctly identifying attributes and objects. Some researchers are starting to work on this problem (e.g., Popescu and Etzioni (2005)). We succeeded in addressing the data-sparseness problem by introducing a latent variable. However, this problem still causes some errors. Precise statistics cannot be obtained for infrequent words. This problem will be solved by incorporating other resources such as thesaurus or a dictionary, or combining our method with other methods using external wider contexts (Suzuki et al., 2006; Turney, 2002; Baron and Hirst, 2004). 4.3 Examples of Obtained Clusters Next, we qualitatively evaluate the proposed methods. For several clusters z, we extract the words that occur more than twice in the whol"
E06-1026,H05-2017,0,\N,Missing
E09-1089,W04-1013,0,0.687494,"s at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (2007). The ﬁrst one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the beginning part of the document (namely, the ﬁrst 100 words). Each probability is estimated with the maximum likelihood principle. The second one is trained weights. These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training data"
E09-1089,W01-0100,0,0.620623,"andomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords. 1 Introduction Automatic text summarization is one of the tasks that have long been studied in natural language processing. This task is to create a summary, or a short and concise document that describes the content of a given set of documents (Mani, 2001). One well-known approach to text summarization is the extractive method, which selects some linguistic units (e.g., sentences) from given documents in order to generate a summary. The extractive method has an advantage that the grammaticality is guaranteed at least at the level of the linguistic units. Since the actual generation of linguistic expressions has not achieved the level of the practical use, we focus on the extractive method in this paper, especially the method based on the sentence extraction. Most of the extractive summarization methods rely on sequentially solving binary classi"
E09-1089,N07-1056,0,0.0405934,"future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation. We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al. (2007). We also plan to design other decoding algorithms for text summarization (e.g., pipage approach (Ageev and Sviridenko, 2004)). As discussed in Section 6.2, integration with similaritybased models is worth consideration. We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection. Deshpande et al. (2007) proposed a selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation. We plan to reﬁne their model so that it can be applied to general text summarization. Table 7: ROUGE-1 of MCKP-Rel with byte constraints, evaluated without stopwords. Underlined are the values signiﬁcantly different from peer65. greedy g-greedy rand100k stack30 exact exactopt peer65 interpolated train 0.374 (0.1) 0.377 (0.4) 0.371 (0.0) 0.385 (0.2) 0.373 (0.2) 0.366 (0.3) 0.384 (0.1) 0.386 (0.3) 0.383 (0.3) 0.384 (0.4) 0.385 (0.1) 0.384 (0.4"
E09-1089,C04-1057,0,0.873064,"uster. They used the centroid to rank sentences, together with the MMR-like redundancy score. Both relevance and redundancy are taken into consideration, but no global viewpoint is given. In CLASSY, which is the best-performing method in DUC’04, Conroy et al. (2004) scored sentences with the sum of tf-idf scores of words. They also incorporated sentence compression based on syntactic or heuristic rules. McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) ﬁrst formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodiﬁed stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimized"
E09-1089,W00-0405,0,0.544217,"Missing"
E09-1089,hovy-etal-2006-automated,0,0.204041,"nguistics 781 Hatzivassiloglou, 2004), which compose the meaning of a sentence. Sentence si is represented by a set of conceptual units {ei1 , · · · , ei|si |}. For example, the sentence “The man bought a book and read it” could be regarded as consisting of two conceptual units “the man bought a book” and “the man read the book”. It is not easy, however, to determine the appropriate granularity of conceptual units. A simple way would be to regard the above sentence as consisting of four conceptual units “man”, “book”, “buy”, and “read”. There is some work on the deﬁnition of conceptual units. Hovy et al. (2006) proposed to use basic elements, which are dependency subtrees obtained by trimming dependency trees. Although basic elements were proposed for evaluation of summaries, they can probably be used also for summary generation. However, such novel units have not proved to be useful for summary generation. Since we focus more on algorithms and models in this paper, we simply use words as conceptual units. The goal of text summarization is to cover as many conceptual units as possible using only a small number of sentences. In other words, the goal is to ﬁnd a subset S(⊂ D) that covers as many conce"
E09-1089,N03-1020,0,0.0421846,"documents, are given. One summary is to be generated for each cluster. Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words. DUC’03 (2003) dataset was used as the training dataset for trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with signiﬁcance level 0.05 was used for the signiﬁcance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact). Table 2: ROUGE of MCKP with trained weights. Underlined ROUGE-1 score"
E17-1112,P14-2134,0,0.0302867,"et al. (2016) also used distributed representations for the same purpose and attempted to reveal the statistical laws of meaning change. They compared the following three methods for creating word embedding: positive pointwise mutual information (PPMI), low-dimensional approximation of PPMI obtained through singular value decomposition, and skip-gram with negative sampling. They suggested that the skip-gram with negative sampling is a reasonable choice for studying meaning changes of words. We decided to follow their work and use the skip-gram with negative sampling to create word embeddings. Bamman et al. (2014) used a similar technique to study differences in word meanings ascribed to geographical factors. They succeeded in correctly recognizing some dialects of English within the United States. Kulkarni et al. (2016) also worked on geographic variations in languages. With some modification, the methods used in the literature (Kulkarni et al., 2014; Hamilton et al., 2016) can be applied to loanword analysis. 3 Methodology We use word embeddings to analyze the semantic changes in Japanese loanwords from the corresponding English. Among the methods of analysis, we chose to use the skip-gram with negat"
E17-1112,W98-0906,0,0.0579155,"e original words were introduced. Moreover, some loanwords did not come directly from English, but from words in other languages, which later became English words. Thus, in this paper, the terms semantic change or meaning change cover all of these semantic differences. 2 Related Work Japanese loanwords have attracted much interest from researchers. Many interesting aspects of Japanese loanwords are summarized in a book written by Irwin (2011). In the field of natural language processing, there have been a number of efforts to capture the behavior of Japanese loanwords including the phonology (Blair and Ingram, 1998; Mao and Hulden, 2016) and segmentation of multi-word loanwords (Breen et al., 2012). The rest of this section explains the computational approaches to semantic changes or variations of words. In particular, there are mainly two different phenomena, namely diachronic change and geographical variation. Jatowt and Duh (2014) used conventional distributional representations of words, i.e., bag-of-context-words, calculated from Google Book (Michel et al., 2011)3 to analyze the diachronic meaning changes of words. They also attempted to capture the change in sentiment of words across time. Kulkarn"
E17-1112,U12-1009,0,0.0197366,"glish, but from words in other languages, which later became English words. Thus, in this paper, the terms semantic change or meaning change cover all of these semantic differences. 2 Related Work Japanese loanwords have attracted much interest from researchers. Many interesting aspects of Japanese loanwords are summarized in a book written by Irwin (2011). In the field of natural language processing, there have been a number of efforts to capture the behavior of Japanese loanwords including the phonology (Blair and Ingram, 1998; Mao and Hulden, 2016) and segmentation of multi-word loanwords (Breen et al., 2012). The rest of this section explains the computational approaches to semantic changes or variations of words. In particular, there are mainly two different phenomena, namely diachronic change and geographical variation. Jatowt and Duh (2014) used conventional distributional representations of words, i.e., bag-of-context-words, calculated from Google Book (Michel et al., 2011)3 to analyze the diachronic meaning changes of words. They also attempted to capture the change in sentiment of words across time. Kulkarni et al. (2014) used distributed representations of words (or word embeddings), inste"
E17-1112,E14-1049,0,0.0345742,"of analysis, we chose to use the skip-gram with negative sampling for the reason discussed in Section 2 with reference to Hamilton et al.’s work (2016). First, we create word embeddings for two languages. We then calculate the similarity or dissimilarity between the embedding (or vector) of a word in a language (say, Japanese) and the embedding of a word in another language (say, English). For this purpose, words in the two languages need to be represented in the same vector space with the same coordinates. There are a number of methods for this purpose (Gouws et al., 2015; Zou et al., 2013; Faruqui and Dyer, 2014; Mikolov et al., 2013a). Among them, we choose the simplest and most computationally efficient one proposed by Mikolov et al. (2013a), where it is assumed that embeddings in one language can be mapped into the vector space of another language by means of a linear transformation represented by W . Suppose we are given trained word embeddings of the two languages and a set of seed pairs of embedding vectors {(xi , zi )|1 ≤ i ≤ n}, each of which is a pair of a vector in one language and a vector in the other language that are translation equiva1196 lents of each other. The transformation matrix"
E17-1112,P16-1141,0,0.17341,"sampling. They suggested that the skip-gram with negative sampling is a reasonable choice for studying meaning changes of words. We decided to follow their work and use the skip-gram with negative sampling to create word embeddings. Bamman et al. (2014) used a similar technique to study differences in word meanings ascribed to geographical factors. They succeeded in correctly recognizing some dialects of English within the United States. Kulkarni et al. (2016) also worked on geographic variations in languages. With some modification, the methods used in the literature (Kulkarni et al., 2014; Hamilton et al., 2016) can be applied to loanword analysis. 3 Methodology We use word embeddings to analyze the semantic changes in Japanese loanwords from the corresponding English. Among the methods of analysis, we chose to use the skip-gram with negative sampling for the reason discussed in Section 2 with reference to Hamilton et al.’s work (2016). First, we create word embeddings for two languages. We then calculate the similarity or dissimilarity between the embedding (or vector) of a word in a language (say, Japanese) and the embedding of a word in another language (say, English). For this purpose, words in t"
E17-1112,W04-3230,0,0.148914,"Missing"
E17-1112,C16-1081,0,0.024566,"troduced. Moreover, some loanwords did not come directly from English, but from words in other languages, which later became English words. Thus, in this paper, the terms semantic change or meaning change cover all of these semantic differences. 2 Related Work Japanese loanwords have attracted much interest from researchers. Many interesting aspects of Japanese loanwords are summarized in a book written by Irwin (2011). In the field of natural language processing, there have been a number of efforts to capture the behavior of Japanese loanwords including the phonology (Blair and Ingram, 1998; Mao and Hulden, 2016) and segmentation of multi-word loanwords (Breen et al., 2012). The rest of this section explains the computational approaches to semantic changes or variations of words. In particular, there are mainly two different phenomena, namely diachronic change and geographical variation. Jatowt and Duh (2014) used conventional distributional representations of words, i.e., bag-of-context-words, calculated from Google Book (Michel et al., 2011)3 to analyze the diachronic meaning changes of words. They also attempted to capture the change in sentiment of words across time. Kulkarni et al. (2014) used di"
E17-1112,I11-1017,0,0.0119276,"e probably due to their infrequency in either the English or the Japanese corpora used for training. 4.6 Evaluation for Educational Use To see if the obtained word embeddings of English and Japanese can assist in language learn16 In Japanese, present usually means a gift, or to give a gift, but hardly to show or introduce. 17 In Japanese, this word usually means to match one’s clothes attractively. 1201 ing, for purposes such as lexical-choice error correction, we evaluate their usefulness by using the writings of Japanese learners of English. Specifically, we use the Lang-8 English data set (Mizumoto et al., 2011)18 to calculate the Dice coefficient instead of JENAAD. This dataset consists of sentences originally written by learners, some of which have been corrected by (presumably) native speakers of English. Because we target embeddings of English and Japanese, we only use English sentences written by Japanese among other learners of English. Of those, approximately one million sentences have corresponding corrections. With these sentence pairs, we calculate the Dice coefficient just as in Section 4.2. The coefficient measures how often a word co-occurs in the original sentences and corresponding cor"
E17-1112,J96-1001,0,0.280173,"and dimeng are respectively the dimensions of the Japanese and English word embeddings; i.e., the dimjpn -dimensional space is mapped to the dimeng -dimensional space. All the coefficients are statistically significant (significance level 0.01). where P (wjpn , weng ) is the probability that this word pair appears in the same sentence pair, and P (wjpn ) and P (weng ) are the generative probabilities of wjpn and weng . All the probabilities were obtained using the maximum likelihood estimation. The Dice coefficient is a measure of coocurrence and can be used to extract translate equivalents (Smadja et al., 1996). If the Dice coefficient of a word pair is low, the words in the pair are unlikely to be translation equivalents of each other. Therefore, if the meaning of a loanword has changed from the original English word, its Dice coefficient should be low. In other words, the cosine similarity should be correlated to the Dice coefficient if the cosine similarity is a good indicator of meaning change. We thus calculate the Pearson’s correlation coefficient between the two. In addition, we calculate the Spearman’s rank-order correlation coefficient to examine the relation of the orders given by the Dice"
E17-1112,P03-1010,0,0.0451319,"the English vector space, we calculate the cosine similarity between each Japanese loanword and its original English word. If the cosine similarity is low for a pair of words, the meaning of the Japanese loanword is different from that of its original English word. 4 Empirical Evaluations 4.2 Evaluation through Correlation To see if the differences in word embeddings are related to the meaning changes of loanwords, we calculate an evaluation measure indicating the global trend. We first extracted one-to-one translation sentence pairs from Japanese-English News Article Alignment Data (JENAAD) (Utiyama and Isahara, 2003). We then use this set of sentence pairs to calculate the Dice coefficient for each pair of a loanword wjpn and its original English word weng , which is defined as 2 × P (wjpn , weng ) , P (wjpn ) + P (weng ) 5 Since it is generally difficult to evaluate methods for capturing semantic changes in words, we conduct a number of quantitative and qualitative evaluations from different viewpoints. 4.1 Data and Experimental Settings The word embeddings of English and Japanese were obtained via the skip-gram with negative sampling (Mikolov et al., 2013b)4 with different dimensions as shown in the res"
E17-1112,D13-1141,0,0.0384264,"Among the methods of analysis, we chose to use the skip-gram with negative sampling for the reason discussed in Section 2 with reference to Hamilton et al.’s work (2016). First, we create word embeddings for two languages. We then calculate the similarity or dissimilarity between the embedding (or vector) of a word in a language (say, Japanese) and the embedding of a word in another language (say, English). For this purpose, words in the two languages need to be represented in the same vector space with the same coordinates. There are a number of methods for this purpose (Gouws et al., 2015; Zou et al., 2013; Faruqui and Dyer, 2014; Mikolov et al., 2013a). Among them, we choose the simplest and most computationally efficient one proposed by Mikolov et al. (2013a), where it is assumed that embeddings in one language can be mapped into the vector space of another language by means of a linear transformation represented by W . Suppose we are given trained word embeddings of the two languages and a set of seed pairs of embedding vectors {(xi , zi )|1 ≤ i ≤ n}, each of which is a pair of a vector in one language and a vector in the other language that are translation equiva1196 lents of each other. Th"
I05-1038,P01-1070,0,0.0176595,"roughly divided into two groups: the ones based on hand-crafted rules and the ones based on machine learning. The system “SAIQA” [1], Xu et al. [2] used hand-crafted rules for question classiﬁcation. However, methods based on pattern matching have the following two drawbacks: high cost of making rules or patterns by hand and low coverage. Machine learning can be considered to solve these problems. Li et al. [3] used SNoW for question classiﬁcation. The SNoW is a multi-class classiﬁer that is speciﬁcally tailored for learning in the presence of a very large number of features. Zukerman et al. [4] used decision tree. Ittycheriah et al. [5] used maximum entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared question classiﬁcation using machine learning methods (decision tree, maximum entropy, SVM) with a rule-based method. The result showed that the accuracy of question classiﬁcation with SVM is the highest of all. According to Suzuki [6], a lot of information is needed to improve the accuracy of question classiﬁcation and SVM is suitable for question classiﬁcation, because SVM can classify questions with high accuracy even when the dimension of the feature space i"
I05-1038,N01-1005,0,0.0709723,"based on hand-crafted rules and the ones based on machine learning. The system “SAIQA” [1], Xu et al. [2] used hand-crafted rules for question classiﬁcation. However, methods based on pattern matching have the following two drawbacks: high cost of making rules or patterns by hand and low coverage. Machine learning can be considered to solve these problems. Li et al. [3] used SNoW for question classiﬁcation. The SNoW is a multi-class classiﬁer that is speciﬁcally tailored for learning in the presence of a very large number of features. Zukerman et al. [4] used decision tree. Ittycheriah et al. [5] used maximum entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared question classiﬁcation using machine learning methods (decision tree, maximum entropy, SVM) with a rule-based method. The result showed that the accuracy of question classiﬁcation with SVM is the highest of all. According to Suzuki [6], a lot of information is needed to improve the accuracy of question classiﬁcation and SVM is suitable for question classiﬁcation, because SVM can classify questions with high accuracy even when the dimension of the feature space is large. Moreover, Zhang et al. [7] compare"
I05-1038,C02-1150,0,\N,Missing
I08-1019,W99-0625,0,0.0190509,"From the table, we can see a large difference in distributions of EQ and no-relation pairs. This difference suggests that the clusterwise classification approach is reasonable. We split the dataset into three clusters: highsimilarity cluster, intermediate-similarity cluster, and low-similarity cluster. Intuitively, we expected that a pair in the high-similarity cluster would have many common bigrams, that a pair in the intermediate-similarity cluster would have many common unigrams but few common bigrams, and that a pair in the low-similarity cluster would have few common unigrams or bigrams. Hatzivassiloglou et al. (1999; 2001) proposed a method based on supervised machine learning to identify whether two paragraphs contain similar information. However, we found it was difficult to accurately identify EQ pairs between two sentences simply by using similarities as features. Zhang et al. (2003) presented a method of classifying CST relations between sentence pairs. However, their method used the same features for every type of CST, resulting in low recall and precision. We thus select better features for each CST type, and for each cluster of EQ. The EQ identification task is apparently related to Textual Entai"
I08-1019,P02-1047,0,0.043184,"and then construct a classifier for each cluster to identify equivalence relations. We also adopt a “coarse-to-fine” approach. We further propose using the identified equivalence relations to address the task of identifying transition relations. 1 Introduction A document generally consists of semantic units called sentences and various relations hold between them. The analysis of the structure of a document by identifying the relations between sentences is called discourse analysis. The discourse structure of one document has been the target of the traditional discourse analysis (Marcu, 2000; Marcu and Echihabi, 2002; Yokoyama et al., 2003), based on rhetorical structure theory (RST) (Mann and Thompson, 1987). § Yasunari Miyabe currently works at Toshiba Solutions Corporation. 141 1. ABC telephone company announced on the 9th that the number of users of its mobile-phone service had reached one million. Users can access the Internet, reserve train tickets, as well as make phone calls through this service. 2. ABC said on the 18th that the number of users of its mobile-phone service had reached 1,500,000. This service includes Internet access, and enables train-ticket reservations and telephone calls. The pa"
I08-1019,J00-3005,0,0.0176184,"imilarities, and then construct a classifier for each cluster to identify equivalence relations. We also adopt a “coarse-to-fine” approach. We further propose using the identified equivalence relations to address the task of identifying transition relations. 1 Introduction A document generally consists of semantic units called sentences and various relations hold between them. The analysis of the structure of a document by identifying the relations between sentences is called discourse analysis. The discourse structure of one document has been the target of the traditional discourse analysis (Marcu, 2000; Marcu and Echihabi, 2002; Yokoyama et al., 2003), based on rhetorical structure theory (RST) (Mann and Thompson, 1987). § Yasunari Miyabe currently works at Toshiba Solutions Corporation. 141 1. ABC telephone company announced on the 9th that the number of users of its mobile-phone service had reached one million. Users can access the Internet, reserve train tickets, as well as make phone calls through this service. 2. ABC said on the 18th that the number of users of its mobile-phone service had reached 1,500,000. This service includes Internet access, and enables train-ticket reservations a"
I08-1019,W03-0507,1,0.833972,"th sentences have a verb or not. The head verbs are extracted using rules proposed by Hatayama (2001). 3. Salient words: This feature indicates whether the salient words of the two sentences are the same or not. We approximate the salient word with the gaor the wa-case word that appears first. 4. Numeric expressions and units (Nanba et al., 2005): The first feature indicates whether the two sentences share a numeric expression or not. The second feature is similarly defined for numeric units. 4 Experiments on identifying EQ pairs We used the Text Summarization Challenge (TSC) 2 and 3 corpora (Okumura et al., 2003) and the Workshop on Multimodal Summarization for Trend Information (Must) corpus (Kato et al., 2005). These two corpora contained 115 sets of related news articles (10 documents per set on average) on various events. A document contained 9.9 sentences on average. Etoh et al. (2005) annotated these two corpora with CST types. There were 471,586 pairs of sentences and 798 pairs of these had EQ. We conducted the experiments with 10-fold cross-validation (i.e., approximately 425,000 pairs on average, out of which approximately 700 pairs are in EQ, are in the training dataset for each fold). The a"
I08-1019,W00-1009,0,0.749983,"Missing"
I08-1039,C04-1121,0,0.00875875,"tion 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which f"
I08-1039,W04-3239,0,0.0262567,"ction 2, we briefly present the related work. In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). The"
I08-1039,P07-1055,0,0.0758729,"n included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which focus on multiple levels in text. Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields. Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents. McDonald et al. (2007) proposed a model for classifying sentences and documents simultaneously. They experimented with joint classification of subjectivity for sentence-level, 297 and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model. Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours. However, all the above methods require annotated corpora for all levels, such as both subjectivity for sentences and sentiments for documents, which are fairly expensive to"
I08-1039,W04-3253,0,0.100844,"t the related work. In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces o"
I08-1039,W02-1011,0,0.0165792,"Missing"
I08-1039,P04-1035,0,0.0690942,"al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which focus on multiple levels in text. Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields. Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents. McDonald et al. (2007) proposed a model for classifying sentences and documents simultaneously. They experimented with joint classification of subjectivity for sentence-level, 297 and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model. Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours. However, all the above methods require annotated corpora f"
I08-1039,P05-1015,0,0.0313786,"be seen as an instance of convolution kernels, which was proposed by Haussler (1999). Convolution kernels are a general class of kernel functions which are calculated on the basis of kernels between substructures of inputs. Our proposed kernel treats sentences as input, and treats sentiment words as substructures of sentences. We can use high degree polynomial kernels as both K which is a kernel between substructures, i.e. sentiment words, of sentences, and K 0 which is a kernel between sentences to make the 300 We used two datasets, customer reviews 1 (Hu and Liu, 2004) and movie reviews 2 (Pang and Lee, 2005) to evaluate sentiment classification of sentences. Both of these two datasets are often used for evaluation in sentiment analysis researches. The number of examples and other statistics of the datasets are shown in Table 1. Our method cannot be applied to sentences which contain no sentiment words. We therefore eliminated such sentences from the datasets. “Available” in Table 1 means the number of examples to which our method can be applied. “Sentiment Words” shows the number of sentiment words that are found in the given sentences. Please remember that sentiment words are defined as those wo"
I11-1093,W95-0107,0,0.0696003,"eal of memory. Therefore, we limit the maximum length of the word-chunks. We use word chunks consisting of up to five or ten words. 2 3.3 Features The features used in our experiment are shown in Table 1. As features for Linear-Chain perceptron, we used the following. Here, k denotes the current word position. wk is the k-th word, and pk is the Part-Of-Speech (POS) tag of k-th word. We used the word and the POS of the k-th word and the words in 2-word windows before and after the k-th word with the current NE-tag tk and the NE tag tk−1 of the previous word. Each NE tag is represented as IOB1 (Ramshaw and Marcus, 1995). This representation uses three tags I, O and B, to represent the inside, outside and beginning of a chunk. B is only used at the beginning of a chunk which immediately follows another chunk that NE class is the same. Each tag is expressed with NE classes, like I-CL, B-CL, where CL is an NE class.4 To realize a fast training speed for Linear-Chain, we only used the valid combination of tk and tk−1 in terms of the chunk representation. • NE Chunking and Classification (NECC, for short) (Carreras et al., 2002): This method consists of two parts. The first part is a base NE recognition as in our"
I11-1093,J95-4004,0,0.415026,"Missing"
I11-1093,W02-2004,0,0.554288,"Missing"
I11-1093,sekine-etal-2002-extended,0,0.707669,"ere L is the upper bound length of the entities. The computational cost might not be a big problem, when we use these learning algorithms to recognize a small number of types of NEs, such as the seven types in MUC (Grishman and Sundheim, 1996), the eight types in IREX (Committee, 1999), and the four types in the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003). However, the computational cost will be higher than ever, when we recognize a large types of classes like Sekine’s extended NE hierarchy that includes about 200 types of NEs for covering several types of needs of IE, QA, and IR (Sekine et al., 2002). This paper proposes a word-chunk-based NE recognition method for creating fast NE recogWe propose a Named Entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. We can obtain features from word chunks, such as the first word of a word chunk and the last word of a word chunk, which cannot be obtained in word-sequence-based recognition methods. However, each word chunk may include a part of an NE or multiple NEs. To solve this problem, we use the following operators: SHIFT for separating the first word from a word chunk, POP for separating the last word"
I11-1093,W02-1001,0,0.682933,"web page is http://chasen-legacy.sourceforge.jp/. Words may include partial NEs because words segmented with ChaSen do not always correspond with NE boundaries. If such problems occur when we segment the training data, we annotated a word chunk with the type of the NE included in the word chunk. We did not deal with the difference between NE boundaries and word boundaries in this experiment. 831 3.2 Algorithms to be Compared We use the multiclass perceptron algorithm for NECC, SR and SPJR. Thus all of the algorithms are based on perceptron (Rosenblatt, 1958). We apply the averaged perceptron (Collins, 2002a) for all the training algorithms. All the learners and NE recognizers were implemented with C + +. We used perceptron-based algorithms because perceptron-based algorithms usually show the faster training speed and lower usage of memory than training algorithms, such as MEMM (McCallum et al., 2000), CRFs (Lafferty et al., 2001), and so on. Actually, when we applied a CRFs implementation based on LBFGS (Liu and Nocedal, 1989) to the training data, the implementation consumed 72GB memory which is our machine memory size. We select the number of the iteration that shows the highest F-measure in"
I11-1093,N03-1028,0,0.305914,"Missing"
I11-1093,C96-1079,0,0.131875,"here K is the number of types of classes and N is the length of the sentence. Semi-Markovbased algorithms, such as semi-Markov perceptron and semi-Markov CRFs, enumerate NE candidates represented by word chunks in advance for capturing features such as the first word of a chunk and the last word of a chunk. Therefore, the computational cost of a semi-Markov perceptron is O(KLN ), where L is the upper bound length of the entities. The computational cost might not be a big problem, when we use these learning algorithms to recognize a small number of types of NEs, such as the seven types in MUC (Grishman and Sundheim, 1996), the eight types in IREX (Committee, 1999), and the four types in the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003). However, the computational cost will be higher than ever, when we recognize a large types of classes like Sekine’s extended NE hierarchy that includes about 200 types of NEs for covering several types of needs of IE, QA, and IR (Sekine et al., 2002). This paper proposes a word-chunk-based NE recognition method for creating fast NE recogWe propose a Named Entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. We can obtain featu"
I11-1093,E99-1023,0,0.0583844,", for short) (Yamada, 2007): This algorithm is based on shift-reduce parsing for wordsequences. It uses two operators. The first one is shift which concatenates a word and its following word chunk. The other is reduce for annotating an NE label to current word chunk.3 The algorithm is different from ours in that the initial inputs of their method are word sequences. Thus, each word chunk is constructed little by little. Therefore, the algorithm cannot use features obtained from word chunks at the early stage. 4 We compared five types of chunk representation: IOB1 , IOB2, IOE1, IOE2 (Tjong Kim Sang and Veenstra, 1999) and Start/End (SE) (Uchimoto et al., 2000) in terms of the number of the NE tags. The number of the NE tags for each representation is as follows; IOB1 is 202, IOB2 is 377, IOE1 is 202, IOE2 is 377, and SE is 730. This experiment uses IOB1 because IOB1 has one of the lowest number of NE tags. The number of NE tags is related to the training speed of Linear-Chain. Actually, Linear-Chain using IOB1-based training data was about 2.4 times faster than Linear-Chain using SE-based training data in our pilot study with small training data. 2 This is because when we ran Semi-Markov without the chunk"
I11-1093,W02-2024,0,0.0377725,"th NE classes, like I-CL, B-CL, where CL is an NE class.4 To realize a fast training speed for Linear-Chain, we only used the valid combination of tk and tk−1 in terms of the chunk representation. • NE Chunking and Classification (NECC, for short) (Carreras et al., 2002): This method consists of two parts. The first part is a base NE recognition as in our method. The second part is NE classification. Unlike in our method, this method just classifies given word chunks without decomposing and concatenating them. This method was used in the best system of the shared task of CoNLL 2002 (Tjong Kim Sang, 2002). • Shift-Reduce Parser for NE Recognition (SR, for short) (Yamada, 2007): This algorithm is based on shift-reduce parsing for wordsequences. It uses two operators. The first one is shift which concatenates a word and its following word chunk. The other is reduce for annotating an NE label to current word chunk.3 The algorithm is different from ours in that the initial inputs of their method are word sequences. Thus, each word chunk is constructed little by little. Therefore, the algorithm cannot use features obtained from word chunks at the early stage. 4 We compared five types of chunk repre"
I11-1093,P10-1110,0,0.0604148,"Missing"
I11-1093,P00-1042,0,0.106228,"Missing"
I11-1093,P08-1067,0,0.0770767,"Missing"
I11-1093,N10-1069,0,0.050053,"Missing"
I11-1093,P06-1059,0,0.423553,"Missing"
I11-1158,P08-2008,0,0.030755,"ense disambiguation is not novel, and dates back to 90’s. Fujii et al. (1998) proposed a method for the verb sense disambiguation, which is based on the k-nearest neighbors. In their method, each instance is represented as a case frame and the similarity of two instances is calculated as a weighted sum of the similarities of case ﬁllers. Fujii et al. also proposed a framework of active learning. To disambiguate verb senses, Chen and Palmer (2009) proposed to use linguistic and semantic features including the voice of the given sentence, the presence of a PP adjunct, and the named entity tags. Dligach and Palmer (2008) proposed to use co-occurrence with other verbs as features. Wagner et al. (2009) proposed to use verb clusters generated with statistics on verb subcategorization. 1382 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1382–1386, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 3 Potts Model We introduce the probability model that we will use for our task. This model gives the probability distribution over a set of nodes associated with random variables, where some pairs of variables are dependent on each other. If the variables can have more"
I11-1158,I08-7018,0,0.0323502,"oach to construct a classiﬁer with those various clues as features. Support vector machines are employed in this work, although other classiﬁers are also applicable. Note that even if a case ﬁller in the test instance did not appear in the training data, the feature ρi (c) corresponding to this case ﬁller conveys the information on the selectional preference of the verb against the case ﬁller. 5 Experiments 5.1 Experimental Settings The proposed method for verb sense disambiguation is evaluated on the white paper part of BCCWJ corpus, the ﬁrst balanced corpus of contemporary written Japanese (Maekawa, 2008), which was also used as a test set for SemEval-2 Japanese word sense disambiguation task (Okumura et al., 2010). The dataset used in this research was created by the preliminary annotation for SemEval2. The senses are deﬁned in the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary. Among three levels of sense IDs deﬁned in this dictionary, the middle-level sense was used in the empirical evaluation, which is the same level of senses used in the SemEval-2. From the dataset above, we selected most ambiguous 14 verbs whose empirical sense distributions (i.e., estimated with the m"
I11-1158,S10-1012,1,0.847663,"in this work, although other classiﬁers are also applicable. Note that even if a case ﬁller in the test instance did not appear in the training data, the feature ρi (c) corresponding to this case ﬁller conveys the information on the selectional preference of the verb against the case ﬁller. 5 Experiments 5.1 Experimental Settings The proposed method for verb sense disambiguation is evaluated on the white paper part of BCCWJ corpus, the ﬁrst balanced corpus of contemporary written Japanese (Maekawa, 2008), which was also used as a test set for SemEval-2 Japanese word sense disambiguation task (Okumura et al., 2010). The dataset used in this research was created by the preliminary annotation for SemEval2. The senses are deﬁned in the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary. Among three levels of sense IDs deﬁned in this dictionary, the middle-level sense was used in the empirical evaluation, which is the same level of senses used in the SemEval-2. From the dataset above, we selected most ambiguous 14 verbs whose empirical sense distributions (i.e., estimated with the maximum likelihood principle) have a high entropy and appear more than 100 times in the dataset. The statistics o"
I11-1158,P05-1017,1,0.914621,"orized probability function is called variational free energy: F (c) = ∑ c = −α −β − i ij i ci n exp(αδ(n, ai ) + β j wij ρj (n)) . (3) The ﬁxed point equation for i ∈ / L can be obtained by removing αδ(c, ai ) from above. This ﬁxed point equation is solved by an iterative computation. In the actual implementation, we represent ρi with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996). Details on the Potts model and its computation can be found in the literature (Nishimori, 2001). 4 Proposed Method 4.1 Construction of Lexical Networks We follow the work by Takamura et al. (2005) to construct a lexical network. We link two words if one word appears in the gloss of the other. Each link belongs to one of two groups: the sameorientation links SL and the different-orientation links DL. If a negation word appears in the gloss of an entry word, the words after the negation word are linked to the entry word with DL. Otherwise, those words are linked with SL. In case of Japanese, the auxiliaries “nai” and “nu” are regarded as negation words. We next set weights W = (wij ) to links : wij =  √ 1   d(i)d(j)     (lij ∈ SL) −√ 1 d(i)d(j) (lij ∈ DL) , (4) 0 otherwise ρi (ci"
I11-1158,N07-1037,1,0.816547,"g instances are used as observed variables, with their index set being L in Equation (2). Pi (c) is estimated for each case. In our experiments on Japanese, surface cases are employed: wo (accusative), ga (nominative), ni (dative/locative), de (locative/instrumental), no (genitive/others), e (locative/illative), to (comitative), kara (elative), yori (comparative), made (terminative). Although our model is also applicable to deep cases, we focus on surface cases since deep case recognition itself is a challenging task. 4.3 Estimation of β In some pieces of previous work (Takamura et al., 2005; Takamura et al., 2007), it has been shown that the optimal β can be obtained by estimating the critical temperature, at which phase transition occurs from paramagnetic phase (variables are randomly oriented) to ferromagnetic phase (most of the variables have the same value). We follow these pieces of previous work. In practice, when the maximum of the spatial averages of the approximated probabilities ∑ maxc i ρi (c)/N exceeds a threshold during increasing β, we consider that the phase transition has occurred. We select the value of β slightly before the phase transition. 4.4 Discriminative Training Probability ρi"
I11-1158,J98-4002,0,\N,Missing
I11-1158,W09-3401,0,\N,Missing
I13-1078,W10-3208,1,0.846591,"on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, Nagoya, Japan, 14-18 October 2013. lized in several applications such as sentiment analysis, opinion mining and emotion analysis. Undoubtedly, manual compilation is the best way to create such an emotion lexicon but is much expensive in terms of time and hum"
I13-1078,esuli-sebastiani-2006-sentiwordnet,0,0.13151,"Missing"
I13-1078,strapparava-valitutti-2004-wordnet,0,0.533599,"Missing"
I13-1078,P07-2034,0,0.0562995,"Missing"
I13-1078,P05-1017,1,0.957128,"ional classes like happy, sad, fear, anger, surprise and disgust. In the previous example, frequent appearance of words from the happy class in a blog document would imply that the writer of the comment is quite happy with the new rule proposed by the Government. Several works have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly availabl"
I13-1078,N07-1037,1,0.824129,"ives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. ferent values of β. Then the accuracies were computed manually as well as using the WordNet Affect lexicon. We also classified the words into two classes, i.e. positive and negative. The accuracies of two classes were calculated using the SentiWordNet. 4.2 5.1 Classification of Words Takamura et al., (2007) used the Potts model for extracting semantic orientation of phrases (pair of adjective and a noun): positive, negative or neutral. In contrast to that, we have used the Potts model for identifying the emotional class (es) of a word. We have used one seed word from each class to start with the experiment. Each seed word is assigned a class manually. We therefore estimate the state of nodes in the lexical network for each class of emotions. The only drawback is that, it could not assign any emotional class to a word which is not present in the lexical network. These words may be referred to as"
I13-1078,J11-2001,0,0.100786,"Missing"
I13-1078,baccianella-etal-2010-sentiwordnet,0,0.423122,"Missing"
I13-1078,H05-1044,0,0.0227693,"ks have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, N"
I13-1078,P97-1023,0,0.0600689,"ork is shown in Table 1. Next, we assign weights W = (wij) to links as follows: By minimizing F(n) under the condition that ( ) ,∑ , we obtain the following fixed point equation for : ( ) Potts Model for Construction of Emotional Lexicon No. of words 20497 3751 55285 8482 Table 1. Statistics of Lexical network We have also constructed another network, the gloss thesaurus network (GT), by linking syno676 nyms, antonyms and hypernyms, in addition to the above linked words. Only antonym links are in DL. We enhanced the gloss-thesaurus network with co-occurrence information extracted from corpus. Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation. Following their method, we connect two adjectives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. fere"
I13-1078,W11-1710,1,0.822398,"Missing"
I13-1078,taboada-etal-2006-methods,0,\N,Missing
I17-1080,P13-1099,0,0.0270186,"heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015"
I17-1080,W04-1013,0,0.0346261,"ngs of the The 8th International Joint Conference on Natural Language Processing, pages 792–800, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a question as an input. We therefore developed a number of methods designed for question summarization: extractive methods based on simple heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al"
I17-1080,D15-1166,0,0.131748,"Missing"
I17-1080,W14-4318,0,0.0253473,"d to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is important in finding th"
I17-1080,W14-4407,0,0.0312026,"highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Confer"
I17-1080,P16-1172,0,0.0216194,"tractive methods based on simple heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks ("
I17-1080,W03-0501,0,0.0909238,"Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for"
I17-1080,D15-1044,0,0.067868,"rao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question a"
I17-1080,W12-1513,0,0.295908,"ctively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is"
I17-1080,P16-1154,0,0.342455,"tractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Ques"
I17-1080,I05-1038,1,0.821674,"zation is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is important in finding the class of this question, which is a yes/no question, but is not appropriate as a summary, because it is impossible to understand what is being asked mer"
I17-1080,C02-1053,0,0.0734977,"of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 201"
I17-1080,P82-1020,0,0.846825,"Missing"
I17-1080,D16-1140,1,0.838233,"hen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a relat"
I17-1080,P00-1041,0,\N,Missing
I17-1080,D10-1050,0,\N,Missing
I17-2002,2016.amta-researchers.10,0,0.0160326,"end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the attention mechanism has"
I17-2002,P97-1003,0,0.247328,"nd Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2S"
I17-2002,D16-1001,0,0.0389053,"data into three parts: The Wall Street Journal (WSJ) sections 02-21 for training, section 22 for development and section 23 for testing. In our models, the dimensions of the input word embeddings, the fed label embeddings, the hidden layers, and an attention vector were respectively set to 150, 30, 200, and 200. The LSTM depth was set to 3. Label set Lcon had a size of 61. The input vocabulary size of PTB was set to 42393. Supervised attention rate λ was set to 1.0. To use entire words as a vocabulary, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate 0.8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997),"
I17-2002,P15-1030,0,0.0325395,"rd LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships along dependency arcs. The f"
I17-2002,C96-1058,0,0.0611496,".8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq"
I17-2002,P14-1022,0,0.0132644,"layer stacked forward LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships al"
I17-2002,P15-1162,0,0.0359884,"Missing"
I17-2002,C16-1291,0,0.145898,"and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the"
I17-2002,D15-1166,0,0.106565,"odel and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention itself did not improve the parsing performances. Furthermore, each AER"
I17-2002,D16-1249,0,0.0316216,"utput tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal an"
I17-2002,J03-1002,0,0.00594689,"size was set to ten. The decoding was performed on both a single model and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention it"
I17-2002,D15-1044,0,0.0712984,"rpus showed that the bracketing F-measure was improved by supervised attention. 1 (NP )S )NP (VP XX XX XX the chef cooks )VP (NP )NP XX XX the soup Figure 1: S-expression format for Vinyals et al. (2015)’s Seq2seq constituency parser. The Seq2seq model employs “&lt;s&gt; (S (NP XX XX )NP (VP XX (NP XX XX )NP )VP )S &lt;/s&gt;” as output tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be a"
I17-2002,W03-3023,0,0.135784,"produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2Seq+span 89.2 89.6 89.2 89.3 88.9 89.4 88.9 89.1 89.1 89.5 89.0 89.2 6.9 1.8 4.7 1.6 89.2 89.4 89.5 89.2 88.1 88.7 88.6 88.4 88.6 89.0 89.1 88.8 6.9 1.7 4.7 1.6 Vinyals et al. (2015) w att† Vinyals et al. (2015) w/o att† Seq2Seq+beam"
K19-1086,D15-1176,0,0.0954788,"Missing"
K19-1086,P16-1100,0,0.0275558,"addition, the improvements can be further obtained when our injection method is used together with the previous methods. We also conducted a comprehensive comparison of these injection methods. Finally, we set up several experiments to check the effects of infrequent words on our model, and we also compared our model with several previous work on 6 common language modeling datasets. Our results show that: network (CNN) to encode characters and then concatenated these encoded character-level representations and word-level representations for part-ofspeech tagging and named entity recognition. Luong and Manning (2016) introduced a characterword neural machine translation model that only consults character-level representations for rare words encoded with a deep LSTM. As research efforts for language models, Kang et al. (2011) used a simple character-word NLM designed for Chinese. Miyamoto and Cho (2016) introduced a gate mechanism between word embeddings and character embeddings obtained from a bidirectional LSTM (BiLSTM) for English. Verwimp et al. (2017) directly concatenated word and character embeddings without other subnetworks to encode the characters for English and Dutch. Although there are a numbe"
K19-1086,N18-1128,0,0.387595,"ethods. - When injecting word-level information into character-aware NLMs, discarding rare words in the training data can help improve the performance. 3 2 Related Work Model Description For language modeling, we basically use a LSTM network (Hochreiter and Schmidhuber, 1997). We denote the hidden state of LSTM for the t-th word wt as ht ∈ Rd , where d is the embedding size. We incorporate word-level information using the neural network shown in Figure 1. We describe the details in the following subsections. Many work have attempted to improve characteraware NLMs in recent years. For example, Assylbekov and Takhanov (2018) proposed several ways of reusing weights in character-aware NLMs. Gerz et al. (2018) achieved an improved result on 50 typologically diverse languages by injecting subword-level information into word vectors at the softmax. For a thorough review of past researches, readers are recommended to read the work by Vania and Lopez (2017), who performed a systematic comparison across different models based on different subword units (characters, character trigrams, BPE, etc.). One direction related to our research is to inject word-level information into character-aware neural models. Aside from lang"
K19-1086,Q17-1010,0,0.657439,"d seen in the training data is less than or equal to θ, we discard it. We refer the model that discards infrequent words as Char-BiLSTMadd-Word-LSTM-Word (g = 0.5, n = 1, θ = 5/15/25). The result is shown in Table 7. 6 Experiments on 6 Common Datasets In addition to the above datasets, we also set up 6 common language modeling datasets: English Penn Treebank (PTB) (Marcus et al., 1993) and 5 non-English datasets with rich morphology from the 2013 ACL Workshop on Machine Translation5 , which have been commonly used for evaluating character-aware NLMs (Botha and Blunsom, 2014; Kim et al., 2016; Bojanowski et al., 2017; Assylbekov and Takhanov, 2018). Since some of previous work has tested their model on PTB, we also included PTB in our experiment. We used the preprocessed small version of nonEnglish datasets by Botha and Blunsom (2014) and followed the same split as the previous work. The data statistics is provided in Table 9. The results of our proposed models and previous work are shown in Table 6. We used CharBiLSTM-LSTM and Char-BiLSTM-add-WordLSTM as baseline models. For our models, we set the frequency threshold θ to 5 and also set n to 2 as these settings help improve our character-aware NLMs, as d"
K19-1086,D16-1209,0,0.0369099,"Missing"
K19-1086,D15-1042,0,0.0705429,"Missing"
K19-1086,W15-3904,0,0.0789621,"Missing"
K19-1086,Q18-1032,0,0.0516269,"Missing"
K19-1086,P17-1184,0,0.26323,"anabu Okumura1 1 Tokyo Institute of Technology 2 National Institute of Advanced Industrial Science and Technology (AIST) {yukun@lr.,kamigaito@lr.,takamura@,oku@}pi.titech.ac.jp Abstract For example, although words “husbandman” and “salesman” share the suffix “man” in their surface forms, standard NLMs cannot capture such information in obtaining the relationship between the two words. A common way to deal with these issues is to use character information of each word to calculate the word representation, and it is often referred to as character-aware NLMs (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017; Gerz et al., 2018). Our research focuses on utilizing advantages of both character-level information and word-level information in characteraware NLMs. Previous work usually combines word-level information and character-level information at the input of LSTM layers through a gating mechanism, or averaging or concatenation of word vectors. Because these approaches generally target at the input vectors, the word-level information cannot be explicitly taken into account at the output layer for predicting the next word. To deal with this problem, we propose an improved character-aware neural lan"
K19-1086,E17-1040,0,0.0450402,"Missing"
L16-1011,P13-3022,0,0.178498,"process can be termed leave-one-language-out. Since features generally have multiple values, each of the features other than the target feature is binarized to make attribute vectors. Each attribute is 1 if the feature of the language is the value associated with the attribute, 0 otherwise. Figure 2: Histogram of the number of languages vs. the number of non-empty features 2.2. Computational analysis on or with WALS Although there is a large amount of literature on the language typology, we name mathematical and computational work with WALS: Daum´e III and Campbell (2007), Daum´e III (2009), Lu (2013), Roy et al. (2014), Murawaki (2015). Daum´e III and Campbell (2007) proposed a probabilistic model of the tendency or universal between linguistic features in WALS, where each feature is associated with a random variable, and the relations between features are captured by the statistical dependency between the random variables. Daum´e III (2009) used a nonparametric bayesian model of linguistic features integrating both geographical and genealogical similarities, and calculated the measure indicating whether each feature value tends to be determined by a geographical reason or a genealogical"
L16-1011,N15-1036,0,0.0890668,"a machine-learning classifier to estimate the value of each feature of each language using the values of the other features, under different choices of training data: all the other languages, or all the other languages except for the ones having the same origin or area with the target language. Keywords: language typology, linguistic feature, WALS, machine learning 1. Introduction In addition to the scientific motivation above, we also have engineering motivations. It is widely known that WALS is sparse; the values of the majority of features are missing (e.g., (Daum´e III and Campbell, 2007; Murawaki, 2015)). Evaluating the values of such features is a laborious task often requiring fieldwork. Our classifier can be used to estimate missing values in the database.2 They may facilitate the statistical analysis on WALS (e.g., Albu (2006)). We also take into account that some features in WALS are dependent on other features in a trivial manner as the order of V and O depends on the order of S, V and O. Such dependent features can obscure the findings pertaining to languages. We propose to remove dependent features from the attribute set for the classifier. We will distribute the resources of depende"
L16-1011,C14-1098,0,0.118373,"n be termed leave-one-language-out. Since features generally have multiple values, each of the features other than the target feature is binarized to make attribute vectors. Each attribute is 1 if the feature of the language is the value associated with the attribute, 0 otherwise. Figure 2: Histogram of the number of languages vs. the number of non-empty features 2.2. Computational analysis on or with WALS Although there is a large amount of literature on the language typology, we name mathematical and computational work with WALS: Daum´e III and Campbell (2007), Daum´e III (2009), Lu (2013), Roy et al. (2014), Murawaki (2015). Daum´e III and Campbell (2007) proposed a probabilistic model of the tendency or universal between linguistic features in WALS, where each feature is associated with a random variable, and the relations between features are captured by the statistical dependency between the random variables. Daum´e III (2009) used a nonparametric bayesian model of linguistic features integrating both geographical and genealogical similarities, and calculated the measure indicating whether each feature value tends to be determined by a geographical reason or a genealogical reason. Lu (2013) f"
N07-1037,P97-1023,0,0.533087,"e the semantic orientations of the adjective-noun pairs. Information from seed words is diffused to unseen nouns on the network. We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classifi"
N07-1037,kamps-etal-2004-using,0,0.1309,"Missing"
N07-1037,W06-1642,0,0.148491,"mantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. 293 Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively re"
N07-1037,P04-3020,0,0.0140355,"Missing"
N07-1037,H05-1052,0,0.0152758,"Missing"
N07-1037,J93-1007,0,0.0254585,"siloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually cr"
N07-1037,P05-1017,1,0.762674,"opose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in t"
N07-1037,E06-1026,1,0.848075,"ation, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method. 1 Introduction Technology for affect analysis of texts has recently gained attention in both academic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. A computational model for the semantic orientations of phrases has been proposed by Takamura et al. (2006). However, their method cannot deal with the words that did not appear in the training data. The purpose of this paper is to propose a method for extracting semantic orientations of phrases, which is applicable also to expressions consisting of unseen words. In our method, we regard this task as the noun classification problem for each adjective; the nouns that become respectively positive (negative, or neutral) when combined with a given adjective are distinguished from the other nouns. We create a lexical network with words being nodes, by connecting two words if one of the two appears in th"
N07-1037,P02-1053,0,0.0201361,"he output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words i"
N07-1037,P06-1134,0,0.0176434,"639/2160) unambiguous ambiguous 94.85 65.25 (736/776) (903/1384) total 69.59 (6086/8746) 79.68 (6969/8746) Table 2: Confusion matrices of classification result with labeled+unlabeled seed set Potts model Gold standard positive neutral negative sum positive 964 198 39 1201 seen nouns neutral negative 254 60 1656 286 397 1544 2307 1890 the difference between them; “high salary” is positive, while “low (cheap) commission” is also positive. 6 sum 1278 2140 1980 5398 positive 126 60 46 232 unseen nouns neutral negative 84 30 427 104 157 350 668 484 sum 240 591 553 1384 ity have strong interaction (Wiebe and Mihalcea, 2006). • The value of α must be properly set, because lower α can be better for the seed words added by the classifier, Conclusion We proposed a method for extracting semantic orientations of phrases (pairs of an adjective and a noun). For each adjective, we constructed a Potts system, which is actually a lexical network extracted from glosses in a dictionary. We empirically showed that the proposed method works well in terms of classification accuracy. Future work includes the following: • We assumed that each word has a semantic orientation. However, word senses and subjectiv298 • To address word"
N07-1037,H05-1044,0,0.0217498,"R good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. 293 Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes. The words that are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerg"
N15-1143,S07-1103,0,0.0166829,"5, 2015. 2015 Association for Computational Linguistics clues and determines the sizes of many physical objects simultaneously. The approach consists of two steps: (i) many different types of clues to the numerical attribute are collected from various linguistics resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tr"
N15-1143,P10-1133,0,0.459083,"resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tried to determine whether a given amount is large, small, or normal as a size of an object, making good use of clue words such as only; The sentence “This laptop weighs only 0.7kg” means that laptops are usually heavier than 0.7kg. 3 Fragmentary clues to sizes 3.1 Physica"
N15-1143,C02-1042,0,0.0462621,"We have therefore developed a mathematical model that uses these 1305 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1305–1310, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics clues and determines the sizes of many physical objects simultaneously. The approach consists of two steps: (i) many different types of clues to the numerical attribute are collected from various linguistics resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects."
N15-1143,P13-1038,0,0.475201,"1), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tried to determine whether a given amount is large, small, or normal as a size of an object, making good use of clue words such as only; The sentence “This laptop weighs only 0.7kg” means that laptops are usually heavier than 0.7kg. 3 Fragmentary clues to sizes 3.1 Physical objects We first collect physical objects, i.e., objects for which the size can be defined. However, the numerical attribute of a word depends on the sense in which the word is being used. We will therefore determine the size of each sense instead of each word. Specifically, we determine the size of each noun synset in the"
N15-1143,W09-3401,0,\N,Missing
N15-1150,D11-1054,0,\N,Missing
N15-1150,W04-3243,0,\N,Missing
N15-1150,C00-2163,0,\N,Missing
N15-1150,J93-2003,0,\N,Missing
N15-1150,J90-2002,0,\N,Missing
N15-1150,P02-1040,0,\N,Missing
N15-1150,P13-1095,0,\N,Missing
N15-1150,P07-2045,0,\N,Missing
N15-1150,N03-1017,0,\N,Missing
N15-1150,2011.iwslt-evaluation.18,0,\N,Missing
N15-1150,P00-1056,0,\N,Missing
N15-1150,W11-2123,0,\N,Missing
P05-1017,J96-1002,0,0.040049,"ground. We have an objective function and its approximation method. We thus have a measure of goodness in model estimation and can use another better approximation method, such as Bethe approximation (Tanaka et al., 2003). The theory tells us which update rule to use. We also have a notion of magnetization, which can be used for hyperparameter estimation. We can use a plenty of knowledge, methods and algorithms developed in the field of statistical mechanics. We can also extend our model to a multiclass model (Q-Ising model). Another interesting point is the relation to maximum entropy model (Berger et al., 1996), which is popular in the natural language processing community. Our model can be obtained by maximizing the entropy of the probability distribution Q(x) under constraints regarding the energy function. 5 Experiments We used glosses, synonyms, antonyms and hypernyms of WordNet (Fellbaum, 1998) to construct an English lexical network. For part-of-speech tagging and lemmatization of glosses, we used TreeTagger (Schmid, 1994). 35 stopwords (quite frequent words such as “be” and “have”) are removed from the lexical network. Negation words include 33 words. In addition to usual negation words such"
P05-1017,H92-1046,0,0.0781048,"ined according to the averages values of the spins. Despite the heuristic flavor of this decision rule, it has a theoretical background related to maximizer of posterior marginal (MPM) estimation, or ‘finite-temperature decoding’ (Iba, 1999; Marroquin, 1985). In MPM, the average is the marginal distribution over xi obtained from the distribution over x. We should note that the finite-temperature decoding is quite different from annealing type algorithms or ‘zero-temperature decoding’, which correspond to maximum a posteriori (MAP) estimation and also often used in natural language processing (Cowie et al., 1992). Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990). Actually, the proposed model can be regarded as a spreading activation model with a specific update rule, as long as we are dealing with 2-class model (2-Ising model). However, there are some advantages in our modelling. The largest advantage is its theoretical background. We have an objective function and its approximation method. We thus have a mea"
P05-1017,P97-1023,0,0.979087,"for extraction of semantic orientations of words. To calculate the association strength of a word with positive (negative) seed words, they used the number of hits returned by a search engine, with a query consisting of the word and one of seed words (e.g., “word NEAR good”, “word NEAR bad”). They regarded the difference of two association strengths as a measure of semantic orientation. They also proposed to use Latent Semantic Analysis to compute the association strength with seed words. An empirical evaluation was conducted on 3596 words extracted from General Inquirer (Stone et al., 1966). Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and 133 Proceedings of the 43rd Annual Meeting of the ACL, pages 133–140, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation. They first classify each conjunctive expression into the same-orientation class or the different-orientation class. They then use the classified expressions to cluster words into the positive class and the negative class. T"
P05-1017,W03-0404,0,0.443684,"Missing"
P05-1017,C90-2067,0,0.0245303,"arroquin, 1985). In MPM, the average is the marginal distribution over xi obtained from the distribution over x. We should note that the finite-temperature decoding is quite different from annealing type algorithms or ‘zero-temperature decoding’, which correspond to maximum a posteriori (MAP) estimation and also often used in natural language processing (Cowie et al., 1992). Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990). Actually, the proposed model can be regarded as a spreading activation model with a specific update rule, as long as we are dealing with 2-class model (2-Ising model). However, there are some advantages in our modelling. The largest advantage is its theoretical background. We have an objective function and its approximation method. We thus have a measure of goodness in model estimation and can use another better approximation method, such as Bethe approximation (Tanaka et al., 2003). The theory tells us which update rule to use. We also have a notion of magnetization, which can be used for h"
P05-1017,kamps-etal-2004-using,0,\N,Missing
P06-1145,W00-0730,0,0.1001,"Missing"
P06-1145,P00-1010,0,0.111276,"Missing"
P06-1145,W01-1311,0,0.0585789,"Missing"
P06-1145,sekine-isahara-2000-irex,0,\N,Missing
P13-1083,D09-1037,0,0.0182123,"ls: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1083,P05-1067,0,0.0375111,"syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they ar"
P13-1083,P07-1035,0,0.105138,"Eiichiro Sumita† , Hiroya Takamura‡ , Manabu Okumura‡ † National Institute of Information and Communications Technology {akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp † Precision and Intelligence Laboratory, Tokyo Institute of Technology {takamura, oku}@pi.titech.ac.jp Abstract [Example 1] 1 あなた は インターネット が 利用 でき ない Japanese POS: noun particle This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. [Example 2] particle noun verb auxiliary verb You can not use the Internet . 私 Japane"
P13-1083,D09-1071,0,0.0564114,"Missing"
P13-1083,P06-1121,0,0.0231334,"guage into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceeding"
P13-1083,I11-1136,0,0.0659352,"Missing"
P13-1083,D11-1125,0,0.0332039,"Missing"
P13-1083,W06-3601,0,0.0610141,"Missing"
P13-1083,P10-1145,0,0.0147666,"d method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the othe"
P13-1083,N03-1017,0,0.00724269,"e included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, th"
P13-1083,C12-1120,0,0.0368962,"Missing"
P13-1083,J03-1002,0,0.0064088,"lish sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the fo"
P13-1083,P03-1021,0,0.159428,"Missing"
P13-1083,W04-3250,0,0.192157,"Missing"
P13-1083,P02-1040,0,0.0860707,"Missing"
P13-1083,W02-2016,0,0.0772343,"Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency str"
P13-1083,P05-1034,0,0.0379419,"between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed withou"
P13-1083,D07-1072,0,0.16601,"nd parameters (e.g., ϕ′k and ϕ′′k ) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities πks and observation ′ distributions (ϕsk , ϕks ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st , and we use the pair (st , zt ) as a state representation. β|γ ∼ GEM(γ), πk |α0 , β ∼ DP(α0 , β), ϕk ∼ H, ϕ′k ∼ H ′ , zt′ |zt ∼ Multinomial(πzt ), xt |zt ∼ F (ϕzt ), x′t |zt ∼ F ′ (ϕ′zt"
P13-1083,W11-2119,0,0.0290777,"Missing"
P13-1083,C04-1090,0,0.0333421,"s represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal"
P13-1083,P06-1077,0,0.0845012,"s in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “"
P13-1083,P08-1066,0,0.025975,"S tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the l"
P13-1083,P09-1063,0,0.0382929,"Missing"
P13-1083,N12-1045,0,0.0450049,"Missing"
P13-1083,D08-1022,0,0.0195747,"ations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual"
P13-1083,P11-1125,1,0.791018,"Missing"
P13-1083,N12-1026,1,0.875093,"Missing"
P13-1083,P08-1064,0,0.0187394,"te two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Asso"
P13-1083,P11-1084,0,0.0126965,"joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851, c Sofia, Bulgaria, August 4"
P13-1083,P11-1087,0,\N,Missing
P13-1083,P07-2045,0,\N,Missing
P13-1101,P11-1049,0,0.399332,"document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 12 (1 − e−1 ). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to"
P13-1101,N06-1023,0,0.0195308,"idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 Lin and Bilmes (2011) Subtree extraction (SbE) Sentence extraction (NC) POURPRE 0.215 0.268 0.278 Precision 0.126 0.238 0.206 Recall 0.201 0.213 0.215 F1 0.135 0.159 0.139 F3 0.174 0.190 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where β is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each"
P13-1101,N10-1134,0,0.345605,"ple, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting"
P13-1101,P11-1052,0,0.206779,"sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem"
P13-1101,P11-2039,1,0.915958,"ompressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompa"
P13-1101,W09-1801,0,0.0282021,"e obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 12 (1 − e−1 ). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed o"
P14-2052,P13-1020,0,0.132407,"Missing"
P14-2052,P13-1101,1,0.60525,"Missing"
P14-2052,P11-1049,0,0.200251,"Missing"
P14-2052,C10-2105,0,0.017709,"n based on RST use EDUs as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese st"
P14-2052,W01-1605,0,0.409306,"Missing"
P14-2052,D13-1156,0,0.0298168,"ta to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and St"
P14-2052,N13-1136,0,0.00537175,"as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted r"
P14-2052,P02-1057,0,0.0824756,"Missing"
P14-2052,P09-1075,0,0.015587,"Missing"
P14-2052,C04-1057,0,0.0689065,"rtant content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; 315 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315–320, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held next month. John was running on a track in the park. ＊ He looks ve"
P14-2052,W08-1105,0,0.109308,"an and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtrees in sentence the relations. Hirao et al. converted RST-DTs compression tasks that compress a single sentence into dependency-based discourse trees (DEP-DTs) with a given compression ratio. However, it is not whose nodes corresponded to EDUs and whose trivial to apply their method to text summarizaedges corresponded to the head modifier relationtion because no compression ratio is given to senships of EDUs. See Hirao et al. for details (Hirao tences. None of these methods use the discourse et al., 2013)"
P14-2052,W09-1802,0,0.0293277,"structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtr"
P14-2052,D13-1158,1,0.609684,"Missing"
P14-2052,W04-1013,0,0.03652,"Missing"
P14-2052,W98-1124,0,0.257936,"ct Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a"
P17-1126,D10-1049,0,0.0239258,"Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 20"
P17-1126,D14-1179,0,0.00917524,"Missing"
P17-1126,P16-1154,0,0.0137433,"t al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations such as subtraction for the third problem because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based compone"
P17-1126,W13-2127,0,0.125326,"core (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b"
P17-1126,P16-1014,0,0.0611281,"generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations such as subtraction for the third problem because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based components for generating stock"
P17-1126,H05-1042,0,0.0145202,"been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents"
P17-1126,N07-1021,0,0.0358973,"ments on the Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generati"
P17-1126,W04-3250,0,0.0108369,"ch size of 100. The dimensions of word embeddings, time embeddings, and hidden states for both the encoder and decoder are set to 128, 64, and 256, respectively. For CNN, we used a single convolutional layer and set the filter size to 3. In the experiments, we conducted three types of evaluation: two for automatic evaluation, and one for human evaluation. For one automatic evaluation, we used BLEU (Papineni et al., 2002) to measure the matching degree between the market comments written by humans as references and output comments generated by our model. We applied paired bootstrap resampling (Koehn, 2004) for a significance test. For the other automatic evaluation metric, we calculate F-measures for time-dependent expressions, using market comments written by humans as references, to investigate whether our model can correctly output timedependent expressions such as “open with” and describe how the price changes compared with the previous period referring to the series of preceding prices such as “continual fall”. Specifically, we calculate F-measures for 13 expressions shown in Figure 3. For the human evaluation, we recruited a specialist in financial engineering as a judge to evaluate the q"
P17-1126,N12-1093,0,0.0138468,"re. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations suc"
P17-1126,D13-1157,0,0.00981197,"Missing"
P17-1126,N16-1086,0,0.0282149,"naee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to sol"
P17-1126,P83-1022,0,0.456216,"because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based components for generating stock reports from a database of daily stock quotes. Although she used several components individually and had to define a number of rules for the generation, our encoder-decoder model can 1375 3 preprocessing 12167.29 12278.83 ... 12451.66 12461.36 lshort hshort encoder xlong 12116.57 12120.94 ... 12145.70 12150.49 concatenation encoder llong hlong (2) Incorporating Time Embedding &lt;s&gt; Generating Market Comments To generate market comments on stock prices, we introduce an encoder-decoder model. Encoderdecoder models have be"
P17-1126,P02-1040,0,0.12253,"network, or a convolutional network is adopted as a basic encoder. In the decoding phase, we feed our model with the delivery time of the market comment to generate the expressions depending on time of day to address the second problem. To address the third problem regarding with numerical values mentioned in the generated text, we allow our model to choose an arithmetic operation such as subtraction or rounding instead of generating a word. The proposed methods are evaluated on the task of generating Japanese market comments on the Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and s"
P17-1126,D16-1128,0,0.0231934,"Missing"
P17-1126,P16-1094,0,0.0224068,"in accordance with price history or the time they are observed. For instance, when the market opens, comments usually mention how much the stock price has increased or decreased compared with the closing price of the previous trading day, as in (1) and (3) in Figure 1. Our model creates vectors called time embedding vectors T on the basis of the time when the comment is delivered (e.g., 9:00 a.m. or 3:00 p.m.). Then a time embedding vector is added to each hidden state s j in decoding so that words are generated depending on time. This mechanism is inspired by speaker embedding introduced by Li et al. (2016). They use an encoder-decoder model for a conversational agent that inherits the characteristics of a speaker, such as his/her manner of speaking. They encode speaker-specific information (e.g., dialect, age, and gender) into speaker embedding vectors and used them in decoding. 3.3 Estimation of Arithmetic Operations Text generation systems based on language models such as RNNLM often generate erroneous words for named entities; that is, they often mention a similar but incorrect entity, e.g., Nissan for Toyota. To overcome this problem, Gulcehre et al. (2016) developed a text generation metho"
P17-1126,P09-1011,0,0.0101461,"Missing"
P17-1126,D09-1042,0,0.0248115,"dence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third"
P17-1126,D15-1044,0,0.0436072,"mponents individually and had to define a number of rules for the generation, our encoder-decoder model can 1375 3 preprocessing 12167.29 12278.83 ... 12451.66 12461.36 lshort hshort encoder xlong 12116.57 12120.94 ... 12145.70 12150.49 concatenation encoder llong hlong (2) Incorporating Time Embedding &lt;s&gt; Generating Market Comments To generate market comments on stock prices, we introduce an encoder-decoder model. Encoderdecoder models have been widely used and proven useful in various tasks of natural language generation such as machine translation (Cho et al., 2014) and text summarization (Rush et al., 2015). Our task is similar to these tasks in that the system takes sequential data and generates text. Therefore, it is natural to use an encoder-decoder model in modeling stock prices. Figure 2 illustrates our model. In describing time-series data, the model is expected to capture various types of change and important values in the given sequence, such as absolute or relative changes and maximum or minimum value, in different time-scales. Moreover, it is necessary to generate time-dependent comments and numerical values that require arithmetic operations for derivation, such as “The closing price"
P17-1126,N07-1022,0,0.0206477,"ly learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as th"
P17-2044,D15-1042,0,0.324961,"by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2044 Figure 1: Dep"
P17-2044,D13-1155,0,0.0721707,"e advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/"
P17-2044,W08-1105,0,0.0582185,"Missing"
P17-2044,C12-1067,0,0.31763,"ant information and grammaticality. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of"
P17-2044,P09-1093,0,0.503233,"ity. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, a"
P17-2044,D16-1140,1,0.93556,"ased on the following three characteristics of the Japanese language: (a) Abbreviation of nouns and nominalization of verbs frequently occur in Japanese. (b) Words that are not verbs can also be the root node especially in headlines. (c) Subjects and objects that can be easily estimated from the context are often omitted. The created training dataset is used to train three models. The first model is the original Filippova et al.’s model, an encoder-decoder model with a long short-term memory (LSTM), which we extend in this paper to make the other two models that can control the output length (Kikuchi et al., 2016), because controlling the output length makes a compressed sentence more informative under the desired length. In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network. 1 Introduction"
P17-2044,W13-3508,0,0.0163057,"tence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network. 1 Introduction Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentenc"
P17-2044,P05-1036,0,0.230908,"ule in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association"
P17-2044,P06-1048,0,\N,Missing
P17-2044,P11-1049,0,\N,Missing
P19-1099,P00-1041,0,0.355572,"Missing"
P19-1099,P18-1063,0,0.0259154,"018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method and Ayana et al.’s: (i) our method uses only the part of the summary generated by a model within the length constraint for calculating the ROUGE score and (ii) it penalizes summaries that exceed the length of the reference regardless of its ROUGE score. 3 Summary Length Controllable Models In this secti"
P19-1099,N16-1012,0,0.174657,"Missing"
P19-1099,W03-0501,0,0.26132,"Missing"
P19-1099,W18-2706,0,0.0799593,", 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method an"
P19-1099,K18-1040,0,0.0630047,"echanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method and Ayana et al.’s: (i) our method uses onl"
P19-1099,P03-1021,0,0.0141561,"y length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training based on a minimum risk training (MRT) under the length constraint. MRT (Och, 2003) is used to optimize a model globally for an arbitrary evaluation metric. It was also applied for optimizing the neural summarization model for headline generation with respect to ROUGE (Ayana et al., 2017), which is based on an overlap of words with reference summaries (Lin, 2004). However, how to use MRT under a length constraint was an open problem; thus we propose a global optimization under length constraint (GOLC) for neural summarization models. We show that neural summarization models trained with GOLC can control the output length better than the existing methods. This is because our"
P19-1099,D15-1044,0,0.179565,"Missing"
P19-1099,D13-1155,0,0.0675463,"Missing"
P19-1099,P17-1099,0,0.603108,"ved post-editing time. 2 Related Work There are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013). Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et"
P19-1099,W08-1105,0,0.0854886,"Missing"
P19-1099,P16-1159,0,0.026186,"used as inputs of a decoder. Thus, the way of decreasing the probability of generating overlength summaries is not trivial. 4.2 Minimum Risk Training In MRT, unlike MLE, the probability of a word at each step is calculated using previously generated words as in the test phase. MRT aims at optimizing a model for an evaluation metric by minimizLM RT (θ) = ∑ ∑ Qθ (y′ |x)∆(y, y′ ), ˜ (x,y)∈D y′ ∈S(x) (2) where Qθ (y′ |x) ∝ pθ (y′ |x)γ . ∆(y, y′ ) is a loss function of the negative ROUGE between a reference summary y and a summary to be evaluated ˜ y′ , γ is a smoothing factor and S(x) = S(x)∪{y} (Shen et al., 2016). S(x) is a set of summaries that can be generated by a model for a given x. Including reference summaries into the set of sampled summaries can increase the probabilities of generating reference summaries, which will be analyzed in Section 6. From Equation (2), we see that the probability of generating a summary is weighted by its ROUGE score. Since MRT optimizes summarization models in terms of a ROUGE score, the length of summaries generated by models depends on the type of a ROUGE score, i.e., summary lengths will be long if we choose ROUGE recall as ∆, while summary lengths will be short"
P19-1099,P16-1154,0,0.016153,"here are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013). Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Sch"
P19-1099,P16-1014,0,0.0452981,"Missing"
P19-1099,D10-1050,0,0.0723689,"Missing"
P19-1099,D16-1140,1,0.890502,"ore, editors have to summarize a source text under a length constraint by reordering and paraphrasing. For summarization, both extractive and abstractive methods have been widely studied. Extractive methods are based on selection of sentences from source texts without using reordering or paraphrasing. In contrast, abstractive methods generate summaries as new sentences. Therefore, abstractive methods can rely on the reordering and paraphrasing required for summary and title generation. However, most abstractive summarization methods are not able to control the summary length. To this problem, Kikuchi et al. (2016) and Liu et al. (2018) proposed abstractive summarization models with a capability of summary length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporati"
P19-1099,P18-1015,0,0.100347,"tly, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen"
P19-1099,W04-1013,0,0.143599,"ect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training based on a minimum risk training (MRT) under the length constraint. MRT (Och, 2003) is used to optimize a model globally for an arbitrary evaluation metric. It was also applied for optimizing the neural summarization model for headline generation with respect to ROUGE (Ayana et al., 2017), which is based on an overlap of words with reference summaries (Lin, 2004). However, how to use MRT under a length constraint was an open problem; thus we propose a global optimization under length constraint (GOLC) for neural summarization models. We show that neural summarization models trained with GOLC can control the output length better than the existing methods. This is because our training procedure makes use of overlength summaries. While the probabilities of generating sum1039 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1039–1048 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computation"
P19-1099,D18-1444,0,0.655386,"rize a source text under a length constraint by reordering and paraphrasing. For summarization, both extractive and abstractive methods have been widely studied. Extractive methods are based on selection of sentences from source texts without using reordering or paraphrasing. In contrast, abstractive methods generate summaries as new sentences. Therefore, abstractive methods can rely on the reordering and paraphrasing required for summary and title generation. However, most abstractive summarization methods are not able to control the summary length. To this problem, Kikuchi et al. (2016) and Liu et al. (2018) proposed abstractive summarization models with a capability of summary length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training bas"
P19-1099,P17-1101,0,0.261948,"Missing"
P19-1202,H05-1042,0,0.183494,"Missing"
P19-1202,P16-1154,0,0.0600758,"ed data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness"
P19-1202,P16-1014,0,0.0614873,"sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries."
P19-1202,D18-1130,0,0.0233321,"that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropr"
P19-1202,D17-1195,0,0.0211576,"rrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through c"
P19-1202,D16-1032,0,0.014764,"that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record."
P19-1202,N16-1099,1,0.856363,"ea is similar to ours in that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salien"
P19-1202,D16-1128,0,0.141635,"Missing"
P19-1202,N18-1204,0,0.0431553,"Missing"
P19-1202,P09-1011,0,0.277927,"Missing"
P19-1202,D15-1166,0,0.0235551,"riptions from structured or non-structured data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metric"
P19-1202,N16-1086,0,0.161392,"Missing"
P19-1202,P02-1040,0,0.103895,"rmation to each of the two stages, we can clearly see which part of the model to which the writer information contributes to. For Puduppully et al. (2019) model, we attach the writer information in the following three ways: 1. concatenating writer embedding w with the input vector for LSTM in the content planning decoder (stage 1); 2. concatenating writer embedding w with the input vector for LSTM in the text generator (stage 2); 3. using both 1 and 2 above. For more details about each decoding stage, readers can refer to Puduppully et al. (2019). 5.3 As evaluation metrics, we use BLEU score (Papineni et al., 2002) and the extractive metrics proposed by Wiseman et al. (2017), i.e., relation generation (RG), content selection (CS), and content ordering (CO) as evaluation metrics. The extractive metrics measure how well the relations extracted from the generated summary match the correct relations6 : 6 5 Our code is available from https://github.com/ aistairc/sports-reporter Evaluation metrics The model for extracting relation tuples was trained on tuples made from the entity (e.g., team name, city name, player name) and attribute value (e.g., “Lakers”, “92”) ex2107 - RG: the ratio of the correct relation"
P19-1202,Q17-1007,0,0.0173455,"phical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries. Puduppully et al. (2019) proposed a two-stage method that first predicts the sequence of data records to be mentioned and then generates a summary condi"
P19-1202,D17-1197,0,0.0124483,"ork for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through careful examination,"
P19-1202,P98-2209,0,0.875231,"Missing"
P19-1635,W14-4012,0,0.0568898,"Missing"
P19-1635,D14-1181,0,0.00509517,"onsidered the magnitude be6308 Magnitude Decimal 1 2 3 4 5 6 &gt;6 Range 0≤m&lt;1 1 ≤ m &lt; 10 10 ≤ m &lt; 102 102 ≤ m &lt; 103 103 ≤ m &lt; 104 104 ≤ m &lt; 105 105 ≤ m &lt; 106 106 ≤ m Ratio 23.24 37.53 25.36 12.21 1.12 0.29 0.23 0.01 Table 3: Distribution of numerals in the dataset. Model LR CNN GRU BiGRU CRNN CNN-capsule GRU-capsule BiGRU-capsule fore the decimal point, i.e., 10.08 was classified as a 2nd magnitude. Finally, we separate the dataset into training set and test set of sizes 500k and 100k, respectively. 4 4.1 Empirical Study Models We adopt seven different architectures for our task, including CNN (Kim, 2014), GRU (Cho et al., 2014), BiGRU, CRNN (Choi et al., 2017), CNNcapsule (Sabour et al., 2017), GRU-capsule, and BiGRU-capsule (Wang et al., 2018b). In our models, each word in the input sentence is represented as a d-dimensional vector with word embeddings, and all the words are concatenated in as a d × l matrix, where l denotes the sentence length. Some preprocessing was performed on the data. We transformed all characters to lowercase. The sentence representation was padded to the maximum length of an instance. The target numeral to be inferred is replaced with a special token &lt;TRT&gt;. Appendice"
P19-1635,P18-1196,0,0.112945,"Missing"
R09-1052,P04-1085,0,0.235831,"Missing"
R09-1052,W00-1303,0,0.0337202,"Missing"
R09-1052,W04-2319,0,0.0224548,"Missing"
R09-1052,P08-1103,0,0.0651582,"Missing"
R09-1052,P05-1012,0,0.135188,"Missing"
R19-1059,N18-1155,1,0.612594,"hical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sentences in To effectively avoid the influence of parse errors and take advantage of the recent adva"
R19-1059,P16-1046,0,0.108004,"nt from the one which they were trained on. Introduction Document summarization is the task of automatically shortening a source document while retaining its salient information. In this paper, we present a recurrent neural network (RNN)-based extractive summarizer taking into account the discourse structure inherent in the source document. 497 Proceedings of Recent Advances in Natural Language Processing, pages 497–506, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_059 2 RNN-based approaches have achieved the stateof-the-art performance in document summarization (Cheng and Lapata, 2016; Nallapati et al., 2017). However, RNN-based summarizers treat the source document just as a sequence of sentences, and ignore the discourse tree structure inherent in the document. The lack of such information limits the ability to correctly compute relative importance between sentences and reduces the coherence of output summaries. Cohan et al. (2018) might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction”"
R19-1059,I17-2002,1,0.785576,"r attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sentences in To effectively avoid the influence of"
R19-1059,N18-2097,0,0.169967,"of Recent Advances in Natural Language Processing, pages 497–506, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_059 2 RNN-based approaches have achieved the stateof-the-art performance in document summarization (Cheng and Lapata, 2016; Nallapati et al., 2017). However, RNN-based summarizers treat the source document just as a sequence of sentences, and ignore the discourse tree structure inherent in the document. The lack of such information limits the ability to correctly compute relative importance between sentences and reduces the coherence of output summaries. Cohan et al. (2018) might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores"
R19-1059,P14-2052,1,0.885116,"Figure 1. In the figure, each node corresponds to a sentence. Regarding the relations between the sentences, sentence S2 elaborates the fact mentioned in sentence S1. In addition, S2 is further elaborated by S3. S4 is a contrast to the mention S1. Such relations are essential cues for generating a concise and coherent summary. For example, elaborated sentences tend to be more important than elaborating sentences, and the elaborated sentences should be included in the summary while the elaborating sentences are not. Several Integer Linear Programming (ILP)based summarizers (Hirao et al., 2013; Kikuchi et al., 2014) use the discourse information given by a discourse parser (Hernault et al., 2010). Thus, the performance of the summarizers is strongly affected by the performance of the discourse parsers. The performance of the parsers deteriorates especially when they are applied to documents of a domain different from the one which they were trained on. Introduction Document summarization is the task of automatically shortening a source document while retaining its salient information. In this paper, we present a recurrent neural network (RNN)-based extractive summarizer taking into account the discourse"
R19-1059,P14-1048,0,0.0304477,"of the source document when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure pr"
R19-1059,W16-5903,0,0.0184934,"cument as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this section, we first explain the base model and give the details of our proposed attention module in the following section. The base model is composed of two main components: a neural network-based hierarchical document encoder and a decoder-based sentence scorer. The document encoder is further split into two components; a sentence reader and a document reader. The"
R19-1059,P14-1003,0,0.0151844,"discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this secti"
R19-1059,W04-1013,0,0.0349711,"t. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we calculated the scores. We used three length constraints; 75 bytes, 275 bytes (Nallapati et al., 2017; Cheng and Lapata, 2016) and the bytes of reference summaries. We truncated generated summaries in the middle to conform to the length constraints. We adopted the last constraint to evaluate whether a model can include sufficient information within the ideal summary length. For the evaluation on out-of-domain data, we report"
R19-1059,D17-1012,0,0.0282195,"ness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their"
R19-1059,W16-3616,0,0.0196532,"ion into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this section, we first explain th"
R19-1059,D15-1166,0,0.137181,"Missing"
R19-1059,C02-1053,0,0.150408,"is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summar"
R19-1059,P17-2043,0,0.0218393,"might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann e"
R19-1059,N18-1158,0,0.0861849,"RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (20"
R19-1059,D13-1158,0,0.0504508,"Missing"
R19-1059,D15-1226,0,0.0544244,"Missing"
R19-1059,prasad-etal-2008-penn,0,0.0459578,"tter than state-of-the-art neural network-based extractive summarizers. 498 resent the discourse dependency tree of x. Specifically, element Ek,l equals 1 if the edge from xk to xl exists in the discourse tree; otherwise Ek,l = 0. Note that we use the discourse structure matrices E only in the training phase. The model does not require the RST annotations of the source document when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et a"
R19-1059,D15-1044,0,0.0277343,"ere have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (2017) later proposed the methods to automatically annotate the binary labels, enabling us to train extractive models. Cohan et al. (2018) demonstrated the usefulness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical t"
R19-1059,C10-1128,0,0.0361701,"orks in the encoder. Refresh is a state-of-the-art method using reinforcement learning (Narayan et al., 2018) 5 . In addition to the above methods, we compared our models with previously reported performances on the DUC2002 test set. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we calculated the scores. We used three length constraints; 75 bytes, 275 bytes (Nallapati et al., 2017; Cheng and Lapata, 2016) and the bytes of reference summaries. We truncated generated summaries"
R19-1059,K15-2002,0,0.0247434,"when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while"
R19-1059,P10-1058,0,0.0401252,"re of the target sentence, while our approach incorporates the information on the parent sentence of the target sentence. NeuralSum is also a neural network-based summarizer which uses convolutional neural networks in the encoder. Refresh is a state-of-the-art method using reinforcement learning (Narayan et al., 2018) 5 . In addition to the above methods, we compared our models with previously reported performances on the DUC2002 test set. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we ca"
R19-1059,N16-1174,0,0.0544093,"l. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (2017) later proposed the methods to automatically annotate the binary labels, enabling us to train extractive models. Cohan et al. (2018) demonstrated the usefulness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while"
R19-1059,E17-1063,0,0.0318384,"N-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sent"
S17-2061,P14-5010,0,0.00333491,"cessing From each dataset, i.e. development, train and test sets, we extract the questions to form threads for subtask B. Each thread contains one original question (orgQ) and the 10 related questions (relQ). We use the term ‘collection of documents’ for the thread, which contains questions (each with subject and body1) as the documents. From each collection of documents, we extract all lemmas and select only content words: nouns, verbs, adjectives, named entities, question words, and foreign words. For this need we use lemmatizer, POS tagger and Named Entity Recognizer from Stanford CoreNLP (Manning et al., 2014). We also count each lemma’s frequency in each collection of documents for each certain thread, not from the whole dataset. Intuitively, in a QA forum, if the frequency of a word is high in a certain thread, the word is likely to be an important matter in the conversation discussed by majority users. For this reason, we rank the words by their frequencies. We list top-N rank of words2 for next process. In our experiments, we set N to 4. 1 If body is empty, we copy the subject for the body. Only words with frequency count ≥ 2 are taken into consideration. 2 2.2 Word Importance Level We first de"
S17-2061,S16-1126,0,0.0222344,", word embeddings obtained with a tool such as word2vec (Mikolov et al., 2013, 2013b) contributed to the best systems for all subtasks. In addition, machine learning based methods were mostly ranked in the top positions for all subtasks. The most popular machine learning approach was SVM for classification, regression and ranking, while neural networks, even though widely used, did not win any subtasks (Nakov et al., 2016). Most machine learning approaches rely on several similarity features as the basis. Various techniques to compute semantic similarity based on word embeddings, were used by Franco-Salvador et al. (2016), Filice et al. (2016), Mohtarami et al. (2016), Wu and Lan (2016), and Mihaylov and Nakov (2016). Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams. Specifically, Franco-Salvador et al. (2016), also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame. Interestingly, Mihaylova et al. (2016) used cosine distance between topic pairs, and text distance for SVM learning features, rather than using similarity features. They also implemented other Boolean and Qatar Living Forum users as tas"
S17-2061,S16-1128,0,0.0489649,"Missing"
S17-2061,S16-1124,0,0.0481197,"Missing"
S17-2061,S16-1083,0,0.0913611,"kov et al., 2017). Since some subtasks are related, we focus only on subtask B, with goal to provide a good basis framework for solving problem in other subtasks. In Task 3 of the previous year, word embeddings obtained with a tool such as word2vec (Mikolov et al., 2013, 2013b) contributed to the best systems for all subtasks. In addition, machine learning based methods were mostly ranked in the top positions for all subtasks. The most popular machine learning approach was SVM for classification, regression and ranking, while neural networks, even though widely used, did not win any subtasks (Nakov et al., 2016). Most machine learning approaches rely on several similarity features as the basis. Various techniques to compute semantic similarity based on word embeddings, were used by Franco-Salvador et al. (2016), Filice et al. (2016), Mohtarami et al. (2016), Wu and Lan (2016), and Mihaylov and Nakov (2016). Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams. Specifically, Franco-Salvador et al. (2016), also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame. Interestingly, Mihaylova et al. ("
S17-2061,S16-1136,0,0.0322392,"the best systems for all subtasks. In addition, machine learning based methods were mostly ranked in the top positions for all subtasks. The most popular machine learning approach was SVM for classification, regression and ranking, while neural networks, even though widely used, did not win any subtasks (Nakov et al., 2016). Most machine learning approaches rely on several similarity features as the basis. Various techniques to compute semantic similarity based on word embeddings, were used by Franco-Salvador et al. (2016), Filice et al. (2016), Mohtarami et al. (2016), Wu and Lan (2016), and Mihaylov and Nakov (2016). Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams. Specifically, Franco-Salvador et al. (2016), also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame. Interestingly, Mihaylova et al. (2016) used cosine distance between topic pairs, and text distance for SVM learning features, rather than using similarity features. They also implemented other Boolean and Qatar Living Forum users as task specific features. Filice et al. (2016) constructed many types of similarity based on text pair"
S17-2061,S16-1129,0,0.0159198,"akov et al., 2016). Most machine learning approaches rely on several similarity features as the basis. Various techniques to compute semantic similarity based on word embeddings, were used by Franco-Salvador et al. (2016), Filice et al. (2016), Mohtarami et al. (2016), Wu and Lan (2016), and Mihaylov and Nakov (2016). Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams. Specifically, Franco-Salvador et al. (2016), also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame. Interestingly, Mihaylova et al. (2016) used cosine distance between topic pairs, and text distance for SVM learning features, rather than using similarity features. They also implemented other Boolean and Qatar Living Forum users as task specific features. Filice et al. (2016) constructed many types of similarity based on text pairs, e.g. n-grams of word lemmas, n-grams of POS tags, parse tree, and LCS for SVM learning features. Then they stack the classifiers across subtasks to solve substasks B and C in such a way that utilizes other subtasks’ results. This task-specific features seem to be the key success for the team to get th"
S17-2061,S16-1172,0,0.0291581,"a tool such as word2vec (Mikolov et al., 2013, 2013b) contributed to the best systems for all subtasks. In addition, machine learning based methods were mostly ranked in the top positions for all subtasks. The most popular machine learning approach was SVM for classification, regression and ranking, while neural networks, even though widely used, did not win any subtasks (Nakov et al., 2016). Most machine learning approaches rely on several similarity features as the basis. Various techniques to compute semantic similarity based on word embeddings, were used by Franco-Salvador et al. (2016), Filice et al. (2016), Mohtarami et al. (2016), Wu and Lan (2016), and Mihaylov and Nakov (2016). Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams. Specifically, Franco-Salvador et al. (2016), also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame. Interestingly, Mihaylova et al. (2016) used cosine distance between topic pairs, and text distance for SVM learning features, rather than using similarity features. They also implemented other Boolean and Qatar Living Forum users as task specific features. F"
W02-2028,J90-1003,0,\N,Missing
W02-2028,J92-4003,0,\N,Missing
W18-6510,W17-3541,0,0.0167717,"so play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific attributes such as user identification (Li et al., 2016b), profile information sets (Zhang et al., 2018; Wang et al., 2017; Herzig et al., 2017), topics (Xing et al., 2017), and speciIntroduction Human-computer conversation is a challenging task in Natural Language Processing (NLP). The aim of conversation models is to generate fluent and relevant responses given an input in a free format, i.e., not just in the form of a question. A large amount of available data on the Internet has sparked the shift in conversation models. Starting with Ritter et al. (2011), completely datadriven models are now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated by Sutskever et al. (2014) has been adapt"
W18-6510,N16-1014,0,0.0620091,"Missing"
W18-6510,P16-1094,0,0.0349368,"Missing"
W18-6510,D16-1127,0,0.175075,"1 how are you ? good morning how are you i’m doing ok i’m good ! ! ! not really good i am excited ! are you sure ? ! come to the party ? yay ! ! ! are you gonna do it ? Table 1: Sample responses from our proposed model involving four different users. notably to machine translation (MT) and response generation. Actual conversations involving humans would be more engaging and the responses are not always general and monotonic. However, neural conversation models tend to generate safe, general, and uninteresting responses, e.g., I don’t know or I’m OK (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016b). We argue that, aside from adding or understanding the context of a conversation, speaking style and response diversity also play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific"
W18-6510,D16-1230,0,0.0526098,"Missing"
W18-6510,D15-1166,0,0.532562,"e now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated by Sutskever et al. (2014) has been adapted to many NLP tasks, 89 Proceedings of The 11th International Natural Language Generation Conference, pages 89–98, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2015) follow the encoder-decoder framework of Sutskever et al. (2014). For response generation, the encoder-decoder models are usually supplemented by the attention mechanism, following the implementation of Bahdanau et al. (2015) or Luong et al. (2015). fied mechanisms (Zhou et al., 2017). In this study, we focus on the issue of “response style.” We intend to let the model learn to generate responses that resemble those of a real person. Given an input utterance and user-specific information, the model will generate a response relevant to the input utterance based on the given userspecific information. The existing methods that exhibit the use of user-specific information (Li et al., 2016b; Zhang et al., 2018), usually require that the users appear in the training data. Therefore, these existing methods cannot handle the unseen users, i.e.,"
W18-6510,D17-1228,0,0.0256033,"Missing"
W18-6510,P02-1040,0,0.100678,"embeddings 93 6.3 cannot cover unseen users. Our model overcomes that issue by using user-info embeddings. The decoder input of both the models can be represented by Equation (6). Since the baseline model does not have user-info embeddings, our model’s atten˜ t is different from theirs. The attional hidden h tentional hidden of the baseline model would be ˜ t is the same as Equation (3), while our model’s h represented by Equation (7). Evaluation Setup Many previous studies on dialogue or response generation models (Li et al., 2016b,a; Sordoni et al., 2015; Xing et al., 2017) relied on BLEU (Papineni et al., 2002) as their automatic evaluation metric. To compute the score, BLEU measures the overlapping words or n-grams between the generated output (hypothesis) and the target output (reference). BLEU was initially intended for machine translation, which tends to have a finite target; therefore, it might not be suitable for evaluating conversation models. According to Liu et al. (2016), BLEU is lowly correlated with human judgments of dialogue systems. Additionally, some other work on response generation (Shang et al., 2015; Li et al., 2016c; Wang et al., 2017; Zhou et al., 2017) did not use BLEU for the"
W18-6510,P18-1205,0,0.12842,"peaking style and response diversity also play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific attributes such as user identification (Li et al., 2016b), profile information sets (Zhang et al., 2018; Wang et al., 2017; Herzig et al., 2017), topics (Xing et al., 2017), and speciIntroduction Human-computer conversation is a challenging task in Natural Language Processing (NLP). The aim of conversation models is to generate fluent and relevant responses given an input in a free format, i.e., not just in the form of a question. A large amount of available data on the Internet has sparked the shift in conversation models. Starting with Ritter et al. (2011), completely datadriven models are now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated"
W18-6510,D14-1162,0,0.0835477,"Missing"
W18-6510,D11-1054,0,0.102165,"Missing"
W18-6510,P15-1152,0,0.0508841,"Missing"
W18-6510,N15-1020,0,0.0556654,"Missing"
W18-6515,P09-1011,0,0.00975975,"of the previous day. Generating Market Comments We describe our model for generating comments. We extend the encoder part of the model proposed by Murakami et al. (2017), which had a limitation in generating informative market comments due to the lack of a capability to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), i"
W18-6515,D15-1166,0,0.00938938,"ity to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T"
W18-6515,E17-1059,0,0.0179969,"on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T gains &lt;price/&gt; main. We encode each of the external resources in addition to the target index, i.e., Nikkei 225, and feed them to the decoder. The experimental results show that o"
W18-6515,P83-1022,0,0.190223,"t Comments We describe our model for generating comments. We extend the encoder part of the model proposed by Murakami et al. (2017), which had a limitation in generating informative market comments due to the lack of a capability to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are gene"
W18-6515,D16-1128,0,0.0147791,"luding weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T gains &lt;price/&gt; main. We encode each of the external resources in addition to the target index, i.e., Nikkei 225, and feed them to the decoder. The experimental results show that our proposed model outperforms the existing single-source model in term"
W19-8640,W18-6554,0,0.020087,"advantages of this approach. 2 periments show the fluency and fidelity of the generated document in terms of BLEU and humanevaluation. Secondly, compared the generated documents between with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templ"
W19-8640,D16-1140,1,0.842464,"abels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al"
W19-8640,D16-1128,0,0.0418044,"Missing"
W19-8640,P16-1094,0,0.0312147,"documents between with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically ext"
W19-8640,P09-1011,0,0.0177701,"tion. Experiments show both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potent"
W19-8640,H05-1042,0,0.0892773,"ormation: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation. Experiments show both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news ar"
W19-8640,N16-1000,0,0.230953,"Missing"
W19-8640,N16-1086,0,0.0182776,"ow both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potential advantages/dis"
W19-8640,C16-1100,0,0.0312858,"al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀects of introducing topic labels in the data-to-document scenario. Besides, Gkatzia et al. (2017) and Portet et al. (2009) proposed non-neural language generation models for the data-to-text task with higher controllability on the output. They assumed that the important contents and their descriptions are determined primar"
W19-8640,W17-5525,0,0.0254598,"tion of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potential advantages/disadvantages of this approach. 2 periments sho"
W19-8640,D18-1356,0,0.0237067,"such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀe"
W19-8640,P19-1193,0,0.0264327,"e University for Advanced Studies ♡ National Institute of Informatics ♠ Tokyo Institute of Technology ♯ Waseda University ♮ The University of Tokyo {g1120501, koba}@is.ocha.ac.jp miyazawa-a@nii.ac.jp {aoki, ishigaki}@lr.pi.titech.ac.jp hiroshi.noji@aist.go.jp keiichi.goshima@aoni.waseda.jp takamura@pi.titech.ac.jp yusuke@is.s.u-tokyo.ac.jp Abstract been applied to versatile data-to-text tasks, because they can generate fluent texts without task-specific knowledge and resources. However, it has also been pointed out that texts generated by neural models suﬀer from low diversity in expressions (Yang et al., 2019). Especially on the data-to-text tasks, since they are developed under the assumption that the important contents could be uniquely determined, previous methods did not focus on controlling the contents in terms of user’s interests. However, each user may expect diﬀerent contents in a summary depending on what they are interested in, and thus it is appealing to develop a method to generate various summaries which reflect user’s interests. This paper investigates a method for guiding data-to-document generation in the finance domain, by referring to a sequence of additional information for inpu"
W19-8640,P18-1080,0,0.0132777,"more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resemble"
W19-8640,D17-1233,0,0.0298286,"and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀects of introducing topic labels in the data-to-document scenario. Besides, Gkatzia et al. (2017) and Portet et al. (2009) proposed non-neural language generation models for the data-to-text task with higher controllability on the output. They assumed that the important contents and their descriptions are determined primarily by experts, an"
W19-8640,N16-1005,0,0.0299349,"n with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designe"
