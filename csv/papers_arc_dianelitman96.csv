2021.findings-emnlp.142,Exploring Multitask Learning for Low-Resource Abstractive Summarization,2021,-1,-1,2,0,6781,ahmed magooda,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"This paper explores the effect of using multitask learning for abstractive summarization in the context of small training corpora. In particular, we incorporate four different tasks (extractive summarization, language modeling, concept detection, and paraphrase detection) both individually and in combination, with the goal of enhancing the target task of abstractive summarization via multitask learning. We show that for many task combinations, a model trained in a multitask setting outperforms a model trained only for abstractive summarization, with no additional summarization data introduced. Additionally, we do a comprehensive search and find that certain tasks (e.g. paraphrase detection) consistently benefit abstractive summarization, not only when combined with other tasks but also when using different architectures and training corpora."
2021.findings-emnlp.175,"Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization",2021,-1,-1,2,0,6781,ahmed magooda,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"This paper explores three simple data manipulation techniques (synthesis, augmentation, curriculum) for improving abstractive summarization models without the need for any additional data. We introduce a method of data synthesis with paraphrasing, a data augmentation technique with sample mixing, and curriculum learning with two new difficulty metrics based on specificity and abstractiveness. We conduct experiments to show that these three techniques can help improve abstractive summarization across two summarization models and two different small datasets. Furthermore, we show that these techniques can improve performance when applied in isolation and when combined."
2021.bea-1.9,Essay Quality Signals as Weak Supervision for Source-based Essay Scoring,2021,-1,-1,2,1,12221,haoran zhang,Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications,0,"Human essay grading is a laborious task that can consume much time and effort. Automated Essay Scoring (AES) has thus been proposed as a fast and effective solution to the problem of grading student writing at scale. However, because AES typically uses supervised machine learning, a human-graded essay corpus is still required to train the AES model. Unfortunately, such a graded corpus often does not exist, so creating a corpus for machine learning can also be a laborious task. This paper presents an investigation of replacing the use of human-labeled essay grades when training an AES system with two automatically available but weaker signals of essay quality: word count and topic distribution similarity. Experiments using two source-based essay scoring (evidence score) corpora show that while weak supervision does not yield a competitive result when training a neural source-based AES model, it can be used to successfully extract Topical Components (TCs) from a source text, which are required by a supervised feature-based AES model. In particular, results show that feature-based AES performance is comparable with either automatically or manually constructed TCs."
2021.argmining-1.14,Self-trained Pretrained Language Models for Evidence Detection,2021,-1,-1,2,0,6783,mohamed elaraby,Proceedings of the 8th Workshop on Argument Mining,0,"Argument role labeling is a fundamental task in Argument Mining research. However, such research often suffers from a lack of large-scale datasets labeled for argument roles such as evidence, which is crucial for neural model training. While large pretrained language models have somewhat alleviated the need for massive manually labeled datasets, how much these models can further benefit from self-training techniques hasn{'}t been widely explored in the literature in general and in Argument Mining specifically. In this work, we focus on self-trained language models (particularly BERT) for evidence detection. We provide a thorough investigation on how to utilize pseudo labels effectively in the self-training scheme. We also assess whether adding pseudo labels from an out-of-domain source can be beneficial. Experiments on sentence level evidence detection show that self-training can complement pretrained language models to provide performance improvements."
2021.argmining-1.15,Multi-task Learning in Argument Mining for Persuasive Online Discussions,2021,-1,-1,2,0,12300,nhat tran,Proceedings of the 8th Workshop on Argument Mining,0,"We utilize multi-task learning to improve argument mining in persuasive online discussions, in which both micro-level and macro-level argumentation must be taken into consideration. Our models learn to identify argument components and the relations between them at the same time. We also tackle the low-precision which arises from imbalanced relation data by experimenting with SMOTE and XGBoost. Our approaches improve over baselines that use the same pre-trained language model but process the argument component task and two relation tasks separately. Furthermore, our results suggest that the tasks to be incorporated into multi-task learning should be taken into consideration as using all relevant tasks does not always lead to the best performance."
2020.lrec-1.130,The Discussion Tracker Corpus of Collaborative Argumentation,2020,43,0,4,0,16877,christopher olshefski,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Although NLP research on argument mining has advanced considerably in recent years, most studies draw on corpora of asynchronous and written texts, often produced by individuals. Few published corpora of synchronous, multi-party argumentation are available. The Discussion Tracker corpus, collected in high school English classes, is an annotated dataset of transcripts of spoken, multi-party argumentation. The corpus consists of 29 multi-party discussions of English literature transcribed from 985 minutes of audio. The transcripts were annotated for three dimensions of collaborative argumentation: argument moves (claims, evidence, and explanations), specificity (low, medium, high) and collaboration (e.g., extensions of and disagreements about others{'} ideas). In addition to providing descriptive statistics on the corpus, we provide performance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research."
2020.coling-main.128,Contextual Argument Component Classification for Class Discussions,2020,-1,-1,2,1,16878,luca lugini,Proceedings of the 28th International Conference on Computational Linguistics,0,"Argument mining systems often consider contextual information, i.e. information outside of an argumentative discourse unit, when trained to accomplish tasks such as argument component identification, classification, and relation extraction. However, prior work has not carefully analyzed the utility of different contextual properties in context-aware models. In this work, we show how two different types of contextual information, local discourse context and speaker context, can be incorporated into a computational model for classifying argument components in multi-party classroom discussions. We find that both context types can improve performance, although the improvements are dependent on context size and position."
2020.coling-demos.10,Discussion Tracker: Supporting Teacher Learning about Students{'} Collaborative Argumentation in High School Classrooms,2020,-1,-1,4,1,16878,luca lugini,Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations,0,"Teaching collaborative argumentation is an advanced skill that many K-12 teachers struggle to develop. To address this, we have developed Discussion Tracker, a classroom discussion analytics system based on novel algorithms for classifying argument moves, specificity, and collaboration. Results from a classroom deployment indicate that teachers found the analytics useful, and that the underlying classifiers perform with moderate to substantial agreement with humans."
2020.bea-1.7,Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing,2020,-1,-1,3,0,22286,tazin afrin,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"Automated writing evaluation systems can improve students{'} writing insofar as students attend to the feedback provided and revise their essay drafts in ways aligned with such feedback. Existing research on revision of argumentative writing in such systems, however, has focused on the types of revisions students make (e.g., surface vs. content) rather than the extent to which revisions actually respond to the feedback provided and improve the essay. We introduce an annotation scheme to capture the nature of sentence-level revisions of evidence use and reasoning (the {`}RER{'} scheme) and apply it to 5th- and 6th-grade students{'} argumentative essays. We show that reliable manual annotation can be achieved and that revision annotations correlate with a holistic assessment of essay improvement in line with the feedback provided. Furthermore, we explore the feasibility of automatically classifying revisions according to our scheme."
2020.acl-main.759,Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring,2020,-1,-1,2,1,12221,haoran zhang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision. However, a neural AES typically does not provide useful feature representations for supporting AWE. This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers. We evaluate performance using a feature-based AES requiring TCs. Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays."
W18-5208,Argument Component Classification for Classroom Discussions,2018,34,3,2,1,16878,luca lugini,Proceedings of the 5th Workshop on Argument Mining,0,"This paper focuses on argument component classification for transcribed spoken classroom discussions, with the goal of automatically classifying student utterances into claims, evidence, and warrants. We show that an existing method for argument component classification developed for another educationally-oriented domain performs poorly on our dataset. We then show that feature sets from prior work on argument mining for student essays and online dialogues can be used to improve performance considerably. We also provide a comparison between convolutional neural networks and recurrent neural networks when trained under different conditions to classify argument components in classroom discussions. While neural network models are not always able to outperform a logistic regression model, we were able to gain some useful insights: convolutional networks are more robust than recurrent networks both at the character and at the word level, and specificity information can help boost performance in multi-task training."
W18-5046,Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue,2018,0,0,2,1,28083,zahra rahimi,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.
W18-0511,Annotating Student Talk in Text-based Classroom Discussions,2018,30,2,2,1,16878,luca lugini,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Classroom discussions in English Language Arts have a positive effect on students{'} reading, writing and reasoning skills. Although prior work has largely focused on teacher talk and student-teacher interactions, we focus on three theoretically-motivated aspects of high-quality student talk: argumentation, specificity, and knowledge domain. We introduce an annotation scheme, then show that the scheme can be used to produce reliable annotations and that the annotations are predictive of discussion quality. We also highlight opportunities provided by our scheme for education and natural language processing research."
W18-0528,Annotation and Classification of Sentence-level Revision Improvement,2018,20,1,2,0,22286,tazin afrin,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders. Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions."
W18-0549,Co-Attention Based Neural Network for Source-Dependent Essay Scoring,2018,0,0,2,1,12221,haoran zhang,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper presents an investigation of using a co-attention based neural network for source-dependent essay scoring. We use a co-attention mechanism to help the model learn the importance of each part of the essay more accurately. Also, this paper shows that the co-attention based neural network model provides reliable score prediction of source-dependent responses. We evaluate our model on two source-dependent response corpora. Results show that our model outperforms the baseline on both corpora. We also show that the attention of the model is similar to the expert opinions with examples."
W17-5006,Predicting Specificity in Classroom Discussion,2017,19,6,2,1,16878,luca lugini,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"High quality classroom discussion is important to student development, enhancing abilities to express claims, reason about other students{'} claims, and retain information for longer periods of time. Previous small-scale studies have shown that one indicator of classroom discussion quality is specificity. In this paper we tackle the problem of predicting specificity for classroom discussions. We propose several methods and feature sets capable of outperforming the state of the art in specificity prediction. Additionally, we provide a set of meaningful, interpretable features that can be used to analyze classroom discussions at a pedagogical level."
P17-3013,Word Embedding for Response-To-Text Assessment of Evidence,2017,15,0,2,1,12221,haoran zhang,"Proceedings of {ACL} 2017, Student Research Workshop",0,"Manually grading the Response to Text Assessment (RTA) is labor intensive. Therefore, an automatic method is being developed for scoring analytical writing when the RTA is administered in large numbers of classrooms. Our long-term goal is to also use this scoring method to provide formative feedback to students and teachers about students' writing quality. As a first step towards this goal, interpretable features for automatically scoring the evidence rubric of the RTA have been developed. In this paper, we present a simple but promising method for improving evidence scoring by employing the word embedding model. We evaluate our method on corpora of responses written by upper elementary students."
P17-1144,A Corpus of Annotated Revisions for Studying Argumentative Writing,2017,24,7,4,1,4658,fan zhang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer{'}s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction."
W16-3615,Extracting {PDTB} Discourse Relations from Student Essays,2016,19,3,3,1,33719,kate forbesriley,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-3635,Towards Using Conversations with Spoken Dialogue Systems in the Automated Assessment of Non-Native Speakers of {E}nglish,2016,18,6,1,1,6782,diane litman,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-0532,Automatically Extracting Topical Components for a Response-to-Text Writing Assessment,2016,20,3,2,1,28083,zahra rahimi,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,We investigate automatically extracting multiword topical components to replace information currently provided by experts that is used to score the Evidence dimension of a writing in response to text assessment. Our goal is to reduce the amount of expert effort and improve the scalability of an automatic scoring system. Experimental results show that scoring performance using automatically extracted data-driven topical components is promising.
P16-1107,Context-aware Argumentative Relation Mining,2016,25,10,2,1,21768,huy nguyen,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-3002,Instant Feedback for Increasing the Presence of Solutions in Peer Reviews,2016,13,7,3,1,21768,huy nguyen,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,None
N16-3008,{A}rg{R}ewrite: A Web-based Revision Assistant for Argumentative Writings,2016,6,8,3,1,4658,fan zhang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"While intelligent writing assistants have become more common, they typically have little support for revision behavior. We present ArgRewrite, a novel web-based revision assistant that focus on rewriting analysis. The system supports two major functionalities: 1) to assist students as they revise, the system automatically extracts and analyzes revisions; 2) to assist teachers, the system provides an overview of studentsxe2x80x99 revisions and allows teachers to correct the automatically analyzed results, ensuring that students get the correct feedback."
N16-1010,Automatic Summarization of Student Course Feedback,2016,16,10,4,1,34667,wencan luo,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
N16-1168,Using Context to Predict the Purpose of Argumentative Writing Revisions,2016,18,6,2,1,4658,fan zhang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1149,The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues,2016,25,4,1,1,6782,diane litman,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1006,An Improved Phrase-based Approach to Annotating and Summarizing Student Course Responses,2016,30,6,3,1,34667,wencan luo,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Teaching large classes remains a great challenge, primarily because it is difficult to attend to all the student needs in a timely manner. Automatic text summarization systems can be leveraged to summarize the student feedback, submitted immediately after each lecture, but it is left to be discovered what makes a good summary for student responses. In this work we explore a new methodology that effectively extracts summary phrases from the student responses. Each phrase is tagged with the number of students who raise the issue. The phrases are evaluated along two dimensions: with respect to text content, they should be informative and well-formed, measured by the ROUGE metric; additionally, they shall attend to the most pressing student needs, measured by a newly proposed metric. This work is enabled by a phrase-based annotation and highlighting scheme, which is new to the summarization task. The phrase-based framework allows us to summarize the student responses into a set of bullet points and present to the instructor promptly."
C16-1246,Inferring Discourse Relations from {PDTB}-style Discourse Labels for Argumentative Revision Classification,2016,17,0,2,1,4658,fan zhang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Penn Discourse Treebank (PDTB)-style annotation focuses on labeling local discourse relations between text spans and typically ignores larger discourse contexts. In this paper we propose two approaches to infer discourse relations in a paragraph-level context from annotated PDTB labels. We investigate the utility of inferring such discourse information using the task of revision classification. Experimental results demonstrate that the inferred information can significantly improve classification performance compared to baselines, not only when PDTB annotation comes from humans but also from automatic parsers."
W15-0603,Incorporating Coherence of Topics as a Criterion in Automatic Response-to-Text Assessment of the Organization of Writing,2015,21,6,2,1,28083,zahra rahimi,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper presents an investigation of score prediction for the Organization dimension of an assessment of analytical writing in response to text. With the long-term goal of producing feedback for students and teachers, we designed a task-dependent model that aligns with the scoring rubric and makes use of the source material. Our experimental results show that our rubric-based model performs as well as baselines on datasets from grades 6-8. On shorter and noisier essays from grades 5-6, the rubric-based model performs better than the baselines. Further, we show that the baseline model (lexical chaining) can be improved if we extend it with information from the source text for shorter and noisier data."
W15-0616,Annotation and Classification of Argumentative Writing Revisions,2015,26,4,2,1,4658,fan zhang,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper explores the annotation and classification of studentsxe2x80x99 revision behaviors in argumentative writing. A sentence-level revision schema is proposed to capture why and how students make revisions. Based on the proposed schema, a small corpus of student essays and revisions was annotated. Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis. Furthermore, features and methods are explored for the automatic classification of revisions. Intrinsic evaluations demonstrate promising performance in high-level revision classification (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writerxe2x80x99s improvement."
W15-0503,Extracting Argument and Domain Words for Identifying Argument Components in Texts,2015,23,38,2,1,21768,huy nguyen,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Argument mining studies in natural language text often use lexical (e.g. n-grams) and syntactic (e.g. grammatical production rules) features with all possible values. In prior work on a corpus of academic essays, we demonstrated that such large and sparse feature spaces can cause difficulty for feature selection and proposed a method to design a more compact feature space. The proposed feature design is based on post-processing a topic model to extract argument and domain words. In this paper we investigate the generality of this approach, by applying our methodology to a new corpus of persuasive essays. Our experiments show that replacing n-grams and syntactic rules with features and constraints using extracted argument and domain words significantly improves argument mining performance for persuasive essays."
N15-3004,Enhancing Instructor-Student and Student-Student Interactions with Mobile Interfaces and Summarization,2015,11,9,5,1,34667,wencan luo,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"Educational research has demonstrated that asking students to respond to reflection prompts can increase interaction between instructors and students, which in turn can improve both teaching and learning especially in large classrooms. However, administering an instructorxe2x80x99s prompts, collecting the studentsxe2x80x99 responses, and summarizing these responses for both instructors and students is challenging and expensive. To address these challenges, we have developed an application called CourseMIRROR (Mobile Insitu Reflections and Review with Optimized Rubrics). CourseMIRROR uses a mobile interface to administer prompts and collect reflective responses for a set of instructorassigned course lectures. After collection, CourseMIRROR automatically summarizes the reflections with an extractive phrase summarization method, using a clustering algorithm to rank extracted phrases by student coverage. Finally, CourseMIRROR presents the phrase summary to both instructors and students to help them understand the difficulties and misunderstandings encountered."
D15-1227,Summarizing Student Responses to Reflection Prompts,2015,26,8,2,1,34667,wencan luo,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose to automatically summarize student responses to reflection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways. First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR."
W14-4324,Evaluating a Spoken Dialogue System that Detects and Adapts to User Affective States,2014,18,7,1,1,6782,diane litman,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"We present an evaluation of a spoken dialogue system that detects and adapts to user disengagement and uncertainty in real-time. We compare this version of our system to a version that adapts to only user disengagement, and to a version that ignores user disengagement and uncertainty entirely. We find a significant increase in task success when comparing both affectadaptive versions of our system to our nonadaptive baseline, but only for male users."
W14-2104,Ontology-Based Argument Mining and Automatic Essay Scoring,2014,12,21,2,0,38702,nathan ong,Proceedings of the First Workshop on Argumentation Mining,0,"Essays are frequently used as a medium for teaching and evaluating argumentation skills. Recently, there has been interest in diagrammatic outlining as a replacement to the written outline that often precedes essay writing. This paper presents a preliminary approach for automatically identifying diagram ontology elements in essays, and demonstrates its positive correlation with expert scores of essay quality."
W14-1812,Improving Peer Feedback Prediction: The Sentence Level is Right,2014,16,7,2,1,21768,huy nguyen,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Recent research aims to automatically predict whether peer feedback is of high quality, e.g. suggests solutions to identified problems. While prior studies have focused on peer review of papers, similar issues arise when reviewing diagrams and other artifacts. In addition, previous studies have not carefully examined how the level of prediction granularity impacts both accuracy and educational utility. In this paper we develop models for predicting the quality of peer feedback regarding argument diagrams. We propose to perform prediction at the sentence level, even though the educational task is to label feedback at a multi-sentential comment level. We first introduce a corpus annotated at a sentence level granularity, then build comment prediction models using this corpus. Our results show that aggregating sentence prediction outputs to label comments not only outperforms approaches that directly train on comment annotations, but also provides useful information for enhancing peer review systems with new functionality."
W14-1818,Sentence-level Rewriting Detection,2014,0,3,2,1,4658,fan zhang,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
C14-1187,Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews,2014,28,17,2,1,34611,wenting xiong,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We propose a novel unsupervised extractive approach for summarizing online reviews by exploiting review helpfulness ratings. In addition to using the helpfulness ratings for review-level filtering, we suggest using them as the supervision of a topic model for sentence-level content scoring. The proposed method is metadata-driven, requiring no human annotation, and generalizable to different kinds of online reviews. Our experiment based on a widely used multi-document summarization framework shows that our helpfulness-guided review summarizers significantly outperform a traditional content-based summarizer in both human evaluation and automated evaluation."
N13-2002,Reducing Annotation Effort on Unbalanced Corpus based on Cost Matrix,2013,33,2,2,1,34667,wencan luo,Proceedings of the 2013 {NAACL} {HLT} Student Research Workshop,0,"Annotated corpora play a significant role in many NLP applications. However, annotation by humans is time-consuming and costly. In this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes. We demonstrate the effectiveness of our approach in the context of one form of unbalanced task: annotation of transcribed human-human dialogues for presence/absence of uncertainty. In two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy. The method is able to reduce human annotation effort by about 80% without a significant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations."
N13-1098,Differences in User Responses to a {W}izard-of-{O}z versus Automated System,2013,12,3,2,0,19594,jesse thomason,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"xc2xa9 2013 Association for Computational Linguistics. Wizard-of-Oz experimental setup in a dialogue system is commonly used to gather data for informing an automated version of that system. Previous work has exposed dependencies between user behavior towards systems and user belief about whether the system is automated or human-controlled. This work examines whether user behavior changes when user belief is held constant and the system's operator is varied. We perform a posthoc experiment using generalizable prosodic and lexical features of user responses to a dialogue system backed with and without a human wizard. Our results suggest that user responses are different when communicating with a wizarded and an automated system, indicating that wizard data may be less reliable for informing automated systems than generally assumed."
W12-2020,An Interactive Analytic Tool for Peer-Review Exploration,2012,15,2,2,1,34611,wenting xiong,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"This paper presents an interactive analytic tool for educational peer-review analysis. It employs data visualization at multiple levels of granularity, and provides automated analytic support using clustering and natural language processing. This tool helps instructors discover interesting patterns in writing performance that are reflected through peer reviews."
W12-1627,"Cohesion, Entrainment and Task Success in Educational Dialog",2012,0,0,1,1,6782,diane litman,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Researchers often study dialog corpora to better understand what makes some dialogs more successful than others. In this talk I will examine the relationship between coherence/entrainment and task success, in several types of educational dialog corpora: 1) one-on-one tutoring, where students use dialog to interact with a human tutor in the physics domain, 2) one-on-one tutoring, where students instead interact with a spoken dialog system, and 3) engineering design, where student teams engage in multi-party dialog to complete a group project. I will first introduce several corpus-based measures of both lexical and acoustic-prosodic dialog cohesion and entrainment, and extend them to handle multi-party conversations. I will then show that the amount of cohesion and/or entrainment positively correlates with measures of educational task success in all of our corpora. Finally, I will discuss how we are using our findings to build better tutorial dialog systems."
W12-1630,Adapting to Multiple Affective States in Spoken Dialogue,2012,32,20,2,1,33719,kate forbesriley,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We evaluate a wizard-of-oz spoken dialogue system that adapts to multiple user affective states in real-time: user disengagement and uncertainty. We compare this version with the prior version of our system, which only adapts to user uncertainty. Our analysis investigates how iteratively adding new affect adaptation to an existing affect-adaptive system impacts global and local performance. We find a significant increase in motivation for users who most frequently received the disengagement adaptation. Moreover, responding to disengagement breaks its negative correlations with task success and user satisfaction, reduces uncertainty levels, and reduces the likelihood of continued disengagement."
N12-1010,Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System,2012,49,13,2,1,33719,kate forbesriley,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a model for detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model (i.e., with respect to a gold standard) yields results on par with prior work. However, since our goal is immediate implementation in a system that already detects and adapts to user uncertainty, we go further than prior work and present an extrinsic evaluation of our model (i.e., with respect to the real-world task). Correlation analyses show crucially that our automatic disengagement labels correlate with system performance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely."
W11-2024,Using Performance Trajectories to Analyze the Immediate Impact of User State Misclassification in an Adaptive Spoken Dialogue System,2011,16,2,2,1,33719,kate forbesriley,Proceedings of the {SIGDIAL} 2011 Conference,0,"We present a method of evaluating the immediate performance impact of user state misclassifications in spoken dialogue systems. We illustrate the method with a tutoring system that adapts to student uncertainty over and above correctness. First we define a ranking of user states representing local performance. Second, we compare user state trajectories when the first state is accurately classified versus misclassified. Trajectories are quantified using a previously proposed metric representing the likelihood of transitioning from one user state to another. Comparison of the two sets of trajectories shows whether user state misclassifications change the likelihood of subsequent higher or lower ranked states, relative to accurate classification. Our tutoring system results illustrate the case where user state misclassification increases the likelihood of negative performance trajectories as compared to accurate classification."
W11-2036,Examining the Impacts of Dialogue Content and System Automation on Affect Models in a Spoken Tutorial Dialogue System,2011,7,3,2,0,42804,joanna drummond,Proceedings of the {SIGDIAL} 2011 Conference,0,"Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users' affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models' performance."
W11-1402,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,2011,-1,-1,2,1,34611,wenting xiong,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
W11-1417,Predicting Change in Student Motivation by Measuring Cohesion between Tutor and Student,2011,-1,-1,2,1,44349,arthur ward,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
P11-2088,Automatically Predicting Peer-Review Helpfulness,2011,17,24,2,1,34611,wenting xiong,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction."
W09-3927,Discourse Structure and Performance Analysis: Beyond the Correlation,2009,19,9,2,1,24249,mihai rotaru,Proceedings of the {SIGDIAL} 2009 Conference,0,"This paper is part of our broader investigation into the utility of discourse structure for performance analysis. In our previous work, we showed that several interaction parameters that use discourse structure predict our performance metric. Here, we take a step forward and show that these correlations are not only a surface relationship. We show that redesigning the system in light of an interpretation of a correlation has a positive impact."
W09-3940,Spoken Tutorial Dialogue and the Feeling of Another{'}s Knowing,2009,14,12,1,1,6782,diane litman,Proceedings of the {SIGDIAL} 2009 Conference,0,"We hypothesize that monitoring the accuracy of the feeling of another's knowing (FOAK) is a useful predictor of tutorial dialogue system performance. We test this hypothesis in the context of a wizarded spoken dialogue tutoring system, where student learning is the primary performance metric. We first present our corpus, which has been annotated with respect to student correctness and uncertainty. We then discuss the derivation of FOAK measures from these annotations, for use in building predictive performance models. Our results show that monitoring the accuracy of FOAK is indeed predictive of student learning, both in isolation and in conjunction with other predictors."
P09-1100,Setting Up User Action Probabilities in User Simulations for Dialog System Development,2009,21,6,2,1,47232,hua ai,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"User simulations are shown to be useful in spoken dialog system development. Since most current user simulations deploy probability models to mimic human user behaviors, how to set up user action probabilities in these models is a key problem to solve. One generally used approach is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks."
P08-1071,Assessing Dialog System User Simulation Evaluation Measures Using Human Judges,2008,12,22,2,1,47232,hua ai,Proceedings of ACL-08: HLT,1,"Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systemsxe2x80x99 logs. However, the validity of these automatic measures has not been fully proven. In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model."
forbes-riley-etal-2008-uncertainty,Uncertainty Corpus: Resource to Study User Affect in Complex Spoken Dialogue Systems,2008,17,6,2,1,33719,kate forbesriley,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present a corpus of spoken dialogues between students and an adaptive Wizard-of-Oz tutoring system, in which student uncertainty was manually annotated in real-time. We detail the corpus contents, including speech files, transcripts, annotations, and log files, and we discuss possible future uses by the computational linguistics community as a novel resource for studying naturally occurring user affect and adaptation in complex spoken dialogue systems."
P07-1046,The Utility of a Graphical Representation of Discourse Structure in Spoken Dialogue Systems,2007,15,2,2,1,24249,mihai rotaru,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In this paper we explore the utility of the Navigation Map (NM), a graphical representation of the discourse structure. We run a user study to investigate if users perceive the NM as helpful in a tutoring spoken dialogue system. From the usersxe2x80x99 perspective, our results show that the NM presence allows them to better identify and follow the tutoring plan and to better integrate the instruction. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics further strengthens these findings."
N07-2001,Comparing User Simulation Models For Dialog Strategy Learning,2007,7,34,3,1,47232,hua ai,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"This paper explores what kind of user simulation model is suitable for developing a training corpus for using Markov Decision Processes (MDPs) to automatically learn dialog strategies. Our results suggest that with sparse training data, a model that aims to randomly explore more dialog state spaces with certain constraints actually performs at the same or better than a more complex model that simulates realistic user behaviors in a statistical way."
N07-2011,Exploring Affect-Context Dependencies for Adaptive System Development,2007,11,10,3,1,33719,kate forbesriley,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We use X2 to investigate the context dependency of student affect in our computer tutoring dialogues, targeting uncertainty in student answers in 3 automatically monitorable contexts. Our results show significant dependencies between uncertain answers and specific contexts. Identification and analysis of these dependencies is our first step in developing an adaptive version of our dialogue system."
N07-1035,Estimating the Reliability of {MDP} Policies: a Confidence Interval Approach,2007,13,15,3,1,191,joel tetreault,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy. In this paper we present a methodology for numerically constructing confidence intervals for the expected cumulative reward for a learned policy. These intervals are used to (1) better assess the reliability of the expected cumulative reward, and (2) perform a refined comparison between policies derived from different Markov Decision Processes (MDP) models. We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace. Our results show that while some of the policies developed in the prior work exhibited very large confidence intervals, the policy developed from the best feature set had a much smaller confidence interval and thus showed very high reliability. xc2xa9 2007 Association for Computational Linguistics."
2007.sigdial-1.23,Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users,2007,23,58,5,1,47232,hua ai,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. In this paper we use Letxe2x80x99s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our first corpus is collected by recruiting subjects to call Letxe2x80x99s Go in a standard laboratory setting, while our second corpus consists of calls from real users calling Letxe2x80x99s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. For example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. In contrast, we find no difference with respect to dialog structure."
W06-1611,Exploiting Discourse Structure for Spoken Dialogue Performance Analysis,2006,20,13,2,1,24249,mihai rotaru,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we study the utility of discourse structure for spoken dialogue performance modeling. We experiment with various ways of exploiting the discourse structure: in isolation, as context information for other factors (correctness and certainty) and through trajectories in the discourse structure hierarchy. Our correlation and PARADISE results show that, while the discourse structure is not useful in isolation, using the discourse structure as context information for other factors or via trajectories produces highly predictive parameters for performance analysis."
W06-1625,{H}umor: Prosody Analysis and Automatic Recognition for {F}*{R}*{I}*{E}*{N}*{D}*{S}*,2006,13,25,2,0,48036,amruta purandare,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We analyze humorous spoken conversations from a classic comedy television show, FRIENDS, by examining acoustic-prosodic and linguistic features and their utility in automatic humor recognition. Using a simple annotation scheme, we automatically label speaker turns in our corpus that are followed by laughs as humorous and the rest as non-humorous. Our humor-prosody analysis reveals significant differences in prosodic characteristics (such as pitch, tempo, energy etc.) of humorous and non-humorous speech, even when accounted for the gender and speaker differences. Humor recognition was carried out using standard supervised learning classifiers, and shows promising results significantly above the baseline."
W06-1321,Discourse and Dialogue Processing in Spoken Intelligent Tutoring Systems,2006,-1,-1,1,1,6782,diane litman,Proceedings of the 7th {SIG}dial Workshop on Discourse and Dialogue,0,None
W06-0607,Manual Annotation of Opinion Categories in Meetings,2006,18,12,4,0,12223,swapna somasundaran,Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006,0,"This paper applies the categories from an opinion annotation scheme developed for monologue text to the genre of multiparty meetings. We describe modifications to the coding guidelines that were required to extend the categories to the new type of data, and present the results of an inter-annotator agreement study. As researchers have found with other types of annotations in speech data, inter-annotator agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Previous work exploited prosodic clues to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories would be a promising line of work."
P06-1025,Dependencies between Student State and Speech Recognition Problems in Spoken Tutoring Dialogues,2006,17,6,2,1,24249,mihai rotaru,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Speech recognition problems are a reality in current spoken dialogue systems. In order to better understand these phenomena, we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state: frustration/anger, certainty and correctness. We apply Chi Square (X2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns. Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems. We also find that tutoring, as a new domain for speech applications, exhibits interesting tradeoffs and new factors to consider for spoken dialogue design."
N06-1034,"Modelling User Satisfaction and Student Learning in a Spoken Dialogue Tutoring System with Generic, Tutoring, and User Affect Parameters",2006,21,24,2,1,33719,kate forbesriley,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We investigate using the PARADISE framework to develop predictive models of system performance in our spoken dialogue tutoring system. We represent performance with two metrics: user satisfaction and student learning. We train and test predictive models of these metrics in our tutoring system corpora. We predict user satisfaction with 2 parameter types: 1) system-generic, and 2) tutoring-specific. To predict student learning, we also use a third type: 3) user affect. Although generic parameters are useful predictors of user satisfaction in other PARADISE applications, overall our parameters produce less useful user satisfaction models in our system. However, generic and tutoring-specific parameters do produce useful models of student learning in our system. User affect parameters can increase the usefulness of these models."
N06-1035,Comparing the Utility of State Features in Spoken Dialogue Using Reinforcement Learning,2006,16,25,2,1,191,joel tetreault,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Recent work in designing spoken dialogue systems has focused on using Reinforcement Learning to automatically learn the best action for a system to take at any point in the dialogue to maximize dialogue success. While policy development is very important, choosing the best features to model the user state is equally important since it impacts the actions a system should make. In this paper, we compare the relative utility of adding three features to a model of user state in the domain of a spoken dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action."
J06-3004,Characterizing and Predicting Corrections in Spoken Dialogue Systems,2006,37,49,1,1,6782,diane litman,Computational Linguistics,0,"This article focuses on the analysis and prediction of corrections, defined as turns where a user tries to correct a prior error made by a spoken dialogue system. We describe our labeling procedure of various corrections types and statistical analyses of their features in a corpus collected from a train information spoken dialogue system. We then present results of machine-learning experiments designed to identify user corrections of speech recognition errors. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best-performing features reduce classification error from baselines of 25.70xe2x80x9328.99% to 15.72%."
E06-1037,Using Reinforcement Learning to Build a Better Model of Dialogue State,2006,14,30,2,1,191,joel tetreault,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Given the growing complexity of tasks that spoken dialogue systems are trying to handle, Reinforcement Learning (RL) has been increasingly used as a way of automatically learning the best policy for a system to make. While most work has focused on generating better policies for a dialogue manager, very little work has been done in using RL to construct a better dialogue state. This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems."
W05-0204,Predicting Learning in Tutoring with the Landscape Model of Memory,2005,5,2,2,1,44349,arthur ward,Proceedings of the Second Workshop on Building Educational Applications Using {NLP},0,"A Landscape Model analysis, adopted from the text processing literature, was run on transcripts of tutoring sessions, and a technique developed to count the occurrence of key physics points in the resulting connection matrices. This point-count measure was found to be well correlated with learning."
2005.sigdial-1.10,Using Bigrams to Identify Relationships Between Student Certainness States and Tutor Responses in a Spoken Dialogue Corpus,2005,-1,-1,2,1,33719,kate forbesriley,Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,0,None
W04-2326,Annotating Student Emotional States in Spoken Tutoring Dialogues,2004,12,26,1,1,6782,diane litman,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"We present an annotation scheme for student emotions in tutoring dialogues. Analyses of our scheme with respect to interannotator agreement and predictive accuracy indicate that our scheme is reliable in our domain, and that our emotion labels can be predicted with a high degree of accuracy. We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing."
P04-3028,Co-training for Predicting Emotions with Spoken Dialogue Data,2004,9,71,2,0,51735,beatriz maeireizo,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"Natural Language Processing applications often require large amounts of annotated training data, which are expensive to obtain. In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues. In order to do so, we have first applied the wrapper approach with Forward Selection and Naive Bayes, to reduce the dimensionality of our feature set. Our results show that Co-training can be highly effective when a good set of features are chosen."
P04-1045,Predicting Student Emotions in Computer-Human Tutoring Dialogues,2004,23,148,1,1,6782,diane litman,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We examine the utility of speech and lexical features for predicting student emotions in computer-human spoken tutoring dialogues. We first annotate student turns for negative, neutral, positive and mixed emotions. We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech. We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions. Our best results yield a 19-36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human-human tutoring dialogues."
N04-3002,{ITSPOKE}: An Intelligent Tutoring Spoken Dialogue System,2004,22,181,1,1,6782,diane litman,Demonstration Papers at {HLT}-{NAACL} 2004,0,"ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its back-end. A student first types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors."
N04-1026,Predicting Emotion in Spoken Dialogue from Multiple Knowledge Sources,2004,19,80,2,1,33719,kate forbesriley,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
W03-0409,Exceptionality and Natural Language Learning,2003,11,2,2,1,24249,mihai rotaru,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"Previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks. In this paper, we first attempt to generalize these results to a new set of language learning tasks from the area of spoken dialog systems and to a different abstraction-based learner. We then examine the utility of various exceptionality measures for predicting where one learner is better than the other. Our results show that generalization of previous results to our tasks is not so obvious and some of the exceptionality measures may be used to characterize the performance of our learners."
W03-0205,A Comparison of Tutor and Student Behavior in Speech Versus Text Based Tutoring,2003,26,11,2,0,2584,carolyn rose,Proceedings of the {HLT}-{NAACL} 03 Workshop on Building Educational Applications Using Natural Language Processing,0,"This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web interface. We present here a comparison between the two on a number of features of dialogue that have been demonstrated to correlate reliably with learning gains with students interacting with the tutor using the text based interface (Rose et al., submitted)."
N03-2018,Towards Emotion Prediction in Spoken Tutoring Dialogues,2003,8,26,1,1,6782,diane litman,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states."
W01-1610,Labeling Corrections and Aware Sites in Spoken Dialogue Systems,2001,15,5,3,0,9456,julia hirschberg,Proceedings of the Second {SIG}dial Workshop on Discourse and Dialogue,0,"This paper deals with user corrections and aware sites of system errors in the TOOT spoken dialogue system. We first describe our corpus, and give details on our procedure to label corrections and aware sites. Then, we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from 'normal' utterances. It appears that some correction types, such as simple repeats, are more likely to be correctly recognized than other types, such as paraphrases. We also present evidence that system dialogue strategy affects users' choice of correction type, suggesting that strategy-specific methods of detecting or coaching users on corrections may be useful. Aware sites tend to be shorter than other utterances, and are also more difficult to recognize correctly for the ASR system."
P01-1048,Predicting User Reactions to System Error,2001,12,22,1,1,6782,diane litman,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper focuses on the analysis and prediction of so-called aware sites, defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error. We describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus, which reveal significant prosodic differences between such turns, compared with turns that 'correct' speech recognition errors as well as with 'normal' turns that are neither aware sites nor corrections. We then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn, a correction, and/or an aware site."
N01-1027,Identifying User Corrections Automatically in Spoken Dialogue Systems,2001,16,35,2,0,9456,julia hirschberg,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present results of machine learning experiments designed to identify user corrections of speech recognition errors in a corpus collected from a train information spoken dialogue system. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best performing features reduce classification error from baselines of 25.70-28.99% to 15.72%."
W00-0304,{NJF}un- A Reinforcement Learning Spoken Dialogue System,2000,5,27,1,1,6782,diane litman,ANLP-NAACL 2000 Workshop: Conversational Systems,0,"This paper describes NJFun, a real-time spoken dialogue system that provides users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users."
C00-1073,Automatic Optimization of Dialogue Management,2000,11,70,1,1,6782,diane litman,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We then show that our approach measurably improves performance in an experimental system.
A00-2028,Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with {H}ow {M}ay {I} {H}elp {Y}ou?,2000,13,86,5,0.310039,6000,marilyn walker,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline."
A00-2029,Predicting Automatic Speech Recognition Performance Using Prosodic Cues,2000,23,67,1,1,6782,diane litman,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the TOOT train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone."
P99-1040,Automatic Detection of Poor Speech Recognition at the Dialogue Level,1999,18,65,1,1,6782,diane litman,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm."
P98-2129,Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent,1998,21,50,1,1,6782,diane litman,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT's cooperative rather than literal strategy contributes to greater performance."
C98-2124,Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent,1998,21,50,1,1,6782,diane litman,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT's cooperative rather than literal strategy contributes to greater performance."
W97-0601,Evaluating Interactive Dialogue Systems: Extending Component Evaluation to Integrated System Evaluation,1997,28,6,2,0.310039,6000,marilyn walker,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,This paper discusses the range of ways in which spoken dialogue system components have been evaluated and discusses approaches to evaluation that attempt to integrate component evaluation into an overall view of system performance. We will argue that the PARADISE (PARAdigm for DIalogue System Evaluation) framework has several advantages over other proposals.
P97-1035,{PARADISE}: A Framework for Evaluating Spoken Dialogue Agents,1997,32,406,2,0.310039,6000,marilyn walker,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity."
J97-1005,Discourse Segmentation by Human and Automated Means,1997,62,171,2,0.367592,721,rebecca passonneau,Computational Linguistics,0,"The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multitutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves."
P95-1015,Combining Multiple Knowledge Sources for Discourse Segmentation,1995,31,99,1,1,6782,diane litman,33rd Annual Meeting of the Association for Computational Linguistics,1,"We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data. We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning)."
W93-0216,Empirical Evidence for Intention-Based Discourse Segmentation,1993,-1,-1,1,1,6782,diane litman,Intentionality and Structure in Discourse Relations,0,None
P93-1020,Intention-Based Segmentation: Human Reliability and Correlation With Linguistic Cues,1993,23,133,2,0.367592,721,rebecca passonneau,31st Annual Meeting of the Association for Computational Linguistics,1,"Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics."
J93-3003,Empirical Studies on the Disambiguation of Cue Phrases,1993,33,244,2,0.3125,9456,julia hirschberg,Computational Linguistics,0,"Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed.This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech."
H92-1103,Extracting Constraints on Word Usage from Large Text Corpora,1992,0,1,2,0,895,kathleen mckeown,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,Our research focuses on the identification of word usage constraints from large text corpora. Such constraints are useful both for the problem of selecting vocabulary for language generation and for disambiguating lexical meaning in interpretation. We are developing systems that can automatically extract such constraints from corpora and empirical methods for analyzing text. Identified constraints will be represented in a lexicon that will be tested computationally as part of a natural language system. We are also identifying lexical constraints for machine translation using the aligned Hansard corpus as training data and are identifying many-to-many word alignments.
C90-2044,Disambiguating Cue Phrases in Text and Speech,1990,18,34,1,1,6782,diane litman,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Cue phrases are linguistic expressions such as 'now' and 'well' that may explicitly mark the structure of a discourse. For example, while the cue phrase 'incidentally' may be used SENTENTIALLY as an adverbial, the DISCOURSE use initiates a digression. In [8], we noted the ambiguity of cue phrases with respect to discourse and sentential usage and proposed an intonational model for their disambiguation. In this paper, we extend our previous characterization of cue phrases and generalize its domain of coverage, based on a larger and more comprehensive empirical study: an examination of all cue phrases produced by a single speaker in recorded natural speech. We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational models of discourse and practical application to the generation of synthetic speech."
P87-1023,Now Let{'}s Talk About Now; Identifying Cue Phrases Intonationally,1987,18,75,2,0.3125,9456,julia hirschberg,25th Annual Meeting of the Association for Computational Linguistics,1,"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent."
P86-1033,Linguistic Coherence: A Plan-Based Alternative,1986,22,23,1,1,6782,diane litman,24th Annual Meeting of the Association for Computational Linguistics,1,"To fully understand a sequence of utterances, one must be able to infer implicit relationships between the utterances. Although the identification of sets of utterance relationships forms the basis for many theories of discourse, the formalization and recognition of such relationships has proven to be an extremely difficult computational task.This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances. Relationships are formulated as discourse plans, which allows their representation in terms of planning operators and their computation via a plan recognition process. By incorporating complex inferential processes relating utterances into a plan-based framework, a formalization and computability not available in the earlier works is provided."
P84-1063,A Plan Recognition Model for Clarification Subdialogues,1984,18,27,1,1,6782,diane litman,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"One of the promising approaches to analyzing task-oriented dialogues has involved modeling the plans of the speakers in the task domain. In general, these models work well as long as the topic follows the task structure closely, but they have difficulty in accounting for clarification subdialogues and topic change. We have developed a model based on a hierarchy of plans and metaplans that accounts for the clarification subdialogues while maintaining the advantages of the plan-based approach."
