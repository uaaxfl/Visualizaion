2020.aacl-demo.7,P18-1128,0,0.0606777,"Missing"
2020.aacl-demo.7,1993.eamt-1.1,0,0.640358,"ter a significance test to determine the relation between sample size and power. There are two scenarios associated with retrospective power analysis: When the values in {ui −vi } are from a known distribution, one can use Monte Carlo simulation to directly simulate from this known distribution. To do this, one has to have an informed guess of the desired effect size (i.e., mean difference) via meta-analysis of previous studies. When the distribution of the sample is unknown a priori, one can resample with replacement from the empirical distribution of the sample (a.k.a. the bootstrap method (Efron and Tibshirani, 1993)) to estimate the power. NLPStatTest implements both methods. Users can employ one or both; NLPStatTest 3.4.1 Prospective Power Analysis Prospective power analysis is used when planning a study (usually in clinical trials) in order to decide how many subjects are needed. In the NLP field, when one constructs or chooses a test corpus for evaluation, it will be beneficial to conduct this type of power analysis to determine how big a corpus needs to be in order to ensure that the significance test reaches the desired power level. In NLPStatTest, prospective power analysis is a preliminary and opt"
2020.aacl-demo.7,W12-0401,0,0.028818,"Missing"
2020.aacl-demo.7,P19-1171,0,0.0189888,"lying distribution, NLPStatTest checks the skewness of {ui − vi } by estimating the sample skewness (γ). Based on the γ value, we use the following rule of thumb (Bulmer, 1979) to determine whether NLPStatTest would recommend the use of mean or median as the test statistic for statistical significance testing: 3.3 Effect Size Effect size can be estimated by different effect size indices, depending on the data types (numerical or categorical) and significance tests. Dror et al. (2020) defined effect size as the unstandardized difference between system performance, while Hauch et al. (2012) and Pimentel et al. (2019) used the standardized difference. NLPStatTest implements the following four indices. Once users select one or more, NLPStatTest will calculate effect size accordingly and display the results. Cohen’s d estimates the standardized mean difference by u ˆ − vˆ d= (2) σ ˆ where vˆ and u ˆ are the sample means and σ ˆ denote standard deviation of u − v. Cohen’s d assumes • |γ |∈ [0, 0.5): roughly symmetric (use mean) • |γ |∈ [0.5, 1): slightly skewed (use median) 43 normality and is one of the most frequently used effect size indices. If Cohen’s d, or any other effect size indices depending on σ ˆ"
2020.aacl-demo.7,2020.acl-main.506,0,0.0406646,"Missing"
2020.aacl-demo.7,W17-4717,0,0.0117609,"ide web interface is written in HTML, CSS, and JavaScript (with JQuery). The server-side code is written in Python, using the Flask web framework. YAML is used for configuration files. KaTeX is used to render mathematical symbols. The Python code uses the SciPy and NumPy libraries to implement statistical tests and Matplotlib to generate the histograms and graphs. 5 Because NLPStatTest currently implements paired testing only, we use the Wilcoxon signedrank test (instead of Wilcoxon rank-sum test) and the BLEU scores (instead of human evaluation scores) when comparing MT systems. According to Bojar et al. (2017), a set of 15 or more sentencelevel evaluation scores constitutes a reliable measure of translation quality; thus, we set the EU size to be 15. We also reshuffled the scores before grouping test instances into evaluation units. Figure 4 shows the results of pairwise comparisons among all 16 Chinese-to-English MT systems (120 system pairs in total). The heatmap is similar to the comparison results in Bojar et al. (2017) (see Figure 5 in that paper). The minor differences of the two heatmaps are due to different evaluation metrics (BLEU vs. human scores), the significant tests (Wilcoxon signed-r"
2020.acl-main.734,D15-1141,0,0.592706,"onally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent ap"
2020.acl-main.734,P17-1110,0,0.372844,"ese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are prove"
2020.acl-main.734,D17-1070,0,0.02872,"reating that n-gram as a segment: DL(D) = − Word Embedding Size Hidden State Size Hidden State Layers Key Embedding Size Value Embedding Size Dropout Rate Bi-LSTM Model Implementation Following previous studies (Sun and Xu, 2011; Chen et al., 2015, 2017; Ma et al., 2018; Qiu et al., 2019), we use four segmentation labels in our experiments, i.e., T = {B, I, E, S}. Among them, B, I, and E indicate a character is the beginning, inside, and the ending of a word and S denotes that the character is a single-character word. Since text representation plays an important role to facilitate many tasks (Conneau et al., 2017; Song et al., 2017, 2018; Sileo et al., 2019), we try two effective and well-known encoders, i.e., Bi-LSTM and BERT4 . In addition, we test WMS EG on a pretrained encoder for Chinese language, i.e., ZEN5 (Diao et al., 2019), which learns n-gram information in its pre-training from large raw corpora and outperforms BERT on many Chinese NLP tasks. Table 5 shows the hyperparameter settings for all the encoders: for the Bi-LSTM encoder, we follow the setting of Chen et al. (2015) and adopt their character embeddings for exi , and for BERT and ZEN encoders, we follow the default settings in their"
2020.acl-main.734,J09-4006,0,0.0296813,"guish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al."
2020.acl-main.734,N19-1423,0,0.20461,"tistics of the five benchmark datasets, in terms of the number of character and word tokens and types in each training and test set. Out-of-vocabulary (OOV) rate is the percentage of unseen word tokens in the test set. where Wo is a trainable parameter and the output ai ∈ R|T |is a weight vector with its each dimension corresponding to a segmentation label. 2.3 Text Encoders and Decoders To ensure wordhood memory networks functionalize, one requires to generate hi for each xi by [h1 , h2 , ..., hi , ..., hl ] = Encoder(X ) (6) where the Encoder can be different models, e.g., Bi-LSTM and BERT (Devlin et al., 2019), to represent a sequence of Chinese characters into vectors. Once all ai are generated from the memory for each xi , a decoder takes them to predict a sequence of segmentation labels Yb = yb1 yb2 · · · ybl for X by Yb = Decoder(A) (7) where A = a1 a2 · · · ai · · · al is the sequence of output from Eq. 5. The Decoder can be implemented by different algorithms, such as softmax: exp(ati ) ybi = arg max P|T | t t=1 exp(ai ) (8) where ati is the value at dimension t in ai . Or one can use CRF for the Decoder: exp(Wc · ai + bc ) yi−1 yi exp(Wc · ai ) + bc ybi = arg max P yi ∈T (9) where Wc ∈ R|T |"
2020.acl-main.734,C18-2004,0,0.0148075,"ther experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.1 1 Introduction Unlike most written languages in the world, the Chinese writing system does not use explicit delimiters (e.g., white space) to separate words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextu"
2020.acl-main.734,D18-1529,0,0.175184,"Missing"
2020.acl-main.734,I05-3017,0,0.495569,"e Decoder can be implemented by different algorithms, such as softmax: exp(ati ) ybi = arg max P|T | t t=1 exp(ai ) (8) where ati is the value at dimension t in ai . Or one can use CRF for the Decoder: exp(Wc · ai + bc ) yi−1 yi exp(Wc · ai ) + bc ybi = arg max P yi ∈T (9) where Wc ∈ R|T |×|T |and bc ∈ R|T |are trainable parameters to model the transition for yi−1 to yi . BC C HAR # W ORD # C HAR T YPE # W ORD T YPE # OOV R ATE 3.1 Experimental Settings Datasets We employ five benchmark datasets in our experiments: four of them, namely, MSR, PKU, AS, and C ITY U, are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the fifth one is CTB6 (Xue et al., 2005). AS and C ITY U are in traditional Chinese characters whereas the other three use simplified MZ NW W EB 275K 483K 403K 443K 342K 184K 287K 258K 260K 210K 3K 3K 4K 3K 4K 12K 23K 26K 21K 21K 3.4 6.0 8.9 5.9 7.1 Table 3: Statistics of CTB7 with respect to five different genres. The OOV rate for each genre is computed based on the vocabulary from all the other four genres. ones. Following previous studies (Chen et al., 2015, 2017; Qiu et al., 2019), we convert traditional Chinese characters in AS and C ITY U into simplified ones.3 For MSR, AS, PKU, and"
2020.acl-main.734,J04-1004,0,0.239292,"quence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by incorporating contextual n-grams (Pei"
2020.acl-main.734,N19-1276,0,0.411604,"for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al.,"
2020.acl-main.734,W99-0701,0,0.099425,"Missing"
2020.acl-main.734,W06-0115,0,0.174747,"he proposed memory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, wh"
2020.acl-main.734,P11-1141,0,0.0261476,"in context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; S"
2020.acl-main.734,P15-1167,0,0.0196888,"ys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress a"
2020.acl-main.734,I13-1181,0,0.0286822,"CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao e"
2020.acl-main.734,D16-1147,0,0.433032,"EG. “N ” denotes a lexicon constructed by wordhood measures. N-grams (keys) appearing in the input sentence “部分居民生活水平” (some residents’ living standard) and the wordhood information (values) of those n-grams are extracted from the lexicon. Then, together with the output from the text encoder, n-grams (keys) and their wordhood information (values) are fed into the memory module, whose output passes through a decoder to get final predictions of segmentation labels for every character in the input sentence. CWS by leveraging wordhood information. In detail, we utilize key-value memory networks (Miller et al., 2016) to incorporate character n-grams with their wordhood measurements in a general sequence labeling paradigm, where the memory module can be incorporated with different prevailing encoders (e.g., BiLSTM and BERT) and decoders (e.g., softmax and CRF). For the memory, we map n-grams and their wordhood information to keys and values in it, respectively, and one can use different wordhood measures to generate such information. Then for each input character, the memory module addresses all the n-grams in the key list that contain the character and uses their corresponding values to generate an output"
2020.acl-main.734,P14-1028,0,0.202698,"ion (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling met"
2020.acl-main.734,C04-1081,0,0.4212,"some extent, that the proposed memory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model pe"
2020.acl-main.734,N16-1176,0,0.0402135,"Missing"
2020.acl-main.734,W14-3916,0,0.0115495,"1; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017), recurrent neural models (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019) and even adversarial learning (Chen et al., 2017). To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et al., 2019), where"
2020.acl-main.734,N19-1351,0,0.0811794,"Missing"
2020.acl-main.734,W06-0137,1,0.813609,"xt. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following th"
2020.acl-main.734,W09-3511,1,0.699376,"Missing"
2020.acl-main.734,C12-2116,1,0.870706,"75.34 70.7 77.33 76.39 95.55 97.2 96.22 96.91 81.40 87.5 73.58 86.91 96.0 95.8 95.95 96.2 96.7 96.4 - 85.4 - WMS EG (BERT-CRF) WMS EG (ZEN-CRF) 98.28 98.40 86.67 84.87 96.51 96.53 86.76 85.36 96.58 96.62 78.48 79.64 97.80 97.93 87.57 90.15 97.16 97.25 88.00 88.46 Table 7: Performance (F-score) comparison between WMS EG (BT-CRF and ZEN-CRF with woodhood memory networks) and previous state-of-the-art models on the test set of five benchmark datasets. 4.2 Cross-Domain Performance As domain variance is always an important factor affecting the performance of NLP systems especially word semgenters (Song et al., 2012; Song and Xia, 2013), in addition to the experiments on benchmark datasets, we also run WMS EG on CTB7 across domains (genres in this case) with and without the memory module. To test on each genre, we use the union of the data from the other four genres to train our segmenter and use AV to extract n-grams from the entire raw text from CTB7 in this experiment. Table 8 reports the results in Fscore and OOV recall, which show a similar trend as that in Table 6, where WMS EG outperforms baselines for all five genres. Particularly, for genres with large domain variance (e.g., the ones with high O"
2020.acl-main.734,P98-2206,0,0.795795,"https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by"
2020.acl-main.734,D11-1090,0,0.705009,"ese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the seq"
2020.acl-main.734,2020.acl-main.735,1,0.413504,"els, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et al., 2019), where Higashiyama et al. (2019) introduced a word attention mechanism to learn from large granular texts during the CWS process. In addition, the studies from Chen et al. (2017) and Qiu et al. (2019) try to improve CWS by learning from data annotated through different segmentation criteria. Moreover, there is a study leveraging auto-analyzed syntactic knowledge obtained from off-the-shelf toolkits to help CWS and part-of-speech tagging (Tian et al., 2020). Compare to these studies, WMS EG offers an alternative solution to robustly enhancing neural CWS models without requiring external resources. 6 Conclusion In this paper, we propose WMS EG, a neural framework for CWS using wordhood memory networks, which maps n-grams and their wordhood information to keys and values in it and appropriately models the values according to the importance of keys in a specific context. The framework follows the sequence labeling paradigm, and the encoders and decoders in it can be implemented by various prevailing models. To the best of our knowledge, this is the"
2020.acl-main.734,I05-3027,0,0.406871,"words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012)."
2020.acl-main.734,I17-1017,0,0.0284743,"coder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrich"
2020.acl-main.734,K17-1016,1,0.448121,"a segment: DL(D) = − Word Embedding Size Hidden State Size Hidden State Layers Key Embedding Size Value Embedding Size Dropout Rate Bi-LSTM Model Implementation Following previous studies (Sun and Xu, 2011; Chen et al., 2015, 2017; Ma et al., 2018; Qiu et al., 2019), we use four segmentation labels in our experiments, i.e., T = {B, I, E, S}. Among them, B, I, and E indicate a character is the beginning, inside, and the ending of a word and S denotes that the character is a single-character word. Since text representation plays an important role to facilitate many tasks (Conneau et al., 2017; Song et al., 2017, 2018; Sileo et al., 2019), we try two effective and well-known encoders, i.e., Bi-LSTM and BERT4 . In addition, we test WMS EG on a pretrained encoder for Chinese language, i.e., ZEN5 (Diao et al., 2019), which learns n-gram information in its pre-training from large raw corpora and outperforms BERT on many Chinese NLP tasks. Table 5 shows the hyperparameter settings for all the encoders: for the Bi-LSTM encoder, we follow the setting of Chen et al. (2015) and adopt their character embeddings for exi , and for BERT and ZEN encoders, we follow the default settings in their papers (Devlin et a"
2020.acl-main.734,P11-1129,0,0.0371253,"al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017), recurrent neural models (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019) and even adversarial learning (Chen et al., 2017). To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et a"
2020.acl-main.734,N18-2028,1,0.651549,"Missing"
2020.acl-main.734,P16-2092,0,0.0294413,"Missing"
2020.acl-main.734,song-xia-2012-using,1,0.821784,"k (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by incorporating contextual n-grams (Pei et al., 2014; Zhou et"
2020.acl-main.734,I13-1071,1,0.72415,".39 95.55 97.2 96.22 96.91 81.40 87.5 73.58 86.91 96.0 95.8 95.95 96.2 96.7 96.4 - 85.4 - WMS EG (BERT-CRF) WMS EG (ZEN-CRF) 98.28 98.40 86.67 84.87 96.51 96.53 86.76 85.36 96.58 96.62 78.48 79.64 97.80 97.93 87.57 90.15 97.16 97.25 88.00 88.46 Table 7: Performance (F-score) comparison between WMS EG (BT-CRF and ZEN-CRF with woodhood memory networks) and previous state-of-the-art models on the test set of five benchmark datasets. 4.2 Cross-Domain Performance As domain variance is always an important factor affecting the performance of NLP systems especially word semgenters (Song et al., 2012; Song and Xia, 2013), in addition to the experiments on benchmark datasets, we also run WMS EG on CTB7 across domains (genres in this case) with and without the memory module. To test on each genre, we use the union of the data from the other four genres to train our segmenter and use AV to extract n-grams from the entire raw text from CTB7 in this experiment. Table 8 reports the results in Fscore and OOV recall, which show a similar trend as that in Table 6, where WMS EG outperforms baselines for all five genres. Particularly, for genres with large domain variance (e.g., the ones with high OOV rates such as MZ a"
2020.acl-main.734,P12-1083,0,0.0759507,"other three use simplified MZ NW W EB 275K 483K 403K 443K 342K 184K 287K 258K 260K 210K 3K 3K 4K 3K 4K 12K 23K 26K 21K 21K 3.4 6.0 8.9 5.9 7.1 Table 3: Statistics of CTB7 with respect to five different genres. The OOV rate for each genre is computed based on the vocabulary from all the other four genres. ones. Following previous studies (Chen et al., 2015, 2017; Qiu et al., 2019), we convert traditional Chinese characters in AS and C ITY U into simplified ones.3 For MSR, AS, PKU, and C ITY U, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Chen et al. (2015); Higashiyama et al. (2019), and only use its test set for the final experiment. Table 2 show the statistics of all datasets in terms of the number of characters and words and the percentage of out-of-vocabulary (OOV) words in the dev/test sets with respect to the training set. In addition, we also use CTB7 (LDC2010T07) to perform our cross-domain experiments. There are five genres in CTB7, including broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and weblog (W EB). The statistics of all the genres are reported in Table 3, where the OOV rate"
2020.acl-main.734,N18-1122,0,0.0848416,"Missing"
2020.acl-main.734,D18-1351,1,0.837204,"t performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.1 1 Introduction Unlike most written languages in the world, the Chinese writing system does not use explicit delimiters (e.g., white space) to separate words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were p"
2020.acl-main.734,D13-1031,0,0.404153,"ainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less atte"
2020.acl-main.734,P16-1040,0,0.0819743,"e first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural"
2020.acl-main.734,W06-0127,0,0.0614675,"emory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observe"
2020.acl-main.734,I08-1002,0,0.236306,"oadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and weblog (W EB). The statistics of all the genres are reported in Table 3, where the OOV rate for each genre is computed according to the union of all other genres. For example, the OOV rate for BC is computed with respect to the union of BN, MZ, NW, and W EB. 3.2 3 BN Wordhood Measures We experiment with three wordhood measures to construct N . The main experiment adopts the aforementioned AV as the measure to rank all ngrams, because AV was shown to be the most effective wordhood measure in previous CWS studies (Zhao and Kit, 2008). Since AV is sensitive to 3 The conversion scripts are from https://github. com/skydark/nstools/tree/master/zhtools AV PMI DLG MSR PKU AS C ITY U CTB6 49K 18K 32K 71K 16K 22K 105K 22K 32K 104K 21K 27K 50K 16K 16K Table 4: The size of lexicon N generated from different wordhood measures under our settings. corpus size, in our experiments we use different AV thresholds when building the lexicon for each dataset: the threshold is 2 for PKU, C ITY U, CTB6 and CTB7, and 5 for MSR and AS. To test the the robustness of WMS EG, we also try two other wordhood measures, i.e., PMI (Sun et al., 1998) and"
2020.acl-main.734,D17-1079,0,0.305042,"y of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to th"
2020.acl-main.735,P06-2013,0,0.0821828,"eatures and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and the knowledge way, are identical in architecture, where each way has a feed-forward attention module (Raffel and Ellis, 2015). For each xi , its Si and Ki are firstly fed into the feature attention way and the knowledge attention way, respectively, then computed within each way, and their final attention vectors are combined to feedback to the backbone model. Take the feature way as an example, the attention 3 Following Chen et al. (2006), the list has 12 syntactic labels, namely, ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, and VP. weight for each context feature si,j is computed by s exp(h&gt; i · ei,j ) asi,j = Pmi &gt; s j=1 exp(hi · ei,j ) (1) where hi is the vector from a text encoder for xi and esi,j the embedding of si,j . Then we have the weighted embedding asi for all si,j in Si via asi = mi X asi,j esi,j (2) j=1 P where denotes a element-wise sum operation. For the knowledge way, the same process is applied to get aki by distinguishing and weighting each knowledge instance ki,j . Finally, the output of the two att"
2020.acl-main.735,N19-1423,0,0.138063,"n Si via asi = mi X asi,j esi,j (2) j=1 P where denotes a element-wise sum operation. For the knowledge way, the same process is applied to get aki by distinguishing and weighting each knowledge instance ki,j . Finally, the output of the two attention ways are obtained through an concatenation of the two vectors: ai = asi ⊕ aki . 2.3 Joint Tagging with Two-way Attentions To functionalize the joint tagging, the two-way attentions interact with the backbone model through the encoded vector hi and its output ai for each xi . For hi , one can apply many prevailing encoders, e.g., Bi-LSTM or BERT (Devlin et al., 2019), to get the vector list [h1 h2 · · · hi · · · hl ] for X . Once ai is obtained, we concatenate it with hi and send it through a fully connected layer to align the dimension of the output for final prediction: oi = W · (hi ⊕ ai ) + b (3) where W and b are trainable parameters. Afterwards, conditional random fields (CRF) is used to estimate the probability for yi over all possible joint CWS and POS tags under xi and yi−1 by exp(Wc · oi + bc ) yi−1 yi exp(Wc · oi + bc ) p(yi |xi ) = P (4) Here, Wc and bc are the weight matrix and the bias vector, respectively, and they are estimated using the (y"
2020.acl-main.735,L18-1550,0,0.0126971,"ntly released Chinese encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-LSTM + POS (SCT) + Sy"
2020.acl-main.735,N09-1032,0,0.0316382,"performs previous studies on the joint task and achieves new state-of-the-art performance on all datasets. While some of the previous studies use auto-analyzed knowledge (Wang et al., 2011; Zhang et al., 2018), they regard such knowledge as gold reference and consequently could suffer from errors in the auto-analyzed results. In contrast, our proposed model is able to selectively model the input information and to discriminate useful knowledge instances through the two-way attentions. 3.4 Cross-Domain Performance Domain variance is an important factor affecting the performance of NLP systems (Guo et al., 2009; McClosky et al., 2010; Song and Xia, 2013). To further demonstrate the effectiveness of T WASP, we conduct cross-domain experiments on the eight genres of CTB9 using BERT and ZEN as the baseline and their enhanced version with POS knowledge from SCT. In doing so, we test on each genre with the models trained on the data from all other genres. The results on both segmentation and the joint task are reported in Table 5, where the SCT results are also included as a reference. The comparison between the baselines and T WASP with POS knowledge clearly shows the consistency of performance improvem"
2020.acl-main.735,D19-1549,0,0.0628014,"oposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model does not treat knowledge as gold references: i"
2020.acl-main.735,D07-1117,0,0.269741,"rface word order, they are much closer in the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learn"
2020.acl-main.735,P09-1059,0,0.329979,"ey are much closer in the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learning from auto-analyz"
2020.acl-main.735,P08-1102,0,0.201522,"Missing"
2020.acl-main.735,P18-1249,0,0.0252787,"c tagset (42 tags for Chinese). We refer to the corpus with the two tagsets as UD1 and UD2, respectively, and use the official splits of train/dev/test in our experiments. The statistics for the aforementioned datasets are in Table 1. 5 We use its version 2.4 downloaded from https:// universaldependencies.org/. 6 The conversation scripts are from https://github. com/skydark/nstools/tree/master/zhtools Implementation To obtain the aforementioned three types of knowledge, we use two off-the-shelf toolkits, Stanford CoreNLP Toolkit (SCT)7 (Manning et al., 2014) and Berkeley Neural Parser (BNP)8 (Kitaev and Klein, 2018): the former tokenizes and parses a Chinese sentence, producing POS tags, phrase structure and dependency structure of the sentence; the latter does POS tagging and syntactic parsing on a pre-tokenized sentence. Both toolkits were trained on CTB data and thus produced CTB POS tags. To extract knowledge, we firstly use SCT to automatically segment sentences and then run both SCT and BNP for POS tagging and parsing. Table 2 shows the size of S and K for all the datasets. We test the model with three encoders: two of them, namely Bi-LSTM and BERT9 (Devlin et al., 2019), are widely used; the third"
2020.acl-main.735,P09-1058,0,0.184232,"Missing"
2020.acl-main.735,N18-2041,0,0.0894766,"fore, S5 = [“结合”, “成”, “分子”] and K5 = [“结合 VP”, “成 VP”, “分子 VP”]. Dependency Relations Given a character xi , let wi be the word that contains xi . The context features Si include wi , wi ’s governor, and wi ’s dependents in the dependency structure; those words combined with their inbound dependency relation labels form Ki . For example, for x6 =“分”, w6 = “分子”, which depends on “结合” with a dependency label dobj. Therefore, S6 = [“分子”, “结 合”], and K6 = [“分子 obj”, “结合 root”]. 2.2 Two-Way Attentions Attention has been shown to be an effective method for incorporating knowledge into NLP systems (Kumar et al., 2018; Margatina et al., 2019) but it cannot be used directly for feature and knowledge in pair-wise forms. Previous studies on the joint task normally directly concatenate the embeddings from context features and knowledge instances into the embeddings of characters (Zhang et al., 2018), which could be problematic for incorporating auto-analyzed, error-prone syntactic knowledge obtained from off-the-shelf toolkits. For both features and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and"
2020.acl-main.735,P17-1111,0,0.391604,"biguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗ Partially done as an intern at Sinovation"
2020.acl-main.735,P14-5010,0,0.00535636,"amely the universal tagset (15 tags) and language-specific tagset (42 tags for Chinese). We refer to the corpus with the two tagsets as UD1 and UD2, respectively, and use the official splits of train/dev/test in our experiments. The statistics for the aforementioned datasets are in Table 1. 5 We use its version 2.4 downloaded from https:// universaldependencies.org/. 6 The conversation scripts are from https://github. com/skydark/nstools/tree/master/zhtools Implementation To obtain the aforementioned three types of knowledge, we use two off-the-shelf toolkits, Stanford CoreNLP Toolkit (SCT)7 (Manning et al., 2014) and Berkeley Neural Parser (BNP)8 (Kitaev and Klein, 2018): the former tokenizes and parses a Chinese sentence, producing POS tags, phrase structure and dependency structure of the sentence; the latter does POS tagging and syntactic parsing on a pre-tokenized sentence. Both toolkits were trained on CTB data and thus produced CTB POS tags. To extract knowledge, we firstly use SCT to automatically segment sentences and then run both SCT and BNP for POS tagging and parsing. Table 2 shows the size of S and K for all the datasets. We test the model with three encoders: two of them, namely Bi-LSTM"
2020.acl-main.735,P19-1385,0,0.0346658,"”, “分子”] and K5 = [“结合 VP”, “成 VP”, “分子 VP”]. Dependency Relations Given a character xi , let wi be the word that contains xi . The context features Si include wi , wi ’s governor, and wi ’s dependents in the dependency structure; those words combined with their inbound dependency relation labels form Ki . For example, for x6 =“分”, w6 = “分子”, which depends on “结合” with a dependency label dobj. Therefore, S6 = [“分子”, “结 合”], and K6 = [“分子 obj”, “结合 root”]. 2.2 Two-Way Attentions Attention has been shown to be an effective method for incorporating knowledge into NLP systems (Kumar et al., 2018; Margatina et al., 2019) but it cannot be used directly for feature and knowledge in pair-wise forms. Previous studies on the joint task normally directly concatenate the embeddings from context features and knowledge instances into the embeddings of characters (Zhang et al., 2018), which could be problematic for incorporating auto-analyzed, error-prone syntactic knowledge obtained from off-the-shelf toolkits. For both features and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and the knowledge way, are i"
2020.acl-main.735,D16-1147,0,0.312264,"posed attention module extracts the context features associated with the character and their corresponding knowledge instances according to the auto-analyzed results, then computes the attentions separately for features and knowledge in each attention way, and finally concatenates the attentions from two ways to guide the tagging process. In doing so, our model can distinguish the important auto-analyzed knowledge based on their contributions to the task and thus avoid being influenced by some inferior knowledge instances. Compared to another prevailing model, i.e., key-value memory networks (Miller et al., 2016), which can learn from pair-wisely organized information, the twoway attentions not only are able to do so, but also fully leverage features and their knowledge rather than using one to weight the other.2 We experiment with three types of knowledge, namely, POS labels, syntactic constituents, and dependency relations, in our experiments. The experimental results on five benchmark datasets illustrate the effectiveness of our model, where state-of-the-art performance for the joint task is achieved on all datasets. We also perform several analyses, which confirm the validity of using two-way atte"
2020.acl-main.735,W04-3236,0,0.643969,"r joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.1 1 Figure 1: An example sentence with CWS and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-of"
2020.acl-main.735,L16-1262,0,0.0225018,"Missing"
2020.acl-main.735,D12-1046,0,0.104152,"Missing"
2020.acl-main.735,P18-2039,0,0.113768,"bone paradigm, the proposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model does not treat knowle"
2020.acl-main.735,I17-1018,0,0.433143,"n color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗ Partially done as an intern at Sinovation Ventures. Correspo"
2020.acl-main.735,P14-2042,0,0.156438,"Missing"
2020.acl-main.735,K17-1016,1,0.380283,"To enhance the backbone paradigm, the proposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model"
2020.acl-main.735,2020.acl-main.734,1,0.413504,"icates the validity of using two attention ways for features and knowledge, i.e., compared to (1), as well as the advantage of learning from both of them, i.e., compared to (2) and (3). Interestingly, in the three settings, (3) outperforms (1), which could be explained by that, with normal attention, mixed feature and knowledge instances in it may make it difficult to weight them for the joint task. There are other methods for using both context features and knowledge in a neural framework, such as key-value memory networks (kvMN) (Miller et al., 2016), which is demonstrated to improve CWS by Tian et al. (2020). Thus we compare our approach with kvMN, in which context features are mapped to keys and knowledge to values. We follow the standard protocol of the kvMN, e.g., addressing keys by Si and reading values from Ki through the corresponding knowledge for each key, computing weights from all key embeddings, and outputting the weighted embeddings from all values. The result from the kvMN is reported at the last row of Table 6, where its performance is not as good as the two-way attentions, and even 8292 Genre BC BN CS DF MZ NW SC WB Seg SCT Joint 96.27 96.98 89.83 91.34 95.69 97.41 84.87 95.99 BERT"
2020.acl-main.735,I11-1035,0,0.789335,"n the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learning from auto-analyzed syntactic knowle"
2020.acl-main.735,P13-1076,0,0.266162,"S and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗"
2020.acl-main.735,D19-1528,1,0.81449,"e encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-LSTM + POS (SCT) + Syn. (SCT) + Dep. (SCT"
2020.acl-main.735,N18-2028,1,0.510158,"9), which is a recently released Chinese encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-L"
2020.acl-main.735,I13-1071,1,0.549957,"sk and achieves new state-of-the-art performance on all datasets. While some of the previous studies use auto-analyzed knowledge (Wang et al., 2011; Zhang et al., 2018), they regard such knowledge as gold reference and consequently could suffer from errors in the auto-analyzed results. In contrast, our proposed model is able to selectively model the input information and to discriminate useful knowledge instances through the two-way attentions. 3.4 Cross-Domain Performance Domain variance is an important factor affecting the performance of NLP systems (Guo et al., 2009; McClosky et al., 2010; Song and Xia, 2013). To further demonstrate the effectiveness of T WASP, we conduct cross-domain experiments on the eight genres of CTB9 using BERT and ZEN as the baseline and their enhanced version with POS knowledge from SCT. In doing so, we test on each genre with the models trained on the data from all other genres. The results on both segmentation and the joint task are reported in Table 5, where the SCT results are also included as a reference. The comparison between the baselines and T WASP with POS knowledge clearly shows the consistency of performance improvement with twoway attentions, where for both B"
2020.acl-main.735,P11-1139,0,0.186691,"nce with CWS and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one a"
2020.coling-main.187,D17-1070,0,0.0298199,"ur Model Len. (b) Results from ZEN Table 3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that ar"
2020.coling-main.187,N19-1423,0,0.221929,"use “大” is a component of s1 and s2 but is (4) 3 not a part of s3 . As a result, for each entire channel, its resulted weight is computed by (k) ai = mk X (k) (k) ai,j ej (3) j=1 Finally, the overall attention of different n-grams for xi is the concatenation of attentions from all channels: M (k) ai = δk ai (4) n with a trainable positive parameter δk to balance the contribution of each channel. 2.2 Joint Tagging with the Attentions To leverage the n-grams through the proposed attention module, we first obtain the hidden vector hi of each xi in the input sequence from the encoder (e.g., BERT (Devlin et al., 2019)) of the backbone model. Next, we feed the resulting hi to the attention module and obtain its output ai , which contains the weighted contextual information carried by the n-gram features. Then, we incorporate such weighted information into the backbone model by concatenating ai with hi and align the resulting vector to the output dimension by a trainable matrix Wd , which is represented by ui = Wd · (hi ⊕ ai ) (5) Afterwards, we pass ui to a conditional random field (CRF) decoder to estimate the joint label ybi for xi . 3 Experiment Settings 3.1 Datasets In our experiments, five Chinese benc"
2020.coling-main.187,J04-1004,0,0.0345089,"N CTB5 DLG PMI 34.5K 13.5K 10.6K AV CTB6 DLG PMI 29.7K 16.6K 13.6K AV CTB7 DLG PMI 27.9K 18.9K 16.6K AV CTB9 DLG PMI 38.1K 27.5K 28.9K AV UD DLG PMI 19.6K 5.3K 1.1K Table 2: The size of lexicon N constructed by AV, DLG, and PMI. 3.2 Lexicon Construction To enhance the joint CWS and POS tagging through the multi-channel attentions, we need to construct the lexicon N which is simply a list of n-grams.6 In this study, we do not want our approach to rely on existing n-gram resources. Therefore, we use three unsupervised methods to obtain n-grams from each datasets, namely, accessor variety (AV) (Feng et al., 2004), description length gain (DLG) (Kit and Wilks, 1999), and pointwise mutual information (PMI) (Sun et al., 1998). Accessor Variety Given a character n-gram s, let left access number Lav (s) be the number of distinct characters that precede s in the corpus. The right access number Rav (s) is defined similarly. The AV score of s is the minimal number of the left and right access numbers: AV (s) = min(Lav (s), Rav (s)) (6) In general, n-grams with higher AV scores are more likely to be words in Chinese. Since AV is sensitive to the size of dataset, in our experiments, we use different thresholds"
2020.coling-main.187,N19-1276,0,0.0243396,"features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimpor"
2020.coling-main.187,P08-1102,0,0.129073,"Missing"
2020.coling-main.187,P09-1059,0,0.0415727,"they may be infrequent in the dataset. 5 Related Work There are basically two approaches to CWS and POS tagging: to perform the two tasks it in a pipeline framework; or to treat them as a joint task where the two tasks are conducted simultaneously, which is known as joint CWS and POS tagging. Ng and Low (2004) provided a comprehensive study to compare the two approaches and found that the joint approach outperform the pipeline one. Therefore, in the past two decades, the majority of studies on CWS and POS tagging applied the joint approach to these tasks (Ng and Low, 2004; Jiang et al., 2008; Jiang et al., 2009; Wang et al., 2011; Sun, 2011; Zeng et al., 2013), where n-grams are widely used as features carrying contextual information to improve model performance. Recently, neural methods, especially the recurrent neural networks (e.g., bi-LSTM) have demonstrated their effectiveness to encode contextual information, and thus significantly improve the model performance in joint CWS and POS tagging. Even though, improvements can still be obtained when n-grams are incorporated into the neural taggers (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018; Tian et al., 2020a). Fo"
2020.coling-main.187,P09-1058,0,0.110448,"Missing"
2020.coling-main.187,P17-1111,0,0.191001,"state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these models, contextual feature"
2020.coling-main.187,W04-3236,0,0.645681,"five benchmark datasets for CWS and POS tagging demonstrate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing n"
2020.coling-main.187,L16-1262,0,0.0291257,"Missing"
2020.coling-main.187,D12-1046,0,0.0530485,"Missing"
2020.coling-main.187,I17-1018,0,0.326212,"ormance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these models, contextual features are leveraged by"
2020.coling-main.187,P14-2042,0,0.042988,"Missing"
2020.coling-main.187,song-xia-2012-using,1,0.920217,"s sentence. 2074 Figure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle"
2020.coling-main.187,I13-1071,1,0.875801,"architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first cate"
2020.coling-main.187,W09-3511,1,0.800858,"terpretation of this sentence. 2074 Figure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel"
2020.coling-main.187,C12-2116,1,0.718021,"ure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In d"
2020.coling-main.187,K17-1016,1,0.664752,"lts from ZEN Table 3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that are used in different"
2020.coling-main.187,N18-2028,1,0.769763,"3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that are used in different channels. The first"
2020.coling-main.187,P98-2206,0,0.47617,"PMI 38.1K 27.5K 28.9K AV UD DLG PMI 19.6K 5.3K 1.1K Table 2: The size of lexicon N constructed by AV, DLG, and PMI. 3.2 Lexicon Construction To enhance the joint CWS and POS tagging through the multi-channel attentions, we need to construct the lexicon N which is simply a list of n-grams.6 In this study, we do not want our approach to rely on existing n-gram resources. Therefore, we use three unsupervised methods to obtain n-grams from each datasets, namely, accessor variety (AV) (Feng et al., 2004), description length gain (DLG) (Kit and Wilks, 1999), and pointwise mutual information (PMI) (Sun et al., 1998). Accessor Variety Given a character n-gram s, let left access number Lav (s) be the number of distinct characters that precede s in the corpus. The right access number Rav (s) is defined similarly. The AV score of s is the minimal number of the left and right access numbers: AV (s) = min(Lav (s), Rav (s)) (6) In general, n-grams with higher AV scores are more likely to be words in Chinese. Since AV is sensitive to the size of dataset, in our experiments, we use different thresholds for the five datasets: 2 for CTB5, 3 for CTB6, 4 for CTB7, 5 for CTB9, and 1 for UD. For each dataset, we collec"
2020.coling-main.187,P11-1139,0,0.1266,"ate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2"
2020.coling-main.187,2020.acl-main.735,1,0.774344,"representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimportant ones. To train"
2020.coling-main.187,2020.acl-main.734,1,0.90683,"representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimportant ones. To train"
2020.coling-main.187,I11-1035,0,0.415347,"OS tagging demonstrate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurit"
2020.coling-main.187,P13-1076,0,0.0939792,"r approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2"
2020.coling-main.187,D10-1082,0,0.034898,"ation and joint tagging F scores of the models (using BERT encoder) with normal and multi-channel attentions are illustrated in (a) and (c), where N is constructed by including n-grams whose frequency is in range [2i , +∞) (1≤i≤10) and whose length is in range [1, n] (1≤n≤10), respectively. The weights (i.e., δk in Eq. 4) assigned to n-gram groups categorized by frequency and length in the multi-channel attention module are shown in (b) and (d), respectively. resources, such as well-defined dictionaries (Wang et al., 2011; Zhang et al., 2014), syntactic features form manual crafted resources (Zhang and Clark, 2010), information of Chinese radicals (Shao et al., 2017), or large auto-processed data (Zhang et al., 2018) are used, our approach only leverages the resource from the datasets, which reduces the cost to train a joint CWS and POS tagger. Overall, the above results demonstrate that weighting n-grams separately is an appropriate approach to improve joint CWS and POS tagging without requiring extra knowledge. 4.2 Effect of N-gram Categorization Methods We analyze the effect of different categorization methods, i.e., n-gram frequency and n-gram length, to the joint task. For frequency-based methods,"
2020.coling-main.187,E14-1062,0,0.300566,"models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these model"
2020.coling-main.187,D13-1061,0,0.390194,"rms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2"
2020.coling-main.63,P18-1063,0,0.0388768,"Missing"
2020.coling-main.63,E17-2110,0,0.0155177,"ed in this study, we show that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patient"
2020.coling-main.63,N19-1423,0,0.0316364,"versation with the two types summaries: “SUM1” for problem statement and “SUM2” for treatment recommendations. SUM2 has two types, i.e., type A and B, which will be explained in the next section. Besides, we propose a hierarchical encoder-tagger (HET) model for extractive summarization to tag each utterance in a medical conversation with regard to whether an utterance is a problem statement or a treatment recommendation. We further enhance the model with end-to-end memory networks (Sukhbaatar et al., 2015) to incorporate the information in relevant utterances in the conversation. We use BERT (Devlin et al., 2019) as the token-level encoder and try several utterance-level encoders and taggers. Experimental results show that HET outperforms strong baselines as well as models from previous studies on this dataset. Analyses are also conducted to better understand our findings from the results. 2 A Corpus of Medical Conversations Medical conversation is a type of task-oriented conversation. Different from ordinary conversations in which topics are often fluid, in task-specific conversations, participants interact to accomplish a projected set of goals and sub-goals (Litman and Allen, 1987; Drew and Heritag"
2020.coling-main.63,N19-1276,0,0.042384,"h is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj ,"
2020.coling-main.63,W16-3648,0,0.0908501,"particular health problems, it is possible to perform the task by identifying important utterances in such conversations. In this study, important utterances refer to the utterances that contain key information for the medical problem or for the treatment. Therefore, our focus is different from existing studies on utterances in conversations, where they pay more attention to assessment of utterances with respect to the functionalities of utterances in the conversations, such as analyzing automatically generated utterances regarding their suitability within particular conversational contexts (Inaba and Takahashi, 2016; Lison and Bibauw, 2017), evaluation of human conversational performance on readability, sensibility, and social involvement (Dascalu et al., 2010), and identification of segments of utterance that are produced with more emphases for certain interactional purposes (Takeuchi et al., 2007). Little research has been done to identify important utterances that contribute to a specific outcome of a conversation, which in this study refers to the content about patient’s problem and recommendation treatment in the conversation. To conduct the medical conversation summarization task, in this paper, we"
2020.coling-main.63,W13-3214,0,0.0244349,"the label PT and DT to generate the summary of patient’s problem (SUM1) and doctor’s diagnoses (SUM2), respectively. 4 Experiments 4.1 Settings We experiment our HET model with and without the memory on our corpus. For model implementation, at the token-level encoder (TE), we use the Chinese version of BERT10 and ZEN (Diao et al., 2019)11 with their default settings, where for both BERT and ZEN, we use 12 layers of multi-head attentions with the dimension of hidden vectors set to 768; for the utterance level, we firstly run experiments with no encoder; then following previous studies such as (Kalchbrenner and Blunsom, 2013; Zhao et al., 2017; Kumar et al., 2018), we experiment with two recurrent neural network models (namely, LSTM and BiLSTM) to encode the utterance sequence for each conversation, where the dimension of hidden states is set to 300 for LSTM and 150 for BiLSTM encoder. In the memory module, the embedding matrix and BiLSTM encoder for obtaining the value vectors vj for uj are applied directly to the Chinese characters in the utterance. All parameters in the embedding matrix and the BiLSTM encoder in the memory module are initialized randomly, with the dimension of embedding and hidden states set t"
2020.coling-main.63,N16-1062,0,0.024137,"lead to unreadable summmaries in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and trea"
2020.coling-main.63,W17-5546,0,0.154587,", it is possible to perform the task by identifying important utterances in such conversations. In this study, important utterances refer to the utterances that contain key information for the medical problem or for the treatment. Therefore, our focus is different from existing studies on utterances in conversations, where they pay more attention to assessment of utterances with respect to the functionalities of utterances in the conversations, such as analyzing automatically generated utterances regarding their suitability within particular conversational contexts (Inaba and Takahashi, 2016; Lison and Bibauw, 2017), evaluation of human conversational performance on readability, sensibility, and social involvement (Dascalu et al., 2010), and identification of segments of utterance that are produced with more emphases for certain interactional purposes (Takeuchi et al., 2007). Little research has been done to identify important utterances that contribute to a specific outcome of a conversation, which in this study refers to the content about patient’s problem and recommendation treatment in the conversation. To conduct the medical conversation summarization task, in this paper, we propose a new benchmark"
2020.coling-main.63,D17-1231,0,0.0180325,"ies in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendati"
2020.coling-main.63,D19-1300,1,0.818553,"SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informative summaries to fac"
2020.coling-main.63,D17-1159,0,0.0187951,"utterance-level encoders, which is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utt"
2020.coling-main.63,W19-1306,0,0.0262848,"recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendations in our study) of the conversations. 6 Conclusion and Future Work In this paper, we proposed a new task of medical conversation summarization, which is performed by identifying important utterances in the conversation between patients and doctors. Based on the re"
2020.coling-main.63,N18-1158,0,0.0157194,"m the patient and the doctor could be more important in generating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a"
2020.coling-main.63,D18-1258,0,0.0453017,"te that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health professionals with the hope that they would alleviate the ever-increasing demands for healthcare services and reduce the inaccessibility of services caused by geographical and socio"
2020.coling-main.63,N19-1373,0,0.019483,"n conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendations in our study) of the conversations. 6 Conclusion and Future Work"
2020.coling-main.63,W18-2305,0,0.0210964,"that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health profess"
2020.coling-main.63,W17-4506,0,0.0220319,"ocus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informative summaries to facilitate the needs of providing useful information to potential patients from the online platform. General"
2020.coling-main.63,song-xia-2012-using,1,0.87114,"the token-level and utterance-level encoders, which is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In"
2020.coling-main.63,N18-2028,1,0.798582,"Missing"
2020.coling-main.63,D07-1048,0,0.119092,"Missing"
2020.coling-main.63,W19-5027,1,0.740721,"s strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health professionals with the hope that they would alleviate the ever-increasing demands for healthcare services and reduce the inaccessibility of services caused by geographical and socio-economic barriers."
2020.coling-main.63,2020.acl-main.735,1,0.710374,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,2020.emnlp-main.487,1,0.559632,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,2020.acl-main.734,1,0.72857,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,L18-1464,1,0.833299,"ly. in our work, this challenge may not be an issue because the redundancy in the original input is limited and directly concatenating selected utterances with their same order in the original conversation does not lead to unreadable summmaries in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of th"
2020.coling-main.63,P19-1214,0,0.0221498,"Missing"
2020.coling-main.63,2020.nlpmc-1.3,1,0.62841,"tand our findings from the results. 2 A Corpus of Medical Conversations Medical conversation is a type of task-oriented conversation. Different from ordinary conversations in which topics are often fluid, in task-specific conversations, participants interact to accomplish a projected set of goals and sub-goals (Litman and Allen, 1987; Drew and Heritage, 1992). Specifically, for conversations in the medical domain from online medical platforms, the projected goal is for the doctor to diagnose and offer treatment recommendation for the patient’s problem (Drew and Heritage, 1992; Robinson, 2012; Wang et al., 2020). Particularly in China, many platforms make such medical conversations publicly available so that new patients with similar problem can search relevant conversations and find helpful information from them. Therefore, summarization of the patient’s problem and doctor’s recommendations in a conversation could be highly important because such summaries can help the new patients locate the key information, especially when a conversation is too long. To conduct such summarization, a straightforward solution is to identity the important utterances that contain key information for problem statements"
2020.coling-main.63,D19-1298,0,0.0185914,"ating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informati"
2020.coling-main.63,N18-1151,1,0.832548,"ls. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in the conversation into their memory vectors and value vectors. The memory vectors (denoted by mj for uj ) are directly copied from the utterance representation obt"
2020.coling-main.63,P19-1499,0,0.0187593,"e important in generating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires t"
2020.coling-main.63,W17-5505,0,0.0263391,"the summary of patient’s problem (SUM1) and doctor’s diagnoses (SUM2), respectively. 4 Experiments 4.1 Settings We experiment our HET model with and without the memory on our corpus. For model implementation, at the token-level encoder (TE), we use the Chinese version of BERT10 and ZEN (Diao et al., 2019)11 with their default settings, where for both BERT and ZEN, we use 12 layers of multi-head attentions with the dimension of hidden vectors set to 768; for the utterance level, we firstly run experiments with no encoder; then following previous studies such as (Kalchbrenner and Blunsom, 2013; Zhao et al., 2017; Kumar et al., 2018), we experiment with two recurrent neural network models (namely, LSTM and BiLSTM) to encode the utterance sequence for each conversation, where the dimension of hidden states is set to 300 for LSTM and 150 for BiLSTM encoder. In the memory module, the embedding matrix and BiLSTM encoder for obtaining the value vectors vj for uj are applied directly to the Chinese characters in the utterance. All parameters in the embedding matrix and the BiLSTM encoder in the memory module are initialized randomly, with the dimension of embedding and hidden states set to 768 and 384, resp"
2020.emnlp-main.487,J07-3004,0,0.229948,"llustrated in the red boxes. In this paper, we propose attentive GCN (AGCN) for CCG supertagging, where its input graph is built based on chunks (n-grams) extracted with unsupervised methods. In detail, two types of edges in the graph are introduced to model word relations within and across chunks and an attention mechanism is applied to GCN to weight those edges. In doing so, different contextual information are discriminatively learned to facilitate CCG supertagging without requiring any external resources. The validity of our approach is demonstrated by experimental results on the CCGbank (Hockenmaier and Steedman, 2007), where state-of-the-art performance is obtained for both tagging and parsing. 2 The Approach We treat CCG supertagging as a sequence labeling task, where the input is a sentence with n words X = x1 x2 · · · xi · · · xn , and the output is a sequence of supertags Yb = yb1 yb2 · · · ybi · · · ybn . Our approach uses attentive GCN (A-GCN) to incorporate information of word pairs through a graph; the graph is built based on n-grams in the input sentence that appear in a lexicon N . This lexicon GCN Normal GCN models with L layers learn from word pairs suggested by the dependency parsing results o"
2020.emnlp-main.487,D19-1549,0,0.0389459,"few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks (Marcheggiani and Titov, 2017; Huang and Carley, 2019; De Cao et al., 2019; Huang et al., 2019); thus we want to determine whether this approach can also help CCG supertagging. However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers. Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequ"
2020.emnlp-main.487,D19-1345,0,0.0285233,"upertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks (Marcheggiani and Titov, 2017; Huang and Carley, 2019; De Cao et al., 2019; Huang et al., 2019); thus we want to determine whether this approach can also help CCG supertagging. However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers. Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequent chunks (n-grams) in the corpus, becaus"
2020.emnlp-main.487,P17-1174,0,0.0274195,"Therefore, in normal GCN, for each xi , all the xj that connect to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexico"
2020.emnlp-main.487,P10-1036,0,0.0814397,"Missing"
2020.emnlp-main.487,N19-1020,0,0.079789,"Missing"
2020.emnlp-main.487,2020.acl-main.735,1,0.432409,"to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted in green; example in-chunk and cross-c"
2020.emnlp-main.487,2020.findings-emnlp.153,1,0.442853,"to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted in green; example in-chunk and cross-c"
2020.emnlp-main.487,N19-1093,1,0.810025,"Missing"
2020.emnlp-main.487,N16-1027,0,0.0365662,"t understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is"
2020.emnlp-main.487,P15-2041,0,0.0379227,"tactic and semantic knowledge for text understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relatio"
2020.emnlp-main.487,N18-1142,0,0.122602,", for each xi , all the xj that connect to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted"
2020.emnlp-main.487,N18-2065,0,0.0150593,"ous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.1 1 Introduction Combinatory categorial grammar (CCG) is a lexicalized grammatical formalism, where the lexical categories (also known as supertags) of the words in a sentence provide informative syntactic and semantic knowledge for text understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards"
2020.emnlp-main.487,P17-1026,0,0.206165,"fore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an eff"
2020.findings-emnlp.153,P17-1152,0,0.0346125,"r approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is es"
2020.findings-emnlp.153,E06-1047,0,0.0497131,"h ability. DATASETS S ENT T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compu"
2020.findings-emnlp.153,D16-1257,0,0.0238305,"the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddings (Kitaev and Klein, 2018)), HPSG information (Zhou and Zhao, 2019; Mrini et al., 2019), or use model ensembles (Kitaev et al., 2019). Compared to these studies, our approach offers an alternative way to enhance constituency parsing with effective leveraging of n-gram information. Moreover, the proposed span attention addresses the limitation of previous studies (Kita"
2020.findings-emnlp.153,P97-1003,0,0.728866,"one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerf"
2020.findings-emnlp.153,D16-1001,0,0.0245578,"ore, in deciding which component (i.e., “compete” or “customer”) the with-PP should attach to, n-grams (e.g., the uni-gram “companies”) may provide useful cues, since ”customers with companies” is less likely than “compete with companies”. 5 Related Work There are two main types of parsing methodologies. One is the transition-based approaches (Sagae and Lavie, 2005); the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddi"
2020.findings-emnlp.153,N19-1423,0,0.0385639,"rge uncased (LU) version of BERT. PARM reports the number of trainable parameters in each model. that the two words co-occur a lot in the dataset and are more likely to form an n-gram. We set the threshold to 0 to determine whether a delimiter should be inserted between the two adjacent words x0 and x00 . In other words, to build the lexicon N from a dataset, we use PMI as an unsupervised segmentation method to segment the dataset and collect all n-grams (n ≤ 5)9 appearing at least twice in the training and development sets combined.10 3.3 Model Implementation In our experiments, we use BERT (Devlin et al., 2019) as the basic encoder for all three languages and use ZEN (Diao et al., 2019) and XLNet-large (Yang et al., 2019) for Chinese and English, respectively.11 For BERT, ZEN, and XLNet, we use the default hyper-parameter settings. (e.g., 24 layers with 1024 dimensional hidden vector for the large models). In addition, following Kitaev et al. (2019), Zhou and Zhao (2019) and Mrini et al. (2019), we 9 We empirically set the max n-gram length to 5 as a unified threshold for all three languages. 10 We show the details of extracting the lexicon with example n-grams in the Appendix. 11 We download BERT m"
2020.findings-emnlp.153,N16-1024,0,0.172676,"V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that emp"
2020.findings-emnlp.153,P19-1031,0,0.043252,"ags), and achieve stateof-the-art performance on all datasets. Compared with Zhou and Zhao (2019) and Mrini et al. (2019) which improve constituency parsing by leveraging the dependency information when training their head phrase structure grammar (HPSG) parser, our approach enhances the task from another direction by incorporating n-gram information through the span attentions as a way to address the limitation of using hidden vector subtraction to represent spans. 4.2 Cross-domain Experiments To further explore whether our approach can be generalized across domains, we follow the setting of Fried et al. (2019) to conduct cross-domain experiments on the Brown and Genia corpus using the models with SA and C AT SA, as well as their corresponding baseline. Note that, for fair comparison, we use BERT-large cased as the encoder without using the predicted POS tags. We follow Fried et al. (2019) to train models on the training set of PTB and evaluate them on the entire Brown corpus and the entire Genia corpus. To construct 14 We use the version of 3.9.2 obtained from https:// stanfordnlp.github.io/CoreNLP/. 15 We obtain their models from https://github.com/ nikitakit/self-attentive-parser. 16 For our mode"
2020.findings-emnlp.153,N18-1091,0,0.0406819,"Missing"
2020.findings-emnlp.153,P06-2038,0,0.292051,"atural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-tr"
2020.findings-emnlp.153,D18-1162,0,0.026786,"Missing"
2020.findings-emnlp.153,C10-1045,0,0.0235998,"T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual infor"
2020.findings-emnlp.153,D19-5323,0,0.0173603,"benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturin"
2020.findings-emnlp.153,P18-1110,0,0.035292,"Missing"
2020.findings-emnlp.153,I17-2002,0,0.0161686,"(F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggest"
2020.findings-emnlp.153,P19-1340,0,0.0640023,"red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transfor"
2020.findings-emnlp.153,P18-1249,0,0.301291,"uhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effectiv"
2020.findings-emnlp.153,Q17-1029,0,0.0146219,"component (i.e., “compete” or “customer”) the with-PP should attach to, n-grams (e.g., the uni-gram “companies”) may provide useful cues, since ”customers with companies” is less likely than “compete with companies”. 5 Related Work There are two main types of parsing methodologies. One is the transition-based approaches (Sagae and Lavie, 2005); the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddings (Kitaev and Klein"
2020.findings-emnlp.153,P18-1116,0,0.0167613,"glish, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Pa"
2020.findings-emnlp.153,P14-5010,0,0.00337342,"RIED ET AL ., 2019) 93.10 87.54 BERT + SA + C AT SA 93.13 93.24 93.29 87.58 87.50 87.53 Table 4: Cross-domain experiment results (F1 scores) from previous studies and our models (based on BERTLC), on the entire Brown and Genia corpora when trained from the training set of PTB. POS tags as the additional input, which suggests that the predicted POS tags may have more conflict with ZEN compared with BERT. Moreover, we run our models on the test set of each dataset and compare the results with previous studies, as well as the ones from prevailing parsers, i.e., Stanford CoreNLP Toolkits (SCT)14 (Manning et al., 2014) and Berkeley Neural Parser (BNP)15 (Kitaev and Klein, 2018). The results are reported in Table 3, where the models using predicted POS tags are marked with “*”.16 Our models with C AT SA outperform previous best performing models from Zhou and Zhao (2019) and Mrini et al. (2019) under different settings (i.e., whether to use the predicted POS tags), and achieve stateof-the-art performance on all datasets. Compared with Zhou and Zhao (2019) and Mrini et al. (2019) which improve constituency parsing by leveraging the dependency information when training their head phrase structure grammar (HPSG"
2020.findings-emnlp.153,J93-2004,0,0.0782316,"so they are suitable to be grouped by such ability. DATASETS S ENT T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Co"
2020.findings-emnlp.153,D14-1162,0,0.0923621,"Missing"
2020.findings-emnlp.153,C10-1100,0,0.0130132,"ion. For instance, Figure 1 illustrates the treelet of an example in the form of “V+NP+PP”. As a classic example of PP-attachment ambiguity, a parser may wrongly attach the “PP” to the “NP” if it only focuses on the words at the boundaries of the text span “flag ... year” and in-between information is not represented properly. In this case, n-grams within that span (e.g., the uni-gram “telescope”) can provide useful cues indicating that the “PP” should be attached to the “V”. Although there are traditional non-neural parsers using n-grams as features to improve parsing (Sagae and Lavie, 2005; Pitler et al., 2010), they are limited in treating them euqally without learning their weights. Therefore, unimportant n-grams may deliver misleading information and lead to wrong predictions. To address this problem, in this paper, we propose a span attention module to enhance chartbased neural constituency parsing by incorporating appropriate n-grams into span representations. Specifically, for each text span we extract all its substrings that appear in an n-gram lexicon; the span attention uses the normal attention mechanism to weight them with respect to their contributions to predict the constituency label o"
2020.findings-emnlp.153,W05-1513,0,0.470284,"fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extr"
2020.findings-emnlp.153,P18-1108,0,0.0141978,"K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a datas"
2020.findings-emnlp.153,W09-3511,1,0.530073,"employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with s"
2020.findings-emnlp.153,N18-2028,1,0.782206,"ling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Associ"
2020.findings-emnlp.153,song-xia-2012-using,1,0.831296,"xt encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with span attention, with"
2020.findings-emnlp.153,P17-1076,0,0.249882,"r than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text en"
2020.findings-emnlp.153,P18-2097,0,0.203982,"TB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggests 4 All the datasets are obtained from the official release of Linguistic Data Consortium. The catalog numbers for ATB part 1-3 are LDC2003T06, LDC2004T02, LDC2005T20, for CTB5 is LDC2005T01,"
2020.findings-emnlp.153,I05-2038,0,0.0420425,"and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggests 4 All the datasets are obtained from the official release of Linguistic Data Consortium. The catalog numbers f"
2020.findings-emnlp.153,C18-1011,0,0.0400782,"Missing"
2020.findings-emnlp.153,2020.acl-main.735,1,0.527704,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,2020.emnlp-main.487,1,0.442853,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,2020.acl-main.734,1,0.563505,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,N03-1033,0,0.287347,"/github.com/zihangdai/xlnet. add three additional token-level self-attention layers to the top of BERT, ZEN, and XLNet. For other settings, we randomly initialize all ngram embeddings used in our attention module12 with their dimension matching that of the hidden vectors obtained from the encoder (e.g., 1024 for BERT-large). Besides, we run our experiments with and without predicted part-of-speech (POS) tags. Following previous studies, for the experiments without POS tags, we take sentences as the only input; for the experiments with POS tags, we obtain the POS tags from Stanford POS Tagger (Toutanova et al., 2003) and incorporate the POS tags by directly concatenating their embeddings with the output of the BERT/ZEN/XLNet encoder. Following previous studies (Suzuki et al., 2018; Kitaev et al., 2019), we use hinge loss during the training process and evaluate different models by by precision, recall, F1 score, and complete match score via the standard evaluation toolkit E VALB13 . During the training process, we try three learning rates, i.e., 5e-5, 1e-5, 5e-6, with a fixed random seed, pick the model with the best F1 score on the development set, and evaluate it on the test set. 12 We also try initiali"
2020.findings-emnlp.153,N18-1142,0,0.151668,"ransformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with span attention, with an example partial"
2020.findings-emnlp.153,N19-1093,1,0.875724,"Missing"
2020.findings-emnlp.153,P19-1230,0,0.240512,"al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many stud"
2020.nlpmc-1.3,J00-3003,0,0.726861,"Missing"
2020.nlpmc-1.3,W18-2309,1,0.830408,"ok? 2 DAD: Okay. Thank you. 3 DOC: You’re welcome. In Ex 4, the physician initiates the closure of the medical conversation by making a future arrangement for the patient’s follow-up visit at line 1. The patient’s father accepts the proposal and the two parties immediately proceed to the terminal exchange (thank you-you’re welcome) at lines 2-3. Besides the closure initiation action (a) making 4.1 Annotating conversation structure Figure 1 illustrates how the hierarchical structure of medical conversation is annotated according to the COSTA scheme. Detail of the COSTA scheme can be found in (Wang et al., 2018). 15 it is not unusual that the physicians and patients go back and forth between these phases. By annotating which phase a turn belongs to, we are not only able to show where the boundaries are among the phases in medical conversation, but also how transitions are coordinated by the participants. Pair and turn dependency Unlike many other types of discourse, conversation is interactive in nature. Thus, turns in conversation cannot be understood alone. Instead, each turn should be understood regarding whether they set up an expectation for a next turn or fulfill the expectation set up by a pri"
2020.nlpmc-1.3,xia-etal-2000-developing,1,0.534917,"ts. Adopting the Conversation Analysis transcribing conventions (Jefferson, 2004), each conversation is segmented into turns at turn-taking positions. Besides capturing the verbatim of each turn, the transcription also captures a series of para-linguistic features (e.g., dysfluencies, intonations, overlaps of turns, noticeable silence in and between turns, non-verbal actions such as nodding, etc.), which are essential aspects of natural spoken language. In addition, the transcribed text is automatically segmented into words using an in-house CRF word segmenter trained on the Chinese Treebank (Xia et al., 2000), so as to provide the necessary basis for conducting related NLP tasks. 3.3 Our Analysis Sequence expansion and making treatment decisions A second aspect of our analysis focuses on sequences within some particular phases in medical conversation. For example, within treatment recommendation phase, we examined how treatment recommendations are delivered and received. Specifically, it is found that treatment decisions (e.g., antibiotic prescriptions) can be negotiated between physicians and patients by patients withholding acceptance of physicians’ recommendations. Thus, the minimal form of ‘re"
bhatia-etal-2010-empty,J05-1004,1,\N,Missing
bhatia-etal-2010-empty,I08-2099,1,\N,Missing
C04-1073,mccord-bernth-1998-lmt,1,\N,Missing
C04-1073,H93-1025,1,\N,Missing
C04-1073,C00-2163,0,\N,Missing
C04-1073,J93-2003,0,\N,Missing
C04-1073,W01-1406,0,\N,Missing
C04-1073,2001.mtsummit-ebmt.4,0,\N,Missing
C04-1073,N03-2036,1,\N,Missing
C04-1073,W01-1403,0,\N,Missing
C04-1073,C00-1078,0,\N,Missing
C04-1073,C92-2101,0,\N,Missing
C04-1073,C00-2131,0,\N,Missing
C04-1073,P02-1040,0,\N,Missing
C04-1073,P02-1039,0,\N,Missing
C04-1073,P93-1004,0,\N,Missing
C04-1073,P03-1057,0,\N,Missing
C04-1073,P96-1021,0,\N,Missing
C04-1073,J89-1003,1,\N,Missing
C04-1073,J80-1003,1,\N,Missing
C10-1044,N09-1067,0,0.0784081,"Missing"
C10-1044,N01-1026,0,0.0316993,"r languages. 1 Introduction While there are more than six thousand languages in the world, only a small portion of these languages have received substantial attention in the field of NLP. With the increase in use of datadriven methods, languages with few or no electronic resources have been difficult to process with current methods. The morphological tagging of Russian using Czech resources as done by (Hana et al., 2004) shows the potential benefit for using the resources of resource-rich languages to bootstrap NLP tools for related languages. Projecting syntactic structures across languages (Yarowsky and Ngai, 2001; Xia and Lewis, 2007) is another possible way to harness existing tools, though such projection is more reliable among languages with similar syntax. Studies such as these show the possible benefits of working with similar languages. A crucial question is how we should define similarity between languages. While genetically related languages tend to have similar typological features as they could inherit the features from their common ancestor, they could also differ a lot due to language change over time. On the other hand, languages with no common ancestor could share many features due to la"
C10-1044,W04-3229,0,0.0602996,"es (WALS), our preliminary experiments show that typologically-based clusters look quite different from genetic groups, but perform as good or better when used to predict feature values of member languages. 1 Introduction While there are more than six thousand languages in the world, only a small portion of these languages have received substantial attention in the field of NLP. With the increase in use of datadriven methods, languages with few or no electronic resources have been difficult to process with current methods. The morphological tagging of Russian using Czech resources as done by (Hana et al., 2004) shows the potential benefit for using the resources of resource-rich languages to bootstrap NLP tools for related languages. Projecting syntactic structures across languages (Yarowsky and Ngai, 2001; Xia and Lewis, 2007) is another possible way to harness existing tools, though such projection is more reliable among languages with similar syntax. Studies such as these show the possible benefits of working with similar languages. A crucial question is how we should define similarity between languages. While genetically related languages tend to have similar typological features as they could i"
C10-1044,N07-1057,1,0.886669,"on While there are more than six thousand languages in the world, only a small portion of these languages have received substantial attention in the field of NLP. With the increase in use of datadriven methods, languages with few or no electronic resources have been difficult to process with current methods. The morphological tagging of Russian using Czech resources as done by (Hana et al., 2004) shows the potential benefit for using the resources of resource-rich languages to bootstrap NLP tools for related languages. Projecting syntactic structures across languages (Yarowsky and Ngai, 2001; Xia and Lewis, 2007) is another possible way to harness existing tools, though such projection is more reliable among languages with similar syntax. Studies such as these show the possible benefits of working with similar languages. A crucial question is how we should define similarity between languages. While genetically related languages tend to have similar typological features as they could inherit the features from their common ancestor, they could also differ a lot due to language change over time. On the other hand, languages with no common ancestor could share many features due to language contact and oth"
C10-1044,H01-1035,0,\N,Missing
C10-2016,C04-1080,0,0.0567204,"Missing"
C10-2016,D07-1031,0,0.160017,"lish, we highlight some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. Comparing to English, we find that all algorithms perform rather poorly in Chinese in 1-to-1 accuracy result but are more competitive in many-to-1 accuracy. We attribute one possible explanation of this to the algorithms’ inability to correctly produce tags that match the desired tag count distribution. 1 Introduction Recently, there has been much work on unsupervised POS tagging using Hidden Markov Models (Johnson, 2007; Goldwater & Griffiths, 2007). Three common approaches are Expectation Maximization (EM), Variational Bayes (VB) and Gibbs Sampling (GS). EM was first used in POS tagging in (Merialdo, 1994) which showed that except in conditions where there are no labeled training data at all, EM performs very poorly. Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. Their experiments on English showed that GS outperforms EM and VB in almost all cases. Other notable studies in the unsupervised and semi-supervised POS domain include the use of p"
C10-2016,P08-1100,0,0.0318164,"Missing"
C10-2016,A94-1009,0,0.244133,"Missing"
C10-2016,D08-1036,1,0.921682,"dy, we analyze and compare the performance of three classes of unsupervised learning algorithms on Chinese and report the experimental results on the CTB. We establish a baseline for unsupervised POS tagging in Chinese. We then compare and analyze the results between Chinese and English, we explore some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. 2 Models In this section, we provide a brief overview of the three unsupervised learning methods for POS tagging as described in (Gao & Johnson, 2008), which all uses a traditional bigram Hidden Markov Model (HMM). HMM is a well135 Coling 2010: Poster Volume, pages 135–143, Beijing, August 2010 known statistical model, used for sequential modeling. To put it formally, let be the set of possible states and be the set of possible observations. In the case for POS tagging using a bigram model, the set corresponds to the set of POS tags and the set corresponds to the set of words in the language. (2) and backward probability . See (Mannings & Schutze, 1999) for details on the calculation. 2.1 Figure 1: Graphical model of an HMM for a bigram POS"
C10-2016,P07-1094,0,0.0387751,"Missing"
C10-2016,N06-1041,0,0.0520223,"Missing"
C10-2016,N09-2054,0,0.0280765,"Missing"
C10-2016,J94-2001,0,0.354707,"Missing"
C10-2016,W04-3236,0,0.0515625,"Missing"
C10-2016,A00-1031,0,0.224737,"Missing"
C10-2016,C02-1145,0,0.095044,"Missing"
C10-2016,W93-0305,0,\N,Missing
C12-2037,W09-3036,1,0.808137,"replaced by Korean counterparts, words in the solid boxes are removed and words in the dotted boxes are collapsed into a single node. Figure 3: The steps of the projection process, as illustrated using an IGT instance from Korean. In this study, we will focus the last two steps. Therefore, for the first two steps, we use word alignment and English parse trees from the gold standard. The last two steps and the evaluation corpora are explained below. 3.1 Corpora We used two different sources of language data for our experiment. The first was a set of guideline sentences for the Hindi treebank (Bhatt et al., 2009). These sentences were provided both in the IGT format, as well as with gold-standard dependency trees in both English and Hindi. The second set was the IGT data used in Xia and Lewis (2007), which includes IGTs for seven languages, plus manually annotated word alignment and dependency trees for both English and the foreign language. In all, eight languages were used for our experiments: Hindi, German, Irish, Hausa, Korean, Malagasy, Welsh, and Yaqui. The size of each data set is given in Table 1. We use the pre-existing, manually annotated parse trees in these two data sets as our gold standa"
C12-2037,J05-1003,0,0.0347087,"syntactic projection information in a discriminative parser generally outperforms deterministic syntactic projection. While this paper uses small IGT corpora for word alignment, our method can be adapted to larger parallel corpora by using statistical word alignment instead. KEYWORDS: Dependency Parsing, Syntactic Projection, Interlinear Glossed Text. Proceedings of COLING 2012: Posters, pages 371–380, COLING 2012, Mumbai, December 2012. 371 1 Introduction The development of large-scale treebanks has significantly improved the performance of statistical parsers (e.g. Klein and Manning, 2001; Collins and Koo, 2005; de Marneffe et al., 2006) Unfortunately, resources such as these typically only exist for a handful of languages, because building large treebanks is very labor-intensive and often cost-prohibitive. As thousands of other languages have no such resources, other techniques are required to attain similar performance. One way in which natural language tools might be created for resource-poor languages is by using resources containing translations between resource-rich and resource-poor languages and use the alignment information to transfer information to the resource-poor ones. Syntactic projec"
C12-2037,de-marneffe-etal-2006-generating,0,0.0121003,"Missing"
C12-2037,J94-4004,0,0.0319931,"et al. (2004) bootstrapped both phrase and dependency parsers. While these systems did not match the performance of supervised systems, they do succeed in demonstrating that even a small amount of information can significantly boost the performance of a baseline system. Syntactic projection, however, suffers from a major flaw—using word alignment to transfer analyses between languages assumes that the language pair represents the similar sentences using similar structures. Hwa et al. (2002) referred to this as the Direct Correspondence Assumption, or DCA. As useful as this assumption may be, Dorr (1994) discussed how languages may be divergent in their representations of similar sentences in semantic, syntactic, or lexical representations. 372 inaugurated manwrIjI ne kiyA minister [erg] did “the manxira kA temple minister inaugurated kiyA did uxGAtana minister of inauguration the temple” the temple manwrIjI minister ne [erg] the uxGAtana ingauguration manxira temple kA of (a) Original IGT representation and word alignment between the three lines. (b) The dependency trees for Hindi and English Figure 1: An example of the light verb construction in Hindi. 2.2 Linguistic Divergence Dorr (1994)"
C12-2037,P09-1042,0,0.0214326,"types of translation divergence. These variations, or linguistic divergence, can be problematic for projection algorithms, as the assumptions that projection relies on may not hold. One example which may affect projections is the light-verb construction. Take as an example the Hindi sentence in Figure 1, where an English verb (“inaugurated”) is represented in Hindi as a light verb (“did”) plus a noun (“inaugurate”). As a result, the dependency structures in English and Hindi are not identical, as illustrated in Figure 1(b). In order to address some of the noise inherent in projection results, Ganchev et al. (2009) took an approach of using “soft” constraints to improve projection results. Rather than commit to a single parse, Ganchev et al. used statistical methods to disambiguate between multiple parse options, and showed significant improvements over a purely deterministic approach. In our previous study, (Georgi et al., 2012), we looked at measuring forms of alignment between dependency structures to quantify the amount of divergence between languages. In this paper we will look at how performance varies among a set of typologically different languages. We hope to investigate the correlation between"
C12-2037,georgi-etal-2012-measuring,1,0.645076,"verb (“inaugurated”) is represented in Hindi as a light verb (“did”) plus a noun (“inaugurate”). As a result, the dependency structures in English and Hindi are not identical, as illustrated in Figure 1(b). In order to address some of the noise inherent in projection results, Ganchev et al. (2009) took an approach of using “soft” constraints to improve projection results. Rather than commit to a single parse, Ganchev et al. used statistical methods to disambiguate between multiple parse options, and showed significant improvements over a purely deterministic approach. In our previous study, (Georgi et al., 2012), we looked at measuring forms of alignment between dependency structures to quantify the amount of divergence between languages. In this paper we will look at how performance varies among a set of typologically different languages. We hope to investigate the correlation between performance in these languages and these quantitative measures in future work. 2.3 Interlinear Glossed Text (IGT) IGT is a resource well-suited to the task of adapting dependency parsing to new languages. As seen in Figure 1a, an IGT instance contains a foreign-language line, an English translation, and a gloss line, w"
C12-2037,P02-1050,0,0.22919,"o such resources, other techniques are required to attain similar performance. One way in which natural language tools might be created for resource-poor languages is by using resources containing translations between resource-rich and resource-poor languages and use the alignment information to transfer information to the resource-poor ones. Syntactic projection takes advantage of existing tools used to annotate a resource-rich language in a corpus, and transfers the analysis to the resource-poor language by means of syntactic projection using word-to-word alignment (Yarowsky and Ngai, 2001; Hwa et al., 2002). While little annotated data typically exists for resource-poor languages, some work has been done examining the utility of interlinear glossed text (IGT) (Xia and Lewis, 2007; Lewis and Xia, 2008), a format used for illustrative examples in linguistic papers. An IGT instance includes a language line which is a phrase or sentence in a foreign language, a gloss line that shows word-to-word translation, and a translation line which is normally in English. We chose IGT for this study because the gloss line can serve as a bridge for aligning the words in the language line and the translation line"
C12-2037,I08-2093,1,0.911376,"g translations between resource-rich and resource-poor languages and use the alignment information to transfer information to the resource-poor ones. Syntactic projection takes advantage of existing tools used to annotate a resource-rich language in a corpus, and transfers the analysis to the resource-poor language by means of syntactic projection using word-to-word alignment (Yarowsky and Ngai, 2001; Hwa et al., 2002). While little annotated data typically exists for resource-poor languages, some work has been done examining the utility of interlinear glossed text (IGT) (Xia and Lewis, 2007; Lewis and Xia, 2008), a format used for illustrative examples in linguistic papers. An IGT instance includes a language line which is a phrase or sentence in a foreign language, a gloss line that shows word-to-word translation, and a translation line which is normally in English. We chose IGT for this study because the gloss line can serve as a bridge for aligning the words in the language line and the translation line. The method proposed in this paper can be easily extended when IGT is replaced by bitexts of sufficient size to train a high-quality word aligner. In this paper, we investigate the possibility of u"
C12-2037,W06-2932,0,0.0343783,"ose from the source. If multiple English nodes align to a single foreign word, the node highest up in the tree is kept, and all others removed. Finally, remaining unaligned words in L F are attached heuristically, following (Quirk et al., 2005). 3.3 The Parser Due to linguistic divergence, projected trees are error-prone. Instead of making hard decisions based on projection, we use information from projected trees as a feature in a discriminative parser. This feature will be highly predictive, but not result in a strictly deterministic method like projection alone. We modified the MST Parser (McDonald et al., 2006) by adding features that check whether certain edges considered by the parser appear in the projected tree. We define two types of features: BOOL, and TAG. The first feature type, BOOL, looks at the current parent→child edge being considered by the parser and returns true if the edge matches one in the projected tree. While this feature was a logical starting point, we also wondered if certain word classes of Englishprojected better than  others, and so the second feature type, TAG, creates a feature for each POS par ent , POSchild pair such that the feature is true if the current parent→chi"
C12-2037,P02-1027,0,0.0326155,"d syntactic projection to bootstrap a dependency parser and improve the resulting parses over projection alone. 2 Background Before presenting our system, we will first describe previous studies on syntactic projection. Next, we will describe the phenomenon of linguistic divergence, and explain why this can affect projection performance. Finally, we will describe interlinear glossed text (IGT) in more detail and highlight the ways in which it is well-suited to this task. 2.1 Syntactic Projection Syntactic projection via word alignment has shown promise in adapting resources between languages. Merlo et al. (2002) demonstrated a technique of classifying verb types via projection, while Yarowsky and Ngai (2001) worked on projecting POS taggers and NP bracketers. Hwa et al. (2004) bootstrapped both phrase and dependency parsers. While these systems did not match the performance of supervised systems, they do succeed in demonstrating that even a small amount of information can significantly boost the performance of a baseline system. Syntactic projection, however, suffers from a major flaw—using word alignment to transfer analyses between languages assumes that the language pair represents the similar sen"
C12-2037,P05-1034,0,0.060945,"parse tree TE for L E , the projection algorithm works as follows. For each English node ei that aligns with foreign word f i , we replace the node for ei with the foreign word. If a single English node ei aligns with multiple foreign words ( f i , f j ), we make multiple 375 copies of ei as siblings in the tree for each source word, then replace the English words with those from the source. If multiple English nodes align to a single foreign word, the node highest up in the tree is kept, and all others removed. Finally, remaining unaligned words in L F are attached heuristically, following (Quirk et al., 2005). 3.3 The Parser Due to linguistic divergence, projected trees are error-prone. Instead of making hard decisions based on projection, we use information from projected trees as a feature in a discriminative parser. This feature will be highly predictive, but not result in a strictly deterministic method like projection alone. We modified the MST Parser (McDonald et al., 2006) by adding features that check whether certain edges considered by the parser appear in the projected tree. We define two types of features: BOOL, and TAG. The first feature type, BOOL, looks at the current parent→child ed"
C12-2037,N01-1026,0,0.425761,"of other languages have no such resources, other techniques are required to attain similar performance. One way in which natural language tools might be created for resource-poor languages is by using resources containing translations between resource-rich and resource-poor languages and use the alignment information to transfer information to the resource-poor ones. Syntactic projection takes advantage of existing tools used to annotate a resource-rich language in a corpus, and transfers the analysis to the resource-poor language by means of syntactic projection using word-to-word alignment (Yarowsky and Ngai, 2001; Hwa et al., 2002). While little annotated data typically exists for resource-poor languages, some work has been done examining the utility of interlinear glossed text (IGT) (Xia and Lewis, 2007; Lewis and Xia, 2008), a format used for illustrative examples in linguistic papers. An IGT instance includes a language line which is a phrase or sentence in a foreign language, a gloss line that shows word-to-word translation, and a translation line which is normally in English. We chose IGT for this study because the gloss line can serve as a bridge for aligning the words in the language line and t"
C12-2037,N07-1057,1,\N,Missing
C12-2116,D11-1033,0,0.0530956,"0; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured cross entropy for the source side of the text; the second is similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; and the third, took into account the bilingual data on both the source and target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. Plank and van Noord (2011) investigated several different trai"
C12-2116,P07-1033,0,0.133576,"Missing"
C12-2116,W99-0701,1,0.863971,"entropy gain (EG), is defined as in Eq 5, where q is a probability distribution estimated from C and q1 is one estimated from C + s, a new corpus formed by adding s to C . Intuitively, if s is similar to C , q1 will be very similar to q and EG(s, c) will be small. EG(s, C) = |H(C + s, q1) − H(C, q) | (5) The measures in Eq 3-5 can all be normalized by sentence length. For instance, Eq 6 shows the normalized entropy gain. We call it Average Entropy Gain (AEG). AEG(s, C) = 3.4 EG(s, C) leng th(s) (6) Descriptive Length Gain (DLG) Description length gain (DLG) is a goodness measure proposed by (Kit and Wilks, 1999) as an unsupervised learning approach to lexical acquisition (Kit, 2005; Kit and Zhao, 2007). Intuitively, the DLG of a string str w.r.t. a corpus C, DLG(str, C), indicates the reduction of description length of C when the characters in str are treated as a unit and all the occurrences of str in C are replaced by the index of the unit. Therefore, the more frequent str is in C and the longer str is, the higher DLG(str,C) is. The DLG calculation resorts to a re-implementation of the suffix array approach to counting n-grams (Kit and Wilks, 1998). Based on this property, we define a similarity me"
C12-2116,J93-2004,0,0.0460848,"ntropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortium No. LDC2010T07 1192 Genre # of chars 275,289 Broadcast Conversation (BC) Broadcast News 482,667 (BN) # of words 184,161 # of files 86 287,442 1,1"
C12-2116,P06-1043,0,0.029327,"Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured"
C12-2116,N10-1004,0,0.0183548,"statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training"
C12-2116,P10-2041,0,0.0575601,"lts on the Chinese Penn Treebank indicate that some of the measures provide a statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. A"
C12-2116,P11-1157,0,0.0869203,"Missing"
C12-2116,song-xia-2012-using,1,0.730643,"Missing"
C12-2116,N03-1033,0,0.0438304,"study, we chose to use the same data split for training and test to facilitate comparison with our other experiments. Training Development Test BC 211,795 30,678 32,816 BN 211,826 30,760 48,317 MZ 211,834 30,708 37,531 NW 211,853 30,726 44,543 WB 211,796 30,746 33,623 Table 2: CTB 7.0 Genre character counts for data splitting. 2.2 System performance and cross entropy In order to determine whether entropy-based measures are helpful in training data selection, we first check whether cross entropy correlates with system performance. For this, we first trained and tested the Stanford POS Tagger2 (Toutanova et al., 2003) on the CTB 7.0. The results are in Table 3, in which the training and testing genres are indicated by row labels and column labels, respectively. In the top part of the table, each cell (i, j) has two numbers, where i is the row and j is the column. The first number is the tagging accuracy, when the tagger is trained on the training data of the genre i , and tested on the test data of the genre j . The second number is cross entropy of the test data, estimated by a trigram language model built from the training data using the CMU-Cambridge LM Toolkit3 . The final row in the table lists the 2"
C12-2116,xia-etal-2000-developing,1,0.750312,"n (CWS) and POS tagging. 2 Methodology In this study, we first test whether there is a strong correlation between system performance and cross entropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortiu"
D17-1205,C16-1058,1,0.830681,"Missing"
D17-1205,D16-1102,1,0.921611,"o produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced. Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role. We therefore postulate that a subset of annotation tasks is in fact suitable for crowd workers, while othe"
D17-1205,P98-1013,0,0.108102,"beling tasks is in fact appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled"
D17-1205,D11-1142,0,0.0349924,"r to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to"
D17-1205,D15-1076,0,0.223489,"entences (see Figure 1). While state-of-the-art SRL will predict many correct labels, some predicted labels will be incorrect, and some labels will be missing. Annotation tasks are therefore designed to detect and correct precision and recall errors. Step 2. We generate two types of annotation tasks for the study, namely C ONFIRM P REDICTION and A DD M ISSING tasks: (1) The first, C ONFIRM P RE DICTION tasks, ask users to confirm, reject or correct each predicted frame or role. This type of task addresses precision issues in the SRL. We present to workers a human-readable questionanswer pair (He et al., 2015) for each predicted label, an example of which is illustrated in Figure 2. (2) The second, A DD M ISSING tasks, address potentially missing annotation, i.e. recall issues in the SRL. We generate a question without a suggested answer and ask workers to either confirm that this role does not appear in the sentence, or supply the correct span. We identify potentially missing annotation using PropBank frame definitions; any unseen core role in a sentence is considered potentially missing. We use a manually created mapping of frameroles to questions to generate these tasks. See Table 1 for a mappin"
D17-1205,D16-1258,0,0.0136308,"ither experts familiar with PropBank, or non-expert crowd workers. Rather than routing tasks to the most appropriate workers, our proposed approach determines which SRL tasks are appropriate for crowdsourcing, and sends the remaining ones to experts. Human-in-the-loop Methods Our method is similar in the spirit of human-in-the-loop learning (Fung et al., 1992; Li et al., 2016). Humanin-the-loop learning generally aims to leverage humans to complete easy commonsense tasks, such as the recognition of objects in images (Patterson et al., 2013). Recent work also proposed humanin-the-loop parsing (He et al., 2016) to include human feedback into parsing. However, unlike these approaches, we aim to combine both experts and non-experts to address the difficulty of some SRL annotation tasks, while leveraging the crowd for the majority of tasks. 6 Conclusion In this paper, we proposed C ROWD - IN - THE -L OOP an approach for creating high-quality annotated 1920 data for SRL that leverages both crowd and expert workers. We conducted a crowdsourcing study and analyzed its results to design a classifier to distinguish between crowd-appropriate and expert-required tasks. Our experimental evaluation showed that"
D17-1205,W11-0404,0,0.481745,"r a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicat"
D17-1205,N06-2015,0,0.0390764,"predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have"
D17-1205,E14-4044,0,0.121404,"Missing"
D17-1205,2013.iwslt-evaluation.5,0,0.0620554,"Missing"
D17-1205,P13-2130,0,0.157588,"uires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced. Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role. We therefore postulate that a subset of annotation tasks is in"
D17-1205,J05-1004,0,0.788261,"act appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of N"
D17-1205,P15-2067,0,0.0779537,"Missing"
D17-1205,D07-1002,0,0.0788148,"icult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing"
D17-1205,D08-1027,0,0.321445,"Missing"
D17-1205,C98-1013,0,\N,Missing
E09-1099,P02-1014,0,0.0143802,"discussed below. 5.2.1 igt1-bin same nearLC prev50 LMw1 LMm1 ... igt1-lew diff nearLC prev50 ... igt1-thp diff prev50 ... ... igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ... igt2-lew diff nearLC prev50 ... igt2-thp diff prev50 ... ... Sequence labeling using traditional classifiers Table 5: Feature vectors for the IGTs in Table 1 when using the CoRef approach with sequence labeling methods One common approach to the CoRef problem processes the mentions sequentially and determine for each mention whether it should start a new entity or be linked to an existing mention (e.g., (Soon et al., 2001; Ng and Cardie, 2002; Luo, 2007)); that is, the approach makes a series of decisions, 5.2.2 Joint Inference Using Markov Logic Recently, joint inference has become a topic of keen interests in both the machine learning and NLP communities (e.g., (Bakir et al., 2007; Sutton et al., 2006; Poon and Domingos, 2007)). There have been increasing interests in formulating coreference resolution in a joint model and conducting joint inference to leverage dependen8 There are minor differences between the language ID and coreference resolution tasks. For instance, each entity in the language ID task must be assigned a langu"
E09-1099,N07-1030,0,0.0145081,"2007; Sutton et al., 2006; Poon and Domingos, 2007)). There have been increasing interests in formulating coreference resolution in a joint model and conducting joint inference to leverage dependen8 There are minor differences between the language ID and coreference resolution tasks. For instance, each entity in the language ID task must be assigned a language code. This means that ambiguous language names will evoke multiple entities, each with a different language code. These differences are reflected in our algorithms. 874 cies among the mentions and entities (e.g., (Wellner et al., 2004; Denis and Baldridge, 2007; Poon and Domingos, 2008)). We have built a joint model for language ID in Markov logic (Richardson and Domingos, 2006). Markov logic is a probabilistic extension of first-order logic that makes it possible to compactly specify probability distributions over complex relational domains. A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probability of a state x i"
E09-1099,P06-1111,0,0.0122329,"uistic documents. The canonical form of an IGT consists Proceedings of the 12th Conference of the European Chapter of the ACL, pages 870–878, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 870 been some preliminary studies that show the benefits of using the resource for NLP. For instance, our previous work shows that automatically enriched IGT data can be used to answer typological questions (e.g., the canonical word order of a language) with a high accuracy (Lewis and Xia, 2008), and the information could serve as prototypes for prototype learning (Haghighi and Klein, 2006). of three lines: a line for the language in question (i.e., the language line), an English gloss line, and an English translation. Table 1 shows the beginning of a linguistic document (Baker and Stewart, 1996) which contains two IGTs: one in lines 3032, and the other in lines 34-36. The line numbers are added for the sake of convenience. 1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE 2: 3: Mark C. Baker and Osamuyimen Thompson Stewart 4: McGill University .... 27: The following shows a similar minimal pair from Edo, 28: a Kwa language spoken in Nigeria (Agheyisi 1990). 29: ` er´i m` 30: (2) a. Em`"
E09-1099,hughes-etal-2006-reconsidering,0,0.327286,"Missing"
E09-1099,I08-2093,1,0.848921,"nguage that the reader may not know much about, and is frequently included in scholarly linguistic documents. The canonical form of an IGT consists Proceedings of the 12th Conference of the European Chapter of the ACL, pages 870–878, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 870 been some preliminary studies that show the benefits of using the resource for NLP. For instance, our previous work shows that automatically enriched IGT data can be used to answer typological questions (e.g., the canonical word order of a language) with a high accuracy (Lewis and Xia, 2008), and the information could serve as prototypes for prototype learning (Haghighi and Klein, 2006). of three lines: a line for the language in question (i.e., the language line), an English gloss line, and an English translation. Table 1 shows the beginning of a linguistic document (Baker and Stewart, 1996) which contains two IGTs: one in lines 3032, and the other in lines 34-36. The line numbers are added for the sake of convenience. 1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE 2: 3: Mark C. Baker and Osamuyimen Thompson Stewart 4: McGill University .... 27: The following shows a similar minimal"
E09-1099,N07-1010,0,0.0142355,"1 igt1-bin same nearLC prev50 LMw1 LMm1 ... igt1-lew diff nearLC prev50 ... igt1-thp diff prev50 ... ... igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ... igt2-lew diff nearLC prev50 ... igt2-thp diff prev50 ... ... Sequence labeling using traditional classifiers Table 5: Feature vectors for the IGTs in Table 1 when using the CoRef approach with sequence labeling methods One common approach to the CoRef problem processes the mentions sequentially and determine for each mention whether it should start a new entity or be linked to an existing mention (e.g., (Soon et al., 2001; Ng and Cardie, 2002; Luo, 2007)); that is, the approach makes a series of decisions, 5.2.2 Joint Inference Using Markov Logic Recently, joint inference has become a topic of keen interests in both the machine learning and NLP communities (e.g., (Bakir et al., 2007; Sutton et al., 2006; Poon and Domingos, 2007)). There have been increasing interests in formulating coreference resolution in a joint model and conducting joint inference to leverage dependen8 There are minor differences between the language ID and coreference resolution tasks. For instance, each entity in the language ID task must be assigned a language code. Th"
E09-1099,D08-1068,1,0.818494,"Poon and Domingos, 2007)). There have been increasing interests in formulating coreference resolution in a joint model and conducting joint inference to leverage dependen8 There are minor differences between the language ID and coreference resolution tasks. For instance, each entity in the language ID task must be assigned a language code. This means that ambiguous language names will evoke multiple entities, each with a different language code. These differences are reflected in our algorithms. 874 cies among the mentions and entities (e.g., (Wellner et al., 2004; Denis and Baldridge, 2007; Poon and Domingos, 2008)). We have built a joint model for language ID in Markov logic (Richardson and Domingos, 2006). Markov logic is a probabilistic extension of first-order logic that makes it possible to compactly specify probability distributions over complex relational domains. A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probability of a state x in such a network is given"
E09-1099,J01-4004,0,0.0242597,"ed to the task, as discussed below. 5.2.1 igt1-bin same nearLC prev50 LMw1 LMm1 ... igt1-lew diff nearLC prev50 ... igt1-thp diff prev50 ... ... igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ... igt2-lew diff nearLC prev50 ... igt2-thp diff prev50 ... ... Sequence labeling using traditional classifiers Table 5: Feature vectors for the IGTs in Table 1 when using the CoRef approach with sequence labeling methods One common approach to the CoRef problem processes the mentions sequentially and determine for each mention whether it should start a new entity or be linked to an existing mention (e.g., (Soon et al., 2001; Ng and Cardie, 2002; Luo, 2007)); that is, the approach makes a series of decisions, 5.2.2 Joint Inference Using Markov Logic Recently, joint inference has become a topic of keen interests in both the machine learning and NLP communities (e.g., (Bakir et al., 2007; Sutton et al., 2006; Poon and Domingos, 2007)). There have been increasing interests in formulating coreference resolution in a joint model and conducting joint inference to leverage dependen8 There are minor differences between the language ID and coreference resolution tasks. For instance, each entity in the language ID task mus"
E09-1099,N07-1057,1,0.828557,"Missing"
E09-1099,I08-1069,1,0.815711,"Missing"
E09-2011,I08-2093,1,0.894564,"Missing"
E09-2011,I08-1069,1,0.833125,"ures. One of the most well known and well studied typological types, or parameters, is that of canonical word order, made famous by Joseph Greenberg (Greenberg, 1963). 2 By constructions, we mean linguistically salient constructions, such as actives, passives, relative clauses, inverted word orders, etc., in particular those we feel would be of the most benefit to linguists and computational linguists alike. 3 The tagset extends the standard BIO tagging scheme. 4 The result is produced by a Maximum Entropy learner. The results by SVM and CRF learners are similar. The details were reported in (Xia and Lewis, 2008). 5 Some IGTs are marked by the authors of the crawled documents as ungrammatical (usually with an asterisk “*” at the beginning of the language line). Those IGTs are kept in ODIN too because they could be useful to other linguists, the same reason that they were included in the original documents. 42 In (Lewis and Xia, 2008), we described a means for automatically discovering the answers to a number of computationally salient typological questions, such as the canonical order of constituents (e.g., sentential word order, order of constituents in noun phrases) or the existence of particular co"
E09-2011,E09-1099,1,0.814336,"Missing"
E09-2011,N07-1057,1,\N,Missing
georgi-etal-2012-measuring,de-marneffe-etal-2006-generating,0,\N,Missing
georgi-etal-2012-measuring,W09-3036,1,\N,Missing
georgi-etal-2012-measuring,N01-1026,0,\N,Missing
georgi-etal-2012-measuring,J03-4003,0,\N,Missing
georgi-etal-2012-measuring,N07-1057,1,\N,Missing
georgi-etal-2012-measuring,J94-4004,0,\N,Missing
georgi-etal-2012-measuring,P02-1050,0,\N,Missing
georgi-etal-2012-measuring,I08-2093,1,\N,Missing
H01-1014,P97-1003,0,0.0305428,"NCY STRUCTURES The notion of head is important in both phrase structures and dependency structures. In many linguistic theories such as X-bar theory and GB theory, each phrase structure has a head that determines the main properties of the phrase and a head has several levels of projections; whereas in a dependency structure the head is linked to its dependents. In practice, the head information is explicitly marked in a dependency Treebank, but not always so in a phrase-structure Treebank. A common way to find the head in a phrase structure is to use a head percolation table, as discussed in [7, 1] among others. For example, the entry (S right S/VP) in the head percolation table says that the head child1 of an S node is the first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the"
H01-1014,P99-1065,0,0.342088,"phrase structure in Figure 3(c). If a head has multiple modifiers, the algorithm could use either a single X’ or stacked X’ [3]. Figure 4 shows the phrase structure for the d-tree in Figure 2, where the algorithm uses a single X’ for multiple modifiers of the same head.2 VP V’ NP N’ VP V’ NNP V’ join Vinken will NP VB MD PP NP P’ DTP N’ DT’ NN DT board N’ IN NP as the N’ DTP DT’ DT a ADJP N’ ADJ’ JJ NP N’ N’ CD NNP 29 NN Nov director nonexecutive Figure 4: The phrase structure built by algorithm 1 for the dtree in Figure 2 3.2 Algorithm 2 Algorithm 2, as adopted by Collins and his colleagues [2] when they converted the Czech dependency Treebank [6] into a phrasestructure Treebank, produces phrase structures that are as flat as possible. It uses the following heuristic rules to build phrase structures: One level of projection for any category: X has only one level of projection: XP. 2 To make the phrase structure more readable, we use N’ and NP as the X’ and XP for all kinds of POS tags for nouns (e.g., NNP, NN, and CD). Verbs and adjectives are treated similarly. Minimal projections for dependents: A dependent Y does not project to Y P unless it has its own dependents. Fixed position"
H01-1014,P95-1037,0,0.0122292,"NCY STRUCTURES The notion of head is important in both phrase structures and dependency structures. In many linguistic theories such as X-bar theory and GB theory, each phrase structure has a head that determines the main properties of the phrase and a head has several levels of projections; whereas in a dependency structure the head is linked to its dependents. In practice, the head information is explicitly marked in a dependency Treebank, but not always so in a phrase-structure Treebank. A common way to find the head in a phrase structure is to use a head percolation table, as discussed in [7, 1] among others. For example, the entry (S right S/VP) in the head percolation table says that the head child1 of an S node is the first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the"
H01-1014,J93-2004,0,0.0366946,"first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the head of each nonhead-child depend on the head of the head-child. Figure 1 shows a phrase structure in the English Penn Treebank [8]. In addition to the syntactic labels (such as NP for a noun phrase), the Treebank also uses function tags (such as SBJ for the subject) for grammatical functions. In this phrase structure, the root node has two children: the NP and the VP. The algorithm would choose the VP as the head-child and the NP as a non-head-child, and make the head Vinkin of the NP depend on the head join of the VP in the dependency structure. The dependency structure of the sentence is shown in Figure 2. A more sophisticated version of the algorithm (as discussed in [10]) takes two additional tables (namely, the argu"
H01-1014,W00-1307,1,0.687762,"a phrase structure in the English Penn Treebank [8]. In addition to the syntactic labels (such as NP for a noun phrase), the Treebank also uses function tags (such as SBJ for the subject) for grammatical functions. In this phrase structure, the root node has two children: the NP and the VP. The algorithm would choose the VP as the head-child and the NP as a non-head-child, and make the head Vinkin of the NP depend on the head join of the VP in the dependency structure. The dependency structure of the sentence is shown in Figure 2. A more sophisticated version of the algorithm (as discussed in [10]) takes two additional tables (namely, the argument table and the tagset table) as input and produces dependency structures with the argument/adjunct distinction (i.e., each dependent is marked in the dependency structure as either an argument or an adjunct of the head). S NP-SBJ NNP Vinken VP MD will VP VB join NP DT NN IN the NP-TMP PP-CLR NP board as DT NNP CD Nov 29 JJ NN director a nonexecutive Figure 1: A phrase structure in the Penn Treebank join Vinken as board will director the a 29 Nov nonexecutive Figure 2: A dependency tree for the sentence in Figure 1. Heads are marked as parents"
I08-1003,P07-1116,0,0.552631,"Missing"
I08-1003,J01-2001,0,0.351855,"Missing"
I08-1003,W02-0603,0,0.179322,"y theory. These studies attempt to minimize lexicon complexity (bit-length in crude MDL) while simultaneously minimizing the complexity (by maximizing the probability) of the corpus given the lexicon (de Marcken, 1996; Goldsmith, 2001; Creutz and Lagus, 2002). Many of the approaches mentioned above utilize a simplistic unigram model of morphology to produce the segmentation of the corpus given the lexicon. Substrings in the lexicon are proposed as morphs within a word based on frequency alone, independently of phrase-, word- and morph-surroundings (de Marcken, 1996; Peng and Schuurmans, 2001; Creutz and Lagus, 2002). There are many approaches, however, which further constrain the segmentation procedure. The work by Creutz and Lagus (2004; 2005; 2006) constrains segmentation by accounting for morphotactics, first assigning mophotactic categories (prefix, suffix, and stem) to baseline morphs, and then seeding and refining an HMM using those category assignments. Other more structured models include Goldsmith’s (2001) work which, instead of inducing morphemes, induces morphological signatures like {ø, s, ed, ing} for English regular verbs. Some techniques constrain possible analyses by employing approximati"
I08-1003,W00-0712,0,0.374395,"Lagus (2004; 2005; 2006) constrains segmentation by accounting for morphotactics, first assigning mophotactic categories (prefix, suffix, and stem) to baseline morphs, and then seeding and refining an HMM using those category assignments. Other more structured models include Goldsmith’s (2001) work which, instead of inducing morphemes, induces morphological signatures like {ø, s, ed, ing} for English regular verbs. Some techniques constrain possible analyses by employing approximations for morphological meaning or usage to prevent false derivations (like singed = sing + ed ). There is work by Schone and Jurafsky (2000; 2001) where meaning is proxied by wordand morph-context, condensed via LSA. Yarowsky and Wicentowski (2000) and Yarowsky et al. (2001) use expectations on relative frequency of aligned inflected-word, stem pairs, as well as POS context features, both of which approximate some sort of meaning. We present a technique for refining a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure. Performance on measures which consider surface-boundary accu"
I08-1003,W04-0106,0,0.665256,"complexity (by maximizing the probability) of the corpus given the lexicon (de Marcken, 1996; Goldsmith, 2001; Creutz and Lagus, 2002). Many of the approaches mentioned above utilize a simplistic unigram model of morphology to produce the segmentation of the corpus given the lexicon. Substrings in the lexicon are proposed as morphs within a word based on frequency alone, independently of phrase-, word- and morph-surroundings (de Marcken, 1996; Peng and Schuurmans, 2001; Creutz and Lagus, 2002). There are many approaches, however, which further constrain the segmentation procedure. The work by Creutz and Lagus (2004; 2005; 2006) constrains segmentation by accounting for morphotactics, first assigning mophotactic categories (prefix, suffix, and stem) to baseline morphs, and then seeding and refining an HMM using those category assignments. Other more structured models include Goldsmith’s (2001) work which, instead of inducing morphemes, induces morphological signatures like {ø, s, ed, ing} for English regular verbs. Some techniques constrain possible analyses by employing approximations for morphological meaning or usage to prevent false derivations (like singed = sing + ed ). There is work by Schone and"
I08-1003,N01-1024,0,0.552884,"Missing"
I08-1003,P01-1063,0,0.702567,"criterion, some utilize heuristics. Pure maximum likelihood (ML) approaches may refine the lexicon with heuristics in lieu of explicit priors (Creutz and Lagus, 2004), or not make categorical refinements at all concerning which morphs are included, only probabilistic refinements through a hierarchical EM procedure (Peng and Schuurmans, 2001). Approaches that optimize the lexicon with respect to priors come in several flavors. There are basic maximum a priori (MAP) approaches that try to maximize the probability of the lexicon against linguistically motivated priors (Deligne and Bimbot, 1997; Snover and Brent, 2001; Creutz and Lagus, 2005). An alternative to 1.2 Allomorphy in UMI Allomorphy, or allomorphic variation, is the process by which a morpheme varies (orthographically or 17 phonologically) in particular contexts, as constrained by a grammar.1 To our knowledge, there is only handful of work within UMI attempting to integrate allomorphy into morpheme discovery. A notable approach is the Wordframe model developed by Wicentowski (2002), which performs weighted edits on root-forms, given context, as part of a larger similarity alignment model for discovering <inflected-form, root-form&gt; pairs. Morphol"
I08-1003,P00-1027,0,0.245559,"tactic categories (prefix, suffix, and stem) to baseline morphs, and then seeding and refining an HMM using those category assignments. Other more structured models include Goldsmith’s (2001) work which, instead of inducing morphemes, induces morphological signatures like {ø, s, ed, ing} for English regular verbs. Some techniques constrain possible analyses by employing approximations for morphological meaning or usage to prevent false derivations (like singed = sing + ed ). There is work by Schone and Jurafsky (2000; 2001) where meaning is proxied by wordand morph-context, condensed via LSA. Yarowsky and Wicentowski (2000) and Yarowsky et al. (2001) use expectations on relative frequency of aligned inflected-word, stem pairs, as well as POS context features, both of which approximate some sort of meaning. We present a technique for refining a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure. Performance on measures which consider surface-boundary accuracy and underlying morpheme consistency indicates this technique leads to improvements over baseline segment"
I08-1003,N07-1020,0,0.242313,"Missing"
I08-1003,H01-1035,0,0.124942,"nd stem) to baseline morphs, and then seeding and refining an HMM using those category assignments. Other more structured models include Goldsmith’s (2001) work which, instead of inducing morphemes, induces morphological signatures like {ø, s, ed, ing} for English regular verbs. Some techniques constrain possible analyses by employing approximations for morphological meaning or usage to prevent false derivations (like singed = sing + ed ). There is work by Schone and Jurafsky (2000; 2001) where meaning is proxied by wordand morph-context, condensed via LSA. Yarowsky and Wicentowski (2000) and Yarowsky et al. (2001) use expectations on relative frequency of aligned inflected-word, stem pairs, as well as POS context features, both of which approximate some sort of meaning. We present a technique for refining a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure. Performance on measures which consider surface-boundary accuracy and underlying morpheme consistency indicates this technique leads to improvements over baseline segmentations for English and Turk"
I08-1069,C04-1080,0,0.0307672,"lisms can be extracted (e.g., (Charniak, 1996)). The English and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g., (Meyers et al., 2000; Menezes, 2002; Xia and McCord, 2004)). There are many ways of using the enriched data to bootstrap NLP tools. Suppose we want to build a POS tagger. Previous studies on unsupervised POS tagging can be divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghighi and Klein (2006) sho"
I08-1069,P06-3002,0,0.0127542,"egories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. The tagged target lines in the enriched IGT data can be incorporated in each category of work mentioned above. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach;"
I08-1069,E03-1009,0,0.0277924,"o several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. The tagged target lines in the enriched IGT data can be incorporated in each category of work mentioned above. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word cluste"
I08-1069,D07-1023,0,0.0122193,"ng to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. The tagged target lines in the enriched IGT data can be incorporated in each category of work mentioned above. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambig"
I08-1069,J94-2001,0,0.144228,"in various formalisms can be extracted (e.g., (Charniak, 1996)). The English and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g., (Meyers et al., 2000; Menezes, 2002; Xia and McCord, 2004)). There are many ways of using the enriched data to bootstrap NLP tools. Suppose we want to build a POS tagger. Previous studies on unsupervised POS tagging can be divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghig"
I08-1069,C00-1078,0,0.02076,"ains (1) the English DS and PS produced by an English parser, (2) the word alignment among the three parts of IGT data, and (3) the target DS and PS produced by the projection algorithm. From the enriched data, various kinds of information can be extracted. For instance, the target syntactic structures form small monolingual treebanks, from which grammars in various formalisms can be extracted (e.g., (Charniak, 1996)). The English and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g., (Meyers et al., 2000; Menezes, 2002; Xia and McCord, 2004)). There are many ways of using the enriched data to bootstrap NLP tools. Suppose we want to build a POS tagger. Previous studies on unsupervised POS tagging can be divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM"
I08-1069,N06-1041,0,0.06091,"s indicated by their name, RPLs suffer from a lack of resources, namely data. Supervised learning techniques generally require large amounts of annotated data, something that is nonexistent or scare for most RPLs. A greater number of RPLs, however, have raw data that is available, and the amount and availability of this raw data is increasing every day as more of it makes its way to the Web. Likewise, advances in un- and semisupervised learning techniques have made raw data more readily viable for tool development. Still, however, such techniques often require “seeds”, or “prototypes” (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. An important question is how to create such seeds for the hundreds to thousands of RPLs. We describe the construction of a resource that taps the large body of linguistically analyzed language data that has made its way to the Web, and propose using this The work described in this document was done while Lewis was faculty at the University of Washington. ∗ resource as a means to bootstrap NLP tool development. Interlinear Glossed Text, or IGT, a semistructured data type quite common to the field of linguistics, is used to present data"
I08-1069,P05-1020,0,0.0398443,"Missing"
I08-1069,E95-1020,0,0.0379288,"divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (Baum et al., 1970) on a large amount of unlabeled data to re-estimate transition and emission probability. The second category uses unlabeled data only (e.g., (Sch¨ utze, 1995; Clark, 2003; Biemann, 2006; Dasgupta and Ng, 2007)). The idea is to cluster words based on morphological and/or distributional cues. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. The tagged target lines in the enriched IGT data can be incorporated in each category of work mentioned above. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by th"
I08-1069,P02-1050,0,0.0193674,"escribed in this document was done while Lewis was faculty at the University of Washington. ∗ resource as a means to bootstrap NLP tool development. Interlinear Glossed Text, or IGT, a semistructured data type quite common to the field of linguistics, is used to present data and analysis for a language and is generally embedded in scholarly linguistic documents as part of a larger analysis. IGT’s unique structure — effectively each instance consists of a bitext between English and some target language — can be easily enriched through alignment and projection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al., 2002)). The reader will note that the IGT instance in Example (1) consists of a bitext between some target language on the first line, or the target line (in this case in Welsh), and a third line in English, the translation line. The canonical IGT form, which this example is representative of, has intervening linguistic annotations and glosses on a second line, the gloss line. Because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, it can serve as a bridge between the target and translation lines; high word a"
I08-1069,I08-2093,1,0.6063,"Missing"
I08-1069,N07-1010,0,0.104041,"Missing"
I08-1069,P95-1037,0,0.0368265,"and providing search over ODIN’s data (as a kind of large-scale multi-lingual search). 3.1 IGT for bootstrapping NLP tools Since the target line in IGT data does not come with annotations (e.g., POS tags), it is first necessary to enrich it. Once enriched, the data can be used as a bootstrap for tools such as taggers. 3.1.1 Enriching IGT In a previous study (Xia and Lewis, 2007), we proposed a three-step process to enrich IGT data: (1) parse the English translation with an English parser and convert English phrase structures (PS) into dependency structures (DS) with a head percolation table (Magerman, 1995), (2) align the target line and the English translation using the gloss line, and (3) project the syntactic structures (both PS and DS) from English onto the target line. For instance, given the IGT example in Ex (1), the enrichment algorithm will produce the word alignment in Figure 1 and the syntactic structures in Figure 2. Target line: Rhoddodd yr athro Gloss line: gave-3sg teacher book Translation: the lyfr i’r to-the The teacher gave a book to bachgen ddoe boy the yesterday boy yesterday Figure 1: Aligning the target line and the English translation with the help of the gloss line 533 (b"
I08-1069,J01-4004,0,0.079608,"Missing"
I08-1069,N07-1057,1,0.791583,"first line, or the target line (in this case in Welsh), and a third line in English, the translation line. The canonical IGT form, which this example is representative of, has intervening linguistic annotations and glosses on a second line, the gloss line. Because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, it can serve as a bridge between the target and translation lines; high word alignment accuracy between the three lines can be achieved without requiring parallel data or bilingual dictionaries (Xia and Lewis, 2007). Furthermore, the gloss line provides additional information about the target language data, such as a variety of grammatical annotations, including verbal and tense markers (e.g., 3sg), case markers, etc., all of which can provide useful knowledge about the language. (1) Rhoddodd yr athro lyfr i’r bachgen ddoe gave-3sg the teacher book to-the boy yesterday “The teacher gave a book to the boy yesterday” (Bailyn, 2001) ODIN, the Online Database of INterlinear text (Lewis, 2006), is a resource built over the past few years from data harvested from scholarly documents. Currently, ODIN has over 4"
I08-1069,C04-1073,1,0.753242,"ced by an English parser, (2) the word alignment among the three parts of IGT data, and (3) the target DS and PS produced by the projection algorithm. From the enriched data, various kinds of information can be extracted. For instance, the target syntactic structures form small monolingual treebanks, from which grammars in various formalisms can be extracted (e.g., (Charniak, 1996)). The English and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g., (Meyers et al., 2000; Menezes, 2002; Xia and McCord, 2004)). There are many ways of using the enriched data to bootstrap NLP tools. Suppose we want to build a POS tagger. Previous studies on unsupervised POS tagging can be divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the Baum-Welch algorithm (B"
I08-1069,N01-1026,0,0.0476367,"opose using this The work described in this document was done while Lewis was faculty at the University of Washington. ∗ resource as a means to bootstrap NLP tool development. Interlinear Glossed Text, or IGT, a semistructured data type quite common to the field of linguistics, is used to present data and analysis for a language and is generally embedded in scholarly linguistic documents as part of a larger analysis. IGT’s unique structure — effectively each instance consists of a bitext between English and some target language — can be easily enriched through alignment and projection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al., 2002)). The reader will note that the IGT instance in Example (1) consists of a bitext between some target language on the first line, or the target line (in this case in Welsh), and a third line in English, the translation line. The canonical IGT form, which this example is representative of, has intervening linguistic annotations and glosses on a second line, the gloss line. Because the gloss line aligns with words and morphemes on the target line, and contains glosses that are similar to words on the translation line, it can serve as a bridge between the target and translatio"
I08-1069,P95-1026,0,0.0504049,"significantly. The tagged target lines in the enriched IGT data can be incorporated in each category of work mentioned above. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). Finally, we can apply semi-supervised learning algorithms (e.g., self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and transductive support vector machines (Vapnik, 1998)), using the tagged sentences as seeds. 3.2 Search One focus of ODIN is and has always been search: how can linguists find the data that they are interested in and how can the data be encoded in such a way as to accommodate the variety of queries that a linguist might ask. We currently allow four types of search queries: search by language name and code, search by language family, search by concept/gram, and search by linguistic constructions. The first allows the user to specify a language name or"
I08-1069,menezes-2002-better,0,0.0134283,"DS and PS produced by an English parser, (2) the word alignment among the three parts of IGT data, and (3) the target DS and PS produced by the projection algorithm. From the enriched data, various kinds of information can be extracted. For instance, the target syntactic structures form small monolingual treebanks, from which grammars in various formalisms can be extracted (e.g., (Charniak, 1996)). The English and target syntactic structures form parallel treebanks, from which transfer rules and translation lexicon can be extracted and used for machine translation (e.g., (Meyers et al., 2000; Menezes, 2002; Xia and McCord, 2004)). There are many ways of using the enriched data to bootstrap NLP tools. Suppose we want to build a POS tagger. Previous studies on unsupervised POS tagging can be divided into several categories according to the kind of information available to the learner. The first category (e.g., (Kupiec, 1992; Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005)) assumes there is a lexicon that lists the allowable tags for each word in the text. The common approach is to use the lexicon to initialize the emission probability in a Hidden Markov Model (HMM), and run the"
I08-2093,W06-0605,0,0.026403,"ugh sources to counteract any linguist-based biases introduced into the data. It is also the case that not all examples are full sentences. A linguist might be exploring the structure of noun phrases for instance, and not provide full sentences. Third, we are basing our analyses on projected structures. The word alignment and syntactic projections are not perfect. Consequently, the trees generated, and the rules read off of them, may be incomplete or inaccurate. 7.2 Relevance to NLP Our efforts described here were inspired by some recent work on low-density languages (Yarowksy and Ngai, 2001; Maxwell and Hughes, 2006; Drabek and Yarowsky, 2006). Until fairly recently, almost all NLP work was done on just a dozen or so languages, with the 689 vast majority of the world’s 6,000 languages being ignored. This is understandable, since in order to do serious NLP work, a certain threshold of corpus size must be achieved. We provide a means for generating small, richly annotated corpora for hundreds of languages using freely available data found on the Web. These corpora can then be used to generate other electronic resources, such as annotated corpora and associated NLP tools. The recent work of (Haghighi and Kl"
I08-2093,P05-1034,0,0.0628326,"grammar induction from raw corpora. 2 Background 2.1 Web-Based Interlinear Data as Resource 1 Introduction There is much recent interest in NLP in “low-density” languages, languages that typically defy standard NLP methodologies due to the absence or paucity of relevant digital resources, such as treebanks, parallel corpora, machine readable lexicons and grammars. Even when resources such as raw or parallel corpora exist, they often cannot be found of sufficient size to allow the use of standard machine learning methods. In some recent grammar induction and MT work (Haghighi and Klein, 2006; Quirk et al., 2005) it has been shown that even a small amount of knowledge about a language, in the form of grammar fragments, treelets or prototypes, can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora. In this paper we present a novel method for discovering knowledge about many of the world’s languages by tapping readily available language data posted to the Web. Building upon our work on structural projections across interlinearized text (Xia and Lewis, 2007), we describe a means for automatically discovering a number of computationally salient typ"
I08-2093,N07-1057,1,0.709184,"e learning methods. In some recent grammar induction and MT work (Haghighi and Klein, 2006; Quirk et al., 2005) it has been shown that even a small amount of knowledge about a language, in the form of grammar fragments, treelets or prototypes, can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora. In this paper we present a novel method for discovering knowledge about many of the world’s languages by tapping readily available language data posted to the Web. Building upon our work on structural projections across interlinearized text (Xia and Lewis, 2007), we describe a means for automatically discovering a number of computationally salient typological features, such as the existence of particular constituents in a language (e.g., ∗ The work described in this document was done while Lewis was faculty at the University of Washington. In linguistics, the practice of presenting language data in interlinear form has a long history, going back at least to the time of the structuralists. Interlinear Glossed Text, or IGT, is often used to present data and analysis on a language that the reader may not know much about, and is frequently included in sc"
I08-2093,N01-1026,0,0.159996,"unts of data and from enough sources to counteract any linguist-based biases introduced into the data. It is also the case that not all examples are full sentences. A linguist might be exploring the structure of noun phrases for instance, and not provide full sentences. Third, we are basing our analyses on projected structures. The word alignment and syntactic projections are not perfect. Consequently, the trees generated, and the rules read off of them, may be incomplete or inaccurate. 7.2 Relevance to NLP Our efforts described here were inspired by some recent work on low-density languages (Yarowksy and Ngai, 2001; Maxwell and Hughes, 2006; Drabek and Yarowsky, 2006). Until fairly recently, almost all NLP work was done on just a dozen or so languages, with the 689 vast majority of the world’s 6,000 languages being ignored. This is understandable, since in order to do serious NLP work, a certain threshold of corpus size must be achieved. We provide a means for generating small, richly annotated corpora for hundreds of languages using freely available data found on the Web. These corpora can then be used to generate other electronic resources, such as annotated corpora and associated NLP tools. The recen"
I08-2093,W05-0807,0,\N,Missing
I08-6003,tiedemann-nygaard-2004-opus,0,0.0318815,"ngual NLP as a basis for the creation of translation models (Brown et. al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Parallel corpora can also benefit monolingual NLP via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (Hwa et. al., 2005; Padó and Lapata, 2005; Yarowsky and Ngai, 2001). For most of the mentioned work, large parallel corpora are required. Often these corpora have limited availability due to licensing restrictions (Tiedemann and Nygaard, 2004) and/or are domain specific (Koehn, 2005). Also parallel corpora are only available for a limited set of language pairs. As a result, researchers look to the World Wide Fei Xia University of Washington Seattle, WA 98195, USA fxia@u.washington.edu Web as a source for parallel corpora (Resnik and Smith, 2003; Ma and Liberman, 1999; Chen and Nie, 2000). Because of the web’s world-wide reach and audience, many websites are bilingual, if not multilingual. The web is therefore a prime candidate as a source for such corpora especially for language pairs including resource-poor languages. Resnik and S"
I08-6003,N01-1026,0,0.027879,"parallel for a certain language pair. 1 Introduction Parallel corpora are invaluable resources in many areas of natural language processing (NLP). They are used in multilingual NLP as a basis for the creation of translation models (Brown et. al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Parallel corpora can also benefit monolingual NLP via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (Hwa et. al., 2005; Padó and Lapata, 2005; Yarowsky and Ngai, 2001). For most of the mentioned work, large parallel corpora are required. Often these corpora have limited availability due to licensing restrictions (Tiedemann and Nygaard, 2004) and/or are domain specific (Koehn, 2005). Also parallel corpora are only available for a limited set of language pairs. As a result, researchers look to the World Wide Fei Xia University of Washington Seattle, WA 98195, USA fxia@u.washington.edu Web as a source for parallel corpora (Resnik and Smith, 2003; Ma and Liberman, 1999; Chen and Nie, 2000). Because of the web’s world-wide reach and audience, many websites are b"
I08-6003,J90-2002,0,0.700129,"Missing"
I08-6003,2005.mtsummit-papers.11,0,0.125645,"ls (Brown et. al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Parallel corpora can also benefit monolingual NLP via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (Hwa et. al., 2005; Padó and Lapata, 2005; Yarowsky and Ngai, 2001). For most of the mentioned work, large parallel corpora are required. Often these corpora have limited availability due to licensing restrictions (Tiedemann and Nygaard, 2004) and/or are domain specific (Koehn, 2005). Also parallel corpora are only available for a limited set of language pairs. As a result, researchers look to the World Wide Fei Xia University of Washington Seattle, WA 98195, USA fxia@u.washington.edu Web as a source for parallel corpora (Resnik and Smith, 2003; Ma and Liberman, 1999; Chen and Nie, 2000). Because of the web’s world-wide reach and audience, many websites are bilingual, if not multilingual. The web is therefore a prime candidate as a source for such corpora especially for language pairs including resource-poor languages. Resnik and Smith (2003) outlined the following three"
I08-6003,1999.mtsummit-1.79,0,0.0672365,"f tools for languages where tools already exist (Hwa et. al., 2005; Padó and Lapata, 2005; Yarowsky and Ngai, 2001). For most of the mentioned work, large parallel corpora are required. Often these corpora have limited availability due to licensing restrictions (Tiedemann and Nygaard, 2004) and/or are domain specific (Koehn, 2005). Also parallel corpora are only available for a limited set of language pairs. As a result, researchers look to the World Wide Fei Xia University of Washington Seattle, WA 98195, USA fxia@u.washington.edu Web as a source for parallel corpora (Resnik and Smith, 2003; Ma and Liberman, 1999; Chen and Nie, 2000). Because of the web’s world-wide reach and audience, many websites are bilingual, if not multilingual. The web is therefore a prime candidate as a source for such corpora especially for language pairs including resource-poor languages. Resnik and Smith (2003) outlined the following three steps for identifying parallel text on the web: (1) Locating pages that might have parallel translations (2) Generating candidate page pairs that might be translations (3) Structural filtering out of non-translation candidate pairs In most of the previous work, Step (1) is performed in an"
I08-6003,H05-1108,0,0.0166315,"ntage of pages that are parallel for a certain language pair. 1 Introduction Parallel corpora are invaluable resources in many areas of natural language processing (NLP). They are used in multilingual NLP as a basis for the creation of translation models (Brown et. al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Parallel corpora can also benefit monolingual NLP via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (Hwa et. al., 2005; Padó and Lapata, 2005; Yarowsky and Ngai, 2001). For most of the mentioned work, large parallel corpora are required. Often these corpora have limited availability due to licensing restrictions (Tiedemann and Nygaard, 2004) and/or are domain specific (Koehn, 2005). Also parallel corpora are only available for a limited set of language pairs. As a result, researchers look to the World Wide Fei Xia University of Washington Seattle, WA 98195, USA fxia@u.washington.edu Web as a source for parallel corpora (Resnik and Smith, 2003; Ma and Liberman, 1999; Chen and Nie, 2000). Because of the web’s world-wide reach and aud"
I08-6003,J03-3002,0,0.630675,"ional Workshop On ""Cross Lingual Information Access"" Addressing the Information Need of Multilingual Societies, 2008 Finding parallel texts on the web using cross-language information retrieval Achim Ruopp University of Washington, Seattle, WA 98195, USA achimr@u.washington.edu Abstract Discovering parallel corpora on the web is a challenging task. In this paper, we use cross-language information retrieval techniques in combination with structural features to retrieve candidate page pairs from a commercial search engine. The candidate page pairs are then filtered using techniques described by Resnik and Smith (2003) to determine if they are translations. The results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for a certain language pair. 1 Introduction Parallel corpora are invaluable resources in many areas of natural language processing (NLP). They are used in multilingual NLP as a basis for the creation of translation models (Brown et. al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Parallel corpora can also benefit monolingual NL"
I08-6003,H91-1026,0,\N,Missing
I08-6003,J03-1002,0,\N,Missing
I11-1138,I08-2099,0,0.0172547,"notation guidelines, the guideline designers need to choose a theoretical framework and a set of linguistic phenomena to be captured. Next, they need to determine a linguistic analysis for each linguistic phenomenon, and demonstrate the analysis with descriptions and examples (e.g., sentences and the corresponding DS or PS trees). Take the HUTB as an example. Because it contains both representation types, DS and PS, it has two sets of guidelines for syntactic annotation, one for each representation type. The DS annotation guidelines follow the Paninian grammatical model (Bharati et al., 1995; Begum et al., 2008). The PS guidelines are inspired by the Principlesand-Parameters methodology, as instantiated by the theoretical developments starting with Government and Binding Theory (Chomsky, 1981). 3 Compatibility and conversion As mentioned in the previous section, annotation guidelines provide linguistic analyses for a set of linguistic phenomena, and they are tied to a representation type (DS or PS). Now given two sets of annotation guidelines (one for DS and the other for PS), the central question is whether automatic conversion between DS and PS is possible; that is, is it possible to write a conver"
I11-1138,P99-1065,0,0.351981,"Missing"
I11-1138,J07-3004,0,0.0397981,"arning. This paper provides a framework in which to think about the question of when such a conversion is possible. 1 Introduction There has been much interest in converting treebanks from one representation to another; for instance, from phrase structure to dependency structure (e.g., motivated by the recent surge in interest in dependency parsing), or from phrase structure to other grammatical frameworks such as LTAG, HPSG, CCG, or LFG. While there has been much work on converting between treebank representations (Collins et al., 1999; Xia and Palmer, 2001; Cahill et al., 2002; Nivre, 2003; Hockenmaier and Steedman, 2007), there has not been a general yet precise discussion of what conditions are necessary for such conversion to happen. In this paper, we provide an analytical framework for determining how difficult it would be to convert representations under one set of annotation guidelines M 1 to representations under another set of guidelines M 2 . We are only interested in cases where annotation guidelines are available for both levels of representation, since it is not clear how one would interpret an undocumented representation, and thus it would not be clear how to evaluate the conversion results. Given"
I11-1138,N10-1049,1,0.871312,"procedure for comparing two sets of annotation guidelines with respect to conversion. Section 5 discuss examples from the HUTB that fall into the two “harder” scenarios for conversion. 1234 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1234–1242, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Important concepts in a treebank This study focuses on the relation between DS and PS treebanks. To understand whether an automatic conversion between DS and PS is possible, it is important to distinguish a few concepts in a treebank. Following (Rambow, 2010), we distinguish three concepts: the linguistic phenomena (what he calls “content”), the representation type, and the linguistic theory (what he calls “syntactic theory”). We reinterpret these concepts and extend them, in terms of the HUTB. 2.1 Linguistic phenomena The linguistic phenomena are what we want to represent about the words which make up our treebank: they are the reason for treebanking. If there were no interesting linguistic phenomena, there would be no reason to create treebanks. The task of treebanking consists of identifying which of the phenomena of interest appear in a given"
I11-1138,H01-1014,1,0.878438,"to another (e.g., dependency) before applying machine learning. This paper provides a framework in which to think about the question of when such a conversion is possible. 1 Introduction There has been much interest in converting treebanks from one representation to another; for instance, from phrase structure to dependency structure (e.g., motivated by the recent surge in interest in dependency parsing), or from phrase structure to other grammatical frameworks such as LTAG, HPSG, CCG, or LFG. While there has been much work on converting between treebank representations (Collins et al., 1999; Xia and Palmer, 2001; Cahill et al., 2002; Nivre, 2003; Hockenmaier and Steedman, 2007), there has not been a general yet precise discussion of what conditions are necessary for such conversion to happen. In this paper, we provide an analytical framework for determining how difficult it would be to convert representations under one set of annotation guidelines M 1 to representations under another set of guidelines M 2 . We are only interested in cases where annotation guidelines are available for both levels of representation, since it is not clear how one would interpret an undocumented representation, and thus"
I13-1071,D11-1033,0,0.512705,"com Abstract tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now we have labeled data f"
I13-1071,P07-1033,0,0.306268,"Missing"
I13-1071,2005.mtsummit-papers.30,0,0.0303374,"erformance of statistical machine translation systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training"
I13-1071,2005.eamt-1.19,0,0.0414119,"n systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including averaged entropy gain (AE"
I13-1071,D07-1036,0,0.0225162,"(2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including averaged entropy gain (AEG), cross entropy,"
I13-1071,P10-2041,0,0.571163,"g, China clksong@gmail.com Abstract tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now"
I13-1071,J05-4003,0,0.0207348,"istical machine translation systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including"
I13-1071,P11-1157,0,0.0774577,"Missing"
I13-1071,song-xia-2012-using,1,0.810696,"AEG when a low percentage of data is selected, while its performance is comparable or slightly lower than AEG when a higher percentage of data is selected. To understand this behavior, we compare some statistics of the data sets, as in Table 5. Since OOV rate is important for CWS and POS tagging, we want to compare our coverage-based method and AEG for this factor, and the results are presented in Table 6. The table shows that when a small percentage 4.3 Chinese Word Segmentation To evaluate feature augmentation on CWS, we use a conditional random fields (CRF) word segmenter as described in (Song and Xia, 2012). A nice property of the segmenter is that it incorporates unsupervised learning to identify possible new words in the test data in order to enhance the segmenter’s performance on OOVs. To be more specific, the segmenter uses description length gain (DLG) (Kit and Wilks, 1999) for lexical acquisition as that was performed in (Kit, 2000; Kit, 2005). Then the decision of the unsupervised word segmentation is represented as features T0i , which indicates the tag of the current character C0 when it belongs to a word whose length i ranges from 1 to 5 charac6 Song et al. (2012) showed that AEG works"
I13-1071,C12-2116,1,0.831268,"limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now we have labeled data from both domains, we can apply feature augmenta"
I13-1071,N03-1033,0,0.0247425,"Missing"
I13-1071,xia-etal-2000-developing,1,0.611785,"n C, In this study, we ran several sets of experiments. the count( ) function is called recursively until a We compared our training data selection with shorter ngram inside the original ngram is found. other methods, and then evaluated our revised feaThe value of the count( ) function is zero only if ture augmentation method on the CWS and POS the token ti itself is an OOV. For the experiments tagging tasks. in this paper, we use trigram to count the ngram coverage. 4.1 Data 3.3 Feature Augmentation The Chinese Penn Treebank (CTB) version 7.04 As we mentioned before, a limitation of feature (Xia et al., 2000) is used in our experiments. It augmentation (Daume III, 2007) is that it requires contains about 1.2 million words from five genres: labeled data from the target domain, and very ofBroadcast Conversation (BC), Broadcast News ten such data is not available. To overcome this (BN), Magazine (MZ), Newswire (NW), and Welimitation, we use training data section on the blog (WB). The details of the five genres of CTB source domain data, treat the selected part of data 7.0 are shown in Table 1. as from a pseudo target domain, and leave the unWe divide the data in each genre into ten folds selected par"
I13-1071,W99-0701,0,\N,Missing
K17-1016,P12-1015,0,0.0187192,"two sentiments. For encoding this dictionary, we design a 18-dimension vector, in which the first 9 dimension represents the positive sentiment while the last 9 for negative sentiment. A word is thus encoded into a binary form where the corresponding dimension is set to 1 with others 0. For the aforementioned word “pretty”, its encoded vector will be “000001000 000000000”, in which the score 0.625 of positive activates the 6th dimension in the vector. In doing so, we form a 83K × 18 regularization matrix for the SWN dictionary. 4 4.1 Experiments Word Similarities Evaluation We use the MEN-3k (Bruni et al., 2012), SimLex999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2002) datasets to perform quantitative comparisons among different approaches to generating embeddings. The cosine scores are computed between the vectors of each pair of words in the datasets8 . The measures adopted are Pearson’s coefficient of product-moment correlation (γ) and Spearman’s rank correlation (ρ), which reflect how The resulting word embeddings based on joint learning as well as retrofitting are evaluated intrinsically and extrinsically. For intrinsic evaluation, we use word similarity benchmark to directly tes"
K17-1016,N15-1184,0,0.0465234,"Missing"
K17-1016,Q16-1002,0,0.0247951,"4; Faruqui et al., 2015; Liu et al., 2015a; Kiela et al., 2015; Wieting et al., 2015; Nguyen et al., 2016) or word distributional information (Maas et al., 2011; Liu et al., 2015b) has been proven as effective in enhancing word embeddings, especially for specific downstream tasks. Bian et al. (2014) proposed to improve embedding learning with different kinds of knowledge, such as morphological, syntactic and 150 semantic information. Wieting et al. (2015) improves embeddings by leveraging paraphrase pairs from the PPDB for learning phrase embeddings in the paraphrasing task. In a similar way, Hill et al. (2016) uses learned word embeddings as supervised knowledge for learning phrase embeddings. Although our approach is conceptually similar to previous work, it is different in several ways. For leveraging unlabeled data, the regularizer in this work is different from applying topic distributions as word vectors (Maas et al., 2011) or treating topics as conditional contexts (Liu et al., 2015b). For leveraging semantic knowledge, our regularizer does not require explicit word relations as used in previous studies (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015), but takes encoded informa"
K17-1016,P16-2074,0,0.110859,"ources to obtain embeddings that are best suited for the target tasks, such as Maas et al. (2011) using a sentiment lexicon to enhance embeddings for sentiment classification. However, learning word embeddings with a particular target makes the approach less generic, also implying that customized adaptation has to be made whenever a new knowledge source is considered. Along the lines of improving embedding quality, semantic resources have been incorporated as guiding knowledge to refine objective functions in a joint learning framework (Bian et al., 2014; Xu et al., 2014; Yu and Dredze, 2014; Nguyen et al., 2016), or used for retrofitting based on word relations defined in the semantic lexicons (Faruqui et al., 2015; Kiela et al., 2015). These approaches, nonetheless, require explicit word relations defined in semantic resources, which is a difficult prerequisite for knowledge preparation. Given the above challenges, we propose a novel framework that extends typical context learning by integrating external knowledge sources for enhancing embedding learning. Compared to a well known work by Faruqui et al. (2015) that focused on tackling the task using a retrofitting1 framework on semantic lexicons, our"
K17-1016,J15-4004,0,0.0344352,"s dictionary, we design a 18-dimension vector, in which the first 9 dimension represents the positive sentiment while the last 9 for negative sentiment. A word is thus encoded into a binary form where the corresponding dimension is set to 1 with others 0. For the aforementioned word “pretty”, its encoded vector will be “000001000 000000000”, in which the score 0.625 of positive activates the 6th dimension in the vector. In doing so, we form a 83K × 18 regularization matrix for the SWN dictionary. 4 4.1 Experiments Word Similarities Evaluation We use the MEN-3k (Bruni et al., 2012), SimLex999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2002) datasets to perform quantitative comparisons among different approaches to generating embeddings. The cosine scores are computed between the vectors of each pair of words in the datasets8 . The measures adopted are Pearson’s coefficient of product-moment correlation (γ) and Spearman’s rank correlation (ρ), which reflect how The resulting word embeddings based on joint learning as well as retrofitting are evaluated intrinsically and extrinsically. For intrinsic evaluation, we use word similarity benchmark to directly test the quality of the learned em"
K17-1016,D14-1162,0,0.101715,"Missing"
K17-1016,D15-1242,0,0.0488127,"Missing"
K17-1016,P15-1145,0,0.0758369,"modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic lexicons (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015) or topic distributions (Maas et al., 2011; Liu et al., 2015b). To provide a more generic solution, we propose a unified framework that learns word embeddings from context (e.g., CBOW or SG) together with the flexibility of incorporating arbitrary external knowledge using the notion of a regularizer. Details are unfolded in following subsections. ternal knowledge has to be clustered beforehand according to their semantic relatedness (e.g., cold, icy, winter, frozen), and words of similar meanings are added as part of context for learning. This may set a high bar for preparing external knowledge since finding the precise word-word relations is required."
K17-1016,P11-1015,0,0.793183,"on (e.g., language modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic lexicons (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015) or topic distributions (Maas et al., 2011; Liu et al., 2015b). To provide a more generic solution, we propose a unified framework that learns word embeddings from context (e.g., CBOW or SG) together with the flexibility of incorporating arbitrary external knowledge using the notion of a regularizer. Details are unfolded in following subsections. ternal knowledge has to be clustered beforehand according to their semantic relatedness (e.g., cold, icy, winter, frozen), and words of similar meanings are added as part of context for learning. This may set a high bar for preparing external knowledge since finding the precise word-word rela"
K17-1016,P14-2089,0,0.225274,"leveraged relevant sources to obtain embeddings that are best suited for the target tasks, such as Maas et al. (2011) using a sentiment lexicon to enhance embeddings for sentiment classification. However, learning word embeddings with a particular target makes the approach less generic, also implying that customized adaptation has to be made whenever a new knowledge source is considered. Along the lines of improving embedding quality, semantic resources have been incorporated as guiding knowledge to refine objective functions in a joint learning framework (Bian et al., 2014; Xu et al., 2014; Yu and Dredze, 2014; Nguyen et al., 2016), or used for retrofitting based on word relations defined in the semantic lexicons (Faruqui et al., 2015; Kiela et al., 2015). These approaches, nonetheless, require explicit word relations defined in semantic resources, which is a difficult prerequisite for knowledge preparation. Given the above challenges, we propose a novel framework that extends typical context learning by integrating external knowledge sources for enhancing embedding learning. Compared to a well known work by Faruqui et al. (2015) that focused on tackling the task using a retrofitting1 framework on"
K17-1016,N13-1090,0,0.244419,"s paper. The resulting embeddings are evaluated by word similarity and sentiment classification. Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods. 1 Fei Xia University of Washington fxia@uw.edu Introduction Distributed representation of words (or word embedding) has been demonstrated to be effective in many natural language processing (NLP) tasks (Bengio et al., 2003; Collobert and Weston, 2008; Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013b,d; Weston et al., 2015). Conventional word embeddings are trained with a single objective function (e.g., language modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic"
K17-1016,baccianella-etal-2010-sentiwordnet,0,\N,Missing
K17-1016,N13-1092,0,\N,Missing
klassen-etal-2014-annotating,E12-2021,0,\N,Missing
L16-1545,klassen-etal-2014-annotating,1,0.853939,"chest x-ray reports. As an example, the text snippet “No change in diffuse lung disease consistent with edema. Patchy bilateral lung consolidation has diminished. No new focal abnormalities” belongs to a chest x-ray note of a patient who is labeled in the gold standard as negative for pneumonia. Our pneumonia prediction system mislabels the patient as positive for pneumonia, because the system primarily used n-grams and UMLS concepts as features and did not capture change-of-state information in the feature representation (Tepper et al., 2013). To address this problem, in our previous study (Klassen et al., 2014), we proposed to annotate change-of-state events in the text snippets with a tuple schema of five fields hlocation, attribute, value, change-of-state, referencei. In this paper, we discuss the extensions we have made to the schema to add annotation of diagnosis events and to handle coordination and negation of tuple fields. We then report on the experimental results of the NLP systems we build to automatically identify change-of-state and diagnosis events. 2. Creating a Corpus for Phenotype Detection Early detection and treatment of ventilator associated pneumonia (VAP) is important as it is t"
L16-1545,P11-1163,0,0.0841632,"Missing"
L16-1545,nilsson-nivre-2008-malteval,0,0.0818922,"Missing"
L16-1545,E12-2021,0,0.102574,"Missing"
L18-1116,W15-3709,1,0.906315,"Missing"
L18-1116,I08-2093,1,0.812078,"Missing"
L18-1464,J97-1002,0,0.615779,"Missing"
L18-1464,J86-3001,0,0.792435,"Missing"
L18-1464,J00-3003,0,0.451514,"tional structures at turn level or between a pair of turns (e.g., by distinguishing Forward Communicative Function and Backward Communicative Function (Core and Allen, 1997; Jurafsky et al., 1997)). Informed by the Conversation Analysis (CA) theory and approach, our annotation scheme allows us to capture hierarchical structures of conversation at several levels, including turn, adjacency pair, sequence, and overall organization. For conversational actions, most of the existing annotation schemes of dialog acts (DAs) were based on speech act theory (Core and Allen, 1997; Jurafsky et al., 1997; Stolcke et al., 2000; Hoxha et al., 2016) ; however, it was found that comprehension of indirect speech acts were very difficulty with the available schemes, primarily due to the fact that classifications of actions were based on the surface format of an utterance. Based on CA theory of actions in conversation, which considers the sequential position of a turn as critical for action recognition and ascription, our annotation scheme allows classifications of actions based on a turn’s structural position in conversation. It thus entails great flexibilities for annotating indirect conversational actions. 5.2. cal an"
L18-1464,xia-etal-2000-developing,1,0.613454,"Missing"
N03-2036,J93-2003,0,0.0217161,"ed models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. T 3 T 2 T 1 S Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability P r(bn1 ) is decomposed into conditional probabilities using the chain rule: n Y P r(bi |bi−1 ) = i=1 n Y pα (bi |bi−1 ) · p(1−α) (bi |bi−1 ) ≈ i=1 n Y pα (bi ) · p(1−α) (bi |bi−1 ) S 2 S 3 S 4 Figure 1: A block sequence that jointly generates 4 target and source phrases. 1 Phrase-based Unigram Model P r(bn1 ) ≈ 1 (1) i=1 We try to find the block sequence that maximizes P r(bn1 ): bn1 = arg maxbn1 P r(bn1 )."
N03-2036,W02-1018,0,0.0230845,"r, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. T 3 T 2 T 1 S Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability P r(bn1 ) is decomposed into conditional probabilities using the chain rule: n Y P r(bi |bi−1 ) = i=1 n Y pα (bi |bi−1 ) · p(1−α) (bi |bi−1 ) ≈ i=1 n Y pα (bi ) · p(1−α) (bi |bi−1 ) S 2 S 3 S 4 Figure 1: A block sequence that jointly generates 4 target and sour"
N03-2036,W99-0604,1,0.796705,"t T 4 In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. T 3 T 2 T 1 S Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability P r(bn1 ) is decomposed into conditional probabilities using the chain rule: n Y P r(bi |bi−1 ) = i=1 n Y pα (bi |bi−1 ) · p(1−α) (bi |bi−1 ) ≈ i=1 n Y pα (bi ) · p(1−α) (bi |bi−1 ) S 2 S 3 S 4 Figure 1: A block sequence that jointly gener"
N03-2036,P02-1040,0,0.116125,"Missing"
N03-2036,P02-1039,0,0.0619993,"e-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. T 3 T 2 T 1 S Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability P r(bn1 ) is decomposed into conditional probabilities using the chain rule: n Y P r(bi |bi−1 ) = i=1 n Y pα (bi |bi−1 ) · p(1−α) (bi |bi−1 ) ≈ i=1 n Y pα (bi ) · p(1−α) (bi |bi−1 ) S 2 S 3 S 4 Figure 1: A block sequence that jointly generates 4 target and source phrases. 1 Phrase-based"
N03-2036,C96-2141,1,\N,Missing
N03-2036,J03-1005,1,\N,Missing
N03-4001,C96-1017,0,0.0944467,"Missing"
N03-4001,J93-2003,0,0.00628603,"Missing"
N03-4001,P03-1051,1,0.838841,"Missing"
N03-4001,J03-1005,1,0.874545,"Missing"
N07-1057,J94-4004,0,0.054448,"such a resource allows one to develop computational artifacts, such as grammars and transfer rules, which can be used as “seed” knowledge for building larger resources. In particular, knowing a little about the structure of a language can help in developing annotated corpora and tools, since a little knowledge can go a long way in inducing accurate structure and annotations (Haghighi and Klein, 2006). Of particular relevance to MT is the issue of struc452 Proceedings of NAACL HLT 2007, pages 452–459, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tural divergence (Dorr, 1994). Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al., 2002). However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data (Fox, 2002; Hwa et al., 2002; Wellington et al., 2006). A larger sample of typologically diverse language data can help test the assumption for hundreds of languages. We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora"
N07-1057,W02-1039,0,0.0587863,"ping annotated corpora and tools, since a little knowledge can go a long way in inducing accurate structure and annotations (Haghighi and Klein, 2006). Of particular relevance to MT is the issue of struc452 Proceedings of NAACL HLT 2007, pages 452–459, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tural divergence (Dorr, 1994). Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al., 2002). However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data (Fox, 2002; Hwa et al., 2002; Wellington et al., 2006). A larger sample of typologically diverse language data can help test the assumption for hundreds of languages. We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora could be of smaller size and the resulting tools more robust, opening the door to the development of tools and resources for a larger number of the world’s languages. Given the imminent death of half of the world’s 6,000 languages (Krauss, 1992), the"
N07-1057,P02-1050,0,0.123199,"used as “seed” knowledge for building larger resources. In particular, knowing a little about the structure of a language can help in developing annotated corpora and tools, since a little knowledge can go a long way in inducing accurate structure and annotations (Haghighi and Klein, 2006). Of particular relevance to MT is the issue of struc452 Proceedings of NAACL HLT 2007, pages 452–459, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tural divergence (Dorr, 1994). Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al., 2002). However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data (Fox, 2002; Hwa et al., 2002; Wellington et al., 2006). A larger sample of typologically diverse language data can help test the assumption for hundreds of languages. We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora could be of smaller size and the resulting tools more robust, opening the door to the development of tools and resourc"
N07-1057,P95-1037,0,0.0122287,"e help of the gloss line. 3. Project the English syntactic structures to obtain the source syntactic structures using word alignment. 3.1 Parsing English sentences There are many English parsers available to the public, and in this experiment we used Charniak’s parser (Charniak, 1997), which was trained on the English Penn Treebank (Marcus et al., 1994). Figure 1(a) shows a parse tree (in the Penn Treebank style) for the English translation in Ex (1). Given a parse tree, (1) Rhoddodd yr athro lyfr i’r bachgen ddoe gave-3sg the teacher book to-the boy yesterday we use a head percolation table (Magerman, 1995) “The teacher gave a book to the boy yesterday” to create the corresponding dependency structure. (Bailyn, 2001) Figure 2(a) shows the dependency structure derived Although IGT is usually embedded in linguistics from the parse tree in Figure 1(a). documents as part of a larger analysis, in and of 3.2 Word alignment itself it contains analysis and interesting informaBecause most of the 700+ languages in ODIN are tion about the source language. In particular, the low-density languages with no on-line bilingual dicgloss line, which is word and morpheme aligned with tionaries or large parallel cor"
N07-1057,H94-1020,0,0.0153765,"syntactic structure in this paper refers to both phrase structure (PS) and dependency structure (DS). The enrichment process has three steps: 1. Parse the English translation using an off-theshelf parser. 2. Align the source sentence and English translation with the help of the gloss line. 3. Project the English syntactic structures to obtain the source syntactic structures using word alignment. 3.1 Parsing English sentences There are many English parsers available to the public, and in this experiment we used Charniak’s parser (Charniak, 1997), which was trained on the English Penn Treebank (Marcus et al., 1994). Figure 1(a) shows a parse tree (in the Penn Treebank style) for the English translation in Ex (1). Given a parse tree, (1) Rhoddodd yr athro lyfr i’r bachgen ddoe gave-3sg the teacher book to-the boy yesterday we use a head percolation table (Magerman, 1995) “The teacher gave a book to the boy yesterday” to create the corresponding dependency structure. (Bailyn, 2001) Figure 2(a) shows the dependency structure derived Although IGT is usually embedded in linguistics from the parse tree in Figure 1(a). documents as part of a larger analysis, in and of 3.2 Word alignment itself it contains anal"
N07-1057,P00-1056,0,0.0543839,"hly likely that the two copies of the same word should be aligned to each other. To help GIZA++ recognize this property, we first identify and collect all such words and then add single word pairs (x,x) to the training data. For instance, from Ex (1), we would add a sentence pair for each morpheme (excepting -3sg which does not appear in the translation line). 3.2.2 Statistical word aligner We create a parallel corpus by using the gloss lines and the translation lines of all the IGT examples for all the languages in ODIN. We then train IBM models (Brown et al., 1993) using the GIZA++ package (Och and Ney, 2000). In addition to the common practice of lowercasing words and combining word 454 Heuristic word aligner Our second word aligner is based on the assumption that if two words (one on the gloss line, the other on the translation line) have the same root form, they are likely to be aligned to one other.We built a simple English morphological analyzer and ran it on the two lines, and then linked the words with the same root form.4 3.3 Tree projection We designed two projection algorithms: one which projects PS and the other which projects DS, both from the English to the source language.5 3.3.1 Pro"
N07-1057,W06-1608,0,0.0149243,"between languages similar to those described in (Fox, 2002). 5.2 Tools and resource building The information that we discover about a language can help with the development of tools for the language. The order of constituents, for instance, can be used to inform prototype-driven learning strategies (Haghighi and Klein, 2006), which can then be applied to raw corpora. It is also possible that small samples of data showing the alignment interactions between source language structures and those of English can provide essential bootstrap information for informing machine translation systems (cf (Quirk and Corston-Oliver, 2006)). Proof of the utility of an enriched corpus built over ODIN will depend crucially on its evaluation, and we feel that an important part of our future work will be the development of parsers that have been trained on projected structures. These parsers can be evaluated against human built corpora such as treebanks (obviously, only for those languages that have treebanks). Proof will also come from linguists who will be able to use the corpus to search for constructions of interest (e.g., passives, relative clauses, etc.), and will likely be able to do so using standard tools such as tgrep.21"
N07-1057,P05-1034,0,0.0721886,"Missing"
N07-1057,P06-1123,0,0.020507,"ols, since a little knowledge can go a long way in inducing accurate structure and annotations (Haghighi and Klein, 2006). Of particular relevance to MT is the issue of struc452 Proceedings of NAACL HLT 2007, pages 452–459, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tural divergence (Dorr, 1994). Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al., 2002). However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data (Fox, 2002; Hwa et al., 2002; Wellington et al., 2006). A larger sample of typologically diverse language data can help test the assumption for hundreds of languages. We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora could be of smaller size and the resulting tools more robust, opening the door to the development of tools and resources for a larger number of the world’s languages. Given the imminent death of half of the world’s 6,000 languages (Krauss, 1992), the development of any language specific tools f"
N07-1057,H05-1107,0,0.0128312,"Missing"
N07-1057,N01-1026,0,0.182444,"Missing"
N07-1057,J93-2003,0,\N,Missing
P13-2055,J94-4004,0,0.250514,"attached as indicated by dotted lines. The words pAnI and se are incorrectly inverted, as indicated by the curved arrow. Figure 1: An example of projecting a dependency tree from English to Hindi. child in the target tree and vice-versa. Finally, spontaneous alignments were those for which a word did not align to any word on the other side. These edge configurations could be detected from simple parent–child edges and the alignment (or lack of) between words in the language pairs. Using these simple, languageagnostic measures allows one to look for divergence types such as those described by Dorr (1994). Georgi et al. (2012b) described a method in which new features were extracted from the projected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new featu"
P13-2055,georgi-etal-2012-measuring,1,0.838392,"indicated by dotted lines. The words pAnI and se are incorrectly inverted, as indicated by the curved arrow. Figure 1: An example of projecting a dependency tree from English to Hindi. child in the target tree and vice-versa. Finally, spontaneous alignments were those for which a word did not align to any word on the other side. These edge configurations could be detected from simple parent–child edges and the alignment (or lack of) between words in the language pairs. Using these simple, languageagnostic measures allows one to look for divergence types such as those described by Dorr (1994). Georgi et al. (2012b) described a method in which new features were extracted from the projected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new features improved parsing"
P13-2055,C12-2037,1,0.749691,"Missing"
P13-2055,H05-1066,0,0.0640071,") between words in the language pairs. Using these simple, languageagnostic measures allows one to look for divergence types such as those described by Dorr (1994). Georgi et al. (2012b) described a method in which new features were extracted from the projected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new features improved parsing performance. In this paper, we use the small training corpus built in Georgi et al. (2012b) to improve the projection algorithm itself. The improved projected trees are in turn fed to the statistical parser to further improve parsing results. 3 neous (unaligned) source words are removed, and the remaining words are replaced with corresponding words in the target side [Fig 1(c)]. Finally, spontaneous target words are re-attached heuris"
P13-2055,P05-1034,0,0.159052,"rge number of electronic resources that can be used to build NLP systems. For instance, some languages may lack treebanks, thus making it difficult to build a high-quality statistical parser. One common approach to address this problem is to take advantage of bitext between a resource-rich language (e.g., English) and a resource-poor language by projecting information from the former to the latter (Yarowsky and Ngai, 2001; Hwa et al., 2004). While pro2 Previous Work For this paper, we will be building upon the standard projection algorithm for dependency structures as outlined in Quirk et al. (2005) and illustrated in Figure 1. First, a sentence pair between resource-rich (source) and resource-poor (target) languages is word aligned [Fig 1(a)]. Second, the source sentence is parsed by a dependency parser for the source language [Fig 1(b)]. Third, sponta306 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 306–311, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics siwA ne Sita erg Sita pAnI se water with filled the GadZe clay-pot clay-pot ko acc with BarA filled water (a) An Interlinear Glossed Text (IGT) instan"
P13-2055,W09-3036,1,\N,Missing
P13-2055,N01-1026,0,\N,Missing
P13-2055,N07-1057,1,\N,Missing
P13-2104,D07-1101,0,0.0204365,". This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of p"
P13-2104,D09-1060,0,0.296398,"o create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of Charniak and Johnson (2005). Reichart and Rappoport (2007) explored self-training when the amount of the annotated data is small 2 Our Approach for Parsing Adaptation Our approach is inspired by Chen et al. (2009)’s work on semi-supervised parsing with additional subtree-based features extracted from unlabeled data and by the feature augmentation method proposed by Daume III (2007). In this section, we first summarize Chen et al.’s work and explain how we extend that for domain adaptation. We will then highlight the similarity and difference between our work and Daume’s method. 2.1 Semi-supervised parsing with subtree-based features One of the most well-known semi-supervised parsing methods is self-training, where a parser trained from the labeled data set is used to parse unlabeled data, and some of t"
P13-2104,P07-1033,0,0.335812,"Missing"
P13-2104,P11-1157,0,0.0436646,"Missing"
P13-2104,W07-2416,0,0.0423289,"Missing"
P13-2104,P10-1001,0,0.015264,"ure augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClo"
P13-2104,P07-1078,0,0.631168,"sers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of Charniak and Johnson (2005). Reichart and Rappoport (2007) explored self-training when the amount of the annotated data is small 2 Our Approach for Parsing Adaptation Our approach is inspired by Chen et al. (2009)’s work on semi-supervised parsing with additional subtree-based features extracted from unlabeled data and by the feature augmentation method proposed by Daume III (2007). In this section, we first summarize Chen et al.’s work and explain how we extend that for domain adaptation. We will then highlight the similarity and difference between our work and Daume’s method. 2.1 Semi-supervised parsing with subtree-based features One of the most w"
P13-2104,C12-2077,1,0.887665,"ch in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) su"
P13-2104,D07-1111,0,0.177528,"quality, and retrain the parser. SrcOnlys TgtOnlyt Src&Tgts,t Self-Trainings,t Co-Trainings,t Feature-Augs,t Chen (2009)s,t this papers,t Per-corpusT Co-Training: In the co-training system, we first train two parsers with the labeled data from the source and target domains, respectively. Then we use the parsers to parse unlabeled data in the target domain and select sentences for which the two parsers produce identical trees. Finally, we add the analyses for those sentences to the union of the source and target labeled data to retrain a new parser. This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al., 2007). WSJ-to-B UAS CM 88.8 43.8 86.6 38.8 89.1 44.3 89.2 45.1 89.2 45.1 89.1 45.1 89.3 45.0 89.5 45.5 89.9 47.0 B-to-WSJ UAS CM 86.3 26.5 88.2 29.3 89.4 31.2 89.8 32.1 89.8 32.7 89.8 32.8 89.7 31.8 90.2 33.4 92.7 42.1 Table 2: Results with the first-order parsing model in the first and second experiments. The superscript indicates the source of labeled data used in training. Feature-Augmentation: This is the approach proposed in (Daume III, 2007). SrcOnlys TgtOnlyt Src&Tgts,t Self-T"
P13-2104,J93-2004,0,0.0430028,"Missing"
P13-2104,P06-1043,0,0.347929,"Missing"
P13-2104,W08-2121,0,0.0406354,"Missing"
P13-2104,E06-1011,0,0.0453708,"ation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed f"
P13-2104,W03-3023,0,0.048585,"Missing"
P13-2104,P05-1012,0,0.34575,"a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of grea"
P13-2104,P09-1043,0,0.0292915,"Missing"
P13-2104,H05-1066,0,0.105294,"a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of grea"
P13-2104,C04-1010,0,0.0109446,"n this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource"
P13-2104,D07-1096,0,\N,Missing
P14-1126,J04-3004,0,0.0270112,"a {(xi , y i )}, the logarithm of the likelihood (a.k.a. the log-likelihood) is given by: X L(λ) = log pλ (y i |xi ) (6) i Maximum likelihood training chooses parameters such that the log-likelihood L(λ) is maximized. However, in our scenario we have no labeled training data for target languages but we have some parallel and unlabeled data plus an English dependency parser. For the purpose of transferring cross-lingual information from the English parser via parallel text, we explore the model training method proposed by Smith and Eisner (2007), which presented a generalization of K function (Abney, 2004), and related it to another semi-supervised learning technique, entropy regularization (Jiao et al., 2006; Mann and McCallum, 2007). The objective K function to be minimized is actually the expected negative loglikelihood: XX K = − p˜(y i |xi ) log pλ (y i |xi ) = X i i yi D(˜ pi ||pλ,i ) + H(˜ pi ) (7) def def where p˜i (·) = p˜(·|xi ) and pλ,i (·) = pλ (·|xi ). p˜(y|x) is the “transferring distribution” that reflects our uncertainty about the true labels, and we are trying to learn a parametric model pλ (y|x) by minimizing the K function. In our scenario, we have a set of aligned parallel da"
P14-1126,P10-1131,0,0.163426,"ms and the oracle (OR). The results of unsupervised DMV model (Klein and Manning, 2004) are from Table 1 of McDonald et al. (2011). Our approach outperforms all these baseline systems and achieves state-of-theart performance on all the eight languages. In order to compare with more previous methods, we also report parsing performance on sentences of length 10 or less after punctuation has been removed. Table 7 shows the results of our system and the results of baseline systems. “USR†” is the weakly supervised system of Naseem et al. (2010). “PGI” is the phylogenetic grammar induction model of Berg-Kirkpatrick and Klein (2010). Both the results of the two systems are cited from Table 4 of McDonald et al. (2011). We also include the results of the unsupervised dependency parsing model with non-parallel multilingual guidance (NMG) proposed by Cohen et al. (2011)8 , and “PR” which is the posterior regularization approach presented in Gillenwater et al. (2010). All the results are shown in Table 7. From Table 7, we can see that among the eight target languages, our approach achieves best parsing performance on six languages — Danish, German, Greek, Italian, Portuguese and Swedish. It should be noted that the “NMG” syst"
P14-1126,D10-1117,0,0.0256421,"and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated scenario, in which we want to build statistical parsers for resource-poo"
P14-1126,W06-2920,0,0.10957,"se resources. The monolingual treebanks in our experiments are from the Google Universal Dependency Treebanks (McDonald et al., 2013), for the reason that the treebanks of different languages in Google Universal Dependency Treebanks have consistent syntactic representations. The parallel data come from the Europarl corpus version 7 (Koehn, 2005) and Kaist Corpus4 . Taking the intersection of languages in the two kinds of resources yields the following seven languages: French, German, Italian, Korean, Portuguese, Spanish and Swedish. The treebanks from CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) appear to be another reasonable choice. However, previous studies (McDonald et al., 2011; McDonald et al., 2013) have demonstrated that a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components, and the heterogenous representations used in CoNLL shared-tasks treebanks weaken any conclusion that can be drawn. 4 http://semanticweb.kaist.ac.kr/home/ index.php/Corpus10 1341 de es fr ko sv Ave DTP 58.50 68.07 70.14 42.37 70.56 61.93 DTP† 58.46 68.72 71.13 43.57 70.59 62.49 PTP† 69.21"
P14-1126,D08-1092,0,0.00776993,"resources from a resource-rich source language (like English).1 We assume that there are absolutely no labeled training data for the target language, but we have access to parallel data with a resource-rich language and a sufficient amount of labeled training data to build an accurate parser for the resource-rich language. This scenario appears similar to the setting in bilingual text parsing. However, most bilingual text parsing approaches require bilingual treebanks — treebanks that have manually annotated tree structures on both sides of source and target languages (Smith and Smith, 2004; Burkett and Klein, 2008), or have tree structures on the source side and translated sentences in the target languages (Huang et 1 For the sake of simplicity, we refer to the resource-poor language as the “target language”, and resource-rich language as the “source language”. In addition, in this study we use English as the source resource-rich language, but our methodology can be applied to any resource-rich languages. 1337 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337–1348, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistic"
P14-1126,D07-1101,0,0.026116,"ges. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 200"
P14-1126,P10-1003,0,0.0162941,"e structures on the source side and translated sentences in the target languages (Huang et 1 For the sake of simplicity, we refer to the resource-poor language as the “target language”, and resource-rich language as the “source language”. In addition, in this study we use English as the source resource-rich language, but our methodology can be applied to any resource-rich languages. 1337 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337–1348, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2009; Chen et al., 2010). Obviously, bilingual treebanks are much more difficult to acquire than the resources required in our scenario, since the labeled training data and the parallel text in our case are completely separated. What is more important is that most studies on bilingual text parsing assumed that the parser is applied only on bilingual text. But our goal is to develop a parser that can be used in completely monolingual setting for each target language of interest. This scenario is applicable to a large set of languages and many research studies (Hwa et al., 2005) have been made on it. Ganchev et al. (20"
P14-1126,N09-1009,0,0.0301851,"2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated scenario, in which we w"
P14-1126,D11-1005,0,0.846572,"e on it. Ganchev et al. (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al., 2007). McDonald et al. (2011) proposed two parser transfer approaches between two different languages — one is directly transferred parser from delexicalized parsers, and the other parser is transferred using constraint driven learning algorithm where constraints are drawn from parallel corpora. In that work, they demonstrate that even the directly transferred delexicalized parser produces significantly higher accuracies than unsupervised parsers. Cohen et al. (2011) proposed an approach for unsupervised dependency parsing with non-parallel multilingual guidance from one or more helper languages, in which parallel data is not used. In this work, we propose a learning framework for transferring dependency grammars from a resource-rich language to resource-poor languages via parallel text. We train probabilistic parsing models for resource-poor languages by maximizing a combination of likelihood on parallel data and confidence on unlabeled data. Our work is based on the learning framework used in Smith and Eisner (2007), which is originally designed for par"
P14-1126,P11-1061,0,0.0365038,"Missing"
P14-1126,P09-1042,0,0.0705249,"Chen et al., 2010). Obviously, bilingual treebanks are much more difficult to acquire than the resources required in our scenario, since the labeled training data and the parallel text in our case are completely separated. What is more important is that most studies on bilingual text parsing assumed that the parser is applied only on bilingual text. But our goal is to develop a parser that can be used in completely monolingual setting for each target language of interest. This scenario is applicable to a large set of languages and many research studies (Hwa et al., 2005) have been made on it. Ganchev et al. (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al., 2007). McDonald et al. (2011) proposed two parser transfer approaches between two different languages — one is directly transferred parser from delexicalized parsers, and the other parser is transferred using constraint driven learning algorithm where constraints are drawn from parallel corpora. In that work, they demonstrate that even the directly transferred delexicalized parser produces significantly higher accuracies than unsupervised parsers. Cohen et al. (2011) proposed a"
P14-1126,P10-2036,0,0.153613,"Missing"
P14-1126,D09-1127,0,0.0179447,"Missing"
P14-1126,P06-1027,0,0.0636433,"og pλ (y i |xi ) (6) i Maximum likelihood training chooses parameters such that the log-likelihood L(λ) is maximized. However, in our scenario we have no labeled training data for target languages but we have some parallel and unlabeled data plus an English dependency parser. For the purpose of transferring cross-lingual information from the English parser via parallel text, we explore the model training method proposed by Smith and Eisner (2007), which presented a generalization of K function (Abney, 2004), and related it to another semi-supervised learning technique, entropy regularization (Jiao et al., 2006; Mann and McCallum, 2007). The objective K function to be minimized is actually the expected negative loglikelihood: XX K = − p˜(y i |xi ) log pλ (y i |xi ) = X i i yi D(˜ pi ||pλ,i ) + H(˜ pi ) (7) def def where p˜i (·) = p˜(·|xi ) and pλ,i (·) = pλ (·|xi ). p˜(y|x) is the “transferring distribution” that reflects our uncertainty about the true labels, and we are trying to learn a parametric model pλ (y|x) by minimizing the K function. In our scenario, we have a set of aligned parallel data P = {xsi , xti , ai } where ai is the word alignment for the pair of source-target sentences (xsi , xt"
P14-1126,D11-1017,0,0.00996941,"resource-poor languages. We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-p"
P14-1126,P04-1061,0,0.775335,"a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider"
P14-1126,2005.mtsummit-papers.11,0,0.188591,"s used to train the English parsing model, and the Treebanks for target languages are used to evaluate the parsing performance of our approach. (ii) Large amounts of parallel text with English on one side. We select target languages based on the availability of these resources. The monolingual treebanks in our experiments are from the Google Universal Dependency Treebanks (McDonald et al., 2013), for the reason that the treebanks of different languages in Google Universal Dependency Treebanks have consistent syntactic representations. The parallel data come from the Europarl corpus version 7 (Koehn, 2005) and Kaist Corpus4 . Taking the intersection of languages in the two kinds of resources yields the following seven languages: French, German, Italian, Korean, Portuguese, Spanish and Swedish. The treebanks from CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) appear to be another reasonable choice. However, previous studies (McDonald et al., 2011; McDonald et al., 2013) have demonstrated that a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components, and the het"
P14-1126,P10-1001,0,0.0424024,"tateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2"
P14-1126,D07-1015,0,0.0389923,"n equation (11) can be rewritten in the following form: X X X p˜(y i |xi ) KP = − log w(e, xi ) X yi pλ (y i |xi ) e∈yi X  fj (e, xi ) (13) e∈yi According to equation (9), p˜(y|x) can also be factored into the multiplication of the weight of each edge, so both KP and its gradient can be calculated by running the O(n3 ) inside-outside algorithm (Baker, 1979; Paskin, 2001) for projective parsing. For non-projective parsing, the analogy to the inside algorithm is the O(n3 ) matrixtree algorithm based on Kirchhoff’s Matrix-Tree Theorem, which is dominated asymptotically by a matrix determinant (Koo et al., 2007; Smith and Smith, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. ′ The second item (KU ) of the K function in equation (11) is the Shannon entropy of the posterior distribution over parsing trees, and can be written into the following form: X X X pλ (y i |xi ) log w(e, xi ) KU = − xi ∈U yi  log Z(xi ) − e∈y i (14) and the gradient of KU is in the following: X ∂pλ (y |xi ) log pλ (y |xi ) ∂KU i i = ∂λj ∂λj xi ∈U X =− pλ (y i |xi ) log pλ (y i |xi )Fj (y i , xi ) 1340 yi + X"
P14-1126,C12-2077,1,0.261799,"nce of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Co"
P14-1126,N07-2028,0,0.418455,") i Maximum likelihood training chooses parameters such that the log-likelihood L(λ) is maximized. However, in our scenario we have no labeled training data for target languages but we have some parallel and unlabeled data plus an English dependency parser. For the purpose of transferring cross-lingual information from the English parser via parallel text, we explore the model training method proposed by Smith and Eisner (2007), which presented a generalization of K function (Abney, 2004), and related it to another semi-supervised learning technique, entropy regularization (Jiao et al., 2006; Mann and McCallum, 2007). The objective K function to be minimized is actually the expected negative loglikelihood: XX K = − p˜(y i |xi ) log pλ (y i |xi ) = X i i yi D(˜ pi ||pλ,i ) + H(˜ pi ) (7) def def where p˜i (·) = p˜(·|xi ) and pλ,i (·) = pλ (·|xi ). p˜(y|x) is the “transferring distribution” that reflects our uncertainty about the true labels, and we are trying to learn a parametric model pλ (y|x) by minimizing the K function. In our scenario, we have a set of aligned parallel data P = {xsi , xti , ai } where ai is the word alignment for the pair of source-target sentences (xsi , xti ), and a set of unlabele"
P14-1126,P13-1028,0,0.100607,"Missing"
P14-1126,E06-1011,0,0.30644,"red-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohe"
P14-1126,P05-1012,0,0.138918,"ependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992;"
P14-1126,C04-1010,0,0.0383587,".0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carro"
P14-1126,J08-4003,0,0.0992895,"Missing"
P14-1126,H05-1066,0,0.672388,"ependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992;"
P14-1126,P05-1044,0,0.0275888,"; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated"
P14-1126,D11-1006,0,0.659962,"Missing"
P14-1126,D07-1070,0,0.330798,"her accuracies than unsupervised parsers. Cohen et al. (2011) proposed an approach for unsupervised dependency parsing with non-parallel multilingual guidance from one or more helper languages, in which parallel data is not used. In this work, we propose a learning framework for transferring dependency grammars from a resource-rich language to resource-poor languages via parallel text. We train probabilistic parsing models for resource-poor languages by maximizing a combination of likelihood on parallel data and confidence on unlabeled data. Our work is based on the learning framework used in Smith and Eisner (2007), which is originally designed for parser bootstrapping. We extend this learning framework so that it can be used to transfer cross-lingual knowledge between different languages. Throughout this paper, English is used as the source language and we evaluate our approach on ten target languages — Danish (da), Dutch (nl), French (fr), German (de), Greek (el), Italian (it), Korean (ko), Portuguese (pt), Spanish (es) and Swedish (sv). Our approach achieves significant improvement over previous state-of-the-art unsupervised and projected parsing systems across all the ten languages, and considerably"
P14-1126,W04-3207,0,0.0439406,"nguages, using existing resources from a resource-rich source language (like English).1 We assume that there are absolutely no labeled training data for the target language, but we have access to parallel data with a resource-rich language and a sufficient amount of labeled training data to build an accurate parser for the resource-rich language. This scenario appears similar to the setting in bilingual text parsing. However, most bilingual text parsing approaches require bilingual treebanks — treebanks that have manually annotated tree structures on both sides of source and target languages (Smith and Smith, 2004; Burkett and Klein, 2008), or have tree structures on the source side and translated sentences in the target languages (Huang et 1 For the sake of simplicity, we refer to the resource-poor language as the “target language”, and resource-rich language as the “source language”. In addition, in this study we use English as the source resource-rich language, but our methodology can be applied to any resource-rich languages. 1337 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337–1348, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association fo"
P14-1126,D10-1120,0,0.236406,"out unlabeled data (-U) presented in this work to those five baseline systems and the oracle (OR). The results of unsupervised DMV model (Klein and Manning, 2004) are from Table 1 of McDonald et al. (2011). Our approach outperforms all these baseline systems and achieves state-of-theart performance on all the eight languages. In order to compare with more previous methods, we also report parsing performance on sentences of length 10 or less after punctuation has been removed. Table 7 shows the results of our system and the results of baseline systems. “USR†” is the weakly supervised system of Naseem et al. (2010). “PGI” is the phylogenetic grammar induction model of Berg-Kirkpatrick and Klein (2010). Both the results of the two systems are cited from Table 4 of McDonald et al. (2011). We also include the results of the unsupervised dependency parsing model with non-parallel multilingual guidance (NMG) proposed by Cohen et al. (2011)8 , and “PR” which is the posterior regularization approach presented in Gillenwater et al. (2010). All the results are shown in Table 7. From Table 7, we can see that among the eight target languages, our approach achieves best parsing performance on six languages — Danish"
P14-1126,D09-1143,0,0.0173393,"thus making it applicable to a wide range of resource-poor languages. We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particul"
P14-1126,N10-1116,0,0.0268667,"oo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated scenario, in which we want to build statistical"
P14-1126,D13-1204,0,0.0185874,"sed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek and Straka, 2013; Spitkovsky et al., 2013), which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers. Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised systems (McDonald et al., 2011). Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated scenario, in which we want to build statistical parsers for resource-poor target languages, using existing resources from a r"
P14-1126,N03-1033,0,0.0435163,"Missing"
P14-1126,D11-1020,0,0.0114211,"We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This"
P14-1126,D13-1093,0,0.0113043,"e data sets when compared with previously studied unsupervised and projected parsing systems. 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). Several supervised dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang et al., 2013) have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability of dependency treebanks in a number of languages (McDonald et al., 2013). However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010; Mareˇcek an"
P14-1126,petrov-etal-2012-universal,0,\N,Missing
P14-1126,D07-1096,0,\N,Missing
P14-1126,P13-2017,0,\N,Missing
P16-4006,W15-3709,1,0.819356,", 2000) backend system that will support future interface extensions. Introduction 2 The current release of the ODIN (Online Database of INterlinear Text) database contains over 150,000 linguistic examples in the form of interlinear glossed text (IGT), an example of which is shown in Fig. 1. These IGT instances are extracted from PDFs found on the web, representing a significant source of data for computational typology, as well as providing information for resource-poor languages (RPLs). These instances are additionally useful for inducing annotation on RPLs, as demonstrated by Georgi et al. (2014, 2015), in which the relationships between words and glosses are identified and encoded for the purpose of enriching the data with annotations not present in the original examples. However, Related Work The system we describe is not the first web-based editor of IGT, but none of the existing systems (for IGT or annotation in general) that we’re aware of fit our use case. T YPE C RAFT1 (Beermann and Mihaylov, 2014) is a wiki-based collaborative IGT editor. As it directly targets IGT, the editor is designed to support tabular annotations and is a useful tool for users creating new IGT. However, it lim"
P16-4006,E12-2021,0,0.0440653,"Missing"
P97-1046,W89-0206,0,0.142805,"Missing"
P97-1046,P96-1023,1,0.806458,"ss of two related • machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the other is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort. I Fei Xia Alshawi Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984)in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of deriving"
P97-1046,J90-2002,0,0.302688,"tiveness of translation systems. This involves assessing the total cost .,f employing a '~ransiation system, including, for example, the cost of manual post-editing. Post-editing &quot;s not an option in speech translation systems for person-to-person communication, and real-time operation is important in this context, so in comparing the two translation models we looked at a variety of other measures, including translation accuracy, speed, and system complexity. Both models underlying the translation systems can be characterized as statistical translation models, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. In fact, the original motivation for the head transducer models was that they are simpler and more amenable to automatic model structure acquisition, while the transfer component of the traditional system was designed with regard to allowing maximum flexibility in mapping between source and target representations to overcome translation divergences (Lindop and Tsujii 1991: Dorr 1994). In practice, it turned out that adopting the simpler transducer models did not invoive sacrificing accuracy, at"
P97-1046,J93-2003,0,0.0120216,"Missing"
P97-1046,J94-4004,0,0.0151466,"lation systems can be characterized as statistical translation models, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. In fact, the original motivation for the head transducer models was that they are simpler and more amenable to automatic model structure acquisition, while the transfer component of the traditional system was designed with regard to allowing maximum flexibility in mapping between source and target representations to overcome translation divergences (Lindop and Tsujii 1991: Dorr 1994). In practice, it turned out that adopting the simpler transducer models did not invoive sacrificing accuracy, at least for our limited domain application. We first describe the transfer and head transducer approaches in Sections 2 and 3 and the method used to assign the numerical parameters of the models in Section 4. In Section 5. we compare experimental systems, based on the two approaches, for Englishto-Chinese translation of air travel enquiries, and we conclude in Section 6. 2 Monolingual Transfer Automata and In this section we review the approach based oll monolingual head automata tog"
P97-1046,J93-1003,0,0.0127674,"valued function taking two arguments, a event e and a context c. The context c is an equivalence class of states under which an action is taken, and the event e is an equivalence class of actions possible from that set of states. We write the value of the function as f(elc ), borrowing notation from the special case of conditional probabilities. The pair (elc) is referred to as a choice. The cost of a solution (i.e., a possible translation of an input string) is the sum of costs for all choices in the derivation of that solution. Discriminative cost functions, including likelihood ratios (cf. Dunning 1993), make use of both positive and negative instances of performing a task. Here we take a positive instance to be the derivation of a &quot;'correct&quot; translation, and a negative instance the derivation of an &quot;incorrect&quot; translation, where correctness is judged by a speaker of both languages. Let n + (e]c) be the count of taking choice (elc) in positive instances resulting from processing the source sentences in a training corpus. Similarly, let n - ( e l c ) be the count of taking (elc) for negative instances. The cost function&quot; used in the experiments is computed as: Word error rate (per cent) Time"
P97-1046,H93-1004,0,0.132116,"sent.) Space (Mbytes/sent.) /(elc) = log(n+(el c) + n-(elc)) -log(n+(ele)). (By comparison, the usual &quot;logprob&quot; cost function using only positive instances would be log(n+(c)) log(n+(elc)).) For unseen choices, we replace the context c and event e with larger equivalence classes. 5 5.1 Transfer Head Transducer 16.2 11.7 1.09 0.17 1.67 0.14 Table 1: Accuracy. time, and space comparison Effectiveness Comparison English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus (Hirschman et al. 1993). By training here we simply mean assignment of the cost functions for fixed model structures. These model structures were coded by hand as monolingual head acceptor and bilingual dependency lexicons for the transfer system and a head transducer lexicon for the transducer system. Positive and negative counts for cost assignment were collected from two sources for both systems and an additional third source for the transfer system. The first set of counts was derived by processing traces using around 1200 sample utterances from the ATIS corpus. This involved running the systems on the sample ut"
P97-1046,C86-1025,0,0.0118349,"ansducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort. I Fei Xia Alshawi Introduction In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata. Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon. The transfer system follows the familiar analysis-transfer-generation architecture (Isabelle and Macklovitch 1986). with mapping of dependency representations (Hudson 1984)in the transfer phase. In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of deriving the target string. Despite ~he simple direct architecture, the head transducer model does embody modern principles of lexicalized recursive grammars and statistical language processing. The context for evaluating both the transducer and transfer models was the development of experimental protot"
song-xia-2012-using,W99-0701,0,\N,Missing
song-xia-2012-using,D11-1033,0,\N,Missing
song-xia-2012-using,N03-1033,0,\N,Missing
song-xia-2012-using,C00-2117,0,\N,Missing
song-xia-2012-using,C02-1126,0,\N,Missing
song-xia-2012-using,P09-1058,0,\N,Missing
song-xia-2012-using,P10-2041,0,\N,Missing
song-xia-2012-using,I11-1035,0,\N,Missing
song-xia-2012-using,D11-1090,0,\N,Missing
song-xia-2012-using,D07-1036,0,\N,Missing
song-xia-2012-using,P06-1043,0,\N,Missing
song-xia-2012-using,P11-1157,0,\N,Missing
song-xia-2012-using,J04-1004,0,\N,Missing
song-xia-2012-using,P11-1139,0,\N,Missing
song-xia-2012-using,O03-4002,0,\N,Missing
song-xia-2012-using,W06-0115,0,\N,Missing
song-xia-2012-using,W06-0127,0,\N,Missing
song-xia-2012-using,I05-3025,0,\N,Missing
song-xia-2012-using,N09-1068,0,\N,Missing
song-xia-2012-using,P07-1033,0,\N,Missing
song-xia-2012-using,D10-1082,0,\N,Missing
song-xia-2014-modern,W99-0701,0,\N,Missing
song-xia-2014-modern,J93-2004,0,\N,Missing
song-xia-2014-modern,W03-1728,0,\N,Missing
song-xia-2014-modern,ogiso-etal-2012-unidic,0,\N,Missing
song-xia-2014-modern,W13-2302,0,\N,Missing
song-xia-2014-modern,rognvaldsson-etal-2012-icelandic,0,\N,Missing
song-xia-2014-modern,erjavec-2012-goo300k,0,\N,Missing
song-xia-2014-modern,song-xia-2012-using,1,\N,Missing
song-xia-2014-modern,I13-1071,1,\N,Missing
song-xia-2014-modern,scheible-etal-2012-gatetogermanc,0,\N,Missing
song-xia-2014-modern,P07-1033,0,\N,Missing
tepper-etal-2012-statistical,W06-3309,0,\N,Missing
tepper-etal-2012-statistical,I08-1050,0,\N,Missing
tepper-etal-2012-statistical,J02-4002,0,\N,Missing
tepper-etal-2012-statistical,W09-3603,0,\N,Missing
tepper-etal-2012-statistical,D10-1082,0,\N,Missing
W00-1208,J94-4004,0,0.0516326,"Missing"
W00-1208,W00-1307,1,0.931318,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,xia-etal-2000-developing,1,0.818684,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,palmer-etal-1998-rapid,1,\N,Missing
W00-1208,J93-2004,0,\N,Missing
W00-1307,2000.iwpt-1.9,0,0.179946,"Missing"
W00-1307,E99-1025,0,0.033721,"Missing"
W00-1307,P97-1003,0,0.241549,"ls. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings: the noun phrase"
W00-1307,C94-2149,0,0.0207537,"types of annotation errors. We have used LexTract for the final cleanup of the Penn Chinese Treebank. Due to space limitation, in this paper we will only discuss the first two tasks. 4.1 Evaluating the coverage of hand-crafted grammars The XTAG grammar (XTAG-Group, 1998) is a hand-crafted large-scale grammar for English, which has been developed at University of Pennsylvania in the last decade. It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by t"
W00-1307,C94-1024,1,0.738348,"Missing"
W00-1307,P98-1115,0,0.111927,"ects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitive elements of an LTAG are elementary trees (etrees). Each etree is associated with a lexical item (called the anchor of the tree) on its frontier. We choose LTAGs as our target grammars (i.e., the grammars to be extracted) be"
W00-1307,P95-1037,0,0.0945816,"o different levels. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings:"
W00-1307,P92-1007,0,0.0295146,"lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitiv"
W00-1307,C96-2103,1,0.838615,"Missing"
W00-1307,W00-2027,0,0.0272379,"It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by the overlap of Gh and a Treebank grammar Gt that is produced by LexTract from T. In this case, we will estimate the coverage of the XTAG grammar on the English Penn Treebank (PTB) using the Treebank grammar G2. There are obvious differences between these two grammars. For example, feature structures and multi-anchor etrees are present only in the XTAG grammar, whereas frequency information"
W00-1307,P92-1022,0,0.0701148,"Missing"
W00-1307,P97-1026,0,0.120564,"entative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG f"
W00-1307,W98-0315,1,0.566444,"e our approaches with related work. 1 Introduction There are various grammar frameworks proposed for natural languages. We take Lexicalized Tree-adjoining Grammars (LTAGs) as representative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalize"
W00-1307,W00-2030,1,0.863403,"Missing"
W00-1307,W00-1208,1,0.84805,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,xia-etal-2000-developing,1,0.69136,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,palmer-etal-1998-rapid,1,\N,Missing
W00-1307,J93-2004,0,\N,Missing
W00-1307,C98-1111,0,\N,Missing
W00-1605,E99-1025,0,0.298469,"Missing"
W00-1605,J93-2004,0,0.0251099,"Missing"
W00-1605,W00-2027,1,0.634896,"ation forest for each sentence which stores, in compact form, all derivations for each sentence. 2 1 0 2 4 6 8 10 12 Sentence length 14 16 18 20 Figure 3: Parse times plotted against sentence length. Coefficient of determination: . (x-axis: Sentence length; y-axis: log(time in seconds)) Since we can easily determine the number of trees selected by a sentence before we start parsing, we can use this number to predict the number of edges that will be proposed by a parser when parsing this sentence, allowing us to better handle difficult cases before parsing. 1 2 Some of these results appear in (Sarkar, 2000). In this section we present some additional data on the previous results and also the results of some new experiments that do not appear in the earlier work. Note that the precise number of edges proposed by the parser and other common indicators of complexity can be obtained only while or after parsing. We are interested in predicting parsing complexity. 39 10 the parser was reduced: 926 sentences (out of the 2250) did not get any parse. This was because some crucial tree was missing in the -best output. The results are graphed in Figure 6. The total number of derivations for all sentences w"
W00-1605,1997.iwpt-1.22,0,0.0335395,"ture for each word in the sentence. This eliminates lexical syntactic ambiguity but does not eliminate attachment ambiguity for the parser. The graph comparing the parsing times is shown in Figure 5. As the comparison shows, the elimination of lexical ambiguity leads to a drastic increase in parsing efficiency. The total time taken to parse all sentences went from 548K seconds to 31.2 seconds. Figure 5 shows us that a model which disambiguates syntactic lexical ambiguity can potentially be extremely useful in terms of parsing efficiency. Thus disambiguation of tree assignment or SuperTagging (Srinivas, 1997) of a sentence before parsing it might be a way of improving parsing efficiency. This gives us a way to reduce the parsing complexity for precisely the sentences which were problematic: the ones which selected too many trees. To test whether parsing times are reduced after SuperTagging we conducted an experiment in which the output of an -best SuperTagger was taken as input to the parser. In our experiment we set to be .3 The time taken to parse the same set of sentences was again dramatically reduced (the total time taken was 21K seconds). However, the disadvantage of this method was that the"
W00-1605,A00-2022,0,\N,Missing
W00-2030,H94-1020,0,0.0546196,"Missing"
W00-2041,C96-1034,0,0.0267563,"Missing"
W00-2041,2000.iwpt-1.9,0,0.02631,"Missing"
W00-2041,H94-1020,0,0.0339469,"Missing"
W00-2041,C96-2103,0,0.0560381,"Missing"
W00-2041,W98-0143,1,0.877555,"Missing"
W06-0104,W99-0608,0,\N,Missing
W06-0104,W99-0606,0,\N,Missing
W06-0104,A00-2005,0,\N,Missing
W06-0104,W96-0213,0,\N,Missing
W06-0104,N01-1006,0,\N,Missing
W06-0104,J95-4004,0,\N,Missing
W06-0104,W01-0701,0,\N,Missing
W06-0104,C02-1145,0,\N,Missing
W06-0104,I05-2033,0,\N,Missing
W06-0104,I05-3005,0,\N,Missing
W08-0202,W02-0104,0,0.0308145,"ter’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well as in the partnershi"
W08-0202,W02-0106,0,0.0316358,"ign of a professional master’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well a"
W08-0202,W02-0112,0,0.0296132,"We present the design of a professional master’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its form"
W08-0202,W05-0110,0,0.0706178,"Missing"
W08-0202,W05-0108,0,0.0306307,"mputational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well as in the partnerships that are an integral"
W08-0202,W08-0206,1,0.826739,"lt a question answering system, which was further developed into a submission to the TREC competition (Jinguji et al., 2006). This year’s class is developing a chatbot to submit to the Loebner Prize competition, an implementation of the Turing Test. Among the required courses, Ling 566 was created a year before the CLMA program started, and has been taught four times. Ling 450 is an established course from our Linguistics curriculum. Ling 570-573 were newly created for this program, and have each been taught three times now. We have put much effort in improving course design, as discussed in (Xia, 2008). 3.2 The prerequisites for the required courses In order to cover the range of methodologies and tasks that our program does in its core sequence, we need to set as a prerequisite the ability to program, including knowledge of data structures and algorithms, and expertise in C++ or Java.2 Another prerequisite is a college-level course in probability and statistics. Without such knowledge, it is all but impossible to discuss the sophisticated statistical models covered in the core NLP courses. For the two Linguistics required courses, the only prerequisite is a college-level introductory cours"
W08-0206,W08-0202,1,0.826386,"two decades, there has been tremendous progress in natural language processing (NLP) and NLP techniques have been applied to various realworld applications such as internet/intranet search and information extraction. Consequently, there has been an increasing demand from the industry for people with special training in NLP. To meet the demand, the University of Washington recently launched a new Professional Masters Program in Computational Linguistics (CLMA). To earn the master’s degree, students must take nine courses and complete a final project. The detail of the program can be found in (Bender et al., 2008). One of the required courses is LING572 (Advanced statistical methods in NLP), a course that I have been teaching every year for the past three years. During the process and especially in Year 3, I have made many changes to course content, assignments, and the usage of discussion board. In this paper, I will describe the evolution of the course and highlight the lessons learned from this experience. Background LING572 is part of the four-course NLP core sequence in the CLMA program. The other three are LING570 (Shallow Processing Techniques for NLP), LING571 (Deep Processing Techniques for NL"
W08-0206,J96-1002,0,0.0359266,"the original papers are often heavy in mathematical proofs and rarely refer to the NLP tasks that our students are familiar with. On the other hand, NLP papers that apply these algorithms to NLP tasks often assume that the readers are already familiar with the algorithms and consequently do not explain the algorithms in detail. Because it is hard to find a suitable paper to cover all the theoretic and application aspects of a learning algorithm, I chose several papers for each algorithm and specified the sections that the students should focus on. For instance, for Maximum Entropy, I picked (Berger et al., 1996; Ratnaparkhi, 1997) for the basic theory, (Ratnaparkhi, 1996) for an application (POS tagging in this case), and (Klein and Manning, 2003) for more advanced topics such as optimization and smoothing. For the more sophisticated learning methods (e.g., MaxEnt and SVM), it is very important for students to read the assigned papers beforehand. However, some students choose not to do so for various reasons; meanwhile, other students might spend too 47 much time trying to understand everything in the papers. To address this problem, in Year 3 I added five reading assignments, one for each of the fo"
W08-0206,J95-4004,0,0.0432814,"ding to Eq (1). If we calculate P (d|c) according to Eq (2), as given in the paper, we have to go through all the features in the feature set F . However, as shown in Eq (3) and (4), the first product in Eq (3), denoted as Z(c) in Eq (4), is a constant with respect to d and can be calculated beforehand and stored with each c. Therefore, to classify d, we only need to go through the features that are present in d. Implementing Eq (4) instead of Eq (2) reduces running time tremendously.5 P (d|c) = c∗ = arg maxc P (c)P (d|c) (1) Y (2) (1 − P (f |c)) f 6∈d = Y The trainer for TBL As described in (Brill, 1995), a TBL trainer picks one transformation in each iteration, applies it to the training data, and repeats the process until no more good transformations can be found. To choose the best transformation, a naive approach would enumerate all the possible transformations, for each transformation go through the data once to calculate the net gain, and choose the transformation with the highest net gain. This approach is very inefficient as the data have to be scanned through multiple times.3 3 Let Nf be the number of features and Nc be the number of classes in a classification task, the number of tr"
W08-0206,N07-5004,0,0.026692,"tudents should be aware of but do not have to understand all the details, and (3) related topics that are not covered in class but are available on additional slides for people who want to learn more by themselves. Taking MaxEnt as an example, Type (1) includes the maximum entropy principle, the modeling, GIS training, and decoding; Type (2) includes regularization and the mathematic proof that shows the relation between maximum likelihood and maximum entropy as provided in (Ratnaparkhi, 1997), and Type (3) includes LBFGS training and the similarity between SVM and MaxEnt with regularization (Klein, 2007). Making this distinction helps students focus on the most essential part of the algorithms and at the same time provides additional material for more advanced students. 4 Reading material One challenge of teaching a statistic NLP course is the lack of good textbooks on the subject; as a result, most of the reading material come from conference and journal papers. The problem is that many of the algorithms covered in class were originally proposed in non-NLP fields such as machine learning and applied mathematics, and the original papers are often heavy in mathematical proofs and rarely refer"
W08-0206,W96-0213,0,0.707375,"rarely refer to the NLP tasks that our students are familiar with. On the other hand, NLP papers that apply these algorithms to NLP tasks often assume that the readers are already familiar with the algorithms and consequently do not explain the algorithms in detail. Because it is hard to find a suitable paper to cover all the theoretic and application aspects of a learning algorithm, I chose several papers for each algorithm and specified the sections that the students should focus on. For instance, for Maximum Entropy, I picked (Berger et al., 1996; Ratnaparkhi, 1997) for the basic theory, (Ratnaparkhi, 1996) for an application (POS tagging in this case), and (Klein and Manning, 2003) for more advanced topics such as optimization and smoothing. For the more sophisticated learning methods (e.g., MaxEnt and SVM), it is very important for students to read the assigned papers beforehand. However, some students choose not to do so for various reasons; meanwhile, other students might spend too 47 much time trying to understand everything in the papers. To address this problem, in Year 3 I added five reading assignments, one for each of the following topics: information theory, Naive Bayes, MaxEnt, SVM,"
W08-0206,N03-5008,0,\N,Missing
W08-0206,N01-1006,0,\N,Missing
W09-0307,N07-1057,1,0.901771,"Missing"
W09-0307,I08-1069,1,0.866672,"Missing"
W09-0307,P05-1046,0,0.01824,"use the IGT detector marks only the span of an instance. For instance, the coindex i in Jani and lii/j on the third line of Ex (1) could easily be mistaken as being part of the word. (2) Language: Citation: L: Coindx: G: T1: T2: Haitian CF (Lefebvre 1998:165) Jan pale ak li (Jan, i), (li, i/j) John speak with he ’John speaks with him’ ’John speaks with himself’ There has been much work on extracting database records from text or semi-structured sources, and the common approach is breaking the text into multiple segments and labeling each segment with a field name (e.g., (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 8 Haitian CF (Lefebvre 1998:165) ak Jani pale lii/j John speak with he (a) ’John speaks with him’ (b) ’John speaks with himself’ 4.2 Enriching IGT Since the language line in IGT data typically does not come with annotations (e.g., POS tags, phrase 9 For instance, in some IGTs, a syntactic structure is added on top of the language line; for instance, the language line in Ex (1) could become something like [IP Jani [VP pale [PP ak lii/j]]] CF here stands for French-lexified creole. 55 The base of several hundred manually mapped grams has provided a reasonably reliable “seman"
W09-0307,E09-1099,1,0.707483,"Missing"
W09-3036,H05-1066,0,0.0147844,"ation of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conversion approach. 2 Two Kinds of Syntactic Structure Two different approaches to describing syntactic structure, dependency structure (DS) (Mel’čuk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides. Formally, in a PS tree,"
W09-3036,P99-1065,0,0.0911636,"ed; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dep"
W09-3036,P03-1054,0,0.00169422,"how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conve"
W09-3036,C02-2025,0,0.0152376,"d theories. The next section highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure version. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al. 2005). A multi-layered approach is also found in the Prague Dependency Treebank (Hajič et al. 2001), or in treebanks based on LFG (King et al. 2003) or HPSG (Oepen et al. 2002). A lesson learned here is that the addition of deeper, more semantic levels may be complicated if the syntactic annotation was not designed with the possibility of multiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of propbanking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not nat"
W09-3036,J05-1004,1,0.175107,"s are carefully coordinated. 1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natural language processing components. Today, treebanks have been constructed for many languages, including Arabic, Chinese, Czech, English, French, German, Korean, Spanish, and Turkish. This paper describes the creation of a Hindi/Urdu multi-representational and multilayered treebank. Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English PropBank (Palmer et al. 2005). Multirepresentational means that we distinguish conceptually what is being represented from how it is represented; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is"
W09-3036,P07-1021,0,\N,Missing
W10-0728,D09-1030,0,0.0244041,"Missing"
W10-0728,N06-4006,0,0.0483726,"Missing"
W10-0728,D08-1027,0,0.0490043,"Missing"
W10-1109,P05-1046,0,0.0773907,"Missing"
W11-0711,W06-3406,0,0.016521,"er, the list is definitely not complete as many informal words in the Enron corpus do not appear in the dictionaries used by Wordnik. 1.00 0.95 0.90 Top 5000 0.85 Top 1000 0.80 At least 5 0.75 At least 10 0.70 Baseline 0.65 0.60 1-gram 2-gram 3-gram 4-gram 5-gram Figure 1: Accuracy of the request classifier with different feature sets 4.3.1 Features for request There has been considerable research into categorizing email messages by function. Cohen, Carvalho, and Mitchell (2004) described the classification of email into ‘email speech acts’, building on the speech act theory of Searle (1975). Carvalho and Cohen (2006) achieved high-precision results categorizing messages into categories such as ’request’ and ’proposal’ when preprocessing the text in certain ways and using unigram, bigram, and trigram features only. Unlike formality, which is more about the style of the messages (e.g., whether the email is all in lowercase), the content words are more relevant for identifying requests. Following the work in (Carvalho and Cohen, 2006), we used word ngrams as features. To prevent the features from being too specific to the small training data, we experimented with two ways of feature selection: by feature cou"
W11-0711,W04-3240,0,0.0641246,"Missing"
W11-0711,P06-2053,0,0.277556,"Missing"
W11-0711,H05-1056,0,0.0702734,"Missing"
W12-4619,I11-1138,1,0.830207,"k1 windows/N(e3) k2 CASEi /NULL vmod suddenly/Adv(e2) Figure 1: The three annotation levels of the HTB: Dependency Structure, PropBank, and Phase Structure (top row); the consistent DS (DSconst ) and the derivation tree (DSderiv ) obtained from these structures (bottom row); ei in the DSderiv tree refers to elementary trees in Figures 5-6. dicating movement from the object position to the subject position using a coindexed trace. (1) 3 khir.kiy˜a: acaanak t.uut.˜ı: windows.F suddenly broke.FPl ‘The windows broke suddenly.’ Consistency between DS and PS In our previous study (Xia et al., 2009; Bhatt et al., 2011), we proposed a DS-to-PS conversion algorithm, which extracts conversion rules from a small number of (DS, PS) pairs, and then applies the rules to a new DS to generate a PS for the DS. The algorithm introduces two concepts: consistency and compatibility. A DS and a PS tree are consistent if and only if there exists an assignment of head words for the internal nodes in the PS such that after merging all the (head child, parent) nodes in the PS, the new PS is identical to the DS. In Figure 1, the DS in (a) and the PS in (c) are not consistent because the empty category CASE appears only in (c)."
W12-4619,W98-0106,0,0.072946,"s in Figure 5 In the case of our unaccusative example, the extracted TAG will include the rule in Figure 6, which replace the first and last rules in Figure 5. The new DSderiv is in Figure 1e., where e2 and e3 are two etrees in Figure 5, and e5 is the etree in Figure 6.2 6 Issues in Word Order It has been known for a long time that in a lexicalized TAG, the derivation tree is a lexical dependency tree (since the nodes are bijectively identified with the words of the sentence), and that this dependency structure is not necessarily the linguistically plausible structure (Rambow and Joshi, 1997; Candito and Kahane, 1998; Rambow et al., 2001). Consider the following example (2): 2 It is not a coincident that in this example DSderiv and DS are isomorphic, as the DS-to-DSconst module involves adding ECs and DSconst -to-DSderiv involves removing ECs. But in theory, DSderiv and DS are not necessarily always isomorphic as the DS-to-DSconst module is not limited to adding ECs. ˜ kitaab˜e mE-ne khariid-nii caah-ii books.F I-Erg buy-Inf.F want-Pfv.F ‘Books, I had wanted to buy.’ ˜ mE-ne yeh kitaab sab-se khariid-ne-ko I-Erg this book all-Instr buy-Inf-Acc kah-aa say-Pfv ‘I told everyone to buy this book.’ In (3), thi"
W12-4619,E99-1029,0,0.0632938,"linguistic interpretation (since buy does not have a clausal argument). Both problems have a common solution: nonlocal multicomponent TAGs with dominance links (nlMC-TAG-DL), for example in the definition given in (Rambow et al., 2001) as D-Tree Substitution Grammars (DSG). In a DSG, there is only the substitution operation, but all links in trees are interpreted as non-immediate dominance links; in fact, the trees can be seen as tree descriptions rather than as trees; this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not pr"
W12-4619,W08-2302,0,0.0136127,"this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not present empirical arguments from Hindi in order to address the issue of what formal complexity is required for the syntactic description of Hindi. We leave those issues to future work. Instead, we assume a simple framework in which we can explore the issue of grammar extraction. The algorithm for extraction of a TAG can be extended straightforwardly to an algorithm for extracting a DSG: whenever in DSconst we have a node labeled with an empty category which is coindexed with a"
W12-4619,J05-2003,0,0.0223303,"here is only the substitution operation, but all links in trees are interpreted as non-immediate dominance links; in fact, the trees can be seen as tree descriptions rather than as trees; this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not present empirical arguments from Hindi in order to address the issue of what formal complexity is required for the syntactic description of Hindi. We leave those issues to future work. Instead, we assume a simple framework in which we can explore the issue of grammar extraction. The"
W12-4619,J01-1004,1,0.658704,"of our unaccusative example, the extracted TAG will include the rule in Figure 6, which replace the first and last rules in Figure 5. The new DSderiv is in Figure 1e., where e2 and e3 are two etrees in Figure 5, and e5 is the etree in Figure 6.2 6 Issues in Word Order It has been known for a long time that in a lexicalized TAG, the derivation tree is a lexical dependency tree (since the nodes are bijectively identified with the words of the sentence), and that this dependency structure is not necessarily the linguistically plausible structure (Rambow and Joshi, 1997; Candito and Kahane, 1998; Rambow et al., 2001). Consider the following example (2): 2 It is not a coincident that in this example DSderiv and DS are isomorphic, as the DS-to-DSconst module involves adding ECs and DSconst -to-DSderiv involves removing ECs. But in theory, DSderiv and DS are not necessarily always isomorphic as the DS-to-DSconst module is not limited to adding ECs. ˜ kitaab˜e mE-ne khariid-nii caah-ii books.F I-Erg buy-Inf.F want-Pfv.F ‘Books, I had wanted to buy.’ ˜ mE-ne yeh kitaab sab-se khariid-ne-ko I-Erg this book all-Instr buy-Inf-Acc kah-aa say-Pfv ‘I told everyone to buy this book.’ In (3), this book is an argument"
W13-2710,P10-1010,0,0.0599253,"Missing"
W13-2710,P07-1009,0,0.145948,"Missing"
W13-2710,W02-1502,1,0.855766,"had bought fruit.’ [sna] (Toews, 2009:34) The annotations in IGT result from deep linguistic analysis and represent much effort on the part of field linguists. These rich annotations include the segmentation of the source line into morphemes, the glossing of those individual morphemes, and the translation into a language of broader communication. The IGT format was developed to compactly display this information to other linguists. Here, we propose to repurpose such data in the automatic development of further resources. The second resource we will be working with is the LinGO Grammar Matrix (Bender et al., 2002; 2010), an open source repository of implemented linguistic analyses. The Grammar Matrix pairs a core grammar, shared across all grammars it creates, with a series of libraries of analyses of cross-linguistically variable phenomena. Users access the system through a web-based questionnaire which elicits linguistic descriptions of languages and then outputs working HPSG (Pollard and Sag, 1994) grammar fragments compatible with DELPH-IN (www.delph-in.net) tools based on those descriptions. For present purposes, this system can be viewed as a function which maps simple descriptions of languages"
W13-2710,C10-1044,1,0.934067,"’s approximately 7,000 languages will become extinct by the year 2100. This is a crisis not only for the field of linguistics—on track to lose the majority of its primary data—but also a crisis for the social sciences more broadly as languages are a key piece of cultural heritage. The field of linguistics has responded with increased efforts to document endangered languages. Language documentation not only captures key linguistic data (both primary data and analytical facts) but also supports language revitalization efforts. It must include both primary data collection (as in Abney and Bird’s (2010) universal corpus) and analytical work elucidating the linguistic structures of each language. As such, the outputs of documentary linguistics are dictionaries, descriptive (prose) grammars as well as transcribed and translated texts (Woodbury, 2003). Traditionally, these outputs were printed artifacts, but the field of documentary linguistics has increasingly realized the benefits of producing digital artifacts as well (Nordhoff and Poggeman, 2012). Bender et al. (2012a) argue that the documentary value of electronic descriptive grammars can be significantly enhanced by pairing them with impl"
W13-2710,N07-1057,1,0.843043,"al is to produce working grammar fragments from IGT produced in documentary linguistics projects. However, in order to evaluate the performance of approaches to answering the high-level questions in the Grammar Matrix questionnaire, we need both IGT and goldstandard answers for a reasonably-sized sample of languages. We have constructed development and test data for this purpose on the basis of work done Other Related Work Our work is also situated with respect to attempts to automatically characterize typological proper1 The details of the algorithm and experimental results were reported in (Xia and Lewis, 2007). 2 76 http://sswl.railsplayground.net/, accessed 4/25/13 Sets of languages Range of testsuite sizes Median testsuite size Language families DEV 1 (n=10) 16–359 91 Indo-European (4), NigerCongo (2), Afro-Asiatic, Japanese, Nadahup, Sino-Tibetan DEV 2 (n=10) 11–229 87 Indo-European (3), Dravidian (2), Algic, Creole, Niger-Congo, Quechuan, Salishan TEST (n=11) 48–216 76 Indo-European (2), Afro-Asiatic, Austro-Asiatic, Austronesian, Arauan, Carib, Karvelian, N. Caucasian, Tai-Kadai, Isolate Table 1: Language families and testsuites sizes (in number of grammatical examples) by students in a class"
W13-2710,I08-1069,1,0.880884,"es. Both projects use the typological database WALS (Haspelmath et al., 2008), which has information about 192 different typological properties and about 2,678 different languages (though the matrix is very sparse). This approach is complementary to ours, and it remains an interesting question whether our results could be improved by bringing in information about other typological properties of the language (either extracted from the IGT or looked up in a typological database). Another strand of related work concerns the collection and curation of IGT, including the ODIN project (Lewis, 2006; Xia and Lewis, 2008), which harvests IGT from linguistics publications available over the web and TypeCraft (Beermann and Mihaylov, 2009), which facilitates the collaborative development of IGT annotations. TerraLing/SSWL2 (Syntactic Structures of the World’s Languages) has begun a database which combines both typological properties and IGT illustrating those properties, contributed by linguists. Finally, Beerman and Hellan (2011) represents another approach to inducing grammars from IGT, by bringing the hand-built linguistic knowledge sources closer together: On the one hand, their cross-linguistic grammar resou"
W13-2710,W09-0307,1,0.83312,"tion algorithmically, instead. Figure 1: Welsh IGT with alignment and projected syntactic structure bootstrapping NLP tools with initial seeds created by projecting syntactic information from resourcerich languages to RPLs through IGT. Projecting syntactic structures has two steps. First, the words in the language line and the translation line are aligned via the gloss line. Second, the translation line is parsed by a parser for the resource-rich language and the parse tree is then projected to the language line using word alignment and some heuristics as illustrated in Figure 1 (adapted from Xia and Lewis (2009)).1 Previous work has applied these projected trees to enhance the performance of statistical parsers (Georgi et al., 2012). Though the projected trees are noisy, they contain enough information for those tasks. The second goal of RiPLes is to use the automatically created resources to perform crosslingual study on a large number of languages to discover linguistic knowledge. For instance, Lewis and Xia (2008) showed that IGT data enriched with the projected syntactic structure could be used to determine the word order property of a language with a high accuracy (see §4). Naseem et al. (2012)"
W13-2710,C96-2120,0,0.0363363,"ted collections of grammatical and ungrammatical examples) and Grammar Matrix choices files. Later on in the class, the students extend the grammar fragments output by the customization system to handle a broader fragment of the language. Accordingly, the testsuites cover phenomena which go beyond the customization system. Testsuites for grammars, especially in their early stages of development, require examples that are simple (isolating the phenomena illustrated by the examples to the extent possible), built out of a small vocabulary, and include both grammatical and ungrammatical examples (Lehmann et al., 1996). The examples included in descriptive resources often don’t fit these requirements exactly. As a result, the data we are working with include examples invented by the students on the basis of the descriptive statements in their resources.3 In total, we have testsuites and associated choices files for 31 languages, spanning 17 language families (plus one creole and one language isolate). The most well-represented family is Indo-European, with nine languages. We used 20 languages, in two dev sets, for algorithm development (including manual error analysis), and saved 11 languages as a held-out"
W13-2710,I08-2093,1,0.879906,"lexical rules. We envision answering the questions regarding morphosyntactic features through an analysis of the grams that appear on the gloss line, with reference to the GOLD ontology (Farrar and Langendoen, 2003). The implementation of such systems in such a way that they are robust to potentially noisy data will undoubtedly be non-trivial. The contribution of this paper is the development of systems to handle one example each of the questions of types (i) and (ii), namely detecting major constituent word order and the underlying case system. For the first, we build directly on the work of Lewis and Xia (2008) (see §2.2). Our experiment can be viewed as an attempt to reproduce their results in the context of the specific view of word order possibilities developed in the Grammar Matrix. The second question (that of case systems) is in some ways more subtle, requiring not only analysis of IGT instances in isolation and aggregation of the results, but also identification of particular kinds of IGT instances and comparison across them. sion grammar fragments. These fragments are relatively modest, yet they relate linguistic strings to semantic representations (and vice versa) and are ready to be built"
W13-2710,P12-1066,0,0.0340909,"Xia and Lewis (2009)).1 Previous work has applied these projected trees to enhance the performance of statistical parsers (Georgi et al., 2012). Though the projected trees are noisy, they contain enough information for those tasks. The second goal of RiPLes is to use the automatically created resources to perform crosslingual study on a large number of languages to discover linguistic knowledge. For instance, Lewis and Xia (2008) showed that IGT data enriched with the projected syntactic structure could be used to determine the word order property of a language with a high accuracy (see §4). Naseem et al. (2012) use this type of information (in their case, drawn from the WALS database (Haspelmath et al., 2008)) to improve multilingual dependency parsing. Here, we build on this aspect of RiPLes and begin to extend it towards the wider range of linguistic phenomena and more detailed classification within phenomena required by the Grammar Matrix questionnaire. 2.3 3 Development and Test Data Our long-term goal is to produce working grammar fragments from IGT produced in documentary linguistics projects. However, in order to evaluate the performance of approaches to answering the high-level questions in"
W13-2710,C12-2037,1,\N,Missing
W14-2206,adolphs-etal-2008-fine,0,0.0121109,"lexicon (dictionary) from the CLRP and thus is effectively created on the basis of a much larger dataset. The GRAM choices files only contain verbs for which a case frame could be identified. If the projected tree was not interpretable by our extraction heuristics or if the example had no overt arguments, then the verb will not be extracted. The MOM choices files, on the 10 9 It is in this relative lack of constraint that BASELINE mostly clearly forms a baseline to improve upon. 11 The vast majority of the incorrect parses for the MOM There are methods for handling unknown lexical items (e.g. Adolphs et al., 2008) in more mature grammars of this type, but these are not applicable at this stage. 49 Choices file ORACLE BASELINE FF - AUTO - NONE FF - DEFAULT- GRAM FF - AUTO - GRAM MOM - DEFAULT- NONE MOM - AUTO - NONE # verb entries 900 3005 3005 739 739 1177 1177 # noun entries 4751 1719 1719 1724 1724 1719 1719 # det entries 0 240 240 240 240 240 240 # verb affixes 160 0 0 0 0 262 262 # noun affixes 24 0 0 0 0 0 0 Table 2: Amount of lexical information in each choices file choices file ORACLE BASELINE FF - AUTO - NONE FF - DEFAULT- GRAM FF - AUTO - GRAM MOM - DEFAULT- NONE MOM - AUTO - NONE Training Dat"
W14-2206,W02-1502,1,0.863376,"Missing"
W14-2206,C12-1016,1,0.828255,"learning precision grammar fragments from existing products of documentary linguistic work. A precision grammar is a grammar which encodes a sharp notion of grammaticality and furthermore relates strings to elaborate semantic representations. Such objects are of interest in the context of documentary linguistics because: (1) they are valuable tools in the exploration of linguistic hypotheses (especially regarding the interaction of various phenomena); (2) they facilitate the search for examples in corpora which are not yet understood; and (3) they can support the development of treebanks (see Bender et al., 2012a). However, they are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and"
W14-2206,W13-2710,1,0.864585,"y are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013) to extract general word"
W14-2206,P04-1041,0,0.04896,"Missing"
W14-2206,2000.iwpt-1.9,0,0.0170731,"ing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of"
W14-2206,W01-0713,0,0.018625,"line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some bas"
W14-2206,W09-2605,0,0.0202486,"h respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of affixes) as well as morphosyntactic information, though here our focus is on the former. Sample excerpts from a choices file are given in Fi"
W14-2206,P13-2055,1,0.899555,"Missing"
W14-2206,P06-1072,0,0.016691,"(Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars"
W14-2206,P06-1111,0,0.0152231,"line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different"
W14-2206,P09-1009,0,0.0178044,"The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to cont"
W14-2206,hockenmaier-steedman-2002-acquiring,0,0.0834147,"Missing"
W14-2206,J07-3004,0,0.0495946,"Missing"
W14-2206,P02-1017,0,0.0331782,"ed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM alg"
W14-2206,P04-1061,0,0.0131047,"projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced g"
W14-2206,N07-1057,1,0.922328,"le for such languages. A third line of research attempts to bootstrap NLP tools for resource-poor languages by taking advantage of IGT data and resources for resourcerich languages. The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006"
W14-2206,P98-1115,0,0.125171,"cs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions c"
W14-2206,I08-2093,1,0.918112,"al., 2012a). However, they are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013)"
W14-2206,C98-1111,0,\N,Missing
W15-3709,bender-2014-language,0,0.0233262,"Missing"
W15-3709,W04-3229,0,0.0731721,"Missing"
W15-3709,W14-2206,1,0.900658,"Missing"
W15-3709,W13-2710,1,0.860502,"gi et al. (2014) used IGT instances to produce sets of dependency trees which were then corrected and used to learn automatic correction rules. 2.2 2.3 Use of INTENT for Linguists In the Spring of 2015 at the University of Washington, our colleague Prof. Emily Bender used the INTENT system as part of a course, Computational Methods in Language Documentation2 . The INTENT system was used to enrich IGT instances from the Language CoLLAGE project (Bender, 2014). The students then worked on methods by which typological phenomenon might be determined from the automatically enriched data, following Bender et al. (2013, 2014). This type of inquiry shows the large-scale enrichment of a wide variety of languages that INTENT is intended for, and how this can be used to answer interesting linguistic questions. Other such uses might be enriching collected IGT instances automatically, to create an annotated corpus from IGT data while being able to greatly reduce the amount of human annotators needed for the task. For these goals to be effective, INTENT must be able to generate sufficiently reliable POS tags on resource-poor languages. Whether or not that is the case is the question we seek to answer in this paper"
W15-3709,I08-2093,1,0.949199,"esource-poor languages. 3.1 Using IGT to Bootstrap Alignment While ODIN contains many IGT instances, it has nowhere approaching the 2 million sentences used in previous projection approaches, such as Yarowsky and Ngai (2001). Thankfully, IGT contains more information than simply the source and target language data—IGT also has gloss lines. The gloss line is a transliteration of the language line, containing many of the same words that are used in the translation, although in a different order. We can use the gloss line as a “pivot” to bootstrap our alignment, as shown in Fig. 2, and following Lewis and Xia (2008, 2010). There are five steps to our process of projecting POS tags using the gloss line of IGT: 1. 2. 3. 4. POS tag the translation line Align the translation line with the gloss line Disambiguate multiply-aligned gloss tokens Attempt to resolve unaligned tokens in the gloss line 5. Project POS tags from gloss line to language line 1 – First, an English-language POS tagger is used to provide the POS tag sequence for the translation line. For our tests, we used the Stanford Tagger (Toutanova et al., 2003) trained on all sections of English Penn Treebank (Marcus et al., 3 This order of preceden"
W15-3709,J93-2004,0,0.0572159,"Missing"
W15-3709,A00-1031,0,0.198987,"Missing"
W15-3709,J92-4003,0,0.291374,"Missing"
W15-3709,D10-1056,0,0.0392613,"Missing"
W15-3709,N03-1033,0,0.0610812,"use the gloss line as a “pivot” to bootstrap our alignment, as shown in Fig. 2, and following Lewis and Xia (2008, 2010). There are five steps to our process of projecting POS tags using the gloss line of IGT: 1. 2. 3. 4. POS tag the translation line Align the translation line with the gloss line Disambiguate multiply-aligned gloss tokens Attempt to resolve unaligned tokens in the gloss line 5. Project POS tags from gloss line to language line 1 – First, an English-language POS tagger is used to provide the POS tag sequence for the translation line. For our tests, we used the Stanford Tagger (Toutanova et al., 2003) trained on all sections of English Penn Treebank (Marcus et al., 3 This order of precedence is: VERB > NOUN > ADV > ADJ > PRON > DET > ADP > CONJ > PRT > NUM > PUNC > X. 4 Further discussion of the noise in these files is found in Xia et al. (2014). 61 however, there is not always a one-to-one alignment between the language line and gloss, and in these cases, we skip processing the IGT instance. In addition to alignment failures, noise is found in the form of a bias toward English, since all projections originate in English, as well as a bias toward unusual phenomena that the author of the pa"
W15-3709,P11-1061,0,0.0182912,"age Technology for Cultural Heritage, Social Sciences, and Humanities, pages 58–67, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing LANG nnisaau daxalna makaatibahunna GLOSS the-women(3.PL.F.)-NOM entered-3.PL.F office(PL.)-ACC-their(F.) TRANS “The women have entered their offices.” Figure 1: An example of Interlinear Glossed Text (IGT) in Arabic from (Nasu, 2001), with an English translation. niques to “project” information from the resourcerich language to the resource-poor one. Yarowsky and Ngai (2001); Das and Petrov (2011) both investigated training POS taggers by projecting labels from one language to another, while Hwa et al. (2004) looked at projecting dependency parsers. In this paper, we focus on using a resource known as Interlinear Glossed Text (IGT) as a possible source of linguistic knowledge for the POS tagging task on resource-poor languages, and apply it to the enrichment of a linguistic resource composed of IGT data. An example of IGT is shown in Fig. 1. IGT is a format used by linguists for giving examples of linguistic phenomena, and since linguists study a large number of languages, IGT instance"
W15-3709,xia-etal-2014-enriching,1,0.885793,"Missing"
W15-3709,N07-1057,1,0.819932,"Missing"
W15-3709,N01-1026,0,0.0976272,"h SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 58–67, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing LANG nnisaau daxalna makaatibahunna GLOSS the-women(3.PL.F.)-NOM entered-3.PL.F office(PL.)-ACC-their(F.) TRANS “The women have entered their offices.” Figure 1: An example of Interlinear Glossed Text (IGT) in Arabic from (Nasu, 2001), with an English translation. niques to “project” information from the resourcerich language to the resource-poor one. Yarowsky and Ngai (2001); Das and Petrov (2011) both investigated training POS taggers by projecting labels from one language to another, while Hwa et al. (2004) looked at projecting dependency parsers. In this paper, we focus on using a resource known as Interlinear Glossed Text (IGT) as a possible source of linguistic knowledge for the POS tagging task on resource-poor languages, and apply it to the enrichment of a linguistic resource composed of IGT data. An example of IGT is shown in Fig. 1. IGT is a format used by linguists for giving examples of linguistic phenomena, and since linguists study a large number of"
W15-3709,N10-1083,0,\N,Missing
W15-3709,petrov-etal-2012-universal,0,\N,Missing
W17-0106,W10-2211,0,0.0319097,"systems will pick this information up from the training data. 4 45 able in need of normalization than there is material that establishes correct practices. On the other hand, there are fewer individual authors, meaning that author identification can potentially lead to greater gains, and supplementary material like audio is likely to be available for at least some of the texts (because much endangered language text is transcribed from audio recordings). The first-pass IGT production shared task resembles earlier shared tasks on morphological analysis, most notably the Morpho Challenge series (Kurimo et al., 2010). It differs, however, in working with words in context (rather than word lists), and in going beyond segmentation of words into morphemes to associating morphemes with particular glosses. The presence of the translation line also provides a new source of information in producing the glosses, not available in previous shared tasks. Finally, the task of producing word-glosses is a novel one, with connections to low-resource machine translation. texts (both old and new) to a consistent format can solve many practical problems communities face. 5 Of the outputs provided by the first-pass IGT prod"
W17-0106,W14-3605,0,0.0207445,"rce scenarios. These contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered l"
W17-0106,W15-4319,0,0.0611856,"Missing"
W17-0106,W06-1421,0,0.386565,"l@alumni.ubc.ca, Shobhana.Chelliah@unt.edu, maxwell@umiacs.umd.edu Abstract ties if we are to actually reap the potential benefits of current research in the former for the latter. We propose that a particularly efficient and effective way to achieve this alignment of interest is through a set of “Shared Task Evaluation Challenges” (STECs) for the speech and language processing communities based on data already collected and annotated in language documentation efforts. STECs have been a primary driver of progress in natural language processing (NLP) and speech technology over several decades (Belz and Kilgarriff, 2006). A STEC involves standardized data for training (or otherwise developing) NLP/speech systems and then a held-out, also standardized, set of test data as well as implemented evaluation metrics for evaluating the systems submitted by the participating groups. This system is productive because the groups developing the algorithms benefit from independently curated data sets to test their systems on as well as independent evaluation of the systems, while the organizers of the shared task are able to focus effort on questions of interest to them without directly funding system development. Organiz"
W17-0106,brugman-russel-2004-annotating,0,0.0478615,"l employ two evaluation metrics: F1 scores averaged across all time segments in the corpus and all speakers in the corpus, respectively. As above, in the case of overlapped speech, both (all) speakers should be labeled. 3. Genre classification For a span of audio, identify which of a fixed inventory of genres, e.g., elicitation, monolog, LRL dialog, reading, or chanting, is present. This inventory will be provided by the organizers along with the training data. Training and test samples will be provided 42 as well as alignment can readily be encoded in formats readable by tools such as ELAN2 (Brugman and Russel, 2004), which has seen increasing adoption for endangered language research and which provides an easy visual interface to timealigned speech and language annotations. The techniques developed through the “Grandma’s hatbox” ensemble of shared tasks have potential benefit for field linguists, researchers in endangered languages, and language archivists. For those collecting data, these techniques can accelerate the process of transcription and alignment of speech data. They can also facilitate consistent metadata extraction from recordings, including language and speaker information, recording dates"
W17-0106,W15-3204,0,0.0147756,"contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered languages often only have"
W17-0106,W11-2838,0,0.0881554,"Missing"
W17-0106,E09-1099,1,0.689325,"hared tasks: Realism Whereas shared tasks in speech and NLP are often somewhat artificial, it is critical to our goals that our shared tasks closely model the actual computational needs of working linguists. It directly follows from this design principle that the software contributed by shared task participants should work off-the-shelf for stakeholders in documentary materials who are interested in using it later (e.g., linguists, speaker Accessibility of the shared task The shared tasks must have relatively low barriers to entry, in 1 There are some notable exceptions, including Xia et al’s (2009) work on language identification in IGT harvested from linguistics papers on the web (Xia et al., 2009), the Zero Resource Speech Challenge (Versteegh et al., 2015), and the recent BABEL (Harper, 2014) program. 40 now turn to the explanation of our three proposed shared tasks. order to encourage broad participation. This can be ensured by having the shared task organizers provide baseline systems which provide working, if not necessarily high-performing, solutions to the task. The baseline systems not only establish the basic feasibility of the tasks, but also provide a starting point for team"
W17-0110,W13-2710,1,0.115361,"g up some noise in the glossing, using the Map Gloss methodology of Lockwood (2016). In the following sections, we provide a description of the methodology and data sets we build on (§2), before laying out the methodology as developed for this paper (§3) and presenting the numerical results (§4). The primary contribution of our paper is in (§5), where we do an error analysis and relate our results to sources of bias. In this paper, we apply two methodologies of data enrichment to predict the case systems of languages from a diverse and complex data set. The methodologies are based on those of Bender et al. (2013), but we extend them to work with a new data format and apply them to a new dataset. In doing so, we explore the effects of noise and inconsistency on the proposed algorithms. Our analysis reveals assumptions in the previous work that do not hold up in less controlled data sets. 1 2 Introduction Background This section briefly overviews the previous work that we build on in this paper: methodology developed by the RiPLes and AGGREGATION projects to extract typological information from IGT (§2.1), the construction and enrichment of the ODIN data set (§2.2), and Map Gloss system for regularizing"
W17-0110,I08-2093,1,0.756023,"data sets—and we believe that this is true for other research purposes as well. In addition to bias in the words themselves used for annotation, additional bias may be introduced if a linguist is collecting data for a specific phenomenon. For example, if a language includes an unrepresentatively large set of intransitive or unergative verbs, the system might not have data with which to identify a nominativeaccusative system. This type of data set bias can be overcome by collecting a diverse set of data from a variety of sources that is large enough to overcome the biases of a particular set (Lewis and Xia, 2008). 5.5 Gold Standard A final contributor to the accuracy measurements was the state of the gold standard itself. We do not consider this a source of error, but rather an inevitability of working with low-resource languages and the very reason a system such as this is useful. Some of the languages classified as having a neutral case system (corresponding to ‘nocase’ in the choices files we use as our gold standard) might be better analyzed as in fact having (non-neutral) case systems. The classification in the gold standard, rather than being an assertion on the part of the grammar engineer who"
W17-0110,W14-2206,1,0.914079,"sses in IGT (§2.3). This work is situated within the AGGREGATION Project whose aim is to facilitate analysis of data collected by field linguists by automatically creating computational grammars on the basis of interlinear glossed text (IGT) and the LinGO Grammar Matrix customization system (Bender et al., 2010). Previous work by the AGGREGATION Project has looked at answering specific, highlevel typological questions for many different languages (Bender et al., 2013) as well as answering as much of the Grammar Matrix customization system’s questionnaire as possible for one specific language (Bender et al., 2014). In this paper, we revisit the case-related experiments done by Bender et al. (2013) in light of new systems for standardizing and enriching IGT and using a broader data set. Specifically, where Bender et al. considered only data from small collections of IGT created by students in a grammar engineering class (Bender, 2014), we will work with the larger and more diverse data sets available from ODIN version 2.1 (Xia et al., 2016). These data sets contain a great deal of noise in terms of inconsistent glossing conventions, missing glosses and data set bias. However, while these data sets are n"
W17-0110,bender-2014-language,1,0.855267,"he AGGREGATION Project has looked at answering specific, highlevel typological questions for many different languages (Bender et al., 2013) as well as answering as much of the Grammar Matrix customization system’s questionnaire as possible for one specific language (Bender et al., 2014). In this paper, we revisit the case-related experiments done by Bender et al. (2013) in light of new systems for standardizing and enriching IGT and using a broader data set. Specifically, where Bender et al. considered only data from small collections of IGT created by students in a grammar engineering class (Bender, 2014), we will work with the larger and more diverse data sets available from ODIN version 2.1 (Xia et al., 2016). These data sets contain a great deal of noise in terms of inconsistent glossing conventions, missing glosses and data set bias. However, while these data sets are noisier, 2.1 Inferring Case Systems from IGT Bender et al. (2013) began work on automatically creating precision grammars on the basis of IGT by using the annotations in IGT to extract largescale typological properties, specifically word order and case system. These typological properties were defined as expected by the Gramm"
W17-0110,H05-1066,0,0.0702967,"Missing"
W17-0110,N07-1057,1,0.703214,"case system based on whether or not certain grams were present (NOM , ACC , ERG , ABS ). The second method, called SAO , collected all of the grams on all intransitive subject (S), transitive subject (A) and transitive object (O) NPs, and then proceeded with the assumption that the most common gram in each role was a case gram. The grammatical role was determined by mapping parses of the English translation (using the Charniak parser (Charniak, 1997)) through the gloss line onto the source language line, according to the methodology set forth in the RiPLes project for resource-poor languages (Xia and Lewis, 2007). Because this method looks for the most frequent gram to identify the case marking gram, it is not essential that the gloss line follow a specific glossing convention, provided that the grams are consistent with each other. As a result, this method was expected to be suited to a wider range of data than GRAM. The data for this experiment comprised 31 languages from 17 different language families. Data was developed in a class in which students use descriptive resources to create testsuites and Grammar Matrix choices files (files that specify characteristics of the language so the Grammar Matr"
W17-0118,W02-1502,1,0.578084,"they are observed to take as input (attach to).7 The degree of overlap is a tuneable parameter of the system. The relationships between the affixes can then be expressed as a graph where groups of morphemes are nodes and input relations are directed edges. The graph can be used to specify the morphotactic component of a precision grammar. Suppose our corpus of IGT consists of the one sentence in (2). help provide hypothesized glosses for as-yet unanalyzed text. Precision grammars are expensive to build, requiring intensive work by highly trained grammar engineers. The Grammar Matrix project (Bender et al., 2002; Bender et al., 2010) aims to reduce the cost of creating precision grammars by producing a starter-kit that automatically creates small precision grammars on the basis of lexical and typological language profiles. The AGGREGATION Project (Bender et al., 2014) is further building on this by applying the methods of Lewis and Xia (2008) and Georgi (2016) to extract language profiles suitable for input into the Grammar Matrix grammar customization system from existing collections of IGT. In this paper, we focus on the morphotactic component of these systems. The Grammar Matrix’s morphotactic sys"
W17-0118,W14-2206,1,0.845609,"anguage documentation by producing summaries of collected data that identify apparent patterns in the data as well as exceptions to those patterns. On the one hand, this can help identify errors in glossing (where apparent exceptions are merely typos or orthographic idiosyncrasies). On the other hand, it can help the linguist understand and model patterns in the data, especially in cases where the phenomena in question have overlapping distributions. In this paper, we undertake a case study of verb classes in Abui [abz] in light of the morphotactic inference system of the AGGREGATION project (Bender et al., 2014; Wax, 2014; Zamaraeva, 2016). We begin with an overview of the phenomenon that is the focus of our case study (§2), formulate the problem and describe the steps to solve it (§3). Next we describe the tools and algorithms we apply to compare the output of the system (which summarizes what is found in the accessible corpus data) with a set of elicited judgments (§4). We PERSON PAT REC LOC GOAL BEN 1S 2S 1 PE 1 PI 2P 3 3I naanipirihadatanoonupu-/poro-/ruhodotoneenipirihedetenoooonuupuu-/pooruu-/roohoodootooneeeeniipiiriiheedeeteeDISTR Table 1: Abui person indexing paradigm An example of the pref"
W17-0118,I08-2093,1,0.767224,"mmar. Suppose our corpus of IGT consists of the one sentence in (2). help provide hypothesized glosses for as-yet unanalyzed text. Precision grammars are expensive to build, requiring intensive work by highly trained grammar engineers. The Grammar Matrix project (Bender et al., 2002; Bender et al., 2010) aims to reduce the cost of creating precision grammars by producing a starter-kit that automatically creates small precision grammars on the basis of lexical and typological language profiles. The AGGREGATION Project (Bender et al., 2014) is further building on this by applying the methods of Lewis and Xia (2008) and Georgi (2016) to extract language profiles suitable for input into the Grammar Matrix grammar customization system from existing collections of IGT. In this paper, we focus on the morphotactic component of these systems. The Grammar Matrix’s morphotactic system (O’Hara, 2008; Goodman, 2013) allows users to specify position classes and lexical rules. Position classes define the order of affixes with respect to each other (and the stem) while lexical rules pair affix forms with morphosyntactic or morphosemantic features. The grammars created on the basis of this information contain morpholo"
W17-0118,C12-1043,0,0.0201973,"Missing"
W17-0118,W16-2021,1,0.656685,"ng summaries of collected data that identify apparent patterns in the data as well as exceptions to those patterns. On the one hand, this can help identify errors in glossing (where apparent exceptions are merely typos or orthographic idiosyncrasies). On the other hand, it can help the linguist understand and model patterns in the data, especially in cases where the phenomena in question have overlapping distributions. In this paper, we undertake a case study of verb classes in Abui [abz] in light of the morphotactic inference system of the AGGREGATION project (Bender et al., 2014; Wax, 2014; Zamaraeva, 2016). We begin with an overview of the phenomenon that is the focus of our case study (§2), formulate the problem and describe the steps to solve it (§3). Next we describe the tools and algorithms we apply to compare the output of the system (which summarizes what is found in the accessible corpus data) with a set of elicited judgments (§4). We PERSON PAT REC LOC GOAL BEN 1S 2S 1 PE 1 PI 2P 3 3I naanipirihadatanoonupu-/poro-/ruhodotoneenipirihedetenoooonuupuu-/pooruu-/roohoodootooneeeeniipiiriiheedeeteeDISTR Table 1: Abui person indexing paradigm An example of the prefix attachment is given in (1)"
W18-2309,J97-1002,0,0.472435,"Missing"
W18-2309,J86-3001,0,0.728681,"ures and Actions with the COSTA Scheme in Medical Conversations † Nan Wang†‡] , Yan Song♠ , Fei Xia‡ University of California-Los Angeles, CA, USA ‡ University of Washington, WA, USA ] Hunan University, Hunan, China ♠ Tencent AI Lab nwang3@ucla.edu Abstract made possible?’(Heritage, 1984; Schegloff, 2007; Sacks et al., 1974). In artificial intelligence, researchers also explored various theories and practices in analyzing conversation structures, based on which intelligent dialog systems can be developed to assist human with various types of tasks (Core and Allen, 1997; Carletta et al., 1997; Grosz and Sidner, 1986; Jurafsky et al., 1997; Stolcke et al., 2000; Mayfield et al., 2014). In medicine, research shows that a thorough understanding of physician-patient communication structure is important for delivering quality health care and achieving optimal health outcomes (Heritage and Maynard, 2006; Zolnierek and Dimatteo, 2009; Stivers, 2007). This paper describes the COSTA scheme for coding structures and actions in conversation. Informed by Conversation Analysis, the scheme introduces an innovative method for marking multi-layer structural organization of conversation and a structure-informed taxonomy"
W18-2309,J00-3003,0,0.769566,"Missing"
W19-5027,D18-1258,0,0.128355,"Missing"
W19-5027,D16-1264,0,0.0597248,"Missing"
W19-5027,C12-2116,1,0.899036,"Q-As, because the systems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same typ"
W19-5027,K17-1016,1,0.708581,"he adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks. 1 Introduction In the big data era, it is often challenging to locate the most helpful information in many realworld applications, such as search engine, customer service, personal assistant, etc. A series of NLP tasks, such as text representation, text classification, summarization, keyphrase extraction, and answer ranking, are able to help QA systems in finding relevant information (Siddiqi and Sharan, 2015; Allahyari et al., 2017; Yang et al., 2016; Joulin et al., 2016; Song et al., 2017, 2018). Currently, most QA corpora are built for the general domain focusing on extracting/generating answers from articles, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), Dureader (He et al., 2017), SearchQA (Dunn et al., 2017), CoQA (Reddy et al., 2018), etc., with few others from community QA forums, 1 https://www.medhelp.org The code for constructing the corpus and the datasets used in this study are available at https://github. com/yuanheTian/ChiMed. 2 250 Proceedings of the BioNLP 2019 workshop, pages 250–260 c Florence, Italy, August 1, 2019. 2019 Associ"
W19-5027,N18-2028,1,0.762664,"ply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same type of input; the only difference is the meaning of class labels (relevancy vs. adopted"
W19-5027,song-xia-2012-using,1,0.876273,"ystems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same type of input; the only"
W19-5027,C18-1181,0,0.021353,") Q-As where a question and both of its answers are input (See Figure 4). ARC-I, DUET, and DRMM are run under the settings of Q-A and Q-As, because the systems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the"
W19-5027,I13-1071,1,0.935533,"Missing"
W19-5027,N18-1140,0,0.0577105,"Missing"
W19-5027,S15-2047,0,0.0492481,"Missing"
W19-5027,D07-1003,0,0.017873,"Missing"
W19-5027,W18-2309,1,0.508459,"uistics University of Washington yhtian@uw.edu Weicheng Ma Computer Science Department New York University wm724@nyu.edu Fei Xia Department of Linguistics University of Washington fxia@uw.edu Yan Song Tencent AI Lab clksong@gmail.com Abstract such as TrecQA (Wang et al., 2007), WikiQA (Yang et al., 2015), and SemEval-2015 (Nakov et al., 2015). In the medical domain, most medial QA corpora consist of scientific articles, such as BioASQ (Tsatsaronis et al., 2012), emrQA (Pampari et al., ˇ 2018), and CliCR (Suster and Daelemans, 2018). Although some studies were done for conversational datasets (Wang et al., 2018a,b), corpora designed for community QA are extremely rare. Meanwhile, given that many online medical service forums have emerged (e.g. MedHelp1 ), there are increasing demands from users to search for answers for their medical concerns. One might be tempted to build QA corpora from such forums. However, in doing so, one must address a series of challenges such as how to ensure the quality of the derived corpus despite the noise in the original forum data. In this paper, we introduce our work on building a Chinese medical QA corpus named ChiMed by crawling data from a big Chinese medical forum"
W19-5027,L18-1464,1,0.682527,"uistics University of Washington yhtian@uw.edu Weicheng Ma Computer Science Department New York University wm724@nyu.edu Fei Xia Department of Linguistics University of Washington fxia@uw.edu Yan Song Tencent AI Lab clksong@gmail.com Abstract such as TrecQA (Wang et al., 2007), WikiQA (Yang et al., 2015), and SemEval-2015 (Nakov et al., 2015). In the medical domain, most medial QA corpora consist of scientific articles, such as BioASQ (Tsatsaronis et al., 2012), emrQA (Pampari et al., ˇ 2018), and CliCR (Suster and Daelemans, 2018). Although some studies were done for conversational datasets (Wang et al., 2018a,b), corpora designed for community QA are extremely rare. Meanwhile, given that many online medical service forums have emerged (e.g. MedHelp1 ), there are increasing demands from users to search for answers for their medical concerns. One might be tempted to build QA corpora from such forums. However, in doing so, one must address a series of challenges such as how to ensure the quality of the derived corpus despite the noise in the original forum data. In this paper, we introduce our work on building a Chinese medical QA corpus named ChiMed by crawling data from a big Chinese medical forum"
W19-5027,D15-1237,0,0.0571836,"Missing"
W19-5044,N18-1132,0,0.362285,"used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained contextualThe recent Transformer-based models have been demonstrated to be a better encoder at NLI than CNN and LSTM by fully attending over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text encoder, a vector representing p and h. Pre-training on l"
W19-5044,S17-2057,0,0.0664922,"Missing"
W19-5044,P03-1003,0,0.643026,"Missing"
W19-5044,W19-1909,0,0.0576562,"Missing"
W19-5044,W19-5039,0,0.0531377,"formation, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset (Romanov and Shivade, 2018) and the MEDIQA 2019 shared task 1 (Ben Abacha et al., 2019) are reported in §6.1 Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when"
W19-5044,N19-1419,0,0.0239519,"over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text encoder, a vector representing p and h. Pre-training on large unlabeled corpora with a language modeling objective has facilitated many recent state-of-the-art advancements (Peters et al., 2018; Radford, 2018; Devlin et al., 2018; Lee et al., 2019; Radford et al., 2019). Inspired by these results, we enhance the MT-DNN representation by further fine-tuning on unlabeled biomedical data to mitigate the lack of in-domain supervision. ized representations contain rich linguistic signals (Hewitt and Manning, 2019; Liu et al., 2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreferen"
W19-5044,Q17-1010,0,0.0383203,"M for Table 2 and 3, and we call it Embedding I. Romanov and Shivade (2018) used embeddings trained on biomedical corpora and observed non-trivial accuracy gain over generaldomain embeddings. Thus, we also experimented with two domain-specific word embeddings that they used and released to initialize the TreeLSTM, and we will call them Embedding II and III. Here is a quick summary of the embeddings: I. GloVe embedding trained on Wikipedia 2014 + Gigaword 5; II. Embedding initialized with common crawl7 GloVe and fine-tuned on BioASQ and then MIMIC-III; III. Embedding initialized with fastText (Bojanowski et al., 2017) trained on Wikipedia and fine-tuned on MIMIC-III. 6.2 Model Enhancement Results We want a diverse set of member models to achieve better ensemble performance. We present ones that lead to better ensemble performance in Table 6. We also report the ensemble models and conflict resolution results in Table 6. Table 4 shows the effect of these embeddings. The first row is the best result from Table 3, which uses Embedding I, and the next two rows are the results when the embedding is changed. The table shows that using specific in-domain embeddings (the second and the third rows in Table 4) does n"
W19-5044,D15-1075,0,0.322514,"he output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1. 1 Sicong Huang Department of ECE University of Washington huangs33@uw.edu Introduction Natural language inference (NLI) (MacCartney and Manning, 2009), also known as textual entailment, is an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of exist"
W19-5044,P18-1224,0,0.0225028,"ate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained contextualThe recent Transformer-based models have been demonstrated to be a better encoder at NLI than CNN and LSTM by fully attending over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text"
W19-5044,D17-1070,0,0.211583,"mble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1. 1 Sicong Huang Department of ECE University of Washington huangs33@uw.edu Introduction Natural language inference (NLI) (MacCartney and Manning, 2009), also known as textual entailment, is an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli e"
W19-5044,C18-1328,0,0.0355147,"Missing"
W19-5044,P17-2057,0,0.0658863,"Missing"
W19-5044,D14-1162,0,0.0829161,"es the constituency parses provided by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encoder that encodes domain and generic string-based features through fully-connected layers; and a softmax classifier that takes in the concatenation (⊕) of the three encoders’ output and generates a prediction. The output of base models is sent to the ensemble and conflict resolution modules (the multimodal attention method is depicted here as an example) to make a final prediction. Xu, 2018). The premise and hypothesis are separately embedded (e.g. via GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018)) and encoded (e.g. via CNN or LSTM). Typically an interaction layer is employed to add information alignment between the premise and the hypothesis. For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018"
W19-5044,N18-1202,0,0.15143,"d by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encoder that encodes domain and generic string-based features through fully-connected layers; and a softmax classifier that takes in the concatenation (⊕) of the three encoders’ output and generates a prediction. The output of base models is sent to the ensemble and conflict resolution modules (the multimodal attention method is depicted here as an example) to make a final prediction. Xu, 2018). The premise and hypothesis are separately embedded (e.g. via GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018)) and encoded (e.g. via CNN or LSTM). Typically an interaction layer is employed to add information alignment between the premise and the hypothesis. For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018)"
W19-5044,W04-1013,0,0.0114582,"entifies and vectorizes biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in a (p, h) pair. In addition to domain knowledge, inspired by Bowman et al. (2015) and Abacha and DemnerFushman (2016), we also extract generic string features and use them to capture the similarity between p and h and then convert the results into vectors. Such similarity information includes ngram overlap, Levenshtein distance (Levenshtein, 1966), Jaccard similarity (Jaccard, 1901), ROUGE (Lin, 2004) and BLEU (Papineni et al., 2001) scores, and absolute length difference.2 To encode the aforementioned features into vectors, each extracted feature is represented by a single scalar and then grouped with others into an array, denoted by v(d) and v(g) for domain and generic features, respectively. Later, they are converted into dense representations by linear transformations and a ReLU nonlinearty. For domain features, this process can be formulated by (d) VF E (p, h) = ReLU(W(d) v(d) + b(d) ) We enhance the base models discussed above with two techniques, namely model ensemble and conflict r"
W19-5044,P19-1189,1,0.90627,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,N19-1112,0,0.370595,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,D18-1187,0,0.111483,") based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset (Romanov and Shivade, 2018) and the MEDIQA 2019 shared task 1 (Ben Abacha et al., 2019) are reported in §6.1 Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models."
W19-5044,N15-1092,0,0.0298709,"For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneo"
W19-5044,K17-1016,1,0.850001,"max Pr (2) r∈{e,c,n} As illustrated in Figure 1, our base model contains three modules. The widely used pre-trained Transformer model (Devlin et al., 2018; Liu et al., 2019c) serves as the basic text encoder to represent p and h. A syntax encoder and a feature encoder are also utilized to augment the basic representation by extracting and encoding more information from the input. The details of these encoders and how they are combined for f are discussed in the following subsections. 3.1 VSE (p) = Tree-LSTM(Parse(p)) Text Encoder Text representation is crucial to facilitate downstream tasks (Song et al., 2017, 2018). As a part of recent advancements in NLP, pre-trained models provide strong baselines for sentence representations and allow great generalizability for the represented text. Therefore, to represent p and h, we adopt a pre-trained Transformer model, MTDNN (Liu et al., 2019c), as the text encoder in our base model. MT-DNN is based on BERT (Devlin et al., 2018) and additionally fine-tuned on GLUE (Wang et al., 2018), a set of NLU datasets including NLI subsets. Through its multi-task learning objective, MT-DNN allows a more general and powerful representation for natural language understa"
W19-5044,P19-1441,0,0.34083,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,N18-2028,1,0.843036,"Missing"
W19-5044,W02-0109,0,0.465772,"Missing"
W19-5044,D18-1455,0,0.0280001,"ubsets. Through its multi-task learning objective, MT-DNN allows a more general and powerful representation for natural language understanding than BERT (Liu et al., 2019c). Formally, one can briefly describe the encoder as VT E (p, h) = MT-DNN(p, h) Syntax Encoder (4) where VSE (p) is the output vector. Once p and h are encoded, the final output of this encoder is the concatenation of the two output vectors VSE (p, h) = VSE (p) ⊕ VSE (h) 3.3 (5) Feature Encoder The explicit integration of entity-level external knowledge has been used to improve many NLP models’ performance (Das et al., 2017; Sun et al., 2018). Domain knowledge has also been demonstrated to be useful for in-domain tasks (Romanov and Shivade, 2018; Lu et al., 2019). Therefore, in addition to generic encoders such as MTDNN and Tree-LSTM, we further enhance the model with domain-specific knowledge through indirectly leveraging labeled biomedical data for (3) 417 4 other tasks. To do that, we propose a domain feature encoder that identifies and vectorizes biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in"
W19-5044,P15-1150,0,0.0703295,"Missing"
W19-5044,D18-1185,0,0.0418285,"Missing"
W19-5044,marelli-etal-2014-sick,0,0.0250401,"al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli et al., 2014; Bowman 2 Related Work A common neural network approach to address the NLI task is sentence pair modeling (Lan and 1 Our code is publicly available at https://github. com/ZhaofengWu/MEDIQA_WTMED 415 Proceedings of the BioNLP 2019 workshop, pages 415–426 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Our overall system. Our base model consists of three encoders and a softmax classifier: a syntax encoder that encodes the constituency parses provided by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encod"
W19-5044,W19-5034,0,0.031164,"n. hi = W(h) pi + b(h) Experiment Settings 5.3 (17) Implementation For MT-DNN, we use its own hyperparameters without modification. By default, we use 300-dimensional GloVe embeddings trained on Wikipedia and Gigawords (Pennington et al., The output probability distribution of i-th pair is f (M A) (p, hi ) = softmax(W(o) h0i + b(o) ) (18) 3 Finally, the prediction is computed by Eq. (2). 419 With the tool https://github.com/jtourille/mimic-tools Text Encoder 2014) to initialize the Tree-LSTM, which reduces each parse tree into a 100-dimensional vector. In the feature encoder, we use scispaCy (Neumann et al., 2019) to extract 38 domain features4 . We also extract 27 linguistic features from the 6 categories specified in §3.3. We project the 38 domain features into 38×20 = 760 dimensions and the 27 linguistic features into 27 × 20 = 540 dimensions with fully-connected layers (See Equation (6)). We fine-tune the text encoder with MIMIC-III discharge summaries using the same objectives as BERT, i.e. masked language model and next sentence prediction, for 8 epochs. For training, we use the AdaMax optimizer (Kingma and Ba, 2014) with learning rate 5 × 10−5 . We use a batch size of 16 and train each model for"
W19-5044,W18-5446,0,0.23131,"InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained co"
W19-5044,2001.mtsummit-papers.68,0,0.0105756,"es biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in a (p, h) pair. In addition to domain knowledge, inspired by Bowman et al. (2015) and Abacha and DemnerFushman (2016), we also extract generic string features and use them to capture the similarity between p and h and then convert the results into vectors. Such similarity information includes ngram overlap, Levenshtein distance (Levenshtein, 1966), Jaccard similarity (Jaccard, 1901), ROUGE (Lin, 2004) and BLEU (Papineni et al., 2001) scores, and absolute length difference.2 To encode the aforementioned features into vectors, each extracted feature is represented by a single scalar and then grouped with others into an array, denoted by v(d) and v(g) for domain and generic features, respectively. Later, they are converted into dense representations by linear transformations and a ReLU nonlinearty. For domain features, this process can be formulated by (d) VF E (p, h) = ReLU(W(d) v(d) + b(d) ) We enhance the base models discussed above with two techniques, namely model ensemble and conflict resolution: ensemble models combin"
W19-5044,N18-1101,0,0.0780091,"Missing"
W19-5044,D18-1009,0,0.0157628,"an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli et al., 2014; Bowman 2 Related Work A common neural network approach to address the NLI task is sentence pair modeling (Lan and 1 Our code is publicly available at https://github. com/ZhaofengWu/MEDIQA_WTMED 415 Proceedings of the BioNLP 2019 workshop, pages 415–426 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Our overall system. Our base model consists of three encoders and a softmax classifier: a syntax encod"
W19-5044,N19-1093,1,0.83268,"2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreference relations (Zhang et al., 2019a,b), could aid the interpretation of a sentence. Syntactic structures are often useful for deciding the entailment of a sentence pair (Chen et al., 2016). There exist numerous NLI examples where a hypothesis is merely the premise with adjunct phrases removed. The syntax encoder also mitigates the outof-vocabulary issue which is common in specific domains (Liu et al., 2019a) by capturing the structural information. Therefore, we include a syntax encoder in our base model. We use Tree-LSTM (Tai et al., 2015) to model constituency parse trees of p and h. For each sentence, we encode it according"
W19-5044,P19-1083,1,0.843284,"2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreference relations (Zhang et al., 2019a,b), could aid the interpretation of a sentence. Syntactic structures are often useful for deciding the entailment of a sentence pair (Chen et al., 2016). There exist numerous NLI examples where a hypothesis is merely the premise with adjunct phrases removed. The syntax encoder also mitigates the outof-vocabulary issue which is common in specific domains (Liu et al., 2019a) by capturing the structural information. Therefore, we include a syntax encoder in our base model. We use Tree-LSTM (Tai et al., 2015) to model constituency parse trees of p and h. For each sentence, we encode it according"
W97-1505,A88-1019,0,0.155472,"Missing"
W97-1505,W94-0104,0,0.0419199,"n performed along either of the hierarchy dimensions. The expansion could be done by general principles (add all trees of a certain subcat frame if any are present), or could be done based on performance of the sub-grammar on held-out training data. Most domains have a rich terminological vocabulary, which if not taken into account can cause prohibitive ambiguity in parsing and interpretation. Identifying and demarcating domain specific terminology is helpful for all of these approaches, since the terms can then be treated as single tokens. This can been done either manually or automatically (Daille, 1994; Jacquemin and Royaut, 1994). Once the sub-grammar has been finalized, strategies for recovering from failure to parse should be developed. One simple strategy is to fall back to the large/whole grammar. A more sophisticated strategy would be to back off using a lexical hierarchy in the same way it was used for generalizing from the training set. hierarchy. Without the description hierarchy, there would be no need to reconcile these differences, since they would be entirely independent pieces of a flat grammar. 3 Tailoring X T A G to t h e W e a t h e r Domain While it is certainly interestin"
W97-1505,C94-1024,1,0.834435,"there is training data (usually unannotated) available; that default mechanisms will be adequate for handling over-specialization (since we know training data will not perfectly reflect the genre) and that the smaller grammar combined with defaults will still be more efficient than the large grammar. Based on these assumptions, the first choice is whether to do full parsing at all in the final application. If the domain contains a large number of fragments, it might be preferable to use a partial parsing approach, in which case development of a sub-grammar will be less crucial. Supertagging (Joshi and Srinivas, 1994) is one such approach; once the supertagger is trained for the domain, it could be used in place of the full parser. If, however, it is determined that full parsing is practicable for the domain, there are still a number of considerations in deriving the sub-grammar. In the ideal situation, there would already be a corrected parsed corpus (treebank), which can be used for crafting a sub-grammar for the domain. This is exceptionally unlikely, and in the more common case, training data will have to be constructed, either manually or automatically. In a lexicalized grammar like LTAG, this turns o"
W97-1505,C92-3145,0,0.0309251,"and more portable implementation of the X-interface to the grammar and all of the supporting tools in CLISP, which is freely available. We also present a methodology for specializing our grammar to a particular domain, and give some results on this effort. 1 1.1 Development of XTAG and Current Status 1.2 Current status of XTAG Working with and developing a large grammar is a challenging process, and the importance of having good visualization tools cannot be over-emphasized. Currently the XTAG system has X-windows based tools for viewing and updating the morphological and syntactic databases (Karp et al., 1992; Egedi and Martin, 1994), and a sophisticated parsing and grammar development interface. This interface includes a tree editor, the ability to vary parameters H i s t o r y of X T A G The XTAG project has been ongoing at Penn in some form or another since 1988. It began with a toy grammar run on LISP machines, and currently has a large English grammar, small grammars in several other languages, a sophisticated X-windows based grammar development environment and numerous satellite tools. Approximately 35 people have worked extensively on the system, and at least that many have worked more peri"
W97-1505,C96-2120,0,0.0494734,"Missing"
W97-1505,C96-1034,0,\N,Missing
W98-0143,C96-1034,0,0.526327,"wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English"
W98-0143,E89-1009,0,0.0214411,"-subject extraction block. 3 We have not yet attempted to extend our coverage to include punctuation, it-clefts, and a few idiosyncratic analyses that are included in the sixty trees we are not generating. . 182 tures, so that the head features will propagate from modifiee to modified node, while non-head features from the predicate as the head of the modifier will be passed to the modified node. 4 Comparison to Other Work Evans, Gazdar and Weir (Evans et al., 1995) also discuss a method for organizing the trees in a TAG hierarchically, using an existing lexical representational system, DATR (Evans and Gazdar, 1989). Since DATR can not capture directly dominance relation in the trees, these must be simulated by using feature equations. There are substantial similarities and significant differences in our approach and Candito&apos;s approach, which she applied primarily to French and Italian. Both systems have built upon the basic ideas expressed in (Vijay-Shanker and Schabes, 1992) for organizing trees hierarchically and the use of tree descriptions that encode substructures found in several trees. The main difference is how Candito uses her dimensions in generating the trees. Her system imposes explicit cond"
W98-0143,P95-1011,0,0.65657,"e of tree substructures, such as wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it gen"
W98-0143,C92-1034,1,0.845874,"aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English grammar as well as a Chinese TAG. We describe the significant properties of both grammars, pointing out the major differences between them, and the methods by which our system is informed about these language-specific prop"
wang-xia-2012-effort,N07-1051,0,\N,Missing
wang-xia-2012-effort,W99-0701,0,\N,Missing
wang-xia-2012-effort,J93-2004,0,\N,Missing
wang-xia-2012-effort,W01-0521,0,\N,Missing
wang-xia-2012-effort,N03-1033,0,\N,Missing
wang-xia-2012-effort,D08-1093,0,\N,Missing
wang-xia-2012-effort,N03-1027,0,\N,Missing
wang-xia-2012-effort,P02-1040,0,\N,Missing
wang-xia-2012-effort,P98-2184,0,\N,Missing
wang-xia-2012-effort,C98-2179,0,\N,Missing
wang-xia-2012-effort,P10-2041,0,\N,Missing
wang-xia-2012-effort,P11-1157,0,\N,Missing
wang-xia-2012-effort,P07-1038,0,\N,Missing
wang-xia-2012-effort,P09-1076,0,\N,Missing
wang-xia-2012-effort,prasad-etal-2008-penn,0,\N,Missing
wang-xia-2012-effort,I08-4017,0,\N,Missing
wang-xia-2012-effort,N10-1004,0,\N,Missing
wang-xia-2012-effort,P07-1033,0,\N,Missing
wang-xia-2012-effort,N06-1020,0,\N,Missing
xia-etal-2010-problems,E09-1099,1,\N,Missing
xia-etal-2010-problems,I08-1069,1,\N,Missing
xia-etal-2010-problems,I08-2093,1,\N,Missing
xia-etal-2014-enriching,xia-etal-2010-problems,1,\N,Missing
xia-etal-2014-enriching,N01-1026,0,\N,Missing
xia-etal-2014-enriching,P13-2055,1,\N,Missing
xia-etal-2014-enriching,N07-1057,1,\N,Missing
xia-etal-2014-enriching,J94-4004,0,\N,Missing
xia-etal-2014-enriching,W13-2710,1,\N,Missing
xia-etal-2014-enriching,I08-1069,1,\N,Missing
xia-etal-2014-enriching,I08-2093,1,\N,Missing
