2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.acl-main.466,N19-1078,0,0.0186388,"as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searc"
2020.acl-main.466,bartolini-etal-2004-semantic,0,0.0859868,"6; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint models like SVM, CNN, GRU (Vacek et al., 2019), or scale-free identifier network (Yan et al., 2017) for promising results. Existing works have made lots of efforts to improve the effect of IE, but we need to pay more attention to the benefits of the extracted information. The extracted symbols have a legal basis and can provide interpretability to legal applications, so we cannot just aim at the performance of methods. Here, we show two examples of utilizing the extracted symbols for interpretability of LegalAI: Relation Extract"
2020.acl-main.466,P19-1424,0,0.509635,"Missing"
2020.acl-main.466,P19-1636,0,0.0419149,"Missing"
2020.acl-main.466,D19-1667,0,0.512858,"lways use hand-crafted rules or features due to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 2018). Meanwhile, other efforts with embedd"
2020.acl-main.466,P15-1017,0,0.0265215,"c legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schild"
2020.acl-main.466,P18-2014,0,0.0292769,"and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyen"
2020.acl-main.466,cvrcek-etal-2012-legal,0,0.0385196,"Missing"
2020.acl-main.466,N19-1423,0,0.487569,"ome researchers tried to embed legal dictionaries (Cvrˇcek et al., 2012), which can be regarded as an alternative method. Secondly, a generalized legal knowledge graph is different in the form with those commonly used in NLP. Existing knowledge graphs concern the relationship between entities and concepts, but LegalAI focuses more on the explanation of legal concepts. These two challenges make knowledge modelling via embedding in LegalAI non-trivial, and researchers can try to overcome the challenges in the future. 2.2 Pretrained Language Models Pretrained language models (PLMs) such as BERT (Devlin et al., 2019) have been the recent focus in many fields in NLP (Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a). Given the success of PLM, using PLM in LegalAI is also a very reasonable and direct choice. However, there are differences between the text used by existing PLMs and legal text, which also lead to unsatisfactory performances when directly applying PLMs to legal tasks. The differences stem from the terminology and knowledge involved in legal texts. To address this issue, Zhong et al. (2019b) propose a language model pretrained on Chinese legal documents, including civil and criminal c"
2020.acl-main.466,L16-1538,0,0.0308904,"problems bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of LQA better in the following parts. Related Work In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal reading comprehension dataset with the same format as SQUAD 2.0 (Rajpurkar et al., 2018), which includes span extraction, yes/no questions, and unanswerable questions. Besides, COLIEE (Kano et al., 2018) contains about 500 yes/no questions. Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam datasets (Fawei et al., 2016; Zhong et al., 2019a) may be quite hard as they require professional legal knowledge and skills. In addition to these datasets, researchers have also worked on lots of methods on LQA. The rulebased systems (Buscaldi et al., 2010; Kim et al., 2013; Kim and Goebel, 2017) are prevalent in early research. In order to reach better performance, researchers utilize more information like the explanation of concepts (Taniguchi and Kano, 2016; Fawei et al., 2015) or formalize relevant documents as graphs to help reasoning (Monroy et al., 2009, 2008; Tran et al., 2013). Machine learning and deep learnin"
2020.acl-main.466,C18-1041,1,0.809907,"minal judgment prediction dataset, C-LJP. The dataset contains over 2.68 million legal documents published by the Chinese government, making C-LJP a qualified benchmark for LJP. C-LJP contains three subtasks, including relevant articles, applicable charges, and the term of penalty. The first two can be formalized as multi-label classification tasks, while the last one is a regression task. Besides, English LJP datasets also exist (Chalkidis et al., 2019a), but the size is limited. With the development of the neural network, many researchers begin to explore LJP using deep learning technology (Hu et al., 2018; Wang et al., 2019; Li et al., 2019b; Liu et al., 2019b; Li et al., 2019a; Kang et al., 2019). These works can be divided into two primary directions. The first one is to use more novel models to improve performance. Chen et al. (2019) use the gating mechanism to enhance the performance of predicting the term of penalty. Pan et al. (2019) propose multi-scale attention to handle the cases with multiple defendants. Besides, other researchers explore how to utilize legal knowledge or the properties of LJP. Luo et al. (2017) use the attention mechanism between facts and law articles to help the p"
2020.acl-main.466,P17-1052,0,0.124736,"element extraction, we have conducted experiments on the dataset, and the results can be found in Table 2. Divorce Labor Loan Model MiF MaF MiF MaF MiF MaF TextCNN DPCNN LSTM BiDAF BERT BERT-MS 78.7 81.3 80.6 83.1 83.3 84.9 65.9 64.0 67.3 68.7 69.6 72.7 76.4 79.8 81.0 81.5 76.8 79.7 54.4 47.4 52.9 59.4 43.7 54.5 80.3 81.4 80.4 80.5 78.6 81.9 60.6 42.5 53.1 63.1 39.5 64.1 Table 2: Experimental results on extracting elements. Here MiF and MaF denotes micro-F1 and macro-F1. We have implemented several classical encoding models in NLP for element extraction, including TextCNN (Kim, 2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and Schmidhuber, 1997), BiDAF (Seo et al., 2016), and BERT (Devlin et al., 2019). We have tried two different versions of pretrained parameters of BERT, including the origin parameters (BERT) and the parameters pretrained on Chinese legal documents (BERT-MS) (Zhong et al., 2019b). From the results, we can see that the language model pretrained on the general domain performs worse 5221 than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts of our paper, we will use BERT pretrained on legal documents for better performance. From the res"
2020.acl-main.466,P16-1200,1,0.806136,"al domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (B"
2020.acl-main.466,D14-1181,0,0.103661,"existing methods on element extraction, we have conducted experiments on the dataset, and the results can be found in Table 2. Divorce Labor Loan Model MiF MaF MiF MaF MiF MaF TextCNN DPCNN LSTM BiDAF BERT BERT-MS 78.7 81.3 80.6 83.1 83.3 84.9 65.9 64.0 67.3 68.7 69.6 72.7 76.4 79.8 81.0 81.5 76.8 79.7 54.4 47.4 52.9 59.4 43.7 54.5 80.3 81.4 80.4 80.5 78.6 81.9 60.6 42.5 53.1 63.1 39.5 64.1 Table 2: Experimental results on extracting elements. Here MiF and MaF denotes micro-F1 and macro-F1. We have implemented several classical encoding models in NLP for element extraction, including TextCNN (Kim, 2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and Schmidhuber, 1997), BiDAF (Seo et al., 2016), and BERT (Devlin et al., 2019). We have tried two different versions of pretrained parameters of BERT, including the origin parameters (BERT) and the parameters pretrained on Chinese legal documents (BERT-MS) (Zhong et al., 2019b). From the results, we can see that the language model pretrained on the general domain performs worse 5221 than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts of our paper, we will use BERT pretrained on legal documents for"
2020.acl-main.466,C16-1087,0,0.015175,"ethods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal do"
2020.acl-main.466,N16-1030,0,0.0292328,"scribe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and e"
2020.acl-main.466,2021.ccl-1.108,0,0.0598105,"Missing"
2020.acl-main.466,D17-1289,0,0.408636,"l, 1963; Segal, 1984; Gardner, 1984) always use hand-crafted rules or features due to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 201"
2020.acl-main.466,P16-1105,0,0.0136318,"volved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including han"
2020.acl-main.466,W16-5607,0,0.0298178,"LAIM https://github.com/thunlp/LegalPapers continuous vector space. Many embedding methods have been proved effective (Mikolov et al., 2013; Joulin et al., 2016; Pennington et al., 2014; Peters et al., 2018; Yang et al., 2014; Bordes et al., 2013; Lin et al., 2015) and they are crucial for the effectiveness of the downstream tasks. In LegalAI, embedding methods are also essential as they can bridge the gap between texts and vectors. However, it seems impossible to learn the meaning of a professional term directly from some legal factual description. Existing works (Chalkidis and Kampas, 2019; Nay, 2016) mainly revolve around applying existing embedding methods like Word2Vec to legal domain corpora. To overcome the difficulty of learning professional vocabulary representations, we can try to capture both grammatical information and legal knowledge in word embedding for corresponding tasks. Knowledge modelling is significant to LegalAI, as many results should be decided according to legal rules and knowledge. Although knowledge graph methods in the legal domain are promising, there are still two major challenges before their practical usage. Firstly, the construction of the knowledge graph in"
2020.acl-main.466,N16-1034,0,0.0187153,"such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint mode"
2020.acl-main.466,P18-2124,0,0.029539,"ons will emphasize on the explanation of some legal concepts, while others may concern the analysis of specific cases. Besides, questions can also be expressed very differently between professionals and non-professionals, especially when describing domain-specific terms. These problems bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of LQA better in the following parts. Related Work In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal reading comprehension dataset with the same format as SQUAD 2.0 (Rajpurkar et al., 2018), which includes span extraction, yes/no questions, and unanswerable questions. Besides, COLIEE (Kano et al., 2018) contains about 500 yes/no questions. Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam datasets (Fawei et al., 2016; Zhong et al., 2019a) may be quite hard as they require professional legal knowledge and skills. In addition to these datasets, researchers have also worked on lots of methods on LQA. The rulebased systems (Buscaldi et al., 2010; Kim et al., 2013; Kim and Goebel, 2017) are prevalent in early research. In order to reach bette"
2020.acl-main.466,D14-1162,0,0.0876394,"Missing"
2020.acl-main.466,N18-1202,0,0.0126811,"as representation learning. Embedding-based methods emphasize on representing legal facts and knowledge in embedding space, and they can utilize deep learning methods for corresponding tasks. 2.1 Character, Word, Concept Embeddings Character and word embeddings play a significant role in NLP, as it can embed the discrete texts into The main contributions of this work are con5219 1 2 https://github.com/thunlp/CLAIM https://github.com/thunlp/LegalPapers continuous vector space. Many embedding methods have been proved effective (Mikolov et al., 2013; Joulin et al., 2016; Pennington et al., 2014; Peters et al., 2018; Yang et al., 2014; Bordes et al., 2013; Lin et al., 2015) and they are crucial for the effectiveness of the downstream tasks. In LegalAI, embedding methods are also essential as they can bridge the gap between texts and vectors. However, it seems impossible to learn the meaning of a professional term directly from some legal factual description. Existing works (Chalkidis and Kampas, 2019; Nay, 2016) mainly revolve around applying existing embedding methods like Word2Vec to legal domain corpora. To overcome the difficulty of learning professional vocabulary representations, we can try to capt"
2020.acl-main.466,D19-1005,0,0.0166133,"pose a language model pretrained on Chinese legal documents, including civil and criminal case documents. Legal domain-specific PLMs provide a more qualified baseline system for the tasks of LegalAI. We will show several experiments comparing different BERT models in LegalAI tasks. For the future exploration of PLMs in LegalAI, researchers can aim more at integrating knowledge into PLMs. Integrating knowledge into pretrained models can help the reasoning ability between legal concepts. Lots of work has been done on integrating knowledge from the general domain into models (Zhang et al., 2019; Peters et al., 2019; Hayashi et al., 2019). Such technology can also be considered for future application in LegalAI. 3 Symbol-based Methods In this section, we describe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emp"
2020.acl-main.466,truyens-van-eecke-2014-legal,0,0.0743219,"Missing"
2020.acl-main.466,W19-2206,0,0.0328903,"2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint models like SVM, CNN, GRU (Vacek et al., 2019), or scale-free identifier network (Yan et al., 2017) for promising results. Existing works have made lots of efforts to improve the effect of IE, but we need to pay more attention to the benefits of the extracted information. The extracted symbols have a legal basis and can provide interpretability to legal applications, so we cannot just aim at the performance of methods. Here, we show two examples of utilizing the extracted symbols for interpretability of LegalAI: Relation Extraction and Inheritance Dispute. Inheritance dispute is a type of cases in Civil Law that focuses on the distributio"
2020.acl-main.466,P18-2118,0,0.0139189,"JEC-QA (Zhong et al., 2019a) as the dataset of the experiments, as it is the largest dataset collected from the bar exam, which guarantees its difficulty. JEC-QA contains 28, 641 multiple-choice and multiple-answer questions, together with 79, 433 relevant articles to help to answer the questions. JEC-QA classifies questions into knowledge-driven questions (KD-Questions) and case-analysis questions (CA-Questions) and reports the performances of humans. We implemented several representative question answering models, including BiDAF (Seo et al., 2016), BERT (Devlin et al., 2019), Co-matching (Wang et al., 2018), and HAF (Zhu et al., 2018). The experimental results can be found in Table 6. From the experimental results, we can learn the 5225 models cannot answer the legal questions well compared with their promising results in open-domain question answering and there is still a huge gap between existing models and humans in LQA. For more qualified LQA methods, there are several significant difficulties to overcome: (1) Legal multi-hop reasoning. As Zhong et al. (2019a) state, existing models can perform inference but not multi-hop reasoning. However, legal cases are very complicated, which cannot be"
2020.acl-main.466,N18-1168,0,0.160824,"e to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 2018). Meanwhile, other efforts with embedding-based methods aim at designing effici"
2020.acl-main.466,Q16-1019,0,0.0794717,"Missing"
2020.acl-main.466,D15-1203,0,0.0112145,"ased methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP tech"
2020.acl-main.466,P19-1139,1,0.850258,"g et al. (2019b) propose a language model pretrained on Chinese legal documents, including civil and criminal case documents. Legal domain-specific PLMs provide a more qualified baseline system for the tasks of LegalAI. We will show several experiments comparing different BERT models in LegalAI tasks. For the future exploration of PLMs in LegalAI, researchers can aim more at integrating knowledge into PLMs. Integrating knowledge into pretrained models can help the reasoning ability between legal concepts. Lots of work has been done on integrating knowledge from the general domain into models (Zhang et al., 2019; Peters et al., 2019; Hayashi et al., 2019). Such technology can also be considered for future application in LegalAI. 3 Symbol-based Methods In this section, we describe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely s"
2020.acl-main.466,E17-2041,0,\N,Missing
2020.acl-main.466,D18-1390,1,\N,Missing
2020.acl-main.540,P17-1187,1,0.873387,"s. In the field of NLP, sememe knowledge bases are built to utilize sememes in practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014"
2020.acl-main.540,D14-1162,0,0.093128,"challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional LSTM (BiLSTM) with max pooling (Conneau et al., 2017) and BERTBASE (BERT) (Devlin et al., 2019). For BiLSTM, its hidden states are 128-dimensional, and it uses 300-dimensional pre-trained GloVe (Pennington et al., 2014) word embeddings. Details of the datasets and the classification accuracy results of the victim models are listed in Table 1. 4.2 Baseline Methods We select two recent open-source word-level adversarial attack models as the baselines, which are typical and involve different search space reduction methods (step 1) and search algorithms (step 2). The first baseline method (Alzantot et al., 2018) uses the combination of restrictions on word embedding distance and language model prediction score to reduce search space. As for search algorithm, it adopts genetic algorithm, another popular metaheuri"
2020.acl-main.540,P19-1561,0,0.0649884,"ity and naturality. Unfortunately, the change of true label will make the adversarial attack invalid. For example, supposing an adversary changes “she” to “he” in an input 6066 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6066–6080 c July 5 - 10, 2020. 2020 Association for Computational Linguistics sentence to attack a gender identification model, although the victim model alters its prediction result, this is not a valid attack. And the adversarial examples with broken grammaticality and naturality (i.e., poor quality) can be easily defended (Pruthi et al., 2019). Various textual adversarial attack models have been proposed (Wang et al., 2019a), ranging from character-level flipping (Ebrahimi et al., 2018) to sentence-level paraphrasing (Iyyer et al., 2018). Among them, word-level attack models, mostly word substitution-based models, perform comparatively well on both attack efficiency and adversarial example quality (Wang et al., 2019b). Word-level adversarial attacking is actually a problem of combinatorial optimization (Wolsey and Nemhauser, 1999), as its goal is to craft adversarial examples which can successfully fool the victim model using a lim"
2020.acl-main.540,P19-1571,1,0.818847,"practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014). Empirical studies have proven it is more efficient than some other optimiza"
2020.acl-main.540,D13-1170,0,0.0290862,"Pmax − where k is a positive constant, xo represents the original input, and E measures the word-level edit distance (number of different words between two n o sentences). E(xD,x ) is defined as the modification rate of an adversarial example. After mutation, the algorithm returns to the Record step. 4 Experiments In this section, we conduct comprehensive experiments to evaluate our attack model on the tasks of sentiment analysis and natural language inference. 4.1 Datasets and Victim Models For sentiment analysis, we choose two benchmark datasets including IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013). Both of them are binary sentiment classification datasets. But the average sentence length of SST-2 (17 words) is much shorter than that of IMDB (234 words), which renders attacks on SST-2 more challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional"
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.acl-main.655,P17-1171,0,0.249533,"entions provide more sparse and focused attention patterns, which are the main source of KGAT’s effectiveness. 2 Related Work The FEVER shared task (Thorne et al., 2018a) aims to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The recently launched FEVER shared task 1.0 is hosted as a competition on Codalab1 with a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It cond"
2020.acl-main.655,P17-1152,0,0.495687,"entions provide more sparse and focused attention patterns, which are the main source of KGAT’s effectiveness. 2 Related Work The FEVER shared task (Thorne et al., 2018a) aims to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The recently launched FEVER shared task 1.0 is hosted as a competition on Codalab1 with a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It cond"
2020.acl-main.655,W19-4828,0,0.120667,"aper focuses on the reasoning stage. Fine-Grained Evidence Propagation. The third analysis studies the distribution of KGATEdge’s attention which is used to propagate the evidence clues in the evidence graph. Figure 5 plots the attention weight distribution of the edge attention scores in KGAT and GAT, one from kernels and one from dot-products. The kernel attentions again are more concentrated: KGAT focuses fewer words while GAT’s dot-product attentions are almost equally distributed among all words. This observation of the scattered dotproduct attention is consistent with previous research (Clark et al., 2019). As shown in the next case study, the edge kernels provide a fine-grained and intuitive attention pattern when combining evidence clues from multiple pieces. Table 5: An example claim (Zhou et al., 2019) whose verification requires multiple pieces of evidence. 6 Case Study Table 5 shows the example claim used in GEAR (Zhou et al., 2019) and the evidence sentences retrieved by ESIM, among which the first two are required evidence pieces. Figure 6 presents the distribution of attentions from the first evidence to the tokens in the second evidence (αi2→1 ) in KGAT (Edge Kernel) and GAT (dot-prod"
2020.acl-main.655,N19-1423,0,0.10054,"mise and hypothesis as either entailment, contradiction or neutral, similar to the FEVER task, though the later requires systems to find the evidence pieces themselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017b), which employs some forms of hard or soft alignment to associate the relevant sub-components between premise and hypothesis. BERT, the pre-trained deep bidirectional Transformer, has also been used for better text representation in FEVER and achieved better performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). The recent development of neural information retrieval models, especially the interaction based ones, have shown promising effectiveness in extracting soft match patterns from query-document interactions (Hu et al., 2014; Pang et al., 2016; Guo et al., 2016; Xiong et al., 2017; Dai et al., 2018). One of the effective ways to model text matches is to leverage matching kernels (Xiong et al., 2017; Dai et al., 2018), which summarize word or phrase interactions in the learned embedding space between query and documents. The kernel extr"
2020.acl-main.655,N18-1132,0,0.0798128,"Missing"
2020.acl-main.655,W18-5516,0,0.122495,"o avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019). This paper presents a new neural structural r"
2020.acl-main.655,2021.ccl-1.108,0,0.155789,"Missing"
2020.acl-main.655,W18-5526,0,0.0920928,"Missing"
2020.acl-main.655,D19-1258,0,0.39007,"textual contents, to prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statement"
2020.acl-main.655,D16-1244,0,0.141858,"Missing"
2020.acl-main.655,N18-1202,0,0.0129804,"regation over claim evidence pairs with 1 https://competitions.codalab.org/ competitions/18814 a graph model (Veliˇckovi´c et al., 2017; Scarselli et al., 2008; Kipf and Welling, 2017). Zhong et al. (2019) further employs XLNet (Yang et al., 2019) and establishes a semantic-level graph for reasoning for a better performance. These graph based models establish node interactions for joint reasoning over several evidence pieces. Many fact verification systems leverage Natural Language Inference (NLI) techniques (Chen et al., 2017b; Ghaeini et al., 2018; Parikh et al., 2016; Radford et al., 2018; Peters et al., 2018; Li et al., 2019) to verify the claim. The NLI task aims to classify the relationship between a pair of premise and hypothesis as either entailment, contradiction or neutral, similar to the FEVER task, though the later requires systems to find the evidence pieces themselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017b), which employs some forms of hard or soft alignment to associate the relevant sub-components between premise and hypothesis. BERT, the pre-trained deep bidirection"
2020.acl-main.655,W18-5501,0,0.135442,"Missing"
2020.acl-main.655,2020.emnlp-main.582,1,0.756424,"etrieved pages. There are two sentence retrieval models in our experiments: ESIM based sentence retrieval and BERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work (Hanselowski et al., 2018; Zhou et al., 2019). The base version of BERT is used to implement our BERT based sentence retrieval model. We use the “[CLS]” hidden state to represent claim and evidence sentence pair. Then a learning to rank layer is leveraged to project “[CLS]” hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work (Zhao et al., 2020; Ye et al., 2020) also employs our BERT based sentence retrieval in their experiments. Claim verification. During training, we set the 2 https://github.com/sheffieldnlp/ fever-scorer 3 https://www.mediawiki.org/wiki/API: Main_page 4 Experimental Methodology 7346 Model Athene (Hanselowski et al., 2018) UCL MRG (Yoneda et al., 2018) UNC NLP (Nie et al., 2019a) BERT Concat (Zhou et al., 2019) BERT Pair (Zhou et al., 2019) GEAR (Zhou et al., 2019) GAT (BERT Base) w. ESIM Retrieval KGAT (BERT Base) w. ESIM Retrieval SR-MRS (Nie et al., 2019b) BERT (Base) (Soleimani et al., 2019) KGAT (BERT Base) BERT (Large) (Solei"
2020.acl-main.655,D18-1010,0,0.0843691,"th a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It conducts reasoning and aggregation over claim evidence pairs with 1 https://competitions.codalab.org/ competitions/18814 a graph model (Veliˇckovi´c et al., 2017; Scarselli et al., 2008; Kipf and Welling, 2017). Zhong et al. (2019) further employs XLNet (Yang et al., 2019) and establishes a semantic-level graph for reasoning for a better performance. These graph based models establish node interact"
2020.acl-main.655,W18-5515,0,0.568683,"d of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019). This paper present"
2020.acl-main.655,N18-1074,0,0.333649,"em. Introduction Online contents with false information, such as fake news, political deception, and online rumors, have been growing significantly and spread widely over the past several years. How to automatically “fact check” the integrity of textual contents, to prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fa"
2020.acl-main.655,P19-1085,1,0.510747,"o prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019"
2020.amta-research.11,J93-2003,0,0.191098,"n languages automatically, is an important task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2"
2020.amta-research.11,P16-1185,1,0.836159,"arning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project developed with Theano, which is no longer updated because MLA put an end to Theano. It implemented the standard attention-based model (RNNsearch) (Bahdanau et al., 2014), minimum risk training (MRT) (Shen et al., 2015) for optimizing model parameters with respect to evaluation metrics, semi-supervised training (SST) (Cheng et al., 2016) for exploiting monolingual ∗Corresponding author. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizers LRP Gradient Aggregation Distributed Training Mixed-Precision TensorBoard Theano RNNsearch MLE, MRT, SST SGD, Adadelta, Adam Yes No No No No TensorFlow Seq2Seq, RNNsearch, Transformer MLE Adam Yes Yes Yes Yes Yes PyTorch Transformer MLE SGD, Adadelta, Adam No Yes Yes Yes Yes Table 1: Available features in different implementations. corpora to learn bi-direc"
2020.amta-research.11,P05-1033,0,0.306244,"nt task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project develope"
2020.amta-research.11,P17-1106,1,0.849312,"4th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizers LRP Gradient Aggregation Distributed Training Mixed-Precision TensorBoard Theano RNNsearch MLE, MRT, SST SGD, Adadelta, Adam Yes No No No No TensorFlow Seq2Seq, RNNsearch, Transformer MLE Adam Yes Yes Yes Yes Yes PyTorch Transformer MLE SGD, Adadelta, Adam No Yes Yes Yes Yes Table 1: Available features in different implementations. corpora to learn bi-directional translation models, and layer-wise relevance propagation (LRP) (Ding et al., 2017) for visualizing and analyzing RNNsearch. • THUMT-TensorFlow: an implementation focuses on performance. It implemented the sequence-to-sequence model (Seq2Seq) (Sutskever et al., 2014), the standard attentionbased model (RNNsearch) (Bahdanau et al., 2014), the Transformer model (Transformer) (Vaswani et al., 2017), and LRP visualization for RNNsearch and Transformer. It also added new features such as multi-GPU training, distributed training, ensemble inference, and TensorBoard visualization. • THUMT-PyTorch: a new implementation developed with PyTorch, which is more flexible and easier to use"
2020.amta-research.11,N03-1017,0,0.194988,"cally, is an important task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original p"
2020.amta-research.11,P02-1040,0,0.110845,"d interpret neural machine translation models (Ding et al., 2017). Figure 1 shows an example of visualizing the Transformer model. Although the transformer model uses attention mechanisms extensively, it is hard to determine the most representative head. For LRP, it is possible to calculate the global relevance between source words and target words, which helps analyze the internal workings of NMT. Please refer to Ding et al. (2017) for more details. 3 Experiments 3.1 Setup We evaluate THUMT on English-German and Chinese-English translation tasks. The evaluation metric is case-sensitive BLEU (Papineni et al., 2002). Following Vaswani et al. (2017), translations are generated via beam-search with a beam size of 4 and a length penalty of 0.6. For English-German, we use the WMT14 training corpus which contains 4.5M sentence pairs with 103M English words and 96M German words. We also use a shared sourcetarget vocabulary of about 37000 tokens encoded by BPE (Sennrich et al., 2016). We use newstest2014 as the test set. For Chinese-English translation, we use the training corpus provided by WMT18 2 . The corpus consists of 24M sentence pairs with 509M Chinese words and 576M English words. We use 32K BPE operat"
2020.amta-research.11,N18-2074,0,0.0261219,"and uses another RNN to generate translation conditioned on the representation. RNNsearch exploits variable representation with attention mechanism and achieves significant improvements over Seq2Seq model. Transformer uses deep self-attention layers instead of RNN layers in both encoder and decoder. It achieves the best performance among the three models. 1 https://github.com/THUNLP-MT/THUMT Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 117 THUMT also implemented relative position embedding (Shaw et al., 2018) in its Transformer implementation. Instead of adding absolute positions to its input, this approach considers relative positions in the self-attention mechanism. We offer two options in our Tensorflow implementation, one can turn on or off the relative position embedding and set the desired maximum relative distance. Note that we only implement relative position embedding in selfattention but not in encoder-decoder attention. 2.2 Training THUMT supports both single machine multi-GPU training and multi-machine distributed training. THUMT-TensorFlow uses the Horovod (Sergeev and Del Balso, 2018"
2020.amta-research.11,P17-4012,0,0.0344285,"wn et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project developed with Theano, which is no longer updated because MLA put an end to Theano. It implemented the standard attention-based model (RNNsearch) (Bahdanau et al., 2014), minimum risk training (MRT) (Shen et al., 2015) for optimizing model parameters with respect to evaluation metrics, semi-supervised training (SST) (Cheng et al., 2016) for exploiting monolingual ∗Corresponding author. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizer"
2020.coling-main.140,N19-1423,0,0.15562,"alized parameters according to the performance on the support set, and selects informative support instances to 1596 contribute more to the adaptation gradients (with meta-parameters φa ), which can be viewed as accurate fine-tuning from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t;"
2020.coling-main.140,D19-1649,1,0.78596,"s to grasp new tasks with only a handful of training data. There are mainly two lines of approaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another directi"
2020.coling-main.140,D18-1514,1,0.742534,"is scaled to 1e-3 in L2 norm, and added to ci to obtain the perturbed representation. 4 Experiments In this section, we empirically evaluate MIML on few-shot relation classification. To evaluate the robustness of MIML, we conduct experiments in the presence of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in"
2020.coling-main.140,P09-1113,0,0.132459,"of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in Han et al. (2018), we consider four types of fewshot settings in evaluation, namely 5-way-1-shot, 5-way-5-shot, 10-way-1-shot and 10-way-5-shot. The N -way-K-shot setting indicates that each evaluation batch has N classes that do not appear in training set and"
2020.coling-main.140,D14-1162,0,0.0851287,"this way, MIML learns meta-parameters that can effectively customize initialization parameters for each class, and select informative support instances for fast adaptation, so as to produce good classification results on the query set. 3.5 Implementation Details All hyper-parameters are selected by grid-search on the development set. The class distribution p(C) is implemented by uniform distribution. We adopt Adam (Kingma and Ba, 2015) to optimize metaparameters. The meta learning rate β is 1 for meta-initializer and meta-querier, and 5e-5 for instance encoder. We employ 50 dimensional GloVe (Pennington et al., 2014) for word embeddings and BERTBASE (Devlin et al., 2019) implemented by Wolf et al. (2019) as the instance encoder. The hidden state dimensions ds and dw are 1, 536 and 50 respectively. The number of adaptation steps T is 150. In virtual adversarial training, we first randomly generate a perturbation vector δ1 for meta-information 1598 Encoder Model 5-way-1-shot 5-way-5-shot 10-way-1-shot 10-way-5-shot CNN Meta Network* GNN* SNAIL* Proto Network* MLMAN* 64.46 ± 0.54 66.23 ± 0.75 67.29 ± 0.26 74.52 ± 0.07 82.98 ± 0.20 80.57 ± 0.48 81.28 ± 0.62 79.40 ± 0.22 88.40 ± 0.06 92.66 ± 0.09 53.96 ± 0.56"
2020.coling-main.140,P19-1279,0,0.0830438,"g from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t; φe ), (1) where xj is the sentence, h and t are head and tail entities respectively. g(·) is the encoder, φe is the parameters of the encoder, and xj ∈ Rds is the instance representation. 3.2 Meta-Information Guided Fast Initializ"
2020.coling-main.140,P19-1277,0,0.0153968,"roaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another direction is to learn how to better initialize parameters for new classes (Finn et al., 2017; Finn e"
2020.coling-main.140,N19-1108,0,0.019921,"ion phase in 5-way and 10-way setting, and ask the model to classify query instances with class-aware initialization parameters. We compare MIML with strong zero-shot classification baselines. DeViSE (Frome et al., 2013) utilizes word embeddings of class names to classify 1600 Setting Random DeViSE SK4 MIML 5-way-0-shot 10-way-0-shot 20.00 10.00 55.90 ± 0.09 42.29 ± 0.08 79.68 ± 0.12 66.17 ± 0.11 79.54 ± 0.06 61.14 ± 0.10 Table 3: Experimental results of zero-shot classification on FewRel development set. instances from unseen classes, and we implement the DeViSE model with BERT encoder. SK4 (Zhang et al., 2019) incorporates rich semantic knowledge of classes, including word embeddings, class descriptions, class hierarchy, and commonsense knowledge graphs. We report the results in Table 3, from which we observe that: Compared to models tailored for zero-shot classification problem, MIML achieves reasonable performance. This is because that the class-aware fast initialization parameters in MIML are guided by meta-information, and thus can potentially be used to severe as classifiers without further adaptation using support instances. In summary, the results show that MIML can effectively integrate hig"
2020.coling-main.155,C14-1151,0,0.0307615,"2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basile et al., 2014; Wang et al., 2020). The graph-based methods is the other major type of knowledge-based WSD approaches, which exploit the structures of the LKB for disambiguation (Agirre et al., 2014; Moro et al., 2014; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018). Besides, a recent method uses both glosses and structural information of LKBs in knowledge-based WSD and achieves state-of-the-art performance (Scarlini et al., 2020). In Chinese WSD, HowNet (Dong and Dong, 2006) is the most widely used LKB (Wu, 2009). Different from other LKBs, HowNet contains neither glosses nor structures of different"
2020.coling-main.155,D15-1084,0,0.0138215,"monstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses ("
2020.coling-main.155,N19-1423,0,0.0594527,"nsupervised WSD based on HowNet. Yang et al. (2001) propose a representative statistical method which utilizes the co-occurrence of sememes of the target word and context to conduct disambiguation. Tang et al. (2015) learn sememe and sense embeddings and disambiguates a word by choosing the sense that has the closest embedding similarity with the context. These methods work well but far from perfectly. In this paper, we propose a new unsupervised HowNet-based WSD model with the help of large pretrained language models. Previous studies have shown that pre-trained language models such as BERT (Devlin et al., 2019) incorporate much sense information (Reif et al., 2019), which can be utilized in WSD. Their pre-training task of masked language model (MLM) is supposed to predict appropriate words for a specified position in the context. In other words, a word with higher MLM prediction score is more suitable for the given context and should have more similar meaning to the original word. Based on this assumption, we design our lexical substitution-based WSD model. For each sense of the target polysemous word, we can find a set of substitution words that involve a sense annotated with the same sememes as th"
2020.coling-main.155,D18-1493,1,0.833008,"y outperforms all the baseline methods and achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses an"
2020.coling-main.155,S07-1004,0,0.656943,"s characters. 3 Experiments In this section, we evaluate our model on the newly built HowNet-based WSD dataset by us. 3.1 Construction of the HowNet-based WSD Dataset To the best of our knowledge, the only HowNet-based Chinese WSD dataset1 is based on an outdated version of HowNet that cannot be found now, which actually makes the dataset unusable. And besides, it is a little small (containing only 1, 173 instances for 20 target polysemous words). Therefore, we build a new and larger HowNet-based Chinese WSD dataset based on the Chinese Word Sense Annotated Corpus used in SemEval-2007 task 5 (Jin et al., 2007), whose sense inventory is Chinese Semantic Dictionary. This corpus comprises 3, 632 word-segmented and part-of-speech tagged instance sentences for 40 Chinese polysemous words (19 nouns and 21 verbs). We ask Chinese native speakers to manually annotate the target polysemous words in each instance sentence of the corpus with corresponding senses of HowNet or a special option of “no appropriate sense”, where each instance is annotated by 3 annotators. Among the 40 target polysemous words, 4 words have only one sense in HowNet and thus the other 36 target words’ 3, 328 instances are annotated in"
2020.coling-main.155,Q14-1019,0,0.0441971,"SD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basile et al., 2014; Wang et al., 2020). The graph-based methods is the other major type of knowledge-based WSD approaches, which exploit the structures of the LKB for disambiguation (Agirre et al., 2014; Moro et al., 2014; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018). Besides, a recent method uses both glosses and structural information of LKBs in knowledge-based WSD and achieves state-of-the-art performance (Scarlini et al., 2020). In Chinese WSD, HowNet (Dong and Dong, 2006) is the most widely used LKB (Wu, 2009). Different from other LKBs, HowNet contains neither glosses nor structures of different senses. Instead, HowNet ∗ Indicates equal contribution. Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/."
2020.coling-main.155,Q18-1044,0,0.0193422,"based WSD dataset. Experimental results demonstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased method"
2020.coling-main.155,P19-1571,1,0.823097,"nd achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses annotated with the same sememes are suppo"
2020.coling-main.155,D19-1009,0,0.0142486,"k which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basil"
2020.coling-main.155,L18-1164,0,0.0171479,"e two most representative ones as the baseline methods. Besides, we compare our model with another three baseline methods that are not specially designed for but can be applied to unsupervised HowNet-based WSD. • SemCo (Yang et al., 2001). This method utilizes the statistics on the co-occurrence of sememes of the target polysemous word and context to conduct WSD. • SemEmbed (Tang et al., 2015). This method first learns sememe embeddings and further obtains sense embeddings, and then employs the embedding similarity between senses of the target word and the context for disambiguation. • Dense (Ustalov et al., 2018). This model is originally designed for WordNet-based WSD, which first obtains sense embeddings from the word embeddings of the corresponding senses’ synonyms and then selects the sense that has the closest embedding similarity with the context. In HowNetbased WSD, we regard the words whose one sense has the same sememes as the target sense as the synonyms. • Random. This baseline method randomly selects a sense of the target word as the WSD result. A common WSD baseline method is choosing the most frequent sense. But HowNet provides no information about the sense frequency. Therefore, we use"
2020.coling-main.155,H05-1097,0,0.0234945,"new and larger HowNet-based WSD dataset. Experimental results demonstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and"
2020.coling-main.155,S07-1044,0,0.0643448,"r the given context and should have more similar meaning to the original word. Based on this assumption, we design our lexical substitution-based WSD model. For each sense of the target polysemous word, we can find a set of substitution words that involve a sense annotated with the same sememes as the target sense. We calculate the MLM prediction score for each substitution word, and the average of prediction scores of a sense’s all substitution words can reflect the probability that the target word conveys this sense in the context. The idea of lexical substitution has been applied to WSD in Yuret (2007). Different from our model, it uses a statistical language model to calculate substitution word scores and more importantly, it is not fully unsupervised and requires some sense-annotated corpora in the WSD procedure. Besides, our model resembles the end-to-end BERT-based lexical substitution model in Zhou et al. (2019). However, it is not aimed at WSD and has different calculation methods of substitution word score from us. In experiments, considering existing HowNet-based WSD dataset is unavailable and based on an outdated version of HowNet, we build a new and larger HowNet-based WSD dataset"
2020.coling-main.155,2020.acl-main.540,1,0.722998,"on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses annotated with the same sememes are supposed to have the same meaning. Therefore, in our model, we select the substitution words"
2020.coling-main.155,P12-1029,0,0.019542,"y better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. L"
2020.coling-main.155,P19-1328,0,0.0220336,". We calculate the MLM prediction score for each substitution word, and the average of prediction scores of a sense’s all substitution words can reflect the probability that the target word conveys this sense in the context. The idea of lexical substitution has been applied to WSD in Yuret (2007). Different from our model, it uses a statistical language model to calculate substitution word scores and more importantly, it is not fully unsupervised and requires some sense-annotated corpora in the WSD procedure. Besides, our model resembles the end-to-end BERT-based lexical substitution model in Zhou et al. (2019). However, it is not aimed at WSD and has different calculation methods of substitution word score from us. In experiments, considering existing HowNet-based WSD dataset is unavailable and based on an outdated version of HowNet, we build a new and larger HowNet-based WSD dataset for evaluation. Experimental results demonstrate that our model significantly outperforms all the baseline methods and achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to How"
2020.emnlp-demos.23,C86-1107,0,0.727398,"Missing"
2020.emnlp-demos.23,N19-1423,0,0.0159031,"ut. The words in the query description are excluded since they are unlikely to be the target word. Different filters, other sort methods and clustering may be further employed to adjust the final results. 3.2 Max-Pooling Word Score Confidence Score Sentence Vector BERT Dictionary Definition / Query Description Part-of-speech Score & Category Score Figure 3: Revised version of the multi-channel reverse dictionary model. Multi-channel Reverse Dictionary Model The multi-channel reverse dictionary model (MRDM) is the core module of our system. We use an improved version of MRDM that employs BERT (Devlin et al., 2019) rather than BiLSTM as the sentence encoder. Figure 3 illustrates the model. For a given query description, MRDM calculates a confidence score for each candidate word in the vocabulary. The confidence score is composed of five parts: (1) The first part is word score. To obtain it, the input query description is first encoded into a sentence vector by BERT, then the sentence vector is mapped into the space of word embeddings by a single-layer perceptron, and finally word score is the dot product of the mapped sentence vector and the candidate word’s embedding. (2) The second part is part-of-spe"
2020.emnlp-demos.23,W19-0421,0,0.0220052,"Missing"
2020.emnlp-demos.23,Q16-1002,0,0.273363,"ased on sentence matching (Bilac et al., 2004; Zock and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four charact"
2020.emnlp-demos.23,P82-1020,0,0.807762,"Missing"
2020.emnlp-demos.23,D18-1221,0,0.0126569,"ck and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four characteristic predictors. The four predictors are used to pre"
2020.emnlp-demos.23,N19-1222,0,0.0153301,"al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four characteristic predictors. The four predictors are used to predict the part-ofspeech, morphemes, word c"
2020.emnlp-demos.23,W04-2105,0,0.0284269,"very user-friendly. It includes multiple filters and sort methods, and can automatically cluster the candidate words, all of which help users find the target words as quickly as possible. 2 Cross-lingual (en-zh/zh-en) =1 (Word) Cross-lingual Dictionary Mode Query Length Monolingual (en/zh) >1 (Sentence) Translation Query Length =1 (Word) >1 (Sentence) Multi-channel Reverse Dictionary Model Word Similarity Thesaurus Confidence Score Related Work Filter / Sort / Cluster There are mainly two methods for reverse dictionary building. The first one is based on sentence matching (Bilac et al., 2004; Zock and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al.,"
2020.emnlp-demos.29,D11-1029,0,0.035567,"Missing"
2020.emnlp-demos.29,W19-1402,0,0.0824936,"Missing"
2020.emnlp-demos.29,W19-1420,0,0.0164561,"the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “horse” from OBS to modern Chinese. Figure 2: An example of an OBS document used in divination. (1) Different from those ancie"
2020.emnlp-demos.29,W16-2103,0,0.0408051,"and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propos"
2020.emnlp-demos.29,P19-1303,0,0.0109063,"s-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “h"
2020.emnlp-demos.29,D18-1063,0,0.0129114,"tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistic"
2020.emnlp-demos.29,D17-1266,0,0.0177155,". (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations),"
2020.emnlp-demos.29,P10-1107,0,0.0277237,"ime-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); B"
2020.emnlp-demos.29,L18-1115,0,0.0281369,"ifice, agriculture, as well as births, illnesses, and deaths of royal members (Flad et al., 2008). Therefore, OBS documents constitute the earliest Chinese textual corpora, and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) a"
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.300,D19-1498,0,0.0662053,"g tasks. Experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy DS data and achieve promising results. The source code of this paper can be found in https://github.com/thunlp/DSDocRE. 1 Now James serves for the Lakers ... [8] James locate_in Lebron James member_of educated_at Vincent–St. Mary High School in his hometown. St. Vincent–St. Mary High School Lakers Figure 1: An example of DocRE. Given a document, DocRE models should capture the relational semantics across sentences to extract multiple relational facts. tence relations (Christopoulou et al., 2019). Fig. 1 gives a brief illustration of DocRE. Relation extraction (RE) aims to identify relational facts between entities from texts. Recently, neural relation extraction (NRE) models have been verified in sentence-level RE (Zeng et al., 2014). Distant supervision (DS) (Mintz et al., 2009) provides large-scale distantly-supervised data that multiplies instances and enables sufficient model training. Sentence-level RE focuses on extracting intrasentence relations between entities in a sentence. However, it is extremely restricted with generality and coverage in practice, since there are plenty"
2020.emnlp-main.300,N19-1423,0,0.174441,"entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. Finally, we fine-tune the model on the human-annotated dataset. 3.1 Document Encoder We adopt BERT (Devlin et al., 2019) as the document encoder to encode documents into representations of entity mentions, entities and relational instances. Let D = {ωi }ni=1 denote the input docu|V | ment which consists of n tokens, and V = {ei }i=1 be the set of entities mentioned in the document, i where entity ei = {mji }lj=1 contains li mentions in the document. Following Soares et al. (2019), we use entity markers [Ei] and [/Ei] for each entity ei . The start marker [Ei] is inserted at the begin of all mentions of entity ei , and the end marker [/Ei] is inserted at the end. 3684 We use BERT to encode the token sequence wit"
2020.emnlp-main.300,P16-1200,1,0.799391,"f DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Ment"
2020.emnlp-main.300,N13-1095,0,0.0321775,"nduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational"
2020.emnlp-main.300,P09-1113,0,0.825376,"aselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact A"
2020.emnlp-main.300,Q17-1008,0,0.0213299,"eep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. F"
2020.emnlp-main.300,P18-1046,0,0.14999,"RE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document En"
2020.emnlp-main.300,P19-1423,0,0.0180819,"gnment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-trai"
2020.emnlp-main.300,P19-1279,0,0.246383,"evel supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the do"
2020.emnlp-main.300,D17-1188,0,0.0179509,"e-tuning. We keep 2Nent entity pairs after pre-denoising for each document during fine-tuning, where Nent is the number of entities mentioned in the document. And we keep 20 entity pairs for each document during pre-training. We train our model with GeForce RTX 2080 Ti. All the special tokens including entity markers and the special blank symbol are implemented with unused tokens in the BERTBASE vocabulary. 4.2 4.4 Baseline We compare our model with the following baselines. (1) CNN/LSTM/BiLSTM (Yao et al., 2019): these models capture relational semantics via various encoder. (2) ContextAware (Sorokin and Gurevych, 2017): it considers the relations’ interactions with attention to jointly learn all entity 4.3 Implementation Details Main Result The main results are shown in Tab. 1. Specifically, D refers to the pre-denoising module and P indicates pre-training tasks. From the results, we can observe that: (1) Our model outperforms all baselines by a significant margin. It is due to the effectiveness of the pre-denoising mechanism 3686 F1 Model Dev IgnF1 F1 58.65 57.00 58.43 56.68 w/o MM w/o RD w/o RA 58.39 57.19 58.48 56.76 55.61 56.73 57.60 56.71 58.13 55.81 54.94 56.30 w/o Inter w/o Intra 58.68 57.78 56.96 56"
2020.emnlp-main.300,P19-1074,1,0.914989,"level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives"
2020.emnlp-main.300,C14-1220,0,0.0453237,"sions and denoising irrelevant information from the document. In experiments, we evaluate our model on an open DocRE benchmark and achieve significant improvement over competitive baselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretra"
2020.emnlp-main.566,W19-1909,0,0.110918,"Missing"
2020.emnlp-main.566,D19-1539,0,0.021328,"r method is both effective and efficient. The source code of this paper can be obtained from https://github. com/thunlp/SelectiveMasking. 1 In-domain Unsupervised Data Random Masking Selective Masking General Pre-train Task-guided Pre-train Downstream Supervised Data Fine-tune Figure 1: The overall three-stage framework. We add task-guided pre-training between general pre-training and fine-tuning to efficiently and effectively learn the domain-specific and task-specific language patterns. Introduction Pre-trained Language Models (PLMs) have achieved superior performances on various NLP tasks (Baevski et al., 2019; Joshi et al., 2020; Liu et al., 2019; Yang et al., 2019; Clark et al., 2020) and have attracted wide research interests. Inspired by the success of GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), most PLMs follow the pre-train-then-fine-tuning paradigm, which adopts unsupervised pre-training on large general-domain † General Unsupervised Data Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific p"
2020.emnlp-main.566,D19-1371,0,0.035946,"ing author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific patterns during pre-training can be helpful to the models for certain domains and tasks. However, conventional pre-training is aimless with respect to specific downstream tasks, and fine-tuning usually suffers from insufficient supervised data, preventing PLMs from effectively capturing these patterns. To learn domain-specific language patterns, some previous works (Beltagy et al., 2019; Huang et al., 2020) pre-train a BERT-like model from scratch using large-scale in-domain data. However, they are computation-intensive and require largescale in-domain data, which is hard to obtain in many domains. To learn task-specific language patterns, some previous works (Phang et al., 2018) add intermediate supervised pre-training after general pre-training, whose pre-training task is similar to the downstream task but has a larger dataset. However, Wang et al. (2019) shows that this kind of intermediate pre-training often negatively impacts 6966 Proceedings of the 2020 Conference on E"
2020.emnlp-main.566,D18-1407,0,0.0417078,"Missing"
2020.emnlp-main.566,2020.acl-main.740,0,0.0383792,"Missing"
2020.emnlp-main.566,2020.tacl-1.5,0,0.0416949,"tive and efficient. The source code of this paper can be obtained from https://github. com/thunlp/SelectiveMasking. 1 In-domain Unsupervised Data Random Masking Selective Masking General Pre-train Task-guided Pre-train Downstream Supervised Data Fine-tune Figure 1: The overall three-stage framework. We add task-guided pre-training between general pre-training and fine-tuning to efficiently and effectively learn the domain-specific and task-specific language patterns. Introduction Pre-trained Language Models (PLMs) have achieved superior performances on various NLP tasks (Baevski et al., 2019; Joshi et al., 2020; Liu et al., 2019; Yang et al., 2019; Clark et al., 2020) and have attracted wide research interests. Inspired by the success of GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), most PLMs follow the pre-train-then-fine-tuning paradigm, which adopts unsupervised pre-training on large general-domain † General Unsupervised Data Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific patterns during pre-t"
2020.emnlp-main.566,2020.emnlp-main.567,0,0.0718477,"Missing"
2020.emnlp-main.566,P05-1015,0,\N,Missing
2020.emnlp-main.566,S14-2004,0,\N,Missing
2020.emnlp-main.566,N18-1112,0,\N,Missing
2020.emnlp-main.566,N19-1423,0,\N,Missing
2020.emnlp-main.566,D19-1628,0,\N,Missing
2020.emnlp-main.582,S17-2001,0,0.0370299,"Missing"
2020.emnlp-main.582,P18-1078,0,0.0246983,"o tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and verification, coreference resolution, and eight tas"
2020.emnlp-main.582,D19-1606,0,0.323037,"9), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference information (Cheng and Erk, 2020; Xu et al., 2020; Zhao et al., 2020). Coreference occurs when two or more expressions in a text refer to the same entity, which is an important element for a coherent understanding of the whole discourse. For example, for comprehending the whole context of “Antoine published The Little Prince in 1943. The book follows a young prince who visits various planets in space.”, we must realize that The book refers to The Little Prince. Therefore, resolving coreference is an essential step"
2020.emnlp-main.582,N19-1423,0,0.584796,"issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT. 1 Introduction Recently, language representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou e"
2020.emnlp-main.582,D19-5801,0,0.0239374,"Missing"
2020.emnlp-main.582,W07-1401,0,0.0718487,"Missing"
2020.emnlp-main.582,P16-1154,0,0.0608072,"Missing"
2020.emnlp-main.582,P18-2058,0,0.0204785,"p((V hk ) hi ) Pr(xj |xi ) = P where denotes element-wise product function and V is a trainable parameter to measure the importance of each dimension for token’s similarity. Moreover, since we split a word into several word pieces as BERT does and we adopt whole word masking strategy for MRP, we need to extend our copy-based objective into word-level. To this end, we apply the token-level copy-based training objective on both start and end tokens of the masked word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Refer"
2020.emnlp-main.582,P82-1020,0,0.813763,"Missing"
2020.emnlp-main.582,P18-1031,0,0.0280754,"language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or o"
2020.emnlp-main.582,D19-1170,0,0.0311913,"ntation Details Following BERT’s setting (Devlin et al., 2019), given the question Q = (q1 , q2 , . . . , qm ) and the passage P = (p1 , p2 , . . . , pn ), we represent them as a sequence X = ([CLS], q1 , q2 , . . . , qm , [SEP], p1 , p2 , . . . , pn , [SEP]), feed the sequence X into the pre-trained encoder and train two classifiers on the top of it to seek answer’s start and end positions simultaneously. For MRQA, CorefBERT maintains the same framework as BERT. For QUOREF, we further employ two extra components to process multiple mentions of the answers: (1) Spurred by the idea from MTMSN (Hu et al., 2019) in handling the problem of multiple answer spans, we utilize the representation of [CLS] to predict the number of answers. After that, we first selects the answer span of the current highest scores, then continues to choose that of the second-highest score with no overlap to previous spans, until reaching the predicted answer number. (2) When answering a question from QUOREF, the relevant mention could possibly be a pronoun, so we attach a reasoning Transformer layer for pronoun resolution before the span boundary classifier. 7174 Model SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQA Averag"
2020.emnlp-main.582,2020.tacl-1.5,0,0.0654805,"on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan a"
2020.emnlp-main.582,P17-1147,0,0.0453347,"Missing"
2020.emnlp-main.582,D19-1588,0,0.0315944,"ese models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference infor"
2020.emnlp-main.582,P16-1086,0,0.0310095,"d word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and"
2020.emnlp-main.582,D14-1181,0,0.00377708,"efBERT outperforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further em"
2020.emnlp-main.582,D19-1439,0,0.273092,"ign a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, researchers have made efforts to explore feature-based unsupervised coreference resolution methods (Bejan et al., 2009; Ma et al., 2016). After that, Word-LM (Trinh and Le, 2018) uncovers that it is natural to resolve pronouns in the sentence according to the probability of language models. Moreover, WikiCREM (Kocijan et al., 2019) builds sentence-level unsupervised coreference resolution dataset for learning coreference discriminator. However, these methods cannot be directly transferred to language representation models since their task-specific design could weaken the model’s performance on other NLP tasks. To address this issue, we introduce a mention reference prediction objective, complementary to masked language modeling, which could make the obtained coreferential reasoning ability compatible with more downstream tasks. 3 Methodology In this section, we present CorefBERT, a language representation model, which a"
2020.emnlp-main.582,D19-1279,0,0.0262174,"learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, res"
2020.emnlp-main.582,P16-1200,1,0.781375,"erforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectio"
2020.emnlp-main.582,2021.ccl-1.108,0,0.305008,"Missing"
2020.emnlp-main.582,P16-1144,0,0.060922,"Missing"
2020.emnlp-main.582,D14-1162,0,0.101657,"Missing"
2020.emnlp-main.582,D19-1005,0,0.114922,"guage representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Da"
2020.emnlp-main.582,N18-1202,0,0.0561499,"eam NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering"
2020.emnlp-main.582,D12-1071,0,0.0699835,"art on FEVER benchmark. It again demonstrates the effectiveness of our model. CorefBERT, which incorporates coreference information in distant-supervised pre-training, contributes to verify if the claim and evidence discuss about the same mentions, such as a person or an object. Coreference Resolution Coreference resolution aims to link referring expressions that evoke the same discourse entity. We examine models’ coreference resolution ability under the setting that all mentions have been detected. We evaluate models on several widely-used datasets, including GAP (Webster et al., 2018), DPR (Rahman and Ng, 2012), WSC (Levesque, 2011), Winogender (Rudinger et al., 2018) and 7 Details are in the appendix due to space limit. LA FEVER BERT Concat GEAR∗ SR-MRS+ KGAT (BERTBASE ) # KGAT (CorefBERTBASE ) 71.01 71.60 72.56 72.81 72.88 65.64 67.10 67.26 69.40 69.82 KGAT (BERTLARGE ) # KGAT (CorefBERTLARGE ) 73.61 74.37 70.24 70.86 KGAT (RoBERTaLARGE ) # KGAT (CorefRoBERTaLarge ) 74.07 75.96 70.38 72.30 ∗ Fact Extraction and Verification Fact extraction and verification aim to verify deliberately fabricated claims with trust-worthy corpora. We evaluate our model on a large-scale public fact verification dataset"
2020.emnlp-main.582,N18-2002,0,0.0595922,"Missing"
2020.emnlp-main.582,D13-1170,0,0.0111053,"Missing"
2020.emnlp-main.582,N18-1074,0,0.0800586,"Missing"
2020.emnlp-main.582,W18-5446,0,0.0194534,"ERT model significantly outperforms BERT-LM, which demonstrates that the intrinsic coreference resolution ability of CorefBERT has been enhanced by involving the mention reference prediction training task. Moreover, it achieves comparable performance with state-of-the-art baseline WikiCREM. Note that, WikiCREM is specially designed for sentence-level coreference resolution and is not suitable for other NLP tasks. On the contrary, the coreferential reasoning capability of CorefBERT can be transferred to other NLP tasks. 4.6 GLUE The Generalized Language Understanding Evaluation dataset (GLUE) (Wang et al., 2018) is designed to evaluate and analyze the performance of models across a diverse range of existing natural language understanding tasks. We evaluate CorefBERT on the main GLUE benchmark used in BERT. Implementation Details Following BERT’s setting, we add [CLS] token in front of the input sentences, and extract its representation on the top layer as the whole sentence or sentence pair’s representation for classification or regression. Results Table 6 shows the performance on GLUE. We notice that CorefBERT achieves comparable results to BERT. Though GLUE does not require much coreference resolut"
2020.emnlp-main.582,D18-1259,0,0.0747868,"Missing"
2020.emnlp-main.582,P19-1074,1,0.726892,"ured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large number of irrelevant entities, which first predicts whether two entities have a relationship and then predicts the specific relation. (4) HinBERT (Tang et al., 2020) proposes a hierarchical inference network to aggregate th"
2020.emnlp-main.582,C14-1220,0,0.111063,".70 53.93 54.54 53.92 55.60 56.27 56.96 BERTLARGE CorefBERTLARGE 56.51 56.82 58.70 59.01 56.01 56.40 58.31 58.83 RoBERTaLARGE CorefRoBERTaLARGE 57.19 57.35 59.40 59.43 57.74 57.90 60.06 60.25 Table 3: Results on DocRED measured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large n"
2020.emnlp-main.582,P19-1139,1,0.848241,"have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, wh"
2020.emnlp-main.582,I05-5002,0,\N,Missing
2020.emnlp-main.582,P07-1107,0,\N,Missing
2020.emnlp-main.582,N16-1030,0,\N,Missing
2020.emnlp-main.582,P16-1072,0,\N,Missing
2020.emnlp-main.582,W17-2623,0,\N,Missing
2020.emnlp-main.582,P17-1152,0,\N,Missing
2020.emnlp-main.582,P17-1019,0,\N,Missing
2020.emnlp-main.582,Q19-1026,0,\N,Missing
2020.emnlp-main.582,P19-1478,0,\N,Missing
2020.emnlp-main.582,P19-1085,1,\N,Missing
2020.emnlp-main.582,N18-2108,0,\N,Missing
2020.emnlp-main.582,N18-1101,0,\N,Missing
2020.emnlp-main.582,P19-1285,0,\N,Missing
2020.emnlp-main.582,D19-1258,0,\N,Missing
2020.emnlp-main.582,Q19-1040,0,\N,Missing
2020.findings-emnlp.216,P17-1171,0,0.0684769,"Missing"
2020.findings-emnlp.216,N19-1423,0,0.0402623,"adden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1) and then continuously train the BERT based model (Sec. 3.2) for the whole. 3.1 Similarly, for the evidence e of the retrieved abstract a"
2020.findings-emnlp.216,2020.acl-main.740,0,0.0450638,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-2.11,0,0.0638369,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-acl.9,0,0.061591,"Missing"
2020.findings-emnlp.216,2021.ccl-1.108,0,0.172535,"Missing"
2020.findings-emnlp.216,2020.acl-main.655,1,0.902622,"h as COVID-19. 2 Related Work Existing fact extraction and verification models usually employ a three-step pipeline system (Chen et al., 2017): document retrieval (abstract retrieval), sentence selection (rationale selection) and fact verification (Thorne et al., 2018; Wadden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section de"
2020.findings-emnlp.216,2020.emnlp-main.609,0,0.0601026,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-acl.17,0,0.0243354,"Missing"
2020.findings-emnlp.216,2020.emnlp-main.582,1,0.887037,"2017): document retrieval (abstract retrieval), sentence selection (rationale selection) and fact verification (Thorne et al., 2018; Wadden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1)"
2020.findings-emnlp.216,P19-1085,1,0.837738,"act verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1) and then continuously train the BERT based model (Sec. 3.2) for the whole. 3.1 Similarly, for the evidence e of the retrieved abstract a, we can get the representation H of"
2020.lrec-1.573,N19-1423,0,0.0139846,"Missing"
2020.lrec-1.573,P18-1082,0,0.0353788,"Missing"
2020.lrec-1.573,P19-3005,1,0.826157,"ing all types of training samples by integrating form information, then present a simple form-stressed weighting method in GPT-2 to strengthen the control to the form of generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University (Guo et al., 2019). Keywords: poetry generation, GPT-2, form control 1. Introduction Chinese poetry is a rich treasure in Chinese traditional culture. For thousands of years, poetry is always considered as the crystallization of human wisdom and erudition by Chinese people and deeply influences the Chinese history from the mental and cultural perspective. In general, a Chinese classical poem is a perfect combination of three aspects, i.e., form, sound, and meaning. Firstly, it must strictly obey a particular form which specifies the number of lines (i.e., sentences) in the poem and the number of characters in e"
2020.lrec-1.573,C16-1100,0,0.0486901,"Missing"
2020.lrec-1.573,K18-1024,1,0.89365,"Missing"
2020.lrec-1.573,D14-1074,0,0.0558955,"Missing"
2021.acl-demo.43,D18-1316,0,0.138032,"Missing"
2021.acl-demo.43,D18-2029,0,0.0717703,"Missing"
2021.acl-demo.43,N19-1423,0,0.0272006,"Missing"
2021.acl-demo.43,P18-2006,0,0.0300371,"Missing"
2021.acl-demo.43,2020.emnlp-main.498,0,0.0205402,"Missing"
2021.acl-demo.43,P82-1020,0,0.727813,"Missing"
2021.acl-demo.43,N18-1170,0,0.0509818,"Missing"
2021.acl-demo.43,D17-1215,0,0.0837115,"Missing"
2021.acl-demo.43,P02-1040,0,0.108913,"Missing"
2021.acl-demo.43,2020.emnlp-main.500,0,0.364903,"Missing"
2021.acl-demo.43,N19-1314,0,0.0266469,"Missing"
2021.acl-demo.43,2020.emnlp-demos.16,0,0.429524,"Missing"
2021.acl-demo.43,D19-1410,0,0.027452,"Missing"
2021.acl-demo.43,P19-1103,0,0.0607744,"Missing"
2021.acl-demo.43,P18-1079,0,0.0424559,"Missing"
2021.acl-demo.43,D13-1170,0,0.00553843,"Missing"
2021.acl-demo.43,D19-1221,0,0.0225437,"Missing"
2021.acl-demo.43,D19-3002,0,0.0416651,"Missing"
2021.acl-demo.43,2020.acl-main.540,1,0.79696,"Missing"
2021.acl-demo.43,P19-1559,0,0.0408221,"Missing"
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.369,W16-2206,0,0.0191707,"Figure 7 shows the results. We divide target tokens into four categories: 1. cPcA: correct prediction & correct alignment; 2. wPcA: wrong prediction & correct alignment; 3. cPwA: correct prediction & wrong alignment; 4. wPwA: wrong prediction & wrong alignment. Compared with other methods, M ASK -A LIGN significantly reduces the alignment errors caused by wrong predictions (wPwA). In addition, the number of the tokens with correct prediction but wrong Related Work Our work is closely related to unsupervised neural word alignment. While early unsupervised neural aligners (Tamura et al., 2014; Alkhouli et al., 2016; Peter et al., 2017) failed to outperform their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), recent studies have made significant progress by inducing alignments from NMT models (Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Our work differs from prior studies in that we design a novel self-supervised model that is capable of utilizing more target context than NMT-based models to generate high quality alignments without using guided training. Our work is also inspired by the success of conditional masked language models ("
2021.acl-long.369,D16-1162,0,0.0186491,"hosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural meth"
2021.acl-long.369,J93-2003,0,0.206312,"Missing"
2021.acl-long.369,2016.amta-researchers.10,0,0.0225016,"d Forward h4 ✕L L✕ Static-KV Attention EncoderLayer w1 + p1 w2 + p2 w3 + p3 w4 + p4 w1+p1 p1 w2+p2 p2 w3+p3 p3 w4+p4 p4 Figure 2: The architecture of M ASK -A LIGN. property of NMT systems (Sutskever et al., 2014), they only leverage part of the target context. This inevitably brings noisy alignments when the prediction is ambiguous. Consider the target sentence in Figure 1. When predicting “Tokyo”, an NMT system may generate “1968” because future context is not observed, leading to a wrong alignment link (“1968”, “Tokyo”). Second, they have to incorporate an additional guided alignment loss (Chen et al., 2016) to outperform GIZA++. This loss requires pseudo alignments of the full training data to guide the training of the model. Although these pseudo alignments can be utilized to partially alleviate the problem of ignoring future context, they are computationally expensive to obtain. In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely M ASK -A LIGN. Our model parallelly masks out each target token and recovers it conditioned on the source and other target tokens. Figure 1 shows an example where the target token “Tokyo” is masked out and re-pre"
2021.acl-long.369,2020.emnlp-main.42,1,0.838872,"1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4781–4791 August 1–6, 2021. ©2021 Association for Computational Li"
2021.acl-long.369,W19-4828,0,0.0171693,"as collectors. As a result of such effect, many target tokens (e.g., the 4783 vanilla attention not 1.0 true 1.0 falsch falsch leaky attention not 0.4 0.6 true 0.2 0.8 [NULL] falsch falsch 0.5 0.5 not true 0.2 0.4 0.4 [NULL] not true Figure 4: An illustrative example of the attention weights from two directional models using vanilla and leaky attention. Leaky attention provides a leak position “[NULL]” to collect extra attention weights. two “in”s in Figure 3) will be incorrectly aligned to the collectors according to the attention weights. This phenomenon has been studied in previous works (Clark et al., 2019; Kobayashi et al., 2020). Kobayashi et al. (2020) show that the norms of the value vectors for the collectors are usually small, making their influence on attention outputs actually limited. We conjecture that this phenomenon is due to the incapability of NMT-based aligners to deal with tokens that have no counterparts on the other side because there is no empty (NULL) token that is widely used in statistical aligners (Brown et al., 1993; Och and Ney, 2003). We propose to explicitly model the NULL token with an attention variant, namely leaky attention. As shown in Figure 4, when calculating"
2021.acl-long.369,W19-5201,0,0.0239257,"and Ly→x are NLL losses, α and β are hyperparameters. 2.3 Inference When extracting alignments, we compute an alignment score Sij for yi and xj as the harmonic mean ij ji of attention weights Wx→y and Wy→x from two directional models: Sij = ij ji 2 Wx→y Wy→x ij ji Wx→y + Wy→x (18) We use the harmonic mean because we assume a ij ji large Sij requires both Wx→y and Wy→x to be large. Word alignments can be induced from the alignment score matrix as follows:  1 if Sij ≥ τ Aij = (19) 0 otherwise (Zenkel et al., 2019, 2020) and used the preprocessing scripts from Zenkel et al. (2019)4 . Following Ding et al. (2019), we take the last 1000 sentences of the training data for these three datasets as validation sets. We used a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 40k merge operations. During training, we filtered out sentences with the length of 1 to ensure the validity of the masking process. 3.2 We implemented our model based on the Transformer architecture (Vaswani et al., 2017). The encoder consists of 6 standard Transformer encoder layers. The decoder is composed of 6 layers, each of which contains static-KV attention while only the last layer is equipped with le"
2021.acl-long.369,P17-1106,1,0.697574,"and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applie"
2021.acl-long.369,N13-1073,0,0.68427,"-art results.1 1 Source : Ich wurde 1968 in Tokio geboren . ___ in 1968 . Figure 1: An example of inducing an alignment link for target token “Tokyo” in M ASK -A LIGN. First, we mask out “Tokyo” and predict it with source and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and"
2021.acl-long.369,D19-1453,0,0.339124,"s in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4781–4791 August 1–"
2021.acl-long.369,D19-1633,0,0.0345485,"et al., 2017) failed to outperform their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), recent studies have made significant progress by inducing alignments from NMT models (Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Our work differs from prior studies in that we design a novel self-supervised model that is capable of utilizing more target context than NMT-based models to generate high quality alignments without using guided training. Our work is also inspired by the success of conditional masked language models (CMLMs) (Ghazvininejad et al., 2019), which have been applied to non-autoregressive machine translation. The CMLM can leverage both previous and future context on the target side for sequence-to-sequence tasks with the masking mechanism. Kasai et al. (2020) extend it with a disentangled context Transformer that predicts every target token conditioned on arbitrary context. By taking the characteristics of word alignment into consideration, we propose to use static-KV attention to achieve masking and aligning in parallel. To the best of our knowledge, this is the first work that incorporates a CMLM into alignment models. 4788 5 Co"
2021.acl-long.369,N18-2081,0,0.0337361,"Missing"
2021.acl-long.369,P00-1056,0,0.726347,"last layer is equipped with leaky attention. We set the embedding size to 512, the hidden size to 1024, and attention heads to 4. The input and output embeddings are shared for the decoder. We trained the models with a batch size of 36K tokens. We used early stopping based on the prediction accuracy on the validation sets. We tuned the hyperparameters via grid search on the ChineseEnglish validation set as it contains gold word alignments. In all of our experiments, we set λ = 0.05 (Eq. (16)), α = 5, β = 1 (Eq. (17)) and τ = 0.2 (Eq. (19)). The evaluation metric is Alignment Error Rate (AER) (Och and Ney, 2000). 3.3 3.1 Baselines We introduce the following unsupervised neural baselines besides two statistical baselines FASTA LIGN and GIZA++: where τ is a threshold. 3 Settings Experiments Datasets We conducted our experiments on four public datasets: German-English (De-En), English-French (En-Fr), Romanian-English (Ro-En) and ChineseEnglish (Zh-En). The Chinese-English training set is from the LDC corpus that consists of 1.2M sentence pairs. For validation and testing, we used the Chinese-English alignment dataset from Liu et al. (2005)3 , which contains 450 sentence pairs for validation and 450 for"
2021.acl-long.369,J03-1002,0,0.286326,"t https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annua"
2021.acl-long.369,E17-2056,0,0.0257607,"Missing"
2021.acl-long.369,N03-1017,0,0.200635,"Missing"
2021.acl-long.369,P19-1124,0,0.0313147,"ce for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages"
2021.acl-long.369,P16-1162,0,0.0936632,"ention weights Wx→y and Wy→x from two directional models: Sij = ij ji 2 Wx→y Wy→x ij ji Wx→y + Wy→x (18) We use the harmonic mean because we assume a ij ji large Sij requires both Wx→y and Wy→x to be large. Word alignments can be induced from the alignment score matrix as follows:  1 if Sij ≥ τ Aij = (19) 0 otherwise (Zenkel et al., 2019, 2020) and used the preprocessing scripts from Zenkel et al. (2019)4 . Following Ding et al. (2019), we take the last 1000 sentences of the training data for these three datasets as validation sets. We used a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 40k merge operations. During training, we filtered out sentences with the length of 1 to ensure the validity of the masking process. 3.2 We implemented our model based on the Transformer architecture (Vaswani et al., 2017). The encoder consists of 6 standard Transformer encoder layers. The decoder is composed of 6 layers, each of which contains static-KV attention while only the last layer is equipped with leaky attention. We set the embedding size to 512, the hidden size to 1024, and attention heads to 4. The input and output embeddings are shared for the decoder. We trained the models"
2021.acl-long.369,N06-1014,0,0.179926,"2, we will show that leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weights from two directions, we apply an agreement loss in the training process to improve the symmetry of our model, which has proven effective in statistical alignment models (Liang et al., 2006; Liu et al., 2015). Given a parallel sentence pair hx, yi, we can obtain the attention weights from two different directions, denoted as W x→y and W y→x . As alignment is bijective, W x→y is supposed to be equal to the transpose of W y→x . We encourage this kind of symmetry through an agreement loss:   La = MSE W x→y , W &gt; y→x (14) where MSE represents the mean squared error. For vanilla attention, La is hardly small because of the normalization constraint. As shown in Figure 4, due to the use of softmax activation, the minimal value of La is 0.25 for vanilla attention. Using leaky attentio"
2021.acl-long.369,D15-1210,1,0.831448,"leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weights from two directions, we apply an agreement loss in the training process to improve the symmetry of our model, which has proven effective in statistical alignment models (Liang et al., 2006; Liu et al., 2015). Given a parallel sentence pair hx, yi, we can obtain the attention weights from two different directions, denoted as W x→y and W y→x . As alignment is bijective, W x→y is supposed to be equal to the transpose of W y→x . We encourage this kind of symmetry through an agreement loss:   La = MSE W x→y , W &gt; y→x (14) where MSE represents the mean squared error. For vanilla attention, La is hardly small because of the normalization constraint. As shown in Figure 4, due to the use of softmax activation, the minimal value of La is 0.25 for vanilla attention. Using leaky attention, our approach can"
2021.acl-long.369,P05-1057,1,0.63668,"Eq. (19)). The evaluation metric is Alignment Error Rate (AER) (Och and Ney, 2000). 3.3 3.1 Baselines We introduce the following unsupervised neural baselines besides two statistical baselines FASTA LIGN and GIZA++: where τ is a threshold. 3 Settings Experiments Datasets We conducted our experiments on four public datasets: German-English (De-En), English-French (En-Fr), Romanian-English (Ro-En) and ChineseEnglish (Zh-En). The Chinese-English training set is from the LDC corpus that consists of 1.2M sentence pairs. For validation and testing, we used the Chinese-English alignment dataset from Liu et al. (2005)3 , which contains 450 sentence pairs for validation and 450 for testing. For other three language pairs, we followed the experimental setup in 3 http://nlp.csai.tsinghua.edu.cn/∼ly/systems/ TsinghuaAligner/TsinghuaAligner.html • NAIVE -ATT (Garg et al., 2019): a method that induces alignments from cross-attention weights of the best (usually penultimate) decoder layer in a vanilla Tranformer. • NAIVE -ATT-L AST: same as NAIVE -ATT except that only the last decoder layer performs cross-attention. • A DD SGD (Zenkel et al., 2019): a method that adds an extra alignment layer to repredict the to-"
2021.acl-long.369,P04-1066,0,0.166114,"uery inputs and hidden states for yi in the l-th layer, respectively. h0i is initialized with pi . We name this variant of attention the static-KV attention. By static-KV, we mean the keys and values are unchanged across different layers in our approach. Our model replaces all selfattention in the decoder with static-KV attention. Leaky Attention Extracting alignments from vanilla cross-attention often suffers from the high attention weights on some specific source tokens such as periods, [EOS], or other high frequency tokens (see Figure 3). This is similar to the “garbage collectors” effect (Moore, 2004) in statistical aligners, where a source token is aligned to too many target tokens. Hereinafter, we will refer to these tokens as collectors. As a result of such effect, many target tokens (e.g., the 4783 vanilla attention not 1.0 true 1.0 falsch falsch leaky attention not 0.4 0.6 true 0.2 0.8 [NULL] falsch falsch 0.5 0.5 not true 0.2 0.4 0.4 [NULL] not true Figure 4: An illustrative example of the attention weights from two directional models using vanilla and leaky attention. Leaky attention provides a leak position “[NULL]” to collect extra attention weights. two “in”s in Figure 3) will be"
2021.acl-long.369,P14-1138,0,0.0587573,"os such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variet"
2021.acl-long.369,P16-1008,1,0.809924,"ct it with source and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been"
2021.acl-long.369,W19-4808,0,0.0275838,"encoder outputs. 2 We use a normal distribution with a mean of 0 and a small 2 A similar attention implementation can be found in https://github.com/pytorch/fairseq/blob/master/fairseq/ modules/multihead attention.py. deviation to initialize kNULL and vNULL to ensure that their initial norms are rather small. When extracting alignments, we only consider the attention matrix without the leak position. Note that leaky attention is different from adding a special token in the source sequence, which will share the same high attention weights with the existing collector instead of calibrating it (Vig and Belinkov, 2019). Our parameterized method is more flexible than Leaky-Softmax (Sabour et al., 2017) which adds an extra dimension with the value of zero to the routing logits. In Section 2.2, we will show that leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weig"
2021.acl-long.369,P13-1017,0,0.0217326,"many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners l"
2021.acl-long.369,2020.acl-main.146,0,0.0709404,"forms cross-attention. • A DD SGD (Zenkel et al., 2019): a method that adds an extra alignment layer to repredict the to-be-aligned target token. • M TL -F ULLC (Garg et al., 2019): a method that supervises an attention head with symmetrized NAIVE -ATT alignments in a multitask learning framework. 4 4785 https://github.com/lilt/alignment-scripts Method Guided De-En En-Fr Ro-En Zh-En FAST-A LIGN (Dyer et al., 2013) GIZA++ (Och and Ney, 2003) N N 25.7 17.8 12.1 6.1 31.8 26.0 18.5 NAIVE -ATT (Garg et al., 2019) NAIVE -ATT-L AST A DD SGD (Zenkel et al., 2019) M TL -F ULLC (Garg et al., 2019) BAO (Zenkel et al., 2020) S HIFT-ATT (Chen et al., 2020) N N N N N N 31.9 28.4 21.2 20.2 17.9 17.9 18.5 17.7 10.0 7.7 8.4 6.6 32.9 32.4 27.6 26.0 24.1 23.9 28.9 26.4 20.2 M TL -F ULLC -GZ (Garg et al., 2019) BAO-G UIDED (Zenkel et al., 2020) S HIFT-AET (Chen et al., 2020) Y Y Y 16.0 16.3 15.4 4.6 5.0 4.7 23.1 23.4 21.2 17.2 M ASK -A LIGN N 14.4 4.4 19.5 13.8 Table 1: Alignment Error Rate (AER) scores on four datasets for different alignment methods. The lower AER, the better. “Guided” denotes whether the guided alignment loss is used during training. All results are symmetrized. We highlight the best results for each"
2021.acl-long.37,N19-1423,0,0.0864666,"Missing"
2021.acl-long.37,P82-1020,0,0.71241,"Missing"
2021.acl-long.37,N18-1170,0,0.0754064,"Missing"
2021.acl-long.37,2020.acl-main.249,0,0.347442,"Missing"
2021.acl-long.37,P14-5010,0,0.00882952,"Missing"
2021.acl-long.37,2021.acl-long.377,1,0.797483,"Missing"
2021.acl-long.37,N19-1144,0,0.0944813,"Missing"
2021.acl-long.37,P18-1079,0,0.052074,"Missing"
2021.acl-long.37,2020.acl-main.540,1,0.792848,"Missing"
2021.acl-long.37,P17-1099,0,0.081181,"Missing"
2021.acl-long.37,J05-2002,0,0.126771,"Missing"
2021.acl-long.37,D13-1170,0,0.0491743,"Missing"
2021.acl-long.377,D18-1316,0,0.0758004,"Missing"
2021.acl-long.377,Q18-1036,0,0.0296289,"Missing"
2021.acl-long.377,N19-1423,0,0.0570118,"Missing"
2021.acl-long.377,2020.coling-main.155,1,0.7636,"Missing"
2021.acl-long.377,2020.acl-main.249,0,0.164488,"Missing"
2021.acl-long.377,P19-1571,1,0.818064,"Missing"
2021.acl-long.377,2021.acl-long.37,1,0.797483,"Missing"
2021.acl-long.377,C86-1107,0,0.422633,"Missing"
2021.acl-long.377,P19-1103,0,0.12002,"Missing"
2021.acl-long.377,W17-1101,0,0.079212,"Missing"
2021.acl-long.377,D13-1170,0,0.0164523,"Missing"
2021.acl-long.377,N19-1144,0,0.0913317,"Missing"
2021.acl-long.377,2020.acl-main.540,1,0.876268,"Missing"
2021.acl-long.377,P19-1559,0,0.0476727,"Missing"
2021.acl-long.446,W17-4772,0,0.0214025,"ation strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De"
2021.acl-long.446,W17-4773,0,0.124791,"that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to i"
2021.acl-long.446,W19-5402,0,0.0160071,"ult of the separated cross-attention sublayer, and the parameters of separated cross-attentions to leverage each source are shared. Finally, a feedforward network is the last sublayer of a decoder layer. In this way, the decoder in our framework can better handle representations of multiple sources. 3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Boja"
2021.acl-long.446,2020.acl-main.747,0,0.0976652,"Missing"
2021.acl-long.446,P19-1292,0,0.0786151,"retrained selfattention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to documentlevel translation, our framework outperforms strong baselines significantly.1 1 B ERT- FUSED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inpu"
2021.acl-long.446,N19-1423,0,0.66529,"ion for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on"
2021.acl-long.446,W16-2360,0,0.0304034,"n tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficien"
2021.acl-long.446,D19-1634,1,0.930675,"ng TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De, Fr) — — 30.0 37.0 (De, Fr) M BART-T RANS (Liu et al., 2020) De mBART M BART-T RANS (Liu et al."
2021.acl-long.446,P19-1653,0,0.0244876,"ED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source"
2021.acl-long.446,W16-2378,0,0.0232202,"le representations of multiple sources. 3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Bojar et al., 2014), 3 There is little difference between the “parallel attention combination strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UA"
2021.acl-long.446,W18-6467,0,0.0195983,"018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De, Fr) — — 30.0 37.0 (De, Fr) M BART-T RANS (Liu et al., 2020) De mB"
2021.acl-long.446,W04-3250,0,0.0195956,"ains 250K tokens. We used minibatch sizes of 256, 1,024, 4,096, and 16,384 tokens for extremely low-, low-, medium-, and high-resource settings, respectively. We used the development set to tune the hyper-parameters and select the best model. In inference, the beamsize was set to 4. Please refer to Appendix A.1 for more details. Evaluation Metrics We used case-sensitive BLEU (multi-bleu.perl) and TER for automatic post-editing. For multi-source translation and document-level translation, S ACRE BLEU5 (Post, 2018) and METEOR6 was adopted for evaluation. We used the paired bootstrap resampling (Koehn, 2004) for statistical significance tests. 3.2 Main Results Table 2 shows the results on the automatic postediting task. Our framework outperforms previous methods without pretraining (i.e., F ORCEDATT, 4 A dual-source example can be obtained by matching two single-source examples. 5 The signature is “BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.14”. 6 https://www.cs.cmu.edu/˜alavie/ METEOR/ Variants None FFN adapter (Guo et al., 2020) Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf = 1) w/o CA = 1) = 2) = 3) #Para. BLEU 0M 100.7M 73.65 73.71 12.5M 16.8M 33.6M 50.4M"
2021.acl-long.446,2020.wmt-1.81,0,0.136117,"re challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Decoder Decoder Encoder Encoder A! B! C C Decoder Decoder Encoder Encoder Encoder C! A B Transfer Multi-source finetuning on labeled data for MSG C Transfer Dec"
2021.acl-long.446,2020.wmt-1.82,0,0.103471,"tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Decoder Decoder Encoder Encoder A! B! C C Decoder Decoder Encoder Encoder Encoder C! A B Transfer Multi-source finetuning on labeled data for MSG C"
2021.acl-long.446,2020.acl-main.703,0,0.544642,"ocument-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Deco"
2021.acl-long.446,N09-1041,0,0.395835,"ny sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation task"
2021.acl-long.446,W18-6326,0,0.0468509,"Missing"
2021.acl-long.446,2020.tacl-1.47,0,0.292357,"information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to documentlevel translation, our framework outperforms strong baselines significantly.1 1 B ERT- FUSED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be he"
2021.acl-long.446,2021.ccl-1.108,0,0.0777017,"Missing"
2021.acl-long.446,D17-1301,0,0.152128,"Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have pro"
2021.acl-long.446,N19-1313,0,0.0486746,"Missing"
2021.acl-long.446,L18-1004,0,0.105585,"3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Bojar et al., 2014), 3 There is little difference between the “parallel attention combination strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and M"
2021.acl-long.446,W18-6319,0,0.0122834,"and the number of heads are the same as mBART. We adopted the vocabulary of mBART, which contains 250K tokens. We used minibatch sizes of 256, 1,024, 4,096, and 16,384 tokens for extremely low-, low-, medium-, and high-resource settings, respectively. We used the development set to tune the hyper-parameters and select the best model. In inference, the beamsize was set to 4. Please refer to Appendix A.1 for more details. Evaluation Metrics We used case-sensitive BLEU (multi-bleu.perl) and TER for automatic post-editing. For multi-source translation and document-level translation, S ACRE BLEU5 (Post, 2018) and METEOR6 was adopted for evaluation. We used the paired bootstrap resampling (Koehn, 2004) for statistical significance tests. 3.2 Main Results Table 2 shows the results on the automatic postediting task. Our framework outperforms previous methods without pretraining (i.e., F ORCEDATT, 4 A dual-source example can be obtained by matching two single-source examples. 5 The signature is “BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.14”. 6 https://www.cs.cmu.edu/˜alavie/ METEOR/ Variants None FFN adapter (Guo et al., 2020) Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf Fine enc"
2021.acl-long.446,D19-1164,0,0.0419344,"Missing"
2021.acl-long.446,D18-1049,1,0.845923,"the fine encoder could capture finer cross-source information, which helps correct translation errors. 4 4.1 Related Work Multi-source Sequence Generation Multi-source sequence generation includes multisource translation (Zoph and Knight, 2016), automatic post-editing (Chatterjee et al., 2017), multidocument summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017), etc. For these tasks, researchers usually leverage multi-encoder architectures to achieve better performance (Zoph and Knight, 2016; Zhang et al., 2018; Huang et al., 2019). To address the data scarcity problem in MSG, some researchers generate pseudo corpora (Negri et al., 2018; Nishimura et al., 2020) to augment the corpus size while others try to make use of pretrained autoencoding models (e.g., BERT 5745 Normal Models M BART-T RANS M BART-T RANS D UAL BART TRICE (De) (Fr) Randomized Fr Randomized De BLEU METEOR BLEU METEOR BLEU METEOR 31.8 34.8 40.2 41.5 33.9 37.9 38.9 39.8 — — 11.3 13.5 — — 13.1 15.0 — — 24.9 23.0 — — 26.4 23.9 Table 8: Adversarial evaluation on the multi-source translation task. “Randomized Fr/De” denotes that the Fr/D"
2021.acl-long.446,N16-1004,0,0.41941,"Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take adv"
2021.emnlp-main.158,P17-1110,0,0.0129712,"ormalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network,"
2021.emnlp-main.158,N19-1423,0,0.15531,"ith Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models themselves. The redundant components get slight improvements that are not as much as the PLMs learning paradigm. 3 Method The overall process of our method is shown in algorithm 1: First, we train a word segmentation model and use it to generate segmentation results. Then, according to the segmentation results, the masked sentence is generated based on certain s"
2021.emnlp-main.158,P16-1039,0,0.0204645,"e Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into"
2021.emnlp-main.158,P17-2096,0,0.0779089,"rning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. I"
2021.emnlp-main.158,P15-1168,0,0.104971,"tistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite impor"
2021.emnlp-main.158,D15-1141,0,0.104748,"tistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite impor"
2021.emnlp-main.158,2020.emnlp-main.317,0,0.0194672,"o improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models themselves. The redundant components get slight improvements that are not as much as the PLMs learning paradigm. 3 Method The overall process of our method is shown in algorithm 1: First, we train a word segmentation model and use it to generate segmentation results. Then, according to the segmentation results, the masked sentence is generated based on certain strategies, and an MLM is trained with the masked sentence. Afterward, we mask the sentences in the training 2069 (t) Figure 2: The architecture of our model. D, D(t) and"
2021.emnlp-main.158,I05-3017,0,0.178287,"Missing"
2021.emnlp-main.158,2020.emnlp-main.318,1,0.713502,"s, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. In this work, we propose a self-supervised CWS approach to enhance the performance of CWS model. In addition, we also investigate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our model consists of two p"
2021.emnlp-main.158,I17-1019,0,0.028262,"Missing"
2021.emnlp-main.158,I08-4010,0,0.0784791,"Missing"
2021.emnlp-main.158,D18-1529,0,0.0255183,"Missing"
2021.emnlp-main.158,P14-1028,0,0.0259696,"methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (201"
2021.emnlp-main.158,C04-1081,0,0.229756,"ms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance th"
2021.emnlp-main.158,P16-1159,1,0.740175,"igate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our model consists of two parts: segmenter and predictor. We leverage the Transformer encoder as a word segmenter. We exploit the revised masked language model (MLM) as a predictor to improve the segmentation model. We generate masked sequences with respect to the segmentation results. Then we exploit MLM to predict the masked part and evaluate the quality of the segmentation based on the quality of the predictions. We leverage an improved version of minimum risk training (MRT) (Shen et al., 2016) to enhance the segmentation. Our contributions are as follows: • We propose a self-supervised method for CWS, which uses the predictions of revised MLM to assist the word segmentation model. • We present an improved version of MRT by adding regularization terms to boost the performance of the word segmentation model. • Experimental results show that our approach outperforms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chines"
2021.emnlp-main.158,2020.acl-main.734,0,0.0325726,"onal Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. In this work, we propose a self-supervised CWS approach to enhance the performance of CWS model. In addition, we also investigate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our mo"
2021.emnlp-main.158,I08-4017,0,0.30877,"ome neural methods try to incorporate external resources to achieve good performance for in-domain and cross-domain CWS (Zhou et al., 2017; Zhang et al., 2018). The previous methods fall into two categories: (1) the statistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetu"
2021.emnlp-main.158,D13-1061,0,0.0814908,"Missing"
2021.emnlp-main.158,D17-1079,0,0.0770808,"curately represent semantic information of Chinese NLP tasks. Besides, the length of the sentence is shortened by word segmentation. The shorter length of a sentence is effective for the deep learning method in some cases. Recently, good performance for CWS has already been achieved in large-scale annotated corpora as reported by related research (Huang and Zhao, 2007; Zhao et al., 2019). Most methods start with data-driven to improve the performance for CWS. For instance, some neural methods try to incorporate external resources to achieve good performance for in-domain and cross-domain CWS (Zhou et al., 2017; Zhang et al., 2018). The previous methods fall into two categories: (1) the statistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To"
2021.emnlp-main.158,I05-3027,0,0.0875816,"with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyp"
2021.emnlp-main.158,O03-4002,0,0.299583,"the predictions of revised MLM to assist the word segmentation model. • We present an improved version of MRT by adding regularization terms to boost the performance of the word segmentation model. • Experimental results show that our approach outperforms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao"
2021.emnlp-main.158,P17-1078,0,0.0163971,"influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models"
2021.emnlp-main.158,N19-1278,0,0.0324817,"Missing"
2021.emnlp-main.158,E14-1062,0,0.0698456,"Missing"
2021.emnlp-main.158,Y98-1020,0,0.762655,"Missing"
2021.emnlp-main.267,J09-1002,0,0.0616764,"ntroduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020)"
2021.emnlp-main.267,D16-1025,0,0.0268183,"ence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires mas"
2021.emnlp-main.267,2021.acl-long.369,1,0.705842,"process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designed for fine-tuning on various multilingual downstream tasks, while our work finetunes a multilingual"
2021.emnlp-main.267,2020.acl-main.747,0,0.03445,"ty Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patter"
2021.emnlp-main.267,D19-1633,0,0.0211716,"ctly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designe"
2021.emnlp-main.267,C18-1266,0,0.0214412,"t MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2"
2021.emnlp-main.267,W19-5406,0,0.131787,"1 0.233 0.263 0.265 SyntheticQE-MLM 0.386 0.368 0.318 0.309 0.204 0.284 0.181 0.208 Ours 0.504 0.463 0.381 0.383 0.242 0.435 0.318 0.338 Results of Ensemble Unsupervised Models SyntheticQE-MT Ensemble 0.488 0.428 0.360 0.339 0.212 0.246 0.274 0.297 SyntheticQE-MLM Ensemble 0.407 0.379 0.318 0.307 0.210 0.299 0.185 0.216 0.508 0.460 0.373 0.362 0.247 0.317 0.262 0.286 SyntheticQE-MT+MLM Ours Ensemble 0.518 0.462 0.395 0.385 0.248 0.453 0.318 0.359 Method Table 3: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2019 sentence- and word-level development and test sets. “*”: we followed Kepler et al. (2019) and implemented the supervised models by fine-tuning the multilingual BERT (Devlin et al., 2019). For the implementation details of the supervised models, please refer to Appendix A.4. Dataset SMT NMT Method SyntheticQE-MT SyntheticQE-MLM Ours SyntheticQE-MT SyntheticQE-MLM Ours Sent 0.469 0.416 0.560 0.526 0.424 0.590 Word 0.417 0.298 0.425 0.444 0.320 0.476 Table 4: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2018 En-Lv test sets. En-Lv SMT NMT uMQE 0.385 0.550 0.176 0.221 BERTScore BERTScore++ 0.213 0.155 0.540 0.580 NMT-QE Ours 0.560 0.590 Method En-De En-Ru NMT NMT 0.375 0"
2021.emnlp-main.267,W17-4763,0,0.0200064,"counterpart without MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language model"
2021.emnlp-main.267,W19-5407,0,0.103634,"o evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multi"
2021.emnlp-main.267,2020.wmt-1.122,0,0.151216,"Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrain"
2021.emnlp-main.267,2020.coling-main.385,0,0.10135,"been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tua"
2021.emnlp-main.267,P16-1162,0,0.110972,"Missing"
2021.emnlp-main.267,2011.eamt-1.12,0,0.0344057,"ni et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has rec"
2021.emnlp-main.267,P13-4014,0,0.0716555,"Missing"
2021.emnlp-main.267,P07-2045,0,0.00803237,"16 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the training data have quality annotations, we did not use these annotations in the experiments. 3 https://opus.nlpl.eu/ 3325 Year Language Pair Domain 2018 En-Lv Biomedical 2019 En-De En-Ru IT IT System SMT NMT NMT NMT Train 313K 365K 217K Dev 1.00K 1.00K 1.00K 1.00K Test 1.32K 1.45K 1.02K 1.02K Table 2: Statistics of the training, development and test datasets in our experiments. Baselines Implementation Details We mainly compared our method with SyntheticQE (Tuan"
2021.emnlp-main.267,2020.wmt-1.118,0,0.0365847,"t al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tuan et al. (2021) train unsupervised QE mode"
2021.emnlp-main.267,D15-1166,0,0.0610192,"lop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for posted"
2021.emnlp-main.267,2020.amta-research.11,1,0.822326,"Missing"
2021.emnlp-main.267,2020.wmt-1.119,0,0.0456977,"Missing"
2021.emnlp-main.267,tiedemann-2012-parallel,0,0.0352985,"nts Setup Data and Preprocessing We mainly conducted experiments on the WMT 2019 QE tasks, which consist of tasks in two different language pairs (En-De and En-Ru). Both tasks are in the IT domain. Since our experiments were conducted in an unsupervised setting, we used parallel corpora without quality annotations as training data2 . Specifically, for En-De, we used indomain parallel data from various sources, including the training data from the WMT 2016 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the traini"
2021.emnlp-main.267,2021.eacl-main.50,0,0.253522,"evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, which can be applied for both sentence- and word"
2021.emnlp-main.267,2020.wmt-1.125,0,0.573067,"QE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, whic"
2021.emnlp-main.267,2020.emnlp-demos.6,0,0.0767023,"Missing"
2021.emnlp-main.267,N19-1242,0,0.0271217,"o be finetuned on labeled training data, while our work conducts unsupervised QE by directly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language m"
2021.emnlp-main.267,2020.emnlp-main.205,0,0.0577838,"Missing"
2021.emnlp-main.267,2021.acl-long.24,0,0.0846356,"Missing"
2021.emnlp-main.366,P19-1279,0,0.0401504,"Missing"
2021.emnlp-main.366,P17-1171,0,0.0267983,"The second challenge is that RE models need to synthesize all information in multiple text paths to obtain the final relation. Open Setting. This setting fully tests the ability of RE in the wild. Given a target entity pair, models need to first retrieve relevant documents for the entity pair from full English Wikipedia corpus (5, 882, 234 documents in total, 3, 646 reasoning text path candidates for each entity pair on average), then perform cross-document reasoning with the retrieved documents to predict the relation. Compared with natural language queries in open domain question answering (Chen et al., 2017), the sparse query information in entity pairs presents unique challenges to document retrieval ability. The second challenge comes from both the quadratic number of potential paths (efficiency), and the finegrained influence of document retrieval on the extraction of relations (effectiveness). 4 Data Analysis In this section, we present data analysis of CodRED, including data statistics, required abilities in our dataset, and cross-document relation instances. Data Statistics. CodRED enjoys diversity in open 4455 Here we use entity names to predict the relations, since we find it can effectiv"
2021.emnlp-main.366,N19-1423,0,0.0353152,"Missing"
2021.emnlp-main.366,Q17-1008,0,0.0179577,"g on shallow correlation between relations and entity names. In this sense, CodRED provides a more reasonable benchmark for knowledge acquisition systems. End-to-end Ent. Ctx. X AUC F1 P@500 10.46 21.19 21.70 X X X 12.72 17.45 25.46 30.54 25.40 30.60 X X X 41.76 47.94 47.33 51.26 58.60 62.80 Table 7: Ablation results on entity names (Ent.) and context (Ctx.). Han et al., 2018; Mesquita et al., 2019) or distant supervision (Riedel et al., 2010; Zhang et al., 2017; Elsahar et al., 2018). (2) Cross-sentence RE datasets focus on extracting cross-sentence relations from documents (Li et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Yao et al., 2019) or dialogues (Yu et al., 2020). Notably, NIST TAC SM-KBP 2019 Track6 aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent"
2021.emnlp-main.366,2020.acl-main.444,0,0.0784142,"s of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., single sentences or gies, where text paths are ranked by the heuristic documents, CodRED presents unique challenges in scores. Specifically, the score of a text path (dh , dt ) document retrieval and cross-document reasoning. is given by: (1) entity count: multiplication of the Intra- and Cross-Document Reasoning. Cross- occurrence number of h in dh and the occurrence number of t in dt , ("
2021.emnlp-main.366,P18-1199,0,0.0127727,", which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inev"
2021.emnlp-main.366,E17-1110,0,0.164516,"an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inevitably limits the coverage of knowledge acqui- and Merneptah is sibling. sition. According to our statistics on Wikipedia Although several datasets have been proposed ∗ for inv"
2021.emnlp-main.366,D15-1203,0,0.0180787,"all text paths. gated representation x. The aggregated entity pair representation x is then fed into a fully connected layer followed by a softmax layer to obtain the distribution of the relation between the entity pair. Besides the entity-level supervision, we also incorporate path-level supervision using an auxiliary classification task, where models need to predict the relation expressed in each path based on pi . 6 Experiments In this section, we assess the challenges of CodRED in both closed and open benchmark settings. 6.1 Evaluation Metrics In closed setting, following previous works (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using aggregate precision-recall curves, and report the area under curve (AUC), the maximum F1 on the curve and Precision@K (P@K). In open setting, we first retrieve relevant documents (top 16 paths) from full Wikipedia corpus, and then use the models trained in the closed setting to infer the relation between the entity pair. We report the mean average precision (MAP), Recall@K (R@K) and mean reciprocal rank (MRR) to show the performance of document retrieval. 5.2.2 End-to-end Model 6.2 Overall Results Despite their simplicity, pipeline models usuall"
2021.emnlp-main.366,C14-1220,0,0.0791534,"Missing"
2021.emnlp-main.366,D17-1004,0,0.157455,"words, presenting challenges for model- a document set D (i.e., full Wikipedia corpus), we ing long text in both efficiency and effectiveness. first find relevant documents to extract their relaWe refer readers to the appendix for more details. tion. Due to the large number of possible docuRequired Abilities. We compare required abili- ments containing h and t respectively, we explore ties of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., sin"
2021.emnlp-main.366,D12-1110,0,0.0561042,"re of each phase, but also 1 Introduction the intrinsic inter-dependence among the phases. Fig. 1 shows an example for cross-doc RE, in Relation extraction (RE), which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or"
2021.emnlp-main.366,Q18-1021,0,0.0152335,"aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent years. Several datasets have been constructed including cross-document question answering (Yang et al., 2018; Welbl et al., 2018) and cross-document summarization (Over and Yen, 2004; Owczarzak and Dang, 2011; Fabbri et al., 2019). In comparison with existing datasets, our dataset is tailored for the task of RE with fine-grained path and evidence annotations, and investigates the more open and challenging scenario of knowledge acquisition. 8 Conclusion A variety of RE datasets have been constructed to promote the development of RE systems in recent years, which can be categorized in two main categories: (1) Sentence-level RE datasets focus on extracting relations on sentence-level, where the composing entities of a rela"
2021.emnlp-main.374,D18-1316,0,0.0347819,"Missing"
2021.emnlp-main.374,P19-1601,0,0.0365553,"Missing"
2021.emnlp-main.374,W18-5102,0,0.0571332,"Missing"
2021.emnlp-main.374,N19-1423,0,0.0481844,"Missing"
2021.emnlp-main.374,J93-3002,0,0.78388,"Missing"
2021.emnlp-main.374,P18-2006,0,0.0385854,"Missing"
2021.emnlp-main.374,N19-1165,0,0.0558301,"Missing"
2021.emnlp-main.374,N19-1320,0,0.0389048,"Missing"
2021.emnlp-main.374,N18-1170,0,0.0263816,"Missing"
2021.emnlp-main.374,D17-1215,0,0.055642,"Missing"
2021.emnlp-main.374,P19-1041,0,0.0352154,"Missing"
2021.emnlp-main.374,2020.emnlp-main.55,0,0.0776638,"Missing"
2021.emnlp-main.374,N18-1169,0,0.0609159,"Missing"
2021.emnlp-main.374,2020.emnlp-main.500,0,0.0267556,"Missing"
2021.emnlp-main.374,P18-1080,0,0.0532628,"Missing"
2021.emnlp-main.374,2021.acl-long.37,1,0.7899,"Missing"
2021.emnlp-main.374,2021.acl-long.377,1,0.769151,"Missing"
2021.emnlp-main.374,2020.acl-main.249,0,0.0691969,"Missing"
2021.emnlp-main.374,N18-1012,0,0.0600824,"Missing"
2021.emnlp-main.374,W16-5603,0,0.060911,"Missing"
2021.emnlp-main.374,D19-1410,0,0.0412958,"Missing"
2021.emnlp-main.374,P19-1103,0,0.0413712,"Missing"
2021.emnlp-main.374,D13-1170,0,0.00581873,"Missing"
2021.emnlp-main.374,D19-1322,0,0.0275882,"Missing"
2021.emnlp-main.374,2020.emnlp-main.495,0,0.0673248,"Missing"
2021.emnlp-main.374,2020.acl-main.540,1,0.841864,"Missing"
2021.emnlp-main.374,N18-1138,0,0.0603791,"Missing"
2021.emnlp-main.374,2020.emnlp-main.417,0,0.0611894,"Missing"
2021.emnlp-main.752,2021.acl-long.37,1,0.815416,"Missing"
2021.emnlp-main.752,2021.acl-long.377,1,0.831006,"Missing"
2021.emnlp-main.752,D13-1170,0,0.00571466,"Missing"
2021.emnlp-main.752,N19-1144,0,0.0601721,"Missing"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.findings-acl.137,N19-1423,0,0.0622221,"Missing"
2021.findings-acl.137,2020.emnlp-main.498,0,0.188847,"Missing"
2021.findings-acl.137,D19-1419,0,0.0442042,"Missing"
2021.findings-acl.137,D19-1423,0,0.13923,"Missing"
2021.findings-acl.137,P19-1561,0,0.0161835,"atus quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"
2021.findings-acl.137,P19-1103,0,0.365226,"ts at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two majo"
2021.findings-acl.137,2021.findings-acl.56,1,0.33783,"sents the resultant decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmen"
2021.findings-acl.137,2020.acl-main.245,0,0.341757,"cent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 Aug"
2021.findings-acl.137,D13-1170,0,0.048696,"Missing"
2021.findings-acl.137,2020.acl-main.263,0,0.275487,"et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example"
2021.findings-acl.137,2020.emnlp-main.500,0,0.399236,"nt decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), whi"
2021.findings-acl.137,2020.emnlp-main.495,0,0.0772852,"e robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial"
2021.findings-acl.137,2020.emnlp-main.417,0,0.0523825,"Missing"
2021.findings-acl.137,2020.acl-main.310,0,0.0965234,"et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space"
2021.findings-acl.137,2020.coling-main.4,1,0.778555,"n the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©20"
2021.findings-acl.137,2020.acl-main.540,1,0.905788,"Missing"
2021.findings-acl.137,P11-1015,0,0.0143693,"Missing"
2021.findings-acl.137,2020.acl-main.590,0,0.0456194,"Missing"
2021.findings-acl.137,2020.acl-main.319,0,0.170701,"ted in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space of word-substitution attacks consists"
2021.findings-acl.153,2020.emnlp-demos.22,0,0.0410419,"ould achieve higher scores on these metrics. We use the “Filtered” setting for all the evaluations, which filters out other true answers from the prediction results to get the final rank for each test case. 5.4 Hyperparameter Settings According to Ruffinelli et al. (2020), performances of KGE methods are sensitive to hyperparameters. Following them, we run 30 quasi-random trails for all models from predefined hyperparameter spaces. We list the hyperparameter spaces we use in Appendix A.5. We run all trails for 100 epochs. For all single-view KE methods, we use the implementations from LibKGE (Broscheit et al., 2020), which utilizes the Ax framework to perform quasi-random hyperparameter search. For AttH, we use the implementation from the authors1 . For JOIE, we use the implementation from the authors2 . We use TransE as the backend and adopt the suggested hyperparameter space from the paper. 6 Experimental Results In this section, we provide the experimental results and further propose several future directions. 6.1 Knowledge Abstraction The results of knowledge abstraction are shown in Table 4. From the results, we can see that AttH has 1 2 https://github.com/HazyResearch/KGEmb https://github.com/Junhe"
2021.findings-acl.153,2020.acl-main.617,0,0.0496769,"Missing"
2021.findings-acl.153,P15-1067,0,0.0372355,") and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations. Thirdly, some datasets provide the full concept graphs (Hao et al., 2019), but both the scale and the depth of the concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type informat"
2021.findings-acl.153,D18-1222,1,0.91582,"EC-KG is shown in Figure 1. During the last decade, there are massive works focusing on learning representations for KGs such as TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and TuckER (Balaˇzevi´c et al., 2019). Though they have achieved promising results on knowledge graph completion, most of them focus on a single graph, especially the entity graph. Beyond modeling a single graph of KGs, recent studies demonstrate that jointly modeling the two graphs in the EC-KG can improve the understanding of each one (Xie et al., 2016; Moon et al., 2017; Lv et al., 2018; Hao et al., 2019). They also propose 1751 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1751–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics several tasks on the EC-KG, such as link prediction and entity typing. These tasks focus on partial aspects of knowledge abstraction, concretization, and completion, which are essential abilities for humans to recognize the world and acquire knowledge. For example, in entity typing, a model may link the entity “Da Vinci” to the concept “painter” which reflects the model’s abstraction ability. Ho"
2021.findings-acl.153,N18-2053,0,0.0200075,"concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type information into KE methods to help the completion of entity graphs. ETE (Moon 1752 et al., 2017) further conducts entity typing, which can be seen as a simplified version of our knowledge abstraction task. Though types of entities can be"
2021.findings-acl.153,2020.emnlp-main.669,0,0.0266774,"on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations."
2021.findings-acl.153,D15-1174,0,0.541521,"archies perform better than general KGE models on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide"
2021.findings-acl.160,W19-5206,0,0.0730147,"tagged AlterBT AlterBT-tagged NIST06 45.94 43.89 46.79 49.07+† 49.40+† NIST02 45.82 44.79 47.11 48.77+† 49.04+† NIST03 45.35 44.40 46.49 48.36+† 48.37+† NIST04 46.88 46.24 47.73 49.51+† 49.10+† NIST05 45.43 45.45 47.17 49.94+† 49.64+† NIST08 36.98 36.45 38.41 40.95+† 40.56+† All 44.40 43.57 45.47 47.68+† 47.49+† Table 1: BLEU scores on the NIST Chinese-English task with 10M additional synthetic corpus. “Base” means only authentic data is used. “BT” corresponds to the back-translation method (Sennrich et al., 2016a). “BT-tagged” corresponds to the tagged BT technique proposed by Caswell et al. (2019). “AlterBT” means alternated training on authentic data and synthetic data using “BT” in each alternation. “AlterBT-tagged” means alternated training on authentic data and synthetic data using “BT-tagged” in each alternation. “+” means significantly better than BT (p &lt; 0.01).“†” means significantly better than BT-tagged (p &lt; 0.01). Scale Base BT BT-tagged AlterBT AlterBT-tagged 1M 34.16 37.36 37.65 38.20+† 37.98+† 4.5M 34.16 36.30 37.42 38.53+† 39.19+† Table 2: BLEU scores on the IWSLT14 GermanEnglish task with 1M and 4.5M additional synthetic corpus. “+” means significantly better than BT (p"
2021.findings-acl.160,P16-1185,1,0.556489,"lps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fac"
2021.findings-acl.160,D18-1045,0,0.0746995,"ely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods ha"
2021.findings-acl.160,P17-2090,0,0.026019,"T model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely"
2021.findings-acl.160,W18-2703,0,0.0617932,"(t) and θˆa successively, which from θˆs to θˆa , θˆs finally leads to a better point with a higher BLEU score. enables authentic data to further redirect the model into a better point with a higher BLEU score. 4 Related Work Our work is based on back-translation (BT), an approach to leverage monolingual data by an additional target-to-source system. BT was proved to be effective in neural machine translation (NMT) systems (Sennrich et al., 2016a). Despite its effectiveness, BT is limited by the accuracy of synthetic data. Noise and translation errors hinder the boosting of model performance (Hoang et al., 2018). The 1831 negative results become more evident when more synthetic data is mixed into training data (Caswell et al., 2019; Wu et al., 2019). Considerable studies have focused on the accuracy problem in synthetic data and further extended back-translation. Imamura et al. (2018) demonstrate that generating source sentences via sampling increases the diversity of synthetic data and benefits the BT system. Edunov et al. (2018) further propose a noisy beam search method to generate more diversified source-side data. Caswell et al. (2019) add a reserved token to synthetic source-side sentences in o"
2021.findings-acl.160,W18-2707,0,0.0762232,"often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed the conventional BT approach, NMT models still suffer from a performance degradation as the size of synthetic data keeps increasing. Hence, how to better take advantage of limited authentic data and abundant synthetic data still remains a grand challenge. In this work, we propose alternated training with synthetic and authentic data for neural machine translation. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent t"
2021.findings-acl.160,P07-2045,0,0.016436,"c bilingual corpus and 10M English-side sentences from WMT17 Chinese-English training set as our monolingual corpus for back-translation. NIST06 was used as the validation set. We use NIST02, 03, 04, 05 and 08 datasets as test sets. For the German-English task, we selected the dataset of IWSLT14 German-English task, which contains 16k parallel sentence pairs for training. We further extracted 4.5M English-side sentences from WMT14 German-English training set as monolingual dataset. We segmented Chinese sentences by THULAC (Sun et al., 2016) and tokenized English and German sentences by Moses (Koehn et al., 2007). The vocabulary was built by Byte Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We used Transformer (Vaswani et al., 2017) implemented in THUMT (Tan et al., 2020) with standard hyperparameters as a base model. We used Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 with the maximum learning rate = 7 × 10−4 . We applied early-stopping to verify convergence of each single S/A-step. If the validation BLEU failed ti exceed the highest score during the certain S/A-step after 10K training iterations, we consider the model converged and alternated"
2021.findings-acl.160,P16-1009,0,0.316106,"visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and"
2021.findings-acl.160,D19-1073,1,0.896517,"target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed the conventional BT approach, NMT models still suffer from a performance degradation as the size of synthetic data keeps increasing. He"
2021.findings-acl.160,D19-1430,0,0.0728883,"ing author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed t"
2021.findings-acl.160,D16-1160,0,0.0201458,"that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations"
2021.findings-acl.160,P16-1162,0,0.687765,"visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and"
2021.findings-acl.160,2020.amta-research.11,1,0.433437,"03, 04, 05 and 08 datasets as test sets. For the German-English task, we selected the dataset of IWSLT14 German-English task, which contains 16k parallel sentence pairs for training. We further extracted 4.5M English-side sentences from WMT14 German-English training set as monolingual dataset. We segmented Chinese sentences by THULAC (Sun et al., 2016) and tokenized English and German sentences by Moses (Koehn et al., 2007). The vocabulary was built by Byte Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We used Transformer (Vaswani et al., 2017) implemented in THUMT (Tan et al., 2020) with standard hyperparameters as a base model. We used Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 with the maximum learning rate = 7 × 10−4 . We applied early-stopping to verify convergence of each single S/A-step. If the validation BLEU failed ti exceed the highest score during the certain S/A-step after 10K training iterations, we consider the model converged and alternated the training set. For the whole training process, we set the maximum training iterations as 250k for ChineseEnglish task and 150k for German-English task. 3.2 Results Figure 1 shows the co"
2021.findings-acl.411,P18-1230,0,0.0607444,"Missing"
2021.findings-acl.411,2020.emnlp-main.369,0,0.0477643,"Missing"
2021.findings-acl.411,J93-2004,0,0.0776744,"Missing"
2021.findings-acl.411,P19-1568,0,0.0581781,"Missing"
2021.findings-acl.411,P17-1187,1,0.819087,"Missing"
2021.findings-acl.411,N19-1222,0,0.0610183,"Missing"
2021.findings-acl.411,P19-1571,1,0.892498,"Missing"
2021.findings-acl.411,D18-1033,1,0.891087,"Missing"
2021.findings-acl.411,D17-1024,0,0.0481256,"Missing"
2021.findings-acl.411,C86-1107,0,0.653547,"Missing"
2021.findings-acl.411,2021.acl-long.377,1,0.808599,"Missing"
2021.findings-acl.411,2020.acl-demos.14,0,0.0408862,"Missing"
2021.findings-acl.411,N18-1101,0,0.022265,"Missing"
2021.findings-acl.411,P19-1103,0,0.0514168,"Missing"
2021.findings-acl.411,D13-1170,0,0.00872208,"Missing"
2021.findings-acl.411,2020.acl-main.540,1,0.833938,"Missing"
2021.findings-acl.411,D15-1031,0,0.0699933,"Missing"
2021.findings-acl.422,W19-5206,0,0.108278,"e bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect"
2021.findings-acl.422,C18-1111,0,0.0181722,"performance drop of backtranslation, as well as the different performances between tagged forward-translation and tagged back-translation (Caswell et al., 2019). In addition, we show that our approach is also beneficial for data augmentation approaches, which can further improve the translation performance over both back-translation and forward-translation. 5.3 Domain Adaptation Since high-quality and domain-specific parallel data is usually scarce or even unavailable, domain adaptation approaches are generally employed for translation in low-resource domains by leveraging out-of-domain data (Chu and Wang, 2018). Languages can be also regarded as different domains, since articles in different languages cover different topics (Bogoychev and Sennrich, 2019). Starting from this intuition, we distinguish examples with different original languages with tagging (Aharoni et al., 2019) and fine-tuning (Luong and Manning, 2015), which are commonly-used in domain adaptation and multi-lingual translation tasks. Our work also benefits domain adaptation: distinguishing original languages in general domain 4785 data consistently improves translation performance of NMT models in several specific domains (Table 16 i"
2021.findings-acl.422,D18-1045,0,0.0469535,"this section, we aim to provide some insights where monolingual data augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6"
2021.findings-acl.422,N19-1388,0,0.0767514,"validation sets. the monolingual data augmentation scenario (Section 4.2), where the language coverage bias problem is more severe due to the newly introduced dataset in source or target language. 4.1 Bilingual Data Utilization In this section, we aim to improve bilingual data utilization through explicitly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total trai"
2021.findings-acl.422,P07-2045,0,0.0080886,"Missing"
2021.findings-acl.422,2021.eacl-main.130,0,0.0136043,"(2020) show that the source-side translationese texts can potentially lead to distortions in automatic and human evaluations. Accordingly, the WMT competition starts to use only source-original test sets for most translation directions since 2019. Our study reconfirms the necessity of distinguishing the source- and target-original examples and takes one step further to distinguish examples in training data. Complementary to previous works, we investigate the effect of language coverage bias on machine translation, which is related to the content bias rather than the language style difference. Shen et al. (2021) also reveal the context mismatch between texts from different original languages. To alleviate this problem, they proposed to combine back- and forward-translation by introducing additional monolingual data, while we focus on better exploiting bilingual data by distinguishing the original languages, which is also helpful for back- and forward-translation. Lembersky et al. (2011, 2012) propose to adapt machine translation systems to generate texts that are more similar to human-translations, while Riley et al. (2020) propose to model human-translated texts and original texts as separate langua"
2021.findings-acl.422,2020.wmt-1.30,0,0.0343531,"Missing"
2021.findings-acl.422,D11-1034,0,0.080019,"y crabs are the most well-known image spokesmen of Bacheng. Both It is the best-known icon of Bacheng. Table 5: An example of the outputs of NMT models trained on different sets of data. Using the targetoriginal data tends to omit content words. naturally arises: can target-original bilingual data improve the fluency of NMT models? To answer the above question, we measure the fluency of outputs with language models trained on the monolingual data as described in Section 2. Previous study finds that different perplexities could be caused by specific contents rather than structural differences (Lembersky et al., 2011). Specifically, some source-original contents are of low frequency in the target-language monolingual data (e.g., “Bacheng” in Table 5), thus the language model trained on the target-language monolingual data tends to assign higher perplexities to outputs containing more source-original content words. To rule out this possibility and check whether the outputs are structurally different, we follow Lembersky et al. (2011) to abstract away from the contentspecific features of the outputs to measure their fluency at the syntactic level. Table 6 shows the results. Although using only the source-ori"
2021.findings-acl.422,W19-6627,0,0.115311,"t the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,E12-1026,0,0.0480962,"Missing"
2021.findings-acl.422,2015.iwslt-evaluation.11,0,0.170579,"ly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total training steps of the pre-training and fine-tuning stages are the same as the baseline. Analysis Recent studies have shown that generating human-translation like texts as opposed to original texts can improve the BLEU score (Riley et al., 2020). To validate that the improvement is partially from alleviating the c"
2021.findings-acl.422,2020.acl-main.532,0,0.193192,"the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted"
2021.findings-acl.422,W19-5208,0,0.392807,"plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,2020.wmt-1.25,0,0.0648905,"Missing"
2021.findings-acl.422,N19-4007,0,0.0685171,"ly, if the language coverage bias exists, the vocabulary distributions of the source- and target-original data should differ greatly from each other, since the covered issues tend to have different frequencies between them (D’Alessio and Allen, 2000). We use the Jensen-Shannon (JS) divergence (Lin, 1991) to measure the difference between two vocabulary En⇒Zh En⇐Zh Origin noun verb adj noun verb adj Target Source Both 67.6 69.7 69.9 52.0 54.0 54.1 64.3 66.2 65.9 53.8 61.8 61.2 38.0 44.1 43.8 57.0 63.9 63.4 Table 4: Translation adequacy of different types of content words measured by F-measure (Neubig et al., 2019). The results are reported on the validation sets. distributions p and q:   p+q p+q 1 KL(p|| ) + KL(q|| ) , JS (p||q) = 2 2 2 where KL(·||·) is the KL divergence (Kullback and Leibler, 1951) of two distributions. Table 2 shows the JS divergence of the vocabulary distributions between the source- and targetoriginal data. We also divide the words into content words and functions words based on their POS tags, since content words are more related to the language coverage bias, while the function words are more related to the stylistic and structural differences between the translationese and or"
2021.findings-acl.422,W19-5333,0,0.0130493,"each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1"
2021.findings-acl.422,W18-6301,0,0.0117812,". For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Se"
2021.findings-acl.422,W18-6319,0,0.0201539,"nce pairs for En⇔De, En⇔Zh, and En⇔Ja, respectively. We used the monolingual data that is publicly available in WMT20 to train the proposed original language detection model (Section 3.1) and data augmentation (Section 4.2). The Appendix lists details about the data preprocessing. For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original la"
2021.findings-acl.422,2020.findings-emnlp.276,0,0.057633,"Missing"
2021.findings-acl.422,2020.acl-main.691,0,0.33119,"followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1 Detecting Original Languages Detection Method Intuitively, we use a largescale monolingual dataset to estimate the distribution of the contents covered by each language. For each training example, we compare its similarities 4779 WMT20 WMT20 En=&gt;Zh En=&gt;Zh En-Zh En-Ja En-De FT Ours 83.6 84.4 83.7 91.5 86.6 88.7 WMT20 WMT20 Zh=&gt;En Zh=&gt;En 37 37 28 28 36.5 36.5 32 32 Table 1: F1 scores of detecting original languages in the test sets. “FT” denotes the forward translation classifier proposed by Riley et al. (2020). 33.2 33.2 31.0 31.0 P (Ds )P (hx, yi|Ds ) , P (hx, yi) P (Dt )P (hx, yi|Dt ) P"
2021.findings-acl.422,P16-1009,0,0.0613289,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.422,P16-1162,0,0.0390796,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-emnlp.263,D14-1067,0,0.0241671,"embed the high-dimension and of different negative instances differs greatly. Reusually discrete features of entities/relations into calling the training loss in previous KRL methods a low-dimension vector space. These learned rep- (such as the margin-based (Chechik et al., 2009) resentations, by encoding the underlying semantic and logistic-based (Gutmann and Hyvärinen, 2010) relationships among entities/relations, are able to loss), it coequally compares each positive instance facilitate various downstream tasks, such as ques- with only one negative instance at each training tion answering (Bordes et al., 2014), recommen- iteration. In this way, it not only restrains the indation (Wang et al., 2019b) and relation extrac- teraction between positive-negative instances, but tion (Bastos et al., 2021) to name some. As a basic also overlooks the different weights of different research topic, KRL has always attracted many at- negative samples to each positive instance, which, tentions of researchers in relevant domains. in general, would lead to bias and slow training ∗ Corresponding author. convergence. Taking the triple (Kobe Bryant, na3061 Findings of the Association for Computational Linguistics: EMNL"
2021.findings-emnlp.263,N18-1133,0,0.0268155,"∈G exp(S(z j , c)/τ ) (13) For better readability, we illustrate the flowchart of our method in Algorithm 1. 4 4.1 Experiments Setup Datasets. We evaluate our C 3 models on two standard link prediction datasets: FB15k237 (Toutanova and Chen, 2015) that is created from FB15K (Bordes et al., 2013) and WN18RR (Dettmers et al., 2018) which is a subset of WN18 (Miller, 1995). Baselines. We compare our C 3 with the following previous state-of-the-art KRL methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), R-GCN (Schlichtkrull et al., 2018), KBGAN (Cai and Wang, 2018), ConvE (Dettmers (11) et al., 2018), SACN (Shang et al., 2019), HypER (Balaževi´c et al., 2019a), RotatE (Sun 3065 WN18RR FB15k-237 MRR H@10 H@3 H@1 MRR H@10 H@3 H@1 TransE (Bordes et al., 2013) DistMult (Yang et al., 2014) ComplEx (Trouillon et al., 2016) R-GCN (Schlichtkrull et al., 2018) KBGAN (Cai and Wang, 2018) ConvE (Dettmers et al., 2018) SACN (Shang et al., 2019) HypER (Balaževi´c et al., 2019a) RotatE (Sun et al., 2019) ConvR (Jiang et al., 2019) VR-GCN (Ye et al., 2019) TuckER (Balaževi´c et al., 2019b) COMPGCN (Vashishth et al., 2019) SANS (Ahrabian et al., 2020) .226 .43 .44 .214"
2021.naacl-main.429,D19-1435,0,0.0134896,"he copying mechels, even for these hypotheses ranked to the anism is also introduced for GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from source senmore grammatical than source sentences. tence to hypothesis sentence. To further accelerate the generation process, some work also comes up • Beam search candidates can provide valuable with non-autoregressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 7"
2021.naacl-main.429,P17-1055,0,0.0316776,"ed with the verification representation Vpk : Evidence Aggregation with Node P (y|wpk ) = softmaxy (Linear((Hpk ◦ Vpk ); Hpk ; Vpk )), (10) Selection Attention where ◦ is the element-wise multiplication and ; is The node selection attention measures node importhe concatenate operator. We average all probabiltance and is used to aggregate supporting evidence ity P (y = 1|wpk ) of token level quality estimation from the fine-grained node representation V l→k of as hypothesis quality estimation score f (s, ck ) for the l-th node. We leverage attention-over-attention the pair hs, ck i: mechanism (Cui et al., 2017) to conduct source hls m+n+2 X and hypotheses hlh representations to calculate the 1 f (s, ck ) = P (y = 1|wpk ). (11) l n + 1 p=m+2 l-th node selection attention score γ . Then we get 5444 3.4 End-to-end Training We conduct joint training with token-level supervision. The source labels and hypothesis labels are used, which denote the grammatical quality of source sentences and GEC accuracy of hypotheses. The cross entropy loss for the p-th token wpk in the k-th node is calculated: L(wpk ) = CrossEntropy(y ∗ , P (y|wpk )), Sentence using the ground truth token labels Then the training loss of"
2021.naacl-main.429,N12-1067,0,0.0931344,"Missing"
2021.naacl-main.429,W13-1703,0,0.168148,"attention mechanisms on the reasoning graph, node interaction attention and node selection attention, to summarize and aggregate necessary GEC evidence from other hypotheses to estimate the quality of tokens. Our experiments show that VERNet can pick up necessary GEC evidence from multi-hypotheses provided by GEC models and help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-trained GEC models with bea"
2021.naacl-main.429,N19-1423,0,0.383601,"mploy language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (Rei, 2017; Rei and Søgaard, 2019) estimate the hypothesis quality on both sentence level (Kaneko et al., 2019) and token level (Yannakoudakis et al., 2017). Chollampatt and Ng (2018b) further estimate GEC quality by considering correction accuracy. They establish sourcehypothesis interactions with the encoder-decoder architecture and learn to directly predict the official evaluation score F0.5 . The pre-trained language model BERT (Devlin et al., 2019) has proven its effectiveness in producing contextual token representations, achieving better quality estimation (Kaneko et al., 2019; Chollampatt et al., 2019) and improving GEC performance by fuse BERT representations (Kaneko et al., 2020). However, existing quality estimation models regard each hypothesis independently and neglect the interactions among multihypotheses, which can also benefit the quality estimation (Fomicheva et al., 2020). 3 Neural Verification Network Source Sentence: &quot;: Do one who suffered from this disease … Hypotheses from the beam search decoding of basic GEC : $1: Do"
2021.naacl-main.429,C16-1079,0,0.0314161,"Missing"
2021.naacl-main.429,2020.acl-main.113,0,0.201106,"du.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Association for Computational Linguistics (a) CoNLL2014 (ann. 1). (b) CoNLL2014 (ann. 2). Figure 2: The GEC Performance of Generated Hypotheses. The hypotheses generated by Kiyono et al. (2019) are evaluated on the CoNLL2014 dataset. The average scores of Precision and Recall are calculated according to the two annotations of CoNLL2014. guistic variation (Fomicheva et al., 2020), which can be used to improve machine translation performance (Wang et al., 2019b). Fomicheva et al. (2020) further leverage multi-hypotheses to make convinced machine translation evaluation, which is more correlated with human judgments. Their work further demonstrates that multi-hypotheses from well-trained neural models have the ability to provide more hints to estimate generation quality. For GEC, the hypotheses from the beam search decoding of well-trained GEC models can provide some valuable GEC evidence. We illustrate the reasons as follows. Existing quality estimation models (Chollamp"
2021.naacl-main.429,P18-1097,0,0.0181342,"esults of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to est"
2021.naacl-main.429,W19-4427,0,0.0173378,"l. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ"
2021.naacl-main.429,2020.lrec-1.835,0,0.0201361,"g GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic a"
2021.naacl-main.429,N18-1055,0,0.0683094,"Missing"
2021.naacl-main.429,W19-4422,0,0.181403,"ks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Associ"
2021.naacl-main.429,2020.acl-main.391,0,0.699224,"hypothesis is compared to the source sentence with a BERT based language model and classified into Win (the hypothesis is better), Tie (the hypothesis and source are same) and Loss (the source is better). The ratios of different classes are plotted with different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding"
2021.naacl-main.429,D19-1119,0,0.0748377,"tion mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/ thunlp/VERNet. 0.8 0.4 0.4 0.2 0.2 0 0 1 2 3 4 5 Ranks in Beam Search (a) BEA19. 1 2 3 4 5 Ranks in Beam Search (b) CoNLL2014. Figure 1: The Grammaticality of Generated Hypotheses. The hypotheses are generated by Kiyono et al. (2019) with beam search decoding. The hypothesis is compared to the source sentence with a BERT based language model and classified into Win (the hypothesis is better), Tie (the hypothesis and source are same) and Loss (the source is better). The ratios of different classes are plotted with different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest"
2021.naacl-main.429,N19-1333,0,0.0144583,"as a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some meth"
2021.naacl-main.429,D19-1510,0,0.0261163,"Missing"
2021.naacl-main.429,I11-1017,0,0.215929,"proposes two kinds of attention mechanisms on the reasoning graph, node interaction attention and node selection attention, to summarize and aggregate necessary GEC evidence from other hypotheses to estimate the quality of tokens. Our experiments show that VERNet can pick up necessary GEC evidence from multi-hypotheses provided by GEC models and help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-tra"
2021.naacl-main.429,P15-2097,0,0.0221129,"on model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one of the state-of-the-art autoreGLEU (Napoles et al., 2015) to evaluate model gressive GEC systems (Kiyono et al., 2019) as our performance on the JFLEG dataset. The official basic GEC model and keep the same setting. The tool ERRANT of the BEA19 shared task (Bryant beam size of our baseline model is set to 5 (Kiyono et al., 2019) is used to calculate Precision, Recall, et al., 2019), and all these beam search hypotheses and F0.5 scores for other datasets. For the CoNLLare reserved in our experiments. 2014 dataset, the M2 evaluation (Dahlmeier and We generate quality estimation labels for tokens Ng, 2012) is also adopted as our main evaluation. in bot"
2021.naacl-main.429,E17-2037,0,0.0146724,"as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one of the state-of-the-art autoreGLEU (Napoles et al., 2015) to evaluate model gressive GEC systems (Kiyono et al., 2019) as our perfo"
2021.naacl-main.429,W14-1701,0,0.0601161,"d with ERRANT according to the golden correction. The words in red color are labeled as incorrect (0) and others are labeled as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Table 1. We do not incorporate lation Coefficient (PCC) between reranking scores additional training corpora for fair comparison. Basic GEC Model. To generate correction hy- and golden scores (F0.5 ) for all hypotheses. To evaluate GEC performance, we adopt potheses, we take one o"
2021.naacl-main.429,2020.bea-1.16,0,0.0373692,"GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from source senmore grammatical than source sentences. tence to hypothesis sentence. To further accelerate the generation process, some work also comes up • Beam search candidates can provide valuable with non-autoregressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation probl"
2021.naacl-main.429,N16-1042,0,0.0593817,"nd help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types. 2 Related Work The GEC task is designed for automatically proofreading. Large-scale annotated corpora (Mizumoto et al., 2011; Dahlmeier et al., 2013; Bryant et al., 2019) bring an opportunity for building fully datadriven GEC systems. Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture (Sutskever et al., 2014) to generate correction hypotheses with beam search decoding (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018a). Transformer-based archi• Beam search candidates are more grammatitectures (Vaswani et al., 2017) show their effectivecal. As shown in Figure 1, the hypotheses ness in NLG tasks and are also employed to achieve from well-trained GEC models with beam convinced correction results (Grundkiewicz et al., search usually win the favor of language mod2019; Kiyono et al., 2019). The copying mechels, even for these hypotheses ranked to the anism is also introduced for GEC models (Zhao rear. It illustrates these hypotheses are usually et al., 2019) to better align tokens from"
2021.naacl-main.429,N19-1014,0,0.0607315,"regressive GEC models and leverages GEC evidence. As shown in Figure 2, the hya single encoder to parallelly detect and correct potheses of different beam ranks have almost grammatical errors (Awasthi et al., 2019; Malmi the same Recall score, which demonstrates all et al., 2019; Omelianchuk et al., 2020). hypotheses in beam search can provide some valuable GEC evidence. Recent research focuses on two directions to im5442 • Beam search can provide better GEC results. The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, Zhao et al. (2019) and Kiyono et al. (2019), the F0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and conf"
2021.naacl-main.429,P17-1194,0,0.0253143,"). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (Rei, 2017; Rei and Søgaard, 2019) estimate the hypothesis quality on both sentence level (Kaneko et al., 2019) and token level (Yannakoudakis et al., 2017). Chollampatt and Ng (2018b) further estimate GEC quality by considering correction accuracy. They establish sourcehypothesis interactions with the encoder-decoder architecture and learn to directly predict the official evaluation score F0.5 . The pre-trained language model BERT (Devlin et al., 2019) has proven its effectiveness in producing contextual token representations, achieving better quality estimation (Kaneko et al., 2019; Chollampatt et al."
2021.naacl-main.429,D19-1412,0,0.0693247,"erence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–11, 2021. ©2021 Association for Computational Linguistics (a) CoNLL2014 (ann. 1). (b) CoNLL2014 (ann. 2). Figure 2: The GEC Performance of Generated Hypotheses. The hypotheses generated by Kiyono et al. (2019) are evaluated on the CoNLL2014 dataset. The average scores of Precision and Recall are calculated according to the two annotations of CoNLL2014. guistic variation (Fomicheva et al., 2020), which can be used to improve machine translation performance (Wang et al., 2019b). Fomicheva et al. (2020) further leverage multi-hypotheses to make convinced machine translation evaluation, which is more correlated with human judgments. Their work further demonstrates that multi-hypotheses from well-trained neural models have the ability to provide more hints to estimate generation quality. For GEC, the hypotheses from the beam search decoding of well-trained GEC models can provide some valuable GEC evidence. We illustrate the reasons as follows. Existing quality estimation models (Chollampatt and Ng, 2018b) for GEC regard hypotheses independently and neglect the potent"
2021.naacl-main.429,D19-1073,1,0.879907,"Missing"
2021.naacl-main.429,N18-1057,0,0.01776,"3.56 and 76.82. prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system (JunczysDowmunt et al., 2018; Kiyono et al., 2019). Various weak-supervision corpora have been leveraged, such as Wikipedia edit history (Lichtarge et al., 2019), Github edit history (Hagiwara and Mita, 2020) and confusing word set (Grundkiewicz et al., 2019). Besides, lots of work generates grammatical errors through generation models or round-trip translation (Ge et al., 2018; Wang et al., 2019a; Xie et al., 2018). Kiyono et al. (2019) further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble (Hoang et al., 2016; Chollampatt and Ng, 2018b) with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019) or grammatical error detection (GED) models to estimate hypothesis quality. GED models (R"
2021.naacl-main.429,P11-1019,0,0.111359,"NUCLE CoNLL-2014 JFLEG Total Correction The 1 a 2 Mobile phone is a marvelous invention to 9 charge 10 the world 12 [SEP] Operation Span Edit Delete 1,2 Replace 9,10 change Insert 12,12 . p=1 Experimental Methodology Table 2: An Example of Token Label Annotation. All sentences are annotated with ERRANT according to the golden correction. The words in red color are labeled as incorrect (0) and others are labeled as correct (1). The “[SEP]” token denotes the end of the sentence. This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE (Yannakoudakis et al., 2011), BEA19 (Bryant et al., 2019) and NUFor the evaluation of sentence-level quality esCLE (Dahlmeier et al., 2013) to construct training timation, we employ the same evaluation metrics and development sets. Four testing scenarios, FCE, BEA19 (Restrict), CoNLL-2014 (Ng et al., 2014) from the previous quality estimation model (Chollampatt and Ng, 2018b), including two evaluation and JFLEG (Napoles et al., 2017), are leveraged to evaluate model performance. Detailed data statis- scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Corretics are presented in Ta"
2021.naacl-main.429,D17-1297,0,0.617309,"th different beam search ranks. Existing GEC systems usually inherit the seq2seq architecture (Sutskever et al., 2014) to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding (Kiyono et al., 2019; Kaneko et al., 2020) or model ensemble (Chollampatt and Ng, 2018a) to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models (Chollampatt and Ng, 2018a; Chollampatt et al., 2019; Yannakoudakis et al., 2017; Kaneko et al., 2019, 2020). Chollampatt and Ng (2018b) further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F0.5 score. The K-best hypotheses from beam search usually derive from model uncertainty (Ott et al., 2018). These uncertainties of multi-hypotheses come from ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) model confidence and potential ambiguity of lin5441 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5441–5452 June 6–1"
2021.naacl-main.452,P15-1034,0,0.023785,"Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to e"
2021.naacl-main.452,P98-1012,0,0.0117585,"(RWHAC) (Elsahar et al., 2017) is the state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectur"
2021.naacl-main.452,P08-1004,0,0.484805,"relation hierarchies with a top-down algorithm. (3) Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Si"
2021.naacl-main.452,P18-2065,0,0.141205,"ering and hierarchy expansion. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/OHRE. Training instances significant person participant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE)"
2021.naacl-main.452,N19-1423,0,0.0216324,"Missing"
2021.naacl-main.452,D18-1247,1,0.819224,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,D18-1514,1,0.942413,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,2020.emnlp-main.299,0,0.713973,"et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of r"
2021.naacl-main.452,Q16-1017,0,0.766042,"ods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical depend"
2021.naacl-main.452,D14-1162,0,0.0860062,"Missing"
2021.naacl-main.452,D19-1021,1,0.212409,"ances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corpora, which plays a"
2021.naacl-main.452,D11-1135,0,0.228983,"cipant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corp"
2021.naacl-main.452,P12-1075,0,0.724872,"Missing"
2021.naacl-main.452,D07-1043,0,0.0263829,"e state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectures. (1) RW-HAC for hierarchy expansion (RW-H"
2021.naacl-main.452,C14-1220,0,0.0267381,"erarchical than ri2 and rj2 in representation space, since ri1 and curriculum learning for robust model training. Pair- rj1 are close to each other in the relation hierarchy. wise virtual adversarial training is also introduced We design a hierarchical triplet objective with to improve the representation generalization ability. a dynamic margin which is determined by the disRelation Embedding Encoder. We adopt CNN to tance between relations in hierarchy. Specifically, encode sentences into relation representations. Fol- the dynamic margin is conducted over the instances lowing previous works (Zeng et al., 2014), given of the relations. As shown in Figure 2, given two a sentence s and target entity pair (eh , et ), each relations ri and rj sampled by hierarchical curricuword in the sentence is first transformed into in- lum training strategy (which will be introduced put representations by the concatenation of word later), we randomly sample two instances (namely embedding and position embedding indicating the anchor instance a and positive instance p) from ri , position of each entity. Then the input representa- and an instance (namely negative instance n) from tion is fed into a convolutional layer"
2021.naacl-main.452,N19-1306,0,0.0189905,"f relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relations to existing hierarchies. Curriculum Learning *#! !! &quot;! #! *#$ !$ &quot;$ #$ layer 1 *&quot;! layer 2 *&quot;$ Dynamic Ma"
2021.naacl-main.452,P19-1133,0,0.0538988,"8), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical inf"
2021.naacl-main.452,N18-1081,0,0.0182471,"world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (20"
A97-1018,C92-1019,0,0.257355,"Missing"
A97-1018,C92-4199,0,0.27213,"Missing"
A97-1018,W93-0305,0,0.0701545,"Missing"
A97-1018,P94-1010,0,0.305441,"Missing"
A97-1018,J96-3004,0,\N,Missing
C02-1055,J98-1001,0,\N,Missing
C02-1055,C92-2070,0,\N,Missing
C02-1055,A97-1018,1,\N,Missing
C10-2081,P91-1022,0,0.818814,"y large aligned corpora. In general, the more aligned text we have, the better result we achieve. Although there is a huge amount of bilingual text on the Internet, most of them are either only aligned at article level or even not aligned at all. Sentence alignment is a process mapping Various sentence alignment algorithms have been proposed, which generally fall into three types: length-based, lexicon-based, and the hybrid of the above two types. Length-based algorithms align sentences according to their length (measured by character or word). The first lengthbased algorithm was proposed in (Brown et al., 1991). This algorithm is fast and has a good performance if there is minimal noise (e.g., sentence or paragraph omission) in the input bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment alg"
C10-2081,P93-1002,0,0.247016,"n) in the input bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algori"
C10-2081,ma-2006-champollion,0,0.52537,"machine translation and cross-language information retrieval. With the rapid growth of online parallel texts, efficient and robust sentence alignment algorithms become increasingly important. In this paper, we propose a fast and robust sentence alignment algorithm, i.e., FastChampollion, which employs a combination of both length-based and lexiconbased algorithm. By optimizing the process of splitting the input bilingual texts into small fragments for alignment, FastChampollion, as our extensive experiments show, is 4.0 to 5.1 times as fast as the current baseline methods such as Champollion (Ma, 2006) on short texts and achieves about 39.4 times as fast on long texts, and Fast-Champollion is as robust as Champollion. 1 Ping Xue The Boeing Company ping.xue@boeing.com Introduction Sentence level aligned parallel corpora are very important resources for NLP tasks including machine translation, cross-language information retrieval and so on. These tasks typically require support by large aligned corpora. In general, the more aligned text we have, the better result we achieve. Although there is a huge amount of bilingual text on the Internet, most of them are either only aligned at article leve"
C10-2081,W96-0201,0,0.0631245,"As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005"
C10-2081,moore-2002-fast,0,0.850009,"ro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005). But both of the two ways suffer from high computational cost and are not fast enough for processing large corpora. Lexical information is necessary for improving robustness of a sentence alignment algorithm, but use of lexical information will introduce higher computational cost and cause a lower speed. A common fact is that the shorter the text is, the less combination possibilities it would introduce and the less computational cost it would need. So if we can first split the input bilingual texts into small aligned fragments reliably with a reasonable amount of computa"
C10-2081,C94-2175,0,0.0793875,"put bilingual texts. As this algorithm does not use any lexical information, it is not robust. Lexicon-based algorithms are usually more robust than the length-based algorithm, because they use the lexical information from source and translation lexicons instead of solely sentence length to determine the translation relationship between sentences in the source text and the target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Va"
C10-2081,P94-1012,0,0.721078,"target text. However, lexicon-based algorithms are slower than length-based sentence alignment algorithms, because they require much more expensive computation. Typical lexiconbased algorithms include (Ma, 2006; Chen, 1993; Utsuro et al., 1994; Melamed, 1996). Sentence length and lexical information are also combined to achieve more efficient algorithms in two ways. One way is to use both sentence length and lex710 Coling 2010: Poster Volume, pages 710–718, Beijing, August 2010 ical information together to determine whether two sentences should be directly aligned or not (Simard et al., 1993; Wu, 1994). The other way is to produce a rough alignment based on sentence length (and possibly some lexical information at the same time), and then build more precise alignment by using more effective lexicon-based algorithms (Moore, 2002; Varga et al., 2005). But both of the two ways suffer from high computational cost and are not fast enough for processing large corpora. Lexical information is necessary for improving robustness of a sentence alignment algorithm, but use of lexical information will introduce higher computational cost and cause a lower speed. A common fact is that the shorter the text"
C12-1105,W06-1615,0,0.0350802,"Missing"
C12-1105,J93-2003,0,0.0924315,"s content c, keyphrase extraction aims to seek a set of keyphrases k that maximizes Pr(k|c).Q By simply assuming the keyphrases are independent conditional over d, we have Pr(k|c) = k∈k Pr(k|c). The optimal set of keyphrases k∗ can be represented as follows: k∗ = arg max Pr(k|c) = arg max k k Y Pr(k|c). (1) k∈k Suppose the number of keyphrases is pre-defined as N k , we can simply find k∗ by ranking 1718 each candidate keyphrase v ∈ V according to its score Pr(v|c) in descending order and selecting top-N k keyphrases. 2.2 Word Trigger Model Word Trigger Model (WTM) is inspired by IBM Model-1 (Brown et al., 1993), the most widely used word alignment models in SMT. WTM assumes the content and the keyphrases of a document are describing the same themes while written in two different languages: document content in one language while keyphrases in the other. From this perspective, keyphrase extraction can be regarded as a translation process from a given document content to keyphrases. In more detail, the translation process is modeled as a trigger process as follows. First, WTM finds several important words in the document content as trigger words. Then, activated by these trigger words, WTM maps the doc"
C12-1105,D11-1146,1,0.819599,"category label. Various methods such as Naive Bayes (Garg and Weber, 2008) and kNN (Li et al., 2009) have been explored. Some researchers proposed using latent topics to build semantic relations between words and tags. The representative methods include TagLDA (Krestel et al., 2009; Si and Sun, 2009) and Content Relevance Model (CRM) (Iwata et al., 2009). However, these methods usually suffer from the over-generalization problem. Recently, a new approach based on word alignment models (WAM) in statistical machine translation (SMT) has been proposed for keyphrase extraction (Ravi et al., 2010; Liu et al., 2011b,a, 2012). WAM-based methods assume the content and keyphrases of a document are describing the same themes but written in different languages. Under this assumption, WAM-based methods regard keyphrase extraction as a translation process from document content to keyphrases. This process is modeled as a trigger from important words in document content to keyphrases according to trigger probabilities between words and keyphrases. WAM-based methods will learn trigger probabilities from sufficient documentkeyphrase pairs. With the trigger probabilities, given a novel document, WAM-based methods a"
C12-1105,W11-0316,1,0.834608,"category label. Various methods such as Naive Bayes (Garg and Weber, 2008) and kNN (Li et al., 2009) have been explored. Some researchers proposed using latent topics to build semantic relations between words and tags. The representative methods include TagLDA (Krestel et al., 2009; Si and Sun, 2009) and Content Relevance Model (CRM) (Iwata et al., 2009). However, these methods usually suffer from the over-generalization problem. Recently, a new approach based on word alignment models (WAM) in statistical machine translation (SMT) has been proposed for keyphrase extraction (Ravi et al., 2010; Liu et al., 2011b,a, 2012). WAM-based methods assume the content and keyphrases of a document are describing the same themes but written in different languages. Under this assumption, WAM-based methods regard keyphrase extraction as a translation process from document content to keyphrases. This process is modeled as a trigger from important words in document content to keyphrases according to trigger probabilities between words and keyphrases. WAM-based methods will learn trigger probabilities from sufficient documentkeyphrase pairs. With the trigger probabilities, given a novel document, WAM-based methods a"
C12-1105,D10-1036,1,0.25901,"ses should primarily rely on their semantic relatedness with the document rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary classification problem (is-"
C12-1105,D09-1027,1,0.643699,"cument rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary classification problem (is-keyphrase 1716 or non-keyphrase). Keyphrase extraction can also be"
C12-1105,D09-1092,0,0.0205123,"en a novel document d with its content c, we can rank each candidate keyphrase v as follows: Pr(v|c) = X w∈c Pr(v|w, ψ) Pr(w|c) = X ψ vw Pr(w|c), (4) w∈c where Pr(w|c) indicates the weight of the word w in c, which can be calculated using the TFIDF score of w in c. From the ranking list in descending order, we can select the topranked ones as keyphrases of the given document. 2.3 Topic Trigger Model WTM triggers keyphrases at the word level. We can also trigger at the topic level, and thus propose Topic Trigger Model (TTM) for keyphrase extraction. TTM is inspired by Polylingual Topic Models (Mimno et al., 2009), which is originally proposed to model parallel documents in multiple languages. TTM is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003). Suppose there are T latent topics in TTM, and the number of topics |T |can be pre-defined by users. TTM assumes that the content c and keyphrases k of a document d share the same distribution over |T |topics (i.e., θd ), which is drawn from a symmetric Dirichlet prior with concentration parameter α. TTM also assumes that each topic t ∈ T corresponds to two different multinomial distributions over words, one for keyphrases (i.e., φ tk )"
C12-1105,voorhees-tice-2000-trec,0,0.0521863,"gher. However, precision/recall/F-measure does not take the order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref can consider the order of extracted keyphrases for evaluation. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase  P |n ranked higher than r| and n is an incorrect keyphrase. It is defined as Bpref = R1 r∈R 1 − . M The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is usually used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the P rank of the first correct keyphrase with all extracted keyphrases, It 1 1 is defined as MRR = |D| d∈D rank , where D is the document set for keyphrase extraction. d 3.2 Case Studies Before quantitative evaluation, we perform case studies by looking into the topics learned by TWTM. By setting T = 100 of TWTM, We select two topics, i.e., Topic-59 and Topic-92 for study. In first several rows of Table 2, we list the top-10 words and top-10 keyphrases given th"
C12-1105,C08-1122,0,0.0306823,"he selection of keyphrases should primarily rely on their semantic relatedness with the document rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary class"
C12-1105,W04-3252,0,\N,Missing
C12-2064,P98-2127,0,0.0642856,"context-aware relations of tags from a large collection of annotation data? (2) After obtaining the context-aware relations of tags, how to construct a tag graph given a resource for random walks? To address the challenges, we propose a probabilistic model to learn the context-aware relations of tags, and propose a random walk algorithm over context-aware relation graphs to suggest tags. To investigate the efficiency of our method, we carry out experiments using real-world datasets. Related work. Measuring semantic relations have been studied in many tasks such as measuring term similarities (Lin, 1998; Gabrilovich and Markovitch, 2007) and query similarities (Wen et al., 2002; Mei et al., 2008). Meanwhile, context-aware setting is being considered in many applications including recommender systems (Adomavicius and Tuzhilin, 2011) and query suggestion (Brown and Jones, 2001), which is a critical research issue for all applications under real-world complex scene. In social tagging, co-occurrencebased tag relations have been explored to group tags into clusters (Wu et al., 2006b; Brooks and Montanez, 2006; Shepitsen et al., 2008), and have been adopted in personalized tag suggestion (Shepitse"
C12-2064,D11-1146,1,0.632126,"d the user-annotated tags as annotation. (a) An example book. (b) TCM Figure 1: (a) An example book. (b) Graphical model of TCM. Social tagging provides a convenient management scheme compared to strict taxonomy in libraries. In order to attract more users to contribute social annotations, many social tagging systems facilitate users through automatic tag suggestion. There are two main approaches: graph-based and content-based. The former approach (Jaschke et al., 2008; Rendle et al., 2009) suggests tags according to users’ annotation history, while the latter approach (Si et al., 2009, 2010; Liu et al., 2011) according to resource meta-data. Since graph-based methods often suffer from the cold-start problem when they face new users or resources, content-based methods are usually regarded as an important component in social tagging systems especially in the initial stage. In this paper we focus on the content-based approach. Many social tagging methods are based on independence assumption, which is widely adopted in computational linguistics (Manning and Schutze, 2000) and information retrieval (Manning et al., 2008). Under this assumption, tags are regarded independent with each other given the re"
C12-2064,D10-1036,1,0.88967,"Missing"
C12-2064,J00-2011,0,0.0715914,"e et al., 2008; Rendle et al., 2009) suggests tags according to users’ annotation history, while the latter approach (Si et al., 2009, 2010; Liu et al., 2011) according to resource meta-data. Since graph-based methods often suffer from the cold-start problem when they face new users or resources, content-based methods are usually regarded as an important component in social tagging systems especially in the initial stage. In this paper we focus on the content-based approach. Many social tagging methods are based on independence assumption, which is widely adopted in computational linguistics (Manning and Schutze, 2000) and information retrieval (Manning et al., 2008). Under this assumption, tags are regarded independent with each other given the resource. Although this makes methods easier to implement, it does not accord with the real world, in which the annotated tags of a resource are usually semantically correlated with each other. Hence, if we can find an effective approach to model tag relations, it may improve the suggestion quality significantly. It is non-trivial to model tag relations. Given a resource, the tag relations are context-aware. Two tags may be more related with each other given a resou"
C12-2064,C98-2122,0,\N,Missing
C12-2066,P06-1002,0,0.0173256,"ignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) ("
C12-2066,J93-2003,0,0.0666932,"based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea,"
C12-2066,J07-3002,0,0.0414709,"Missing"
C12-2066,N04-1035,0,0.0518495,"L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taska"
C12-2066,P09-1104,0,0.216333,"ey, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohi"
C12-2066,P07-2045,0,0.00833936,"in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-world data. To alleviate this problem, many pruning methods have been proposed to reduce the computational complexity of synchronous parsing by pruning less promising cells. Zhang and Gildea (2005) introduce a tic-tac-toe pruning method based on IBM model 1 probabilities. Haghighi et al. (2009) use posterior predictions from simpler alignment models for identifying degenerate cells. Liu et al. (2010a) propose a discriminative fra"
C12-2066,W09-0424,0,0.0189591,"ich link will result in an ITG alignment before calling the ITG(a) procedure. We leave this for future work. 3.2 Translation Evaluation For the translation evaluation, we used 138K sentence pairs that have at most 40 words from the FBIS corpus as the training set, NIST 2002 dataset as the development set, and NIST 2005 dataset as the test set. As the biparsing algorithm runs too slow on the training data, we only compared our algorithm with biparsing+pruning in terms of average time per sentence pair and BLEU. Moses (Koehn et al., 2007) (a state-of-the-art phrase-based SMT system) and Joshua (Li et al., 2009) (a state-of-the-art hierarchial phrase-based SMT system) are used in our experiments. Both of them are used with default settings, except that word alignments are produced by “biparsing+pruning” and “beam search” respectively rather than GIZA++. Table 2 shows the average aligning time as well as the BLEU scores obtained by Moses and Joshua. Our system runs 20 times faster than the baseline without significant loss in translation quality. algorithm biparsing+pruning beam search setting t = 10−5 b = 10 average time (s) 7.57 0.35 Moses 23.86 23.95 Joshua 23.77 23.38 Table 2: Comparison of averag"
C12-2066,P10-1033,0,0.0772598,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,C10-2084,0,0.10331,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,P05-1057,1,0.80375,"ligning time and model score over various sentence lengths. 2. biparsing + pruning: the bilingual parsing algorithm with tic-tac-toe pruning (Zhang and Gildea, 2005). For simplicity, we used the IBM model 4 translation probabilities trained on the FBIS corpus (6.5M+8.4M words) to approximate ITG lexical probabilities in the following experiments: p( f , e) ≈ pm4 ( f |e) × pm4 (e |f )/2, p( f , ε) ≈ pm4 ( f |ε), p(ε, e) ≈ pm4 (e|ε). 3.1 Alignment Evaluation For the alignment evaluation, we selected 461 sentence pairs that contain at most 50 words on both sides from the hand-aligned dataset of (Liu et al., 2005). The three ITG alignment methods are compared in terms of average time per sentence pair, average model score per sentence pair, and AER. The results are shown in Table 1. Although achieving the best model score and AER, the biparsing algorithm runs too slow: 126.164 seconds per sentence pair on average. This is impractical for dealing with large scale real-world data that usually contains millions of sentence pairs. The tic-tac-toe pruning method (biparsing + pruning) does increase the speed by two orders of magnitude (3.571 seconds per sentence pair), which confirms the effectiveness of cel"
C12-2066,J10-3002,1,0.283909,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,H05-1011,0,0.0257162,"cessfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and transla"
C12-2066,P06-1065,0,0.020514,"udies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney,"
C12-2066,J04-4002,0,0.077746,"© ÛŽ{Œ±3õ‘ªžmS|¢ Viterbiéà§ÙOŽE,Ý•, p§J±?n•é õ•éf ý¢êâ""•d§·‚JÑ˜«{ük Î|¢Ž{""TŽ{±˜éà•å :§`kÀJ•Ð ë‚V éà¥§†–Ã{Jp .VÇ•Ž""3Ç=êâþ ¢ (JL§·‚ Ž{'¦^}{Eâ VŠ©ÛŽ{¯˜‡êþ?§Óž ± éà Ú€È Ÿþ"" KEYWORDS: word alignment, inversion transduction grammar, beam search. KEYWORDS IN L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of sour"
C12-2066,H05-1010,0,0.0310368,"2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b"
C12-2066,C96-2141,0,0.252895,"and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi"
C12-2066,J97-3002,0,0.791372,"; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterb"
C12-2066,P06-1066,0,0.171229,"et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-"
C12-2066,P03-1019,0,0.0390805,"e et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal w"
C12-2066,P05-1059,0,0.374016,"rown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of suc"
C12-2066,W06-1627,0,0.0409849,"Missing"
C12-2066,J07-2003,0,\N,Missing
C12-2069,J93-2003,0,0.0409633,"uggestion has been well studied (Gupta et al., 2010). There are two approaches for social tag suggestion: graphbased approach and content-based approach. Since we have to suggest tags according to the content of m, we follow the content-based approach. The specialty of our problem compared to previous problems lies in: (1) the method should be robust to noise and informal format of microblog messages; and (2) m is short with no more than 140 Chinese characters in Sina Weibo. Taking the specialty in consideration, we propose to use word alignment model (WAM) in statistical machine translation (Brown et al., 1993) for social tag suggestion, which has been verified to outperform other existing content-based methods (Liu et al., 2011, 2012). Here we give a brief introduction to WAM, and introduce some important extensions to make the method appropriate to suggest social tags for microblog messages. WAM for Social Tag Suggestion. Given P a message m, WAM ranks candidate tags by computing their likelihood Pr WAM (t|m) = w∈m Pr(t|w) Pr(w|m), where Pr(w|m) is the weight of the word w in m, and Pr(t|w) is the translation probability from w to t obtained from the translation models. Pr(w|m) is estimated using"
C12-2069,C08-1049,0,0.0549553,"Missing"
C12-2069,D11-1146,1,0.842685,"ch and content-based approach. Since we have to suggest tags according to the content of m, we follow the content-based approach. The specialty of our problem compared to previous problems lies in: (1) the method should be robust to noise and informal format of microblog messages; and (2) m is short with no more than 140 Chinese characters in Sina Weibo. Taking the specialty in consideration, we propose to use word alignment model (WAM) in statistical machine translation (Brown et al., 1993) for social tag suggestion, which has been verified to outperform other existing content-based methods (Liu et al., 2011, 2012). Here we give a brief introduction to WAM, and introduce some important extensions to make the method appropriate to suggest social tags for microblog messages. WAM for Social Tag Suggestion. Given P a message m, WAM ranks candidate tags by computing their likelihood Pr WAM (t|m) = w∈m Pr(t|w) Pr(w|m), where Pr(w|m) is the weight of the word w in m, and Pr(t|w) is the translation probability from w to t obtained from the translation models. Pr(w|m) is estimated using term-frequency and inverse message frequency (TFIMF), which is similar to TFIDF. According to the ranking scores, we sug"
C12-2069,J03-1002,0,0.0040868,"ity of the tag t. Using messages and their corresponding extracted tags, we build the translation pairs for WAM training. We use IBM Model 1 (Brown et al., 1993) for WAM training. IBM Model 1 is a widely used word alignment algorithm which does not require linguistic knowledge for two languages. We have also tested more sophisticated word alignment algorithms such as IBM Model 3 for tag suggestion. However, these methods do not achieve better performance than IBM Model 1. Therefore, in this paper we only demonstrate the experimental results using IBM Model 1. In experiments, we select GIZA++ (Och and Ney, 2003) to train IBM Model 1. 3.2 Measuring Authority Some works have been devoted to authority analysis of social media (Pal and Counts, 2011). The basic conclusion is that a microblog user has more authority if it has more followers and posts more original messages. Therefore, in this paper, we simply compute authority of a user e as:  |F | Pr(e) = P log e∈Em e |Ae | log × log(|Me |) |Fe | |Ae |  × log(|Me |) , (3) where Fe is the follower set of e and Ae is the user set followed by e. The score is normalized over all experts in Em . 3.3 Restricting Candidate Expert Set for TSC We denote the name"
C12-2069,D11-1147,0,0.0419291,"8). However, quantitative studies of rumors have just begun, and microblog services provide a chance. Recently, researchers have developed different approaches to study rumors or misinformation. Some researchers devoted to finding information diffusion patterns over social networks (Kempe et al., 2003; Gruhl et al., 2004; Leskovec et al., 2009; Romero et al., 2011) and limiting the spread of misinformation by means of network structure (Budak et al., 2011). The spread patterns of rumors with respect to the content and conversations were also studied (Ennals et al., 2010; Mendoza et al., 2010; Qazvinian et al., 2011; Castillo et al., 2011). On one hand, most of these methods all focused on external features of rumors, which cannot ultimately determine whether a message is misinformation. On the other hand, the features can be obtained only after the information has spread over social networks. Existing methods find experts based on either people relations (graph-based approach) or people meta-data (content-based approach). In the graph-based approach, users are ranked according to their authority scores computed by the algorithms such as HITS and PageRank (Zhang et al., 2007; Jurczyk and Agichtein, 2007)"
C12-2069,N10-1072,0,0.042198,"Missing"
C12-2074,D11-1146,1,0.713637,"Missing"
C12-2074,D09-1026,0,0.142585,"Missing"
C12-3040,P01-1004,0,0.830801,"Missing"
C12-3040,C00-1006,0,0.507065,"Missing"
C12-3040,C10-1054,0,0.0422361,"Missing"
C12-3040,P07-2045,0,0.0140445,"Missing"
C12-3040,N03-1017,0,0.00994328,"Missing"
C12-3040,W02-1018,0,0.0922608,"Missing"
C12-3040,H05-1086,0,0.0278739,"Missing"
C12-3040,P03-1021,0,0.0414804,"Missing"
C12-3040,P02-1038,0,0.445065,"Missing"
C14-1179,P06-1067,0,0.0207309,"Missing"
C14-1179,P12-1050,0,0.195704,"Missing"
C14-1179,N13-1003,0,0.320689,"tion (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering mode"
C14-1179,P11-1105,0,0.0610031,"performance. We leave this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phr"
C14-1179,P13-2071,0,0.211141,"this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words"
C14-1179,C10-2033,1,0.935129,"Missing"
C14-1179,D08-1089,0,0.228527,"zed reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previ"
C14-1179,N10-1129,0,0.238196,"model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the"
C14-1179,W11-2123,0,0.00870943,"s directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive autoencoders to the softmax layer. Taking “resident population” as an example, there are three vectors in the binary ∑ tree used by the corresponding recursive autoencoder, denoted as x ˆ1 , x ˆ2 and x ˆ3 . The average vector is computed as x ¯ = 13 3i=1 x ˆi . 3 As"
C14-1179,P07-1019,0,0.013884,"error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU"
C14-1179,P08-1067,0,0.00956818,"lgorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In"
C14-1179,2010.eamt-1.28,0,0.0153363,"reordering probabilities for long phrase pairs that are usually observed only once in the training data. On the contrary, short phrase pairs that occur in the training data for many times tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ⟨“yingyun”, “business”⟩ is observed to have different orientations in different contexts. It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding contexts keep changing. Previous study shows that considering more contexts into reordering modeling improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity. 3 A Neural Reordering Model 3.1 The Model Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering model is given by P (o|f , e, a) ≈ n ∏ P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) (4) i=1 where ⟨f˜ai−1 , e˜i−1 ⟩ is the previous phrase pair. Including the previous phrase pairs improves the context sensiti"
C14-1179,J99-4005,0,0.0691019,"Missing"
C14-1179,N03-1017,0,0.318751,"tions reordering probabilities on the words of both the current and previous phrase pairs. Including the words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we build one classifier for all phrase pairs, which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to n"
C14-1179,P07-2045,0,0.119238,"-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations"
C14-1179,D13-1054,1,0.864535,"propose a neural reordering classifier that takes the current and previous phrase pairs as input. The neural network consists of four recursive autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e., f˜ai , e˜i , f˜ai−1 , e˜i−1 ) into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that the recursive autoencoders for the same language share with the same parameters. Our neural network is similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜"
C14-1179,J06-4004,0,0.0801973,"Missing"
C14-1179,N13-1090,0,0.0101041,"alized models in predicting long-distance reordering. 1903 # examples 100,000 200,000 300,000 400,000 500,000 3,000,000 BLEU 25 24 23 22 neural lexicalized 2 4 6 Distortion Limit 8 Figure 3: BLEU with various distortion limits. Vectors ours word2vec MT06 31.03 30.44 MT02 33.03 32.28 Accuracy 83.55 84.40 84.55 84.95 85.25 85.55 BLEU 30.92 31.03 31.01 30.93 31.27 31.03 Table 3: Effect of training corpus size. MT03 32.48 32.00 MT04 32.52 32.07 MT05 31.11 30.24 MT08 25.20 24.54 Table 4: Comparison of neural reordering models trained based on word vectors produced by our model (ours) and word2vec (Mikolov et al., 2013). 4.5 The Effect of Training Corpus Size Table 3 shows the classification accuracy and translation performance with various number of randomly sampled reordering examples for training the neural classifier. The classification accuracy and translation performance generally rise as the number of reordering example increases.6 Surprisingly, both the classification accuracy and translation performance of using 500,000 reordering examples are close to using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples are enough for training a robust classifier. 4."
C14-1179,2009.mtsummit-papers.10,0,0.0894341,"Missing"
C14-1179,J04-4002,0,0.0403147,"Missing"
C14-1179,P03-1021,0,0.0215854,"nd decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) + bo ), (7) where W o is a weight matrix, bo is a bias vector, c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) is the concatenation of the vectors of the four phrases. 2 Following Och (2003), we use a linear model in our decoder with conventional features (e.g., translation probabilities and n-gram language model). The neural reordering model is incorporated into the discriminative framework as an additional feature. 3.2 Training Training the neural reordering model involves minimizing the following two kinds of errors: • Reconstruction error: It measures how well the computed vector space representations represent the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees formed during computing the vector space representation for a"
C14-1179,D11-1014,0,0.731154,"dering as classification. Instead of maintaining a reordering probability distribution for each phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013). This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as training examples. We find that 500, 000 reordering examples suffice to train a robust classifier (Section 4.5). 2. Continuous space representation. Instead of using a symbolic representation of phrases, we use a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al., 2011b; Li et al., 2013). Consider two phrases “in London” and “in Centara Grand”. It is usually easy to predict the orientations of “in London” because it might be observed in the training data for many times. This is not the case for “in Centara Grand” as it might occur only once. However, if the two phrases happen to have very similar continuous space representations, “in Centara Grand” is likely to have a similar reordering probability distribution with “in London”. To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive autoencoders. Given two word"
C14-1179,N04-4026,0,0.195174,"ts show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities condi"
C14-1179,P06-1066,0,0.0967446,"orientations but are totally unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled data hardly helps in training the neural reordering model. Table 4 shows the results when we replace the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automa"
C14-1179,2010.iwslt-papers.19,0,0.177035,"d using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the performance. We leave this f"
C14-1179,P09-1038,0,0.0516363,"Missing"
C14-1179,C04-1030,0,0.228504,"Missing"
C14-1179,N04-1021,0,\N,Missing
C14-1192,C00-1006,0,0.225443,"corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 20"
C14-1192,P01-1004,0,0.203375,"that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in"
C14-1192,P08-1115,0,0.025281,"nd Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hiera"
C14-1192,D09-1115,1,0.845273,"Missing"
C14-1192,C12-3040,1,0.946269,"y used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records that are pairs of source-language and target-language strings. Given an input source string, the retrieval system returns a translation record of maximum similarity to the input on the source side. Although these methods prove to be effective in example-based and memory-based translation systems, they heavily rely on parallel corpora that are limited both in size and domain. More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends only on monolingual corpora. Given an input source string, their system retrieves translation candidates from a set of target-language sentences. This can be done by combining machine translation (MT) and information retrieval (IR): machine translation is used to transform the input source string to a coarse translation, which serves as a query to retrieve the most probable translation in the monolingual corpus. Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora that are"
C14-1192,D08-1076,0,0.0329923,"age and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are us"
C14-1192,J05-4003,0,0.0321661,"w that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the out-of-domain test set. 4.2 Evaluation on Parallel Corpus Mining In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel corpus from a comparable corpus. 4.2.1 Experimental Setup The comparable corpus for extracting parallel sentences contains news articles published by Xinhua News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and 1.7M English articles. We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system. 2038 language Chinese English articles 1.2M 1.7M sentences 18.5M 17.8M words 441.2M 440.2M vocabulary 2.1M 3.4M Table 4: The Xinhua News Comparable Corpus from 1995 to 2010 Munteanu and Marcu (2005) English words Chinese words BLEU 5.00M 4.12M 22.84 10.00M 8.20M 25.10 15.00M 12.26M 25.41 20.00M 16.30M 25.56 this work English words Chinese Words 5.00M 3.98M 10.00M 8.17M 15.00M 12.49M 20.00M 16.90M BLEU 25.44 26.62 26.49 26.87 Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system. Given a comparable corpus (see Table 4), both"
C14-1192,1993.tmi-1.4,0,0.801428,"entences from comparable corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 20"
C14-1192,P02-1038,0,0.108194,"entence e is the translation of a sourcelanguage sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by 2032 introducing a coarse translation q as a hidden variable: X P (q, e|f ) P (e|f ) = (1) q∈Q(f ) X = P (q|f ) × P (e|q, f ) (2) q∈Q(f ) where P (q|f ) is a translation sub-model, P (e|q, f ) is a retrieval sub-model, and Q(f ) is the set of all possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model. To take advantage of various translation and retrieval information sources, we use a log-linear model (Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned on a source sentence f parameterized by a real-valued vector θ: exp(θ · h(q, e, f )) P 0 0 q0 ∈Q(f ) e0 ∈E exp(θ · h(q , e , f )) P (q, e|f ; θ) = P where h(·) is a vector of feature functions and θ is the corresponding feature weight vector. Accordingly, the decision rule for the latent variable model is given by ( ) X ˆ = arg max e exp(θ · h(q, e, f )) e∈E (3) (4) q∈Q(f ) As there are exponentially many queries, it is efficient to approximate the summation over all possible queries by using maximization i"
C14-1192,P03-1021,0,0.527429,"“with”}. This set serves as a coarse single query and the retrieval system returns a list of target sentences that contain these words: 2035 Training Dev in-domain out-of-domain Test in-domain out-of-domain query document query document query document query document Chinese 1.21M 5K 5K 5K 5K English 1.21M 2.23M 2.23M 2.23M 2.23M Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based translation model and language model for Moses (Koehn et al., 2007). The development set is used to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To examine the effect of domains on retrieval performance, we used two development and test sets: indomain and out-domain. President Bush gave a talk at a meeting Bush held a meeting with Sharon Sharon and Bush attended a meeting held at London Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences (scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we can match each retri"
C14-1192,P02-1040,0,0.0905267,"variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice, we further distinguish between search graph and translation option graph. They are generated by Moses with the default setting. We use both translation and retrieval features in the experiments. The translation features include phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, language models, and word penalty. Besides the conventional IR features such as term frequency and inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002): the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity penalty. These features impose structural constraints on retrieval and ensure translation closeness of retrieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss function we used in our experiment is 1−P@n. Note that using translation option graph as query lattice does not include language models and distance-based lexicalized reordering models as features. 4.1.2 Evaluation Results Table 2 shows the results on the in-domain test set. The “#"
C14-1192,N04-1017,0,0.0407263,"ponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are used as a compact representation of multiple speech recognition transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004; Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that uses the bag-of-words model because translation retrieval must take structure and dependencies in text into account to ensure translational equivalence. 3 3.1 Query Lattice for Translation Retrieval Translation Retrieval Let f be a source-language string, E be a set of target-language strings, the problem is how to find the ˆ from E. Note that E is a monolingual corpus rather than a parallel corpus. most probable translation e Therefore, string matching on the source side (Sato and Nagao, 1990;"
C14-1192,C90-3044,0,0.400766,"extracting parallel sentences from comparable corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidat"
C14-1192,D08-1065,0,0.0259949,", translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). I"
C14-1192,2005.iwslt-1.18,1,0.777298,"emory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958)"
C14-1192,N06-1053,0,0.0305477,"n candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are used as a compact representation of multiple speech recognition transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004; Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that uses the bag-of-words model because translation retrieval must take structure and dependencies in text into account to ensure translational equivalence. 3 3.1 Query Lattice for Translation Retrieval Translation Retrieval Let f be a source-language string, E be a set of target-language strings, the problem is how to find the ˆ from E. Note that E is a monolingual corpus rather than a parallel corpus. most probable translation e Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al.,"
C14-1192,P07-2045,0,\N,Missing
C16-1300,C96-1005,0,0.0570209,"ong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li a"
C16-1300,P15-1072,0,0.0132745,"har et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016): Their work applies the Earth Mover’s Distance to the post-processing of fixed bilingual word embeddings to retrieve word translation, while ours strives to 3195 train better bilingual word embeddings with the EMD. In addition, we also explore the feasibility of using the EMD for bilingual lexicon induction from non-parallel data. In computer vision, there have been a few"
C16-1300,D14-1110,1,0.82059,"esources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complem"
C16-1300,D15-1131,0,0.102346,"lish translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Sinc"
C16-1300,E14-1049,0,0.0413951,"ons in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a si"
C16-1300,W04-3208,0,0.15272,"Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in t"
C16-1300,P04-1067,0,0.779039,", 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For"
C16-1300,N15-1157,0,0.0811635,"7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense emb"
C16-1300,C14-1048,0,0.0154618,"their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016): Their work applies the Earth Mover’s Distance"
C16-1300,P08-1088,0,0.0443622,"al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanize"
C16-1300,P12-1092,0,0.0357477,"biguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambig"
C16-1300,P15-1010,0,0.0124639,"ed. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016)"
C16-1300,N15-1070,0,0.0124216,"tiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados"
C16-1300,W02-0902,0,0.217772,"ocessing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple pos"
C16-1300,P14-2037,0,0.18134,"Missing"
C16-1300,Q15-1016,0,0.0151673,"of the EMD regularization. We are inclined to take small and many gradient ascent steps, so we fix M = 10, 000 and tune λe on the validation set. Finally, the learning rate is decayed linearly at the end of each epoch. 4.2 Adding Context Vectors In the previous section, we have presented our model with word vectors W S and W T as the parameters. In reality, each word is associated with a context vector as well (Mikolov et al., 2013c). While the usual representation of a word for evaluation is simply a word vector, some authors have suggested adding the context vector (Pennington et al., 2014; Levy et al., 2015). Previously this means a simple postprocessing step during evaluation, but in our setting we can bring the trick to training. Specifically, using Euclidean distance as the ground distance, we would have parametrized Cts in the EMD term (4) as Cts = WtT − WsS . (7) Considering the context vectors U S and U T , we now reformulate the ground distance as   Cts = WtT + UtT − WsS + UsS . (8) This modification affects both steps in the alternating optimization procedure. In addition, the seed term also encourages corresponding context vectors to be close. 5 Experimental Setup 5.1 Data In our exper"
C16-1300,D15-1200,0,0.0122641,"1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how"
C16-1300,N15-1028,0,0.109226,"because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the"
C16-1300,W15-1521,0,0.0519279,"ountain Table 5: English translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau"
C16-1300,D14-1113,0,0.0242166,"h a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieve"
C16-1300,J03-1002,0,0.0055145,"e into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon. 1 Introduction Bilingual lexica provide word-level semantic equivalence information across languages, and prove to be valuable for a range of cross-lingual natural language processing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and"
C16-1300,D14-1162,0,0.0780849,"ntly affect the strength of the EMD regularization. We are inclined to take small and many gradient ascent steps, so we fix M = 10, 000 and tune λe on the validation set. Finally, the learning rate is decayed linearly at the end of each epoch. 4.2 Adding Context Vectors In the previous section, we have presented our model with word vectors W S and W T as the parameters. In reality, each word is associated with a context vector as well (Mikolov et al., 2013c). While the usual representation of a word for evaluation is simply a word vector, some authors have suggested adding the context vector (Pennington et al., 2014; Levy et al., 2015). Previously this means a simple postprocessing step during evaluation, but in our setting we can bring the trick to training. Specifically, using Euclidean distance as the ground distance, we would have parametrized Cts in the EMD term (4) as Cts = WtT − WsS . (7) Considering the context vectors U S and U T , we now reformulate the ground distance as   Cts = WtT + UtT − WsS + UsS . (8) This modification affects both steps in the alternating optimization procedure. In addition, the seed term also encourages corresponding context vectors to be close. 5 Experimental Setup 5"
C16-1300,P99-1067,0,0.445284,"language processing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language wo"
C16-1300,P11-1002,0,0.0276315,"As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanized) Chinese word “qiche”"
C16-1300,N10-1013,0,0.01923,"al setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different trans"
C16-1300,P15-1173,0,0.0234585,"Missing"
C16-1300,P15-2093,1,0.836973,"the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embed"
C16-1300,Q13-1001,0,0.124303,"Missing"
C16-1300,C14-1016,0,0.0130171,"(Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prio"
C16-1300,N13-1011,0,0.48435,"Missing"
C16-1300,D13-1168,0,0.151132,"Missing"
C16-1300,P15-2118,0,0.149214,"Missing"
C16-1300,P11-2084,0,0.188188,"Missing"
C16-1300,D13-1141,0,0.0966579,"e competitor customer luxury middle part Ours car automobile auto foundation interior brick onwards building hill mountain Table 5: English translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within l"
C18-1041,D14-1067,0,0.0156667,"res or manual labels which are hard to gather on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and leg"
C18-1041,J81-4005,0,0.745887,"Missing"
C18-1041,P15-1001,0,0.027774,"on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified f"
C18-1041,D14-1181,0,0.121215,"text or case profiles. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th Inter"
C18-1041,O12-5004,0,0.412552,"Missing"
C18-1041,D17-1289,0,0.600661,"ords, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguistics, pages 487–498 Santa Fe, New Mexico, USA, August 20-26, 2018. Fact On June 24, 2015, the defendant pry o"
C18-1041,D15-1167,0,0.0235342,", some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguis"
C18-1041,N16-1174,0,0.414897,"t. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified framework. In the fol"
C18-1041,D17-1099,0,0.0278547,"ttribute-based classification to the label-embedding task. Jayaraman and Grauman (2014) introduces a random forest method stressing the unreliability of attribute prediction for unseen classes. They also extend it to the few-shot scenario. Other than attributes, other external information can also be introduced to promote zero-shot classification. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge between text features and visual features. Zero-shot learning has also been used in applications besides object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et al., 2014). 2.2 Charge Prediction Researchers in the legal area have been working on automatically making the legal judgment for a long time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they are restricted to a small"
C18-1099,P17-1110,0,0.0236166,"t adverarial training strategies to transfer the features of one source domain to its corresponding target domain. Inspired by Ganin et al. (2016), adversarial training has also been explored in some typical NLP tasks for multi-feature fusion. Park and Im (2016) propose a multi-modal representation learning model based on adversarial training. Then, Liu et al. (2017a) employ adversarial training to construct a multi-task learning model for text classification by extending the original binary adversarial training to the multiclass version. And a similar adversarial framework is also adapted by Chen et al. (2017) to learn features from different datasets for chinese word segmentation. In this paper, we adopt adversarial training to boost feature fusion to grasp the consistency among different languages. 3 Methodology In this section, we introduce the overall framework of our proposed AMNRE in detail. As shown in Figure 1, for each entity pair, AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns. Meanwhile, a unified space is also set up to encode consistent features among languages. By explicitly encoding the consist"
C18-1099,W14-4012,0,0.133982,"Missing"
C18-1099,N15-1151,0,0.0761329,"roblems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training G"
C18-1099,P11-1055,0,0.162418,"versarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations"
C18-1099,P16-1200,1,0.944037,"lications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a mult"
C18-1099,P17-1004,1,0.867008,"the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a multi-lingual attention to capture the pattern consistency and complementarity among languages. Although MNRE achieves great success in multi-lingual RE, it still has some problems. MNRE learns a single representation for each sentence in various languages, which cannot well capture both the consistency and diversity of relation patterns in different"
C18-1099,P17-1001,0,0.0571961,"Missing"
C18-1099,D17-1189,0,0.248594,"d models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this"
C18-1099,P09-1113,0,0.957424,"take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same"
C18-1099,P15-1061,0,0.0339267,"dividual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016)"
C18-1099,D12-1110,0,0.069634,"ifferences between individual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervisi"
C18-1099,N16-1103,0,0.0745242,"eng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training Goodfellow et al. (2015)"
C18-1099,D17-1187,0,0.141618,"suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose"
C18-1099,C14-1220,0,0.647507,"nlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction"
C18-1099,D15-1203,0,0.599989,"xample, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual att"
C18-1099,D17-1186,1,0.868287,"distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to e"
C18-1099,D15-1031,0,0.0138338,"at our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attent"
C98-2201,C92-1019,0,0.0122325,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,P94-1010,0,0.0369469,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,P97-1041,0,0.038462,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,A97-1018,1,0.727926,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,J96-3004,0,\N,Missing
D07-1081,P06-1069,1,0.822384,"aluate the approach, during which some empirical rules are observed to complete the approach; Section 6 makes some further observations and discussions based on Section 5; Section 7 gives a concluding remark. 2 Experiment Settings 2.1 Document Collections Two document collections are used in this study. CE (Chinese Encyclopedia): This is from the electronic version of the Chinese Encyclopedia. We choose a Chinese corpus as the primary document collection because Chinese text (as well as other Asian languages) has a very large term set and a satisfying subset is usually not smaller than 50000 (Li et al., 2006); on the contrary, a dimensionality lower than 10000 suffices a general English text categorization (Yang and Pedersen, 1997; Rogati and Yang, 2002). For computational cost reasons mentioned in Section 1, Chinese text categorization would benefit more from an high-performing aggressive term selection. This collection contains 55 categories and 71674 documents (9:1 split to training set and test set). Each documents belongs to only one category. Each category contains 399– 3374 documents. This collection was also used by Li et al. (2006). 20NG (20 Newsgroups1 ): This classical English document"
D09-1027,W08-1404,0,0.48597,"he web era with enormous information. The rest of the paper is organized as follows. In Section 2, we introduce and discuss the related work in this area. In Section 3, we give an overview of our method for keyphrase extraction. From Section 4 to Section 7, the algorithm is described in detail. Empirical experiment results are demonstrated in Section 8, followed by our conclusions and plans for future work in Section 9. 2 Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphras"
D09-1027,C08-1122,0,0.631285,"g methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphrase extraction algorithm for single document. Motivated by similar idea, Wan and Xiao (Wan and Xiao, 2008a) propose to adopt clustering methods to find a small number of similar documents to provide more knowledge for building word graphs for keyword extraction. Moreover, after our submission of this paper, we find that a method using community detection on semantic term graphs is proposed for keyphrase extraction from multi-theme documents (Grineva et al., 2009). In addition, some practi"
D09-1027,W04-3252,0,\N,Missing
D09-1027,W03-1028,0,\N,Missing
D10-1036,W03-1028,0,0.911462,"h P word w as its random jump probability with w∈V pz (w) = 1. The words that are more relevant to topic z will be assigned larger probabilities when performing the PageRank. For topic z, the topic-specific PageRank scores are defined as follows: Both PageRank and TPR are all iterative algorithms. We terminate the algorithms when the number of iterations reaches 100 or the difference of each vertex between two neighbor iterations is less than 0.001. 3.3 Extract Keyphrases Using Ranking Scores After obtaining word ranking scores using TPR, we begin to rank candidate keyphrases. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun phrases. We thus select noun phrases from a document as candidate keyphrases for ranking. The candidate keyphrases of a document is obtained as follows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1 . Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. I"
D10-1036,W08-1404,0,0.781596,"ent, alleged document, Arab government, slaying Wazir, State Department spokesman Charles Redman, Khalil Wazir(+) TPR, Rank 3 Topic on “terrorism” terrorist attacks(+), PLO leader Yasser Arafat(+), Abu Jihad, United States(+), alleged document, U.S. government document, Palestine Liberation Organization leader, State Department spokesman Charles Redman, political assassination(+), full cooperation Table 6: Extracted keyphrases by TPR. Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The clustering-based method performed well on short abstracts (with F-measure 0.382 on RESEARCH) but poorly on long articles (NEWS with F-me"
D10-1036,D09-1027,1,0.436746,"Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The clustering-based method performed well on short abstracts (with F-measure 0.382 on RESEARCH) but poorly on long articles (NEWS with F-measure score 0.216) due to two non-trivial issues: (1) how to determine the number of clus374 TFIDF (+5) PLO leader Yasser Arafat(+), PLO attacks, PLO offices, PLO officials(+), PLO leaders, Abu Jihad, terrorist attacks(+), Khalil Wazir(+), slaying wazir, political assassination(+) PageRank (+3) PLO leader Yasser Arafat(+), PLO officials(+), PLO attacks, United States(+), PLO offices, PLO leaders, State Department spokesman Charles Redman, U.S. government"
D10-1036,J00-2011,0,0.531618,"nd extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and change dynamically, which demands keyphrase extraction to be efficient and adaptable. However, since human labeling is time consuming, it is impractical to label training set from time to tim"
D10-1036,voorhees-tice-2000-trec,0,0.0555382,"order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref is desirable to evaluate the performance considering the order in which the extracted keyphrases are ranked. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase and n is an incorrect keyphrase, Bpref is defined as follows, Bpref = 1 X |n ranked higher than r| . (7) 1− R M r∈R The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the rank of the first correct keyphrase with all extracted keyphrases, MRR is defined as follows, MRR = 1 X 1 , |D| rankd (8) d∈D 4.2 Evaluation Metrics For evaluation, the words in both standard and extracted keyphrases are reduced to base forms using 2 http://wanxiaojun1979.googlepages.com. It was obtained from the author. 4 http://en.wikipedia.org/wiki/Wikipedia_ database. 3 370 where D is the document set for keyphrase extraction. Note that although the evaluation s"
D10-1036,C08-1122,0,0.828591,"t noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In PageRank for keyphrase extraction, the ranking X e(wj , wi ) Rz (wi ) = λ Rz (wj )+(1−λ)pz (wi ). score of a candidate keyphrase p is computed by O(wj ) j:wj →wi summing up the ranking scores of all words within (3) the phrase: R(p) = P wi ∈p R(wi ) (Mihalcea and TaIn Fig. 1, we show an example with two topics. In rau, 2004; Wan and Xiao, 2008a; Wan and Xiao, this figure, we use the size of circles to indicate how 2008b). Then candidate keyphrases are ranked in relevant the word is to the topic. In the PageRanks descending order of ranking scores. The top M canof the two topics, high preference values will be as- didates are selected as keyphrases. signed to different words with respect to the topic. In TPR for keyphrase extraction, we first comFinally, the words will get different PageRank val- pute the ranking scores of candidate keyphrases sepues in the two PageRanks. arately for each topic. That is for each topic z we The setti"
D10-1036,W04-3252,0,\N,Missing
D11-1146,P00-1041,0,0.0105252,"in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare description-annotation pairs for learning translation probab"
D11-1146,J93-2003,0,0.068816,"in Table 1 are triggered to the tags in annotation. • Some tags in the annotation do appear in the corresponding description, but they may not be statistically significant. Figure 1: An example of the word trigger method for suggesting tags given a description. • Some tags may even not appear in the description. 2 It is not trivial to reduce the vocabulary gap and find the semantic correspondence between descriptions and annotations. By regarding both the description and the annotation as parallel summaries of a resource, we use word alignment models in statistical machine translation (SMT) (Brown et al., 1993) to estimate the translation probabilities between the words in descriptions and annotations. SMT has been successfully applied in many applications to bridge vocabulary gap. For detailed descriptions of related work, readers can refer to Section 2.2. In this paper, besides employing word alignment models to social tagging, we also propose a method to efficiently build description-annotation pairs for sufficient learning translation probabilities by word alignment models. Based on the learned translation probabilities between words in descriptions and annotations, 1578 Related Work 2.1 Social"
D11-1146,P03-1003,0,0.0202143,"ggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag"
D11-1146,D09-1027,1,0.796786,"those specific tags such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap betw"
D11-1146,D09-1051,0,0.0140124,"those specific tags such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap betw"
D11-1146,J10-3002,0,0.101687,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,D10-1036,1,0.904207,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,P10-1085,0,0.0388215,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,W11-0316,1,0.776677,"ities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical"
D11-1146,J03-1002,0,0.0208445,"iption is usually limited to hundreds of words. Meanwhile, it is common that some popular resources are annotated by multiple users with thousands of tags. For example, the tag Dumas is annotated by 2, 748 users for the book in Table 1. We have to deal with the lengthunbalance between a resource description and its corresponding annotation for two reasons. • It is impossible to list all annotated tags on the annotation side of a description-annotation pair. The performance of word alignment models will also suffer from the unbalanced length of sentence pairs in the parallel training data set (Och and Ney, 2003). • Moreover, the annotated tags may have different importance for the resource. It would be unfair to treat these tags without distinction. Here we propose a sampling method to prepare length-balanced description-annotation pairs for word alignment. The basic idea is to sample a bag of tags from the annotation according to tag weights and make the generated bag of tags with comparable length with the description. We consider two parameters when sampling tags. First, we have to select a tag weighting type for sampling. In this paper, we investigate two straightforward sampling types, including"
D11-1146,W04-3219,0,0.0127481,"ription. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources,"
D11-1146,J10-3010,0,0.0367492,"vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a c"
D11-1146,P07-1059,0,0.00788117,"resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe th"
D11-1146,C08-1093,0,0.0276697,"from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-ann"
D11-1146,P08-1082,0,0.0133568,"We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method ("
D11-1146,C10-1149,0,0.0122965,"tions of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare de"
D11-1146,C10-1148,0,0.013484,"tions of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare de"
D11-1146,W04-3252,0,\N,Missing
D13-1054,P06-1067,0,0.014521,"ns for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language mo"
D13-1054,P12-1050,0,0.12561,"ing’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phra"
D13-1054,N13-1003,0,0.0456632,"ts on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based system"
D13-1054,C10-2033,1,0.891478,"nformation from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translatio"
D13-1054,D08-1089,0,0.423945,"t syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since"
D13-1054,N10-1129,0,0.0324751,"eural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to righ"
D13-1054,P13-1088,0,0.022199,"et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network consists of four autoencoder"
D13-1054,P12-1092,0,0.00995628,"y for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders"
D13-1054,P08-1067,0,0.0105112,"As recursive autoencoders are capable of producing vector space representations for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1.07 BLEU points over the baseline on the NIST 2008 Chinese-English dataset. There are a number of interesting directions we would like to pursue in the near future. First, replacing the MaxEnt classifier with a neural one redefines the conditions for risk-free hypothesis recombination. We find that the number of hypotheses that can be recombined reduces in our system. Therefore, we plan to use forest reranking (Huang, 2008) to alleviate this problem. Second, it is interesting to follow Socher et al. (2013) to combine linguistically-motivated labels with recursive neural networks. Another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by RAEs. It is necessary to investigate more efficient decoding algorithms. Finally, it is possible to apply our method to other phrase-based and even syntax-based systems. Acknowledgments This research is supported by the 863 Program under the grant No. 2012AA011102, by the Boeing Tsinghua"
D13-1054,J99-4005,0,0.0200293,"e recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memo"
D13-1054,N03-1017,0,0.0747853,"significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, esp"
D13-1054,P07-2045,0,0.0116822,"ing orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into p"
D13-1054,J04-4002,0,0.104451,"es over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reorder"
D13-1054,N04-1021,0,0.0119397,"are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translation"
D13-1054,W05-0908,0,0.0293904,"aseline by replacing the MaxEnt reordering model with a neural model. Both the systems have the same pruning settings: the threshold pruning parameter is set to 0.5 and the histogram pruning parameter to 40. For minimum-error-rate training, both systems generate 200-best lists. 4.2 MT Evaluation Table 1 shows the case-insensitive BLEU-4 scores of the baseline system and our system on the development and test sets. Our system outperforms the baseline system by 1.21 BLEU points on the development set and 1.07 on the test set. Both the differences are statistically significant at p = 0.01 level (Riezler and Maxwell, 2005). Table 2 shows the number of sentences that our system has a higher (>), equal (=) or lower (&lt;) BLEU score on the NIST 2008 dataset. We find that our system is superior to the baseline system for long 3 The choice of α is very important for achieving high BLEU scores. We tried a number of intervals and found that the classification accuracy is most stable in the interval [0.100,0.125]. 574 neural maxent 5 10 15 20 Length 25 30 35 Figure 4: Comparison of reordering classification accuracies between the MaxEnt and neural classifiers over varying phrase lengths. “Length” denotes the sum of the l"
D13-1054,D11-1014,0,0.255232,"Missing"
D13-1054,D12-1110,0,0.0166609,"ither manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering c"
D13-1054,P13-1045,0,0.0698655,". As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network"
D13-1054,N04-4026,0,0.093585,"erating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word ins"
D13-1054,J97-3002,0,0.42934,"T) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. They use the CKY algorithm to recursively merge two blocks (i.e., a pair of source and target strings"
D13-1054,P06-1066,0,0.651966,"which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamless"
D13-1054,I08-1066,0,0.274945,"lingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it po"
D13-1054,J10-3009,0,0.0150292,"MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manua"
D13-1054,C04-1030,0,0.0511187,"space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omission"
D13-1054,J09-4009,0,\N,Missing
D14-1110,S07-1054,0,0.00826163,"We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1 -measure on the Semeval-2007 coarsegrained all-words dataset (Navigli et al., 2007). Table 5 report"
D14-1110,W06-1663,0,0.0148635,"e disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and"
D14-1110,P12-1092,0,0.864337,"chnology Department of Computer Science and Technology Tsinghua University, Beijing 100084, China cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn Abstract Most word representation methods assume each word owns a single vector. However, this is usually problematic due to the homonymy and polysemy of many words. To remedy the issue, Reisinger and Mooney (2010) proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors within the cluster. Huang et al. (2012) followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations. These cluster-based models conduct unsupervised word sense induction by clustering word contexts and, thus, suffer from the following issues: Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguat"
D14-1110,S07-1068,0,0.0182737,"ich are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1 -measure on the Semeval-2007 coarsegrai"
D14-1110,H05-1053,0,0.00865122,"es traditional PageRank over the semantic graph based on WordNet and obtains a context-independent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7 We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. 1030 Algorithm Random BL MFS BL k-NN Static PR Personalized PR Degree Our Model Sports Recall 19.5 19.6 30.3 20.1 35.6 42.0 57.3 Finance Recall 19.6 37.1 43.4 39.6 46.9 47.8 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the"
D14-1110,P10-1116,0,0.00544857,"s on domain-specific WSD, and achieves competitive performance on coarsegrained all-words WSD. Our model only requires large-scale unlabeled text corpora and a sense inventory for WSD, thus it can be easily applied to other corpora and tasks. There are still several open problems that should be investigated further: 1033 1. Because the senses of words change over time (new senses appear), we will incorporate cluster-based methods in our model to find senses that are not in the sense inventory. 2. We can explore other WSD methods based on sense vectors to improve our performance. For example, (Li et al., 2010) used LDA to perform data-driven WSD in a manner similar to our model. We may integrate the advantages of these models and our model together to build a more powerful WSD system. Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In Proceedings of ACL, pages 92–97. 3. To learn better sense vectors, we can exploit the semantic relations (such as the hypernym and hyponym relations defined in WordNet) between senses in our model. Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources"
D14-1110,D10-1036,1,0.478393,"Missing"
D14-1110,D11-1146,1,0.436874,"proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object"
D14-1110,W11-0316,1,0.276159,"proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object"
D14-1110,H93-1061,0,0.90814,"ependent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7 We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. 1030 Algorithm Random BL MFS BL k-NN Static PR Personalized PR Degree Our Model Sports Recall 19.5 19.6 30.3 20.1 35.6 42.0 57.3 Finance Recall 19.6 37.1 43.4 39.6 46.9 47.8 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the dataset, and the best results are shown in bold. The differences between other results a"
D14-1110,P13-1045,0,0.0196764,"confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 4.1 Related Work Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word re"
D14-1110,C14-1016,0,0.524868,"architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labe"
D14-1110,P10-1040,0,0.156264,"en ε = 0.0, we use every disambiguation result to update the context vector. When ε 6= 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 4.1 Related Work Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in ve"
D14-1110,S07-1006,0,0.042888,"81.6 82.5 85.3 All words F1 62.7 78.9 77.0 82.5 83.2 81.7 73.9 75.8 79.6 82.6 Table 5: Performance on Semeval-2007 coarsegrained all-words WSD. In the type column, U, Semi and S stand for unsupervised, semisupervised and supervised, respectively. The differences between the results in bold in each column of the table are not statistically significant at p < 0.05. methods rely greatly on high-quality semantic relations or annotated data, which are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SS"
D14-1110,D14-1113,0,0.508916,"roach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such a"
D14-1110,P10-1154,0,0.0289566,"Missing"
D14-1110,N10-1013,0,0.421671,"pgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model’s is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-paramet"
D14-1110,C14-1048,0,\N,Missing
D14-1110,P10-2017,0,\N,Missing
D15-1082,D13-1080,0,0.031571,"s. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown that PTransE can achiev"
D15-1082,P11-1055,0,0.238952,"Missing"
D15-1082,D11-1049,0,0.241022,"n in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown"
D15-1082,D12-1093,0,0.0709095,"s of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation"
D15-1082,P09-1113,0,0.211437,"Missing"
D15-1082,P08-1028,0,0.00866539,"ource obtained from the entity n. For each relation path p, we set the initial resource in h as Rp (h) = 1. By performing resource allocation recursively from h through the path p, the tail entity t eventually obtains the resource Rp (t) which indicates how much information of the head entity h can be well translated. We use Rp (t) to measure the reliability of the path p given (h, t), i.e., R(p|h, t) = Rp (t). 2.3 BornInCity p = r1 · . . . · rl . (6) Both addition and multiplication operations are simple and have been extensively investigated in semantic composition of phrases and sentences (Mitchell and Lapata, 2008). Recurrent Neural Network (RNN). RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation"
D15-1082,P15-1016,0,0.503149,"NN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015). For a multiple-step relation path triple (h, p, t), we could have followed TransE and define the energy function as E(h, p, t) = ||h + p − t||. However, since we have minimized ||h + r − t|| with the direct relation triple (h, r, t) to make sure r ≈ t−h, we may directly define the energy function of (h, p, t) as 2.5 For optimization, we employ stochastic gradient descent (SGD) to minimize the loss function. We randomly select a valid triple from the training set iteratively for learning. In the implementation, we also enforce constraints on the norms of the embeddings h, r, t. That is, we se"
D15-1082,D12-1042,0,0.0629002,"21, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Rev+Path PTransE (ADD, 2-step) -TransE -Path PTransE (MUL, 2-step) PTransE (RNN, 2-step) PTransE (ADD, 3-step) 3.3 Mean Rank Raw Filter 2.8 2.5 2.6 2.3 2.4 1.9 1.7 1.2 135.8 135.3 2.0 1.6 2.5 2.0 1.9 1.4 1.8 1.4 Predicting Tail Entities (Hits@10) 1-to-1 1-to-N N-to-1 N-to-N 34.9 14.6 68.3 41.3 32.7 14.9 61.6 43.3 28.2 13.1 76.0 41.8 43.7 19.7 66.7 50.0 65.5 39.8 83.3 67.2 79.2 37.4 90.4 72.1 71.5 49.0 85.0 72.9 9"
D15-1082,D13-1136,0,0.130463,"8 89.2 34.1 69.2 74.6 86.6 43.7 70.6 91.0 92.8 60.9 83.8 89.0 86.8 57.6 79.8 88.9 84.0 56.3 84.5 90.1 92.0 58.7 86.1 tion paths. As compared with TransE, the inferior of PTransE-TransE also indicates that entity representations are informative and crucial for relation prediction. as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Re"
D15-1144,J07-2003,0,0.822202,"hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the standard alignment tools are not optimal for training syntax-based models. As a result, they have to resort to realigning. On the other hand, the consistency constraint used in most translation rule extraction algorithms tolerate wrong links within consistent phrase pairs. Chiang (2007) uses the union of two unidirectional alignments, which usually has a low precision, for extracting hierarchical phrases. Therefore, it is important to include both alignment model score and the consistency constraint in the optimization objective of word alignment. In this work, we propose to use coverage, which measures how well extracted phrases can 1228 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1228–1237, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Figure 1: (a) An alignment resulting in a set"
D15-1144,D09-1037,0,0.12616,"n often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only"
D15-1144,P08-2007,0,0.132395,"anslation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and bal"
D15-1144,P10-1147,0,0.0768572,"p between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that maximizing alignment accuracy does not necessarily lead to improved translation performance. 3 Modeling Consistency in Word Alignment Our intuition is that including the consistency constraint in word alignment can hopefully reduce the discrepancy between alignment and translation. While this idea has been suggested by a number of authors (e.g., (Deng and Zhou, 2009; DeNero and Klein, 2010)), our goal is to optimize arbitrary alignment models with respect to end-toend translation in the search phase without labeled data (see Related Work for detailed comparison). A natural way is to include consistency in the optimization objective as a regularization term. However, as consistency is only defined at the phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and effici"
D15-1144,P09-2058,0,0.26144,"s a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that maximizing alignment accuracy does not necessarily lead to improved translation performance. 3 Modeling Consistency in Word Alignment Our intuition is that including the consistency constraint in word alignment can hopefully reduce the discrepancy between alignment and translation. While this idea has been suggested by a number of authors (e.g., (Deng and Zhou, 2009; DeNero and Klein, 2010)), our goal is to optimize arbitrary alignment models with respect to end-toend translation in the search phase without labeled data (see Related Work for detailed comparison). A natural way is to include consistency in the optimization objective as a regularization term. However, as consistency is only defined at the phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short),"
D15-1144,J07-3002,0,0.0292539,"rpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the stand"
D15-1144,P06-1121,0,0.304,"discriminative alignment approaches across various languages and translation models. 1 Introduction Word alignment, which aims to identify the correspondence between words in two languages, plays an important role in statistical machine translation (Brown et al., 1993). Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a lo"
D15-1144,P04-1064,0,0.0363259,"eps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, s"
D15-1144,P06-1002,0,0.0282657,"ility of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted becaus"
D15-1144,P09-1104,0,0.0448938,"Missing"
D15-1144,J93-2003,0,0.0539631,"Missing"
D15-1144,H05-1012,0,0.0401707,"roves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may n"
D15-1144,P04-1023,0,0.0949041,"traction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constra"
D15-1144,P07-1036,0,0.0227503,"Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the stand"
D15-1144,D07-1091,0,0.294215,"n English sentence “EU and Russia hold summit in Moscow”. We use black circles to denote links. The link (1, 1) indicates that the first Chinese word “oumeng” and the first English word “EU” are translations of each other. 1229 1. No words in the source phrase are aligned with words outside the target phrase and vice versa: ∀(j, i) ∈ a : j1 ≤ j ≤ j2 ↔ i1 ≤ i ≤ i2 , 2. At least one word in the source phrase is aligned with at least one word in the target phrase: ∃(j, i) ∈ a : j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 . Alignment consistency forms the basis of translation rule extraction in modern SMT systems (Koehn and Hoang, 2007; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). In Figure 1, (f13 , e31 ) is consistent with the alignment because all words in “oumeng he eluosi” are aligned with all words in “EU and Russia”. In contrast, in Figure 1(b), “huiwu shounao” and “hold summit” are not consistent with the alignment because “hold” is also aligned to a word “juxing” outside. However, alignment consistency only defines a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 )"
D15-1144,N03-1017,0,0.0633295,"Missing"
D15-1144,P06-1096,0,0.1416,"e phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and efficient to calculate during search (Deng and Zhou, 2009). Unfortunately, optimizing with respect to phrase count is prone to yield alignments with very few links in a biased way, which result in a large number of bilingual phrases extracted from a small fraction of the training data. Another alternative is reachability (Liang et al., 2006a; Yu et al., 2013) that indicates whether there exists a full derivation to recover the training data. However, calculating reachability faces a major problem: a large portion of training data cannot be fully recovered due to noisy alignments and the distortion limit (Yu et al., 2013). In this work, we propose coverage, which reflects how well extracted phrases can recover the training data, to measure the sentence-level consistency. In the following, we will introduce a number of definitions to facilitate the exposition. Definition 5 A source word fj is said to be covered by a bilingual phra"
D15-1144,N06-1014,0,0.0399502,"eter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted becaus"
D15-1144,P06-1077,1,0.862866,"We use black circles to denote links. The link (1, 1) indicates that the first Chinese word “oumeng” and the first English word “EU” are translations of each other. 1229 1. No words in the source phrase are aligned with words outside the target phrase and vice versa: ∀(j, i) ∈ a : j1 ≤ j ≤ j2 ↔ i1 ≤ i ≤ i2 , 2. At least one word in the source phrase is aligned with at least one word in the target phrase: ∃(j, i) ∈ a : j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 . Alignment consistency forms the basis of translation rule extraction in modern SMT systems (Koehn and Hoang, 2007; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). In Figure 1, (f13 , e31 ) is consistent with the alignment because all words in “oumeng he eluosi” are aligned with all words in “EU and Russia”. In contrast, in Figure 1(b), “huiwu shounao” and “hold summit” are not consistent with the alignment because “hold” is also aligned to a word “juxing” outside. However, alignment consistency only defines a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that"
D15-1144,J10-3002,1,0.893926,"4 ( Similarly, we also distinguish between Cs+t and Cs+l depending on the tightness of extracted phrases. P|f (s) | Algorithm 2 Updating the set of extracted bilingual phrases after adding a link. a∈A(f ,e) (5) where A(f , e) is a set of all possible alignments for the sentence pair. Algorithm 1 shows the consistency-aware search algorithm for word alignment. The input of the algorithm includes a source sentence f , a target sentence e, a set of model parameters θ, phrase length limit w, pruning parameters β and b, and the number of most likely alignments to be retaind n (line 1). Inspired by Liu et al. (2010), 2 Note that training algorithms are unchanged. We only introduce a new search algorithm that takes coverage into consideration. We leave consistency-aware training algorithms for arbitrary alignment models for future work. 1231 the algorithm starts with an empty alignment a together with an empty phrase set B. We use open to store active alignments during search and N to store top-n alignments after search (lines 2-4). The procedure A DD(open, ha, Bi, β, b) adds ha, Bi to open and discards any alignment that has a score worse than β multiplied by the best score in the list or the score of th"
D15-1144,W02-1018,0,0.271474,"Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) sho"
D15-1144,J03-1002,0,0.0563828,"h” denotes “hard”, “s” denotes “soft”, “l” denotes “loose”, and “t” denotes “tight”. The BLEU scores were calculated on the development set. For quick validation, we used a small fraction of the training data to train the phrase-based model. 5 5.1 Experiments Setup 5.1.1 Languages and Datasets We evaluated our approach in terms of alignment and translation quality on five language pairs: Chinese-English (ZH-EN), Czech-English (CSEN), German-English (DE-EN), Spanish-English (ES-EN), and French-English (FR-EN). The evaluation metrics for alignment and translation are alignment error rate (AER) (Och and Ney, 2003) and case-insensitive BLEU (Papineni et al., 2002), respectively. For Chinese-English, the training data consists of 1.2M pairs of sentences with 30.9M Chinese words and 35.5M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set (Liu and Sun, 2015). 3 For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005 and 2008 datas"
D15-1144,J04-4002,0,0.798185,"ware search algorithm significantly outperforms both generative and discriminative alignment approaches across various languages and translation models. 1 Introduction Word alignment, which aims to identify the correspondence between words in two languages, plays an important role in statistical machine translation (Brown et al., 1993). Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality."
D15-1144,P02-1040,0,0.0963257,"otes “loose”, and “t” denotes “tight”. The BLEU scores were calculated on the development set. For quick validation, we used a small fraction of the training data to train the phrase-based model. 5 5.1 Experiments Setup 5.1.1 Languages and Datasets We evaluated our approach in terms of alignment and translation quality on five language pairs: Chinese-English (ZH-EN), Czech-English (CSEN), German-English (DE-EN), Spanish-English (ES-EN), and French-English (FR-EN). The evaluation metrics for alignment and translation are alignment error rate (AER) (Och and Ney, 2003) and case-insensitive BLEU (Papineni et al., 2002), respectively. For Chinese-English, the training data consists of 1.2M pairs of sentences with 30.9M Chinese words and 35.5M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set (Liu and Sun, 2015). 3 For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005 and 2008 datasets as the test sets. For other languages, the tra"
D15-1144,P08-1066,0,0.0179969,"In this work, we have presented a general framework for optimizing word alignment with respect to machine translation. We introduce coverage to measure how well extracted bilingual phrases can recover the training data. We develop a consistency-aware search algorithm that calculates coverage on the fly during search efficiently. Experiments show the our approach is effective in both alignment and translation tasks across various alignment models, translation models, and language pairs. In the future, we plan to apply our approach to syntax-based models (Galley et al., 2006; Liu et al., 2006; Shen et al., 2008) and include the constituency constraint in the optimization objective. It is also interesting to develop consistency-aware training algorithms for word alignment. Acknowledgements Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808) and the National Natural Science Foundation of China (No. 61331013 and No. 61432013). Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by"
D15-1144,J10-2004,0,0.0155466,"ed alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the standard alignment tools are not optimal for training syntax-based models. As a result, they have to resort to realigning. On the other hand, the consistency constraint used in most translation rule extraction algorithms tolerate wrong links within consistent phrase pairs. Chiang (2007) uses the union of two unidirectional alignments, which usually has a low precision, for extracting hierarchical phrases. Therefore, it is important to include both alignment model score and the consistency constraint in the optimization objective of word alignment. In this work, we propose to use"
D15-1144,J97-3002,0,0.460318,"ndicate that using forced decoding to select reachable sentences with an unlimited distortion limit runs in O(2n n3 ) time. In contrast, calculating coverage is much easier and more efficient by ignoring the dependency between phrases but still retains the spirit of measuring recovery. 6.2 Structural Constraints for Alignment Modeling structural constraints in alignment has received intensive attention in the community, either directly modeling phrase-to-phrase alignment (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009) or intersecting synchronous grammars with alignment (Wu, 1997; Zhang and Gildea, 2005; Haghighi et al., 2009). Our work is in spirit most close to (Deng and Zhou, 2009) and (DeNero and Klein, 2010). Deng and Bowen (2009) cast combining IBM Model 4 alignments in two directions as an optimization problem driven by an effectiveness function. They evaluate the impact of adding or removing a link with respect to phrase extraction using the effectiveness function of phrase count. The major difference is that we generalize their idea to arbitrary alignment models in the search phase rather than bidirectional alignment combination in the post-processing phase."
D15-1144,D13-1112,0,0.0671753,"efinition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and efficient to calculate during search (Deng and Zhou, 2009). Unfortunately, optimizing with respect to phrase count is prone to yield alignments with very few links in a biased way, which result in a large number of bilingual phrases extracted from a small fraction of the training data. Another alternative is reachability (Liang et al., 2006a; Yu et al., 2013) that indicates whether there exists a full derivation to recover the training data. However, calculating reachability faces a major problem: a large portion of training data cannot be fully recovered due to noisy alignments and the distortion limit (Yu et al., 2013). In this work, we propose coverage, which reflects how well extracted phrases can recover the training data, to measure the sentence-level consistency. In the following, we will introduce a number of definitions to facilitate the exposition. Definition 5 A source word fj is said to be covered by a bilingual phrase B = (fjj12 , eii"
D15-1144,P05-1059,0,0.0630576,"at using forced decoding to select reachable sentences with an unlimited distortion limit runs in O(2n n3 ) time. In contrast, calculating coverage is much easier and more efficient by ignoring the dependency between phrases but still retains the spirit of measuring recovery. 6.2 Structural Constraints for Alignment Modeling structural constraints in alignment has received intensive attention in the community, either directly modeling phrase-to-phrase alignment (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009) or intersecting synchronous grammars with alignment (Wu, 1997; Zhang and Gildea, 2005; Haghighi et al., 2009). Our work is in spirit most close to (Deng and Zhou, 2009) and (DeNero and Klein, 2010). Deng and Bowen (2009) cast combining IBM Model 4 alignments in two directions as an optimization problem driven by an effectiveness function. They evaluate the impact of adding or removing a link with respect to phrase extraction using the effectiveness function of phrase count. The major difference is that we generalize their idea to arbitrary alignment models in the search phase rather than bidirectional alignment combination in the post-processing phase. In addition, we find tha"
D15-1196,N09-1003,0,0.0123629,"presentation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1 , and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 × 10−6 . 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Far"
D15-1196,D14-1110,1,0.714851,"Missing"
D15-1196,P14-5004,0,0.0135374,"09) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1689 1 https://code.google.com/p/word2vec/ Model Skip-Gram RNN NNSE OIWE-NPG OIWE-IPG WS-203 67.35 49.28 51.06 63.71 71.74 RG-65 50.49 50.19 56.48 56.85 57.16 MEN 52.56 43.44 57.60 56.68 meanings of these dimensions. One can also refer to http://github.com/skTim/OIWE to find top-5 words for all dimensions. No. 1 2 Table 1:"
D15-1196,P12-1092,0,0.0305669,"Missing"
D15-1196,C12-1118,0,0.582632,"l (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE realizes interpretable word embeddings by applying non-negative constraints for word embeddings. Although NNSE learns word embeddings with good interpretabilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data. Inspired by the characteristics of NMF methods (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabi"
D15-1196,D14-1162,0,0.130543,"paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗ Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what"
D15-1196,P12-1027,0,0.0324909,"s (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent (Lin, 2007) for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also employ adaptive gradient descent (Sun et al., 2012) to speedup learning convergence. We name the proposed models as online interpretable word embeddings (OIWE). 1687 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. For experiments, we implement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE."
D15-1196,P10-1040,0,0.0188767,"ization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF appr"
D15-1210,J93-2003,0,0.172973,"gnificant improvements over two state-ofthe-art alignment methods. 1 Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these mod"
D15-1210,N10-1015,0,0.0575352,"Missing"
D15-1210,P10-1065,0,0.0238281,"n word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viterbi EM algorithm to train the joint model."
D15-1210,J07-2003,0,0.502152,"between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricte"
D15-1210,P06-1121,0,0.0607179,"tric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one align"
D15-1210,D07-1091,0,0.657698,"telligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China ‡ Samsung R&D Institute of China, Beijing 100028, China {liuchunyang2012,liuyang.china,luanhuanbo}@gmail.com, sms@tsinghua.edu.cn h0517.yu@samsung.com Abstract one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Exp"
D15-1210,N03-1017,0,0.105236,"Missing"
D15-1210,N06-1014,0,0.273714,"t. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as"
D15-1210,C10-1080,1,0.851806,"that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. W"
D15-1210,P06-1077,1,0.755036,"o directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al."
D15-1210,J03-1002,0,0.269429,"ey Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China ‡ Samsung R&D Institute of China, Beijing 100028, China {liuchunyang2012,liuyang.china,luanhuanbo}@gmail.com, sms@tsinghua.edu.cn h0517.yu@samsung.com Abstract one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the infer"
D15-1210,P02-1040,0,0.0951639,".2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. G IZA ++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al."
D15-1210,C96-2141,0,0.919358,"associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the translation accuracy, especially for distantly-related la"
D15-1210,P13-1106,0,0.0353913,"Missing"
D15-1210,C10-1135,1,0.818061,"case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other late"
D16-1171,D15-1263,0,0.00545718,"her et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification mod"
D16-1171,I13-1156,0,0.0472259,"i )2 , (13) where T is the numbers of predicted sentiment ratings that are identical with gold sentiment ratings, 1654 Baselines We compare our NSC model with several baseline methods for document sentiment classification: Majority regards the majority sentiment category in training set as the sentiment category of each document in test set. Trigram trains a SVM classifier with unigrams, bigrams and trigrams as features. TextFeature extracts text features including word and character n-grams, sentiment lexicon features, etc, and then train a SVM classifier. UPF extracts use-leniency features (Gao et al., 2013) and corresponding product features from training data, which is further concatenated with the features in Trigram an TextFeature. AvgWordvec averages word embeddings in a document to obtain document representation which is fed into a SVM classifier as features. SSWE generates features with sentiment-specific word embeddings (SSWE) (Tang et al., 2014) and then trains a SVM classifier. RNTN + RNN represents each sentence with the Recursive Neural Tensor Network (RNTN) (Socher et al., 2013) and feeds sentence representations into IMDB Yelp2013 Acc. RMSE Acc. RMSE Models without user and product"
D16-1171,D14-1181,0,0.0694938,"ge processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on th"
D16-1171,D15-1278,0,0.0124337,"k models to learn representations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Wor"
D16-1171,P14-5010,0,0.0086801,"Missing"
D16-1171,W02-1011,0,0.0421348,"p/NSC. 1 In this work, we focus on the task of documentlevel sentiment classification, which is a fundamental problem of sentiment analysis. Document-level sentiment classification assumes that each document expresses a sentiment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motiva"
D16-1171,D11-1014,0,0.171796,"s a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and prod"
D16-1171,D12-1110,0,0.0813482,"the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics"
D16-1171,D13-1170,0,0.290543,"ng and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant inf"
D16-1171,J11-2001,0,0.0271202,"ment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentime"
D16-1171,P15-1150,0,0.0119584,"epresentations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence"
D16-1171,P14-1146,0,0.099911,"Missing"
D16-1171,D15-1167,0,0.531648,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,P15-1098,0,0.869986,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,N16-1174,0,0.252278,"phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification model. tence or sentences from a document (Yang et al., 2016). Most existing sentiment classification models ignore the global user preference and product characteristics, which have crucial effects on the sentiment polarities. To address this issue, (Tang et al., 2015b) propose to add user/product preference matrices and representation vectors into CNN models. Nevertheless, it suffers from high model complexity and only considers word-level preference rather than semantic levels. In contrast, we propose an efficient neural sentiment classification model with users and products to serve as attentions in both word and semantic levels. 3 Methods In this s"
D17-1186,D13-1080,0,0.00904531,"Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embedd"
D17-1186,D15-1038,0,0.0104799,"arning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ago. Figure 2: The architecture of CNN used for text encoder. 3.1.1 Input Vector First, we transform the words {w1 , w2 , · · · , wl } in sentence s into vector"
D17-1186,W09-2415,0,0.0260005,"Missing"
D17-1186,P11-1055,0,0.352923,"the source of distant supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014)"
D17-1186,D11-1049,0,0.0351932,"rmation from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Represe"
D17-1186,D12-1093,0,0.0531395,"f each relation. 3.1 Text Encoder As shown in Fig. 2, we use a CNN to extract information from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non"
D17-1186,D15-1082,1,0.176633,"rst transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ag"
D17-1186,P16-1200,1,0.504287,"annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we know that “h is the f"
D17-1186,P09-1113,0,0.875034,"ntly, petabytes of natural-language text containing thousands of different structure types are readily available, which is an important resource for automatically finding unknown relational facts. Hence, relation extraction (RE), defined as the task of extracting structured information from plain text, has attracted much interest. Most existing supervised RE systems usually suffer from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained"
D17-1186,P16-1105,0,0.0561127,"Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural models could also be easil"
D17-1186,P15-1016,0,0.0143213,"s of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married"
D17-1186,D15-1044,0,0.0415961,"ltilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entit"
D17-1186,C14-1008,0,0.00772067,"(Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn"
D17-1186,P15-1061,0,0.200168,"d relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. F"
D17-1186,P13-1045,0,0.0177896,"et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which dir"
D17-1186,D12-1110,0,0.15958,"from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful i"
D17-1186,D12-1042,0,0.306622,"supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al."
D17-1186,D15-1206,0,0.0616817,"lation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural mo"
D17-1186,P17-1166,0,0.0427788,"Missing"
D17-1186,D15-1203,0,0.654901,"ining data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we kn"
D17-1186,C14-1220,0,0.549229,"acks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help"
D17-1207,D16-1250,0,0.196759,"eted as translations. Moreover, this interpretation naturally handles multiple alternative translations. For example, the Chinese word “mao” can be translated to “cat” or “kitten”, as shown in Figure 1. 2.2 The Form of the Transformation The approximate isomorphism across embedding spaces inspires researchers to use a simple form of transformation. For example, Mikolov et al. (2013a) chose to use a linear transformation, i.e. the transformation G parametrized by a matrix. Later, proposals for using an orthogonal transformation are supported empirically (Xing et al., 2015; Zhang et al., 2016c; Artetxe et al., 2016) and theoretically (Smith et al., 2017). Indeed, an orthogonal transformation has desirable properties in this setting. If G is an orthogonal matrix that transforms the source embeddings into the target space, then its transpose (also its inverse) G> 1935 performs transformation in the reverse direction. In that case, any word embedding a can be recovered by transforming back and forth because G> Ga = a. Moreover, computing the cosine similarity between a source embedding a and a target embedding b will be independent of the semantic space in which the similarity is measured, > because > > > b"
D17-1207,W16-1208,0,0.0182111,"ing. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embeddings and apply the EMD program as a postprocessing step to automatically produce multiple alternative"
D17-1207,W16-1614,0,0.0828985,"the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces to matching their means and variances, but this assumption is hard to justify and interpret. In contrast, our proposal does not make any assumption on the distributions, and directly matches the transformed source embedding distribution with the target distribution by minimizing their earth mover’s distance. Another attempt to learn cross-lingual embedding transformation without supervision is (Barone, 2016). Architectures of generative adversarial nets and adversarial autoencoders (Makhzani et al., 2015) are experimented, but the reported results are not positive. We tried the publicly available code on our data and obtained negative results as well. This outcome is likely caused by the training difficulty pointed out by (Arjovsky and Bottou, 2017), as traditional GAN training minimizes Jensen-Shannon divergence between distributions, which can provide pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architect"
D17-1207,C16-1171,0,0.0888885,"), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces to matching their means and variances, but this assumption is hard to justify and interpret. In contrast, our proposal does not make any assumption on the distributions, and directly matches the transformed source embedding distribution with the target distribution by minimiz"
D17-1207,D15-1131,0,0.0101445,"a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that wor"
D17-1207,D12-1025,0,0.0141087,"Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P e"
D17-1207,D13-1173,0,0.148339,"Missing"
D17-1207,P15-1081,0,0.0146857,"ction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and"
D17-1207,C16-1331,0,0.0736013,"ive language pairs considered in this paper, as well as their typology dissimilarity and geographical distance. The typology dissimilarity is computed from features in the WALS database (Dryer and Haspelmath, 2013). It is defined as one minus relative Hamming similarity, which is in turn defined as the number of agreeing features divided by the number of total features available for the language pair (Albu, 2006; Cysouw, 2013b). As a rough approximation, the geographical distance is measured by the distance between the capital cities of the countries where the considered languages are spoken (Eger et al., 2016). The typology dissimilarity reflects genealogical influence on the divergence between languages, while the geographical distance indicates the effect of language contact. Both play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment f"
D17-1207,E14-1049,0,0.740932,"ic universals manifest themselves at various levels of linguistic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above q"
D17-1207,P04-1067,0,0.0364016,"tant roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parall"
D17-1207,W98-1217,0,0.117098,"butions, which can provide pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embedding"
D17-1207,P14-2037,0,0.0366263,"Missing"
D17-1207,P15-1027,0,0.332622,"istic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above question in the affirmative. The key insight is to view embed"
D17-1207,N15-1028,0,0.189155,"hemselves at various levels of linguistic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above question in the af"
D17-1207,W15-1521,0,0.0692121,"anguage, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distribution"
D17-1207,W12-0209,0,0.021362,"pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embeddings and apply the EMD program as a postp"
D17-1207,N15-1157,0,0.00994847,"gual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to b"
D17-1207,P15-2118,0,0.0193638,"Missing"
D17-1207,J03-1002,0,0.0343742,"the divergence between languages, while the geographical distance indicates the effect of language contact. Both play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from"
D17-1207,P16-2080,0,0.0168343,"attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states"
D17-1207,N15-1104,0,0.271726,"transport scheme can be readily interpreted as translations. Moreover, this interpretation naturally handles multiple alternative translations. For example, the Chinese word “mao” can be translated to “cat” or “kitten”, as shown in Figure 1. 2.2 The Form of the Transformation The approximate isomorphism across embedding spaces inspires researchers to use a simple form of transformation. For example, Mikolov et al. (2013a) chose to use a linear transformation, i.e. the transformation G parametrized by a matrix. Later, proposals for using an orthogonal transformation are supported empirically (Xing et al., 2015; Zhang et al., 2016c; Artetxe et al., 2016) and theoretically (Smith et al., 2017). Indeed, an orthogonal transformation has desirable properties in this setting. If G is an orthogonal matrix that transforms the source embeddings into the target space, then its transpose (also its inverse) G> 1935 performs transformation in the reverse direction. In that case, any word embedding a can be recovered by transforming back and forth because G> Ga = a. Moreover, computing the cosine similarity between a source embedding a and a target embedding b will be independent of the semantic space in which t"
D17-1207,P99-1067,0,0.248459,"h play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on super"
D17-1207,P15-2093,1,0.643394,"has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces"
D17-1207,C16-1300,1,0.890233,"Missing"
D17-1207,N16-1156,0,0.40728,"1. In order to connect the separate embedding spaces, we can try to transform the source embeddings so that they align well with target ones. Naturally, we need a measure for the quality of the alignment to guide our search for the transformation. As we aim to eliminate the need for crosslingual supervision from word translation pairs, the measure cannot be defined at the word level as in previous work (Mikolov et al., 2013a). Rather, it should quantify the difference between the entire distributions of embeddings. With this in mind, we find the earth mover’s distance to be a suitable choice (Zhang et al., 2016b). Its workings are illustrated in the right part of Figure 1. We can think of target embeddings as piles of earth, and transformed source embeddings as holes to be filled. Then the earth mover’s distance computes the minimal cost of moving the earth to fill the holes. Clearly, if the two sets of embeddings align well, the earth mover’s distance will be small. Therefore, we can try to find the transformation that minimizes the earth mover’s distance. Another desirable feature of the earth mover’s distance is that the computed transport scheme can be readily interpreted as translations. Moreov"
D17-1207,D13-1141,0,0.0146437,"approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the"
D17-1207,L16-1521,0,0.0065638,"V X s=1 t=1 (k) Tst cos GwsS , wtT (13)  + const S = min − G∈O(d) B T V X V X s=1 t=1  (k) Tst cos GwsS , wtT . Data Preparation B.1 Non-Parallel Corpora for Training Embeddings http://linguatools.org/tools/corpora/wikipediacomparable-corpora 3 https://github.com/BYVoid/OpenCC 4 http://thulac.thunlp.org 5 http://www.nltk.org 6 http://www.cis.uni-muenchen.de/˜schmid/tools/ TreeTagger and Moens, 2013). For the Japanese corpus, we use MeCab7 for word segmentation and POS tagging. For Turkish, we utilize the preprocessing tools (tokenization and POS tagging) provided in LORELEI Language Packs (Strassel and Tracey, 2016), and its English side is preprocessed by NLTK. The statistics of the preprocessed corpora is given in Table 3. B.2 The data for training monolingual word embeddings comes from Wikipedia comparable corpora.2 Following (Vuli´c and Moens, 2013), we retain only nouns with at least 1,000 occurrences except for Turkish-English, whose frequency cutoff threshold is 100, as the amount of data is relatively small in this low-resource setting. For the Chinese side, we first use OpenCC3 to normalize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC.4 The"
D17-1207,Q13-1001,0,0.0351308,"Missing"
D17-1207,P16-1024,0,0.0615432,"Missing"
D17-1207,N13-1011,0,0.0493086,"Missing"
D18-1033,P17-1042,0,0.0122721,"16). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism"
D18-1033,D15-1131,0,0.0220224,"Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally"
D18-1033,P15-1027,0,0.0324231,"from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive p"
D18-1033,J09-4006,1,0.731001,"wNet annotates sememes for 118, 346 Chinese words and 104, 025 English words. The number of sememes in total is 1, 983. Since some sememes only appear few times in HowNet, which are expected to be unimportant, we filter out those low-frequency sememes. Specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400. In our experiments, Chinese is source language and English is target language. To learn Chinese and English monolingual word embeddings, we extract about 2.0G text from Sogou-T1 and Wikipedia2 respectively. And we use THULAC3 (Li and Sun, 2009) for Chinese word segmentation. As for seed lexicon, we build it in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of wor"
D18-1033,D16-1136,0,0.0173221,", 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given"
D18-1033,P15-1145,0,0.019521,"no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vu"
D18-1033,N15-1184,0,0.415789,"st language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov"
D18-1033,O02-2003,0,0.939939,"ifferent from WordNet (Miller, 1995) which focuses on the relations between senses, it annotates each word with one or more relevant sememes. As illustrated in Fig. 1, the word apple has two senses including apple (fruit) and apple (brand) in HowNet. The sense apple (fruit) has one sememe fruit, and the sense apple (brand) has five sememes including computer, PatternValue, able, bring and SpecificBrand. There exist about 2, 000 sememes and over 100 thousand labeled Chinese and English words in HowNet. HowNet has been widely used in various NLP applications such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Zhang et al., 2005), question classification (Sun et al., 2007) and sentiment classification (Dang and Zhang, 2010). However, most languages do not have such sememe-based linguistic KBs, which prevents us understanding and utilizing human languages to a greater extent. Therefore, it is important to build sememe-based linguistic KBs for various languages. Manual construction for sememebased linguistic KBs requires efforts of many linguistic experts, which is time-consuming and Words are regarded as the smallest meaningful unit of speech or writing that can stand by"
D18-1033,E14-1049,0,0.0357019,"corporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at max"
D18-1033,N15-1028,0,0.0172286,"so have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditi"
D18-1033,W15-1521,0,0.0159648,"ns are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate eith"
D18-1033,D18-1493,1,0.722699,"mental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Be"
D18-1033,J15-4004,0,0.0334436,"source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our model which jointly trains word and sememe embeddings (named CLSP-SE) with a baseline method BiLex (Z"
D18-1033,P17-1187,1,0.598384,"he effectiveness of our model. Experimental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pai"
D18-1033,P18-1227,1,0.393378,"al sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation"
D18-1033,D14-1162,0,0.0869063,"sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plent"
D18-1033,S12-1049,0,0.0262445,"in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our m"
D18-1033,P14-2037,0,0.0204874,"e information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention o"
D18-1033,P15-2093,1,0.853601,"stic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of cont"
D18-1033,P16-1157,0,0.0124002,"le performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong"
D18-1033,P16-1024,0,0.0127337,". Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given a training word sequence {w"
D18-1033,P15-2118,0,0.0164701,"15; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our mo"
D18-1033,D13-1141,0,0.039799,"onal information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was"
D18-1049,N18-1118,0,0.216407,"ce and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the"
D18-1049,D11-1084,1,0.945014,"ology, Tsinghua University, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the"
D18-1049,D12-1108,0,0.535179,"versity, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, o"
D18-1049,P15-1001,0,0.0438623,"h et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the"
D18-1049,D13-1163,1,0.886079,"tlevel context to the encoder and decoder (see Section 2.3). It is clear that integrating document-level context into the encoder (Eq. 12) brings significant improvements (i.e., 45.97 vs. 47.51). Similarly, it is also beneficial to integrate document-level context into the decoder (Eq. 16). Combining both leads to further improvements. This observation suggests that documentlevel context does help to improve Transformer. 3.9 4 Developing document-level models for machine translation has been an important research direction, both for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012; Xiong et al., 2013a,b; Garcia et al., 2014) and NMT (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018). Most existing work on document-level NMT has focused on integrating document-level context into the RNNsearch model (Bahdanau et al., Effect of Context Gating As shown in Table 9, we also validated the effectiveness of context gating (see Section 2.3.3). We find that replacing residual connections with context gating leads to an overall improvement of 0.38 BLEU point. 3.10 Related Work Anal"
D18-1049,P18-1249,0,0.0320133,"nt-level context often includes several sentences, it is important to capture long-range dependencies and identify relevant information. We use multi-head self-attention (Vaswani et al., 2017) to compute the representation of documentlevel context because it is capable of reducing the maximum path length between long-range dependencies to O(1) (Vaswani et al., 2017) and determining the relative importance of different locations in the context (Bahdanau et al., 2015). Because of this property, multi-head self-attention has proven to be effective in other NLP tasks such as constituency parsing (Kitaev and Klein, 2018). where C(1) ∈ RD×M is the annotation of X&lt;k af(1) ter the first layer, A·,m ∈ RD×1 is the column vector for the m-th contextual word, and FNN(·) is a position-wise fully connected feed-forward network (Vaswani et al., 2017). This process iterates Nc times as follows:   A(n) = MultiHead C(n−1) , C(n−1) , C(n−1) , (8) h i (n) (n) C(n) = FNN(A·,1 ); . . . ; FNN(A·,M ) , (9) 535 (k) where A(n) and C(n) (n = 1, . . . , Nc ) are the hidden state and annotation at the n-th layer, respectively. Note that C(0) = Xc . 2.3 where y0 ∈ RD×1 is the vector representation of a begin-of-sentence token and Y"
D18-1049,P17-4012,0,0.171804,"alculated on the development set. filter size is set to 2,048. The multi-head attention has 8 individual attention heads. We set N = Ns = Nt = 6. In training, we use Adam (Kingma and Ba, 2015) for optimization. Each mini-batch contains approximately 24K words. We use the learning rate decay policy described by Vaswani et al. (2017). In decoding, the beam size is set to 4. We use the length penalty (Wu et al., 2016) and set the hyper-parameter α to 0.6. We use four Tesla P40 GPUs for training and one Tesla P40 GPU for decoding. We implement our approach on top of the open-source toolkit THUMT (Zhang et al., 2017). 4 3.2 2. (Kuang et al., 2017): using a cache which stores previous translated words and topical words to incorporate document-level context into the RNNsearch model. They use a document-level parallel corpus containing 2.8M sentence pairs. Table 3 gives the BLEU scores reported in their paper. 3. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit documentlevel context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. Effect of Context Le"
D18-1049,P18-1118,0,0.40399,"al for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 20"
D18-1049,D16-1163,0,0.0330673,"h 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the newly introduced mod"
D18-1049,2012.eamt-1.60,0,0.196011,"2M Chinese-English sentence pairs with 54.8M Chinese words and 60.8M English words. 3 The document-level parallel corpus is a subset of the full training set, including 41K documents with 940K sentence pairs. On average, each document in the training set contains 22.9 sentences. We use the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, 2008 datasets as test sets. The development and test sets contain 588 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 51"
D18-1049,P16-1162,0,0.260437,"88 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 20"
D18-1049,P16-1159,1,0.842461,"ion. To address this problem, we replace the residual connections after the context attention sub-layer with a position-wise context gating sub-layer: this step. P (y|x; θs ) is identical to the original Transformer model, which is a special case of our model. In the second step, document-level parameters θd are estimated on the document-level parallel corpus Dd only: X log P (Y|X; θˆs , θd ). (25) θˆd = argmax Gating(H) = λH + (1 − λ)SubLayer(H). (21) θd The gating weight is given by λ = σ(Wi H + Ws SubLayer(H)), Our approach is also similar to pre-training which has been widely used in NMT (Shen et al., 2016; Tu et al., 2018). The major difference is that our approach keeps θˆs fixed when estimating θd to prevent the model from overfitting on the relatively smaller document-level parallel corpora. (22) where σ(·) is a sigmoid function, Wi and Ws are model parameters. 2.4 Training Given a document-level parallel corpus Dd , the standard training objective is to maximize the loglikelihood of the training data: ( ) X θˆ = argmax log P (Y|X; θ) . (23) θ 3 Setup We evaluate our approach on Chinese-English and French-English translation tasks. In ChineseEnglish translation task, the training set contai"
D18-1049,W17-4811,0,0.179065,"Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use"
D18-1049,P18-1117,0,0.365642,"onal SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context"
D18-1049,D17-1301,0,0.15683,"Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored"
D18-1121,D15-1103,0,0.0488134,"labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this"
D18-1121,E17-2119,0,0.131243,"Missing"
D18-1121,P09-1113,0,0.0272593,"Missing"
D18-1121,D14-1200,0,0.0221804,"the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarze"
D18-1121,N15-1054,0,0.0241505,"this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current"
D18-1121,D14-1162,0,0.0812471,"IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidden size h of LSTM used in the language model. We perform (5) (6) where L is the matrix of all label embeddings, and Jlm is loss function of the language model used in the training phase. In order to ensure that label embeddings are in the same semantic space with word embeddings, L is initialized with word embeddings of the labels’ names. In the training phase, parameters"
D18-1121,D16-1144,0,0.702433,"tion. On the other hand, entity typing aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Y"
D18-1121,Q13-1030,0,0.0465428,"Missing"
D18-1121,E17-1119,0,0.216419,"language model predicts high probability for a reasonable sentence. Before applying the LME module to enhance the ET module, the language model is pre-trained with sentences from the training set. The loss function for s in the pre-train phase is: Jpre = LM({l1 , l2 , ..., e, r1 , r2 , ...}), 3 3.1 PT i=1 yi Li , Jlm = LM({l1 , l2 , ..., h, r1 , r2 , ...}), (4) Dataset W IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidd"
D18-1121,P12-1076,0,0.0756863,"Missing"
D18-1121,N18-1002,0,0.290333,"aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al.,"
D18-1121,D15-1083,0,0.0905318,"Missing"
D18-1121,P15-2048,0,0.36254,"idered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this paper. 2 In the W IKI d"
D18-1121,C12-2133,0,0.0329572,"of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified int"
D18-1235,P17-1171,0,0.0472406,"Missing"
D18-1235,P17-1055,0,0.0340931,"our work. Great effort has been put into the development of sophisticated neural models for machine reading comprehension. The attention mechanism was first introduced by Hermann et al. (2015) into reading comprehension and soon became the dom2110 inating model. Wang and Jiang (2017) proposed to solve machine comprehension using MatchLSTM and answer pointer. Seo et al. (2017) and Xiong et al. (2017) applied different ways to match the question and the context with bidirectional attention. Hu et al. (2017) used iterative aligner to match the question and the passage with feature-rich encoder. Cui et al. (2017) employed one more layer of attention over the bi-directional attention mechanism. Wang et al. (2017) applied a self-matching mechanism to aggregate evidence from the context. Tan et al. (2018) proposed to generate answer from extracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages,"
D18-1235,P17-1147,0,0.0217511,"re the source of answers is a short passage with a few hundred words, the DuReader dataset provides up to 5 full documents, which may contain up to 100K words. This incurs exorbitant demand on memory and training time. To deal with this issue, previous approaches select a single representative paragraph for each document, on which the answer extraction is performed. The original paper of DuReader (He et al., 2017) employed a simple heuristic strategy, and Wang et al. (2018b) trained a paragraph ranking model, while Clark and Gardner (2017) applied TF-IDF based method for the TriviaQA dataset (Joshi et al., 2017) which was in a similar situation. However, answers could come from more than one paragraph. We apply a simple yet effective method to extract contents from multiple paragraphs of the document, aiming to include as much information for the answer extraction as possible. We concatenate the title and the whole document as the passage if it is shorter than a predefined maximum length. If not, we employ passage extraction in the following way: • The title of the document is extracted. Whether a document is relevant to the question could be easily seen from the title. • We compute BLEU-4 score of e"
D18-1235,D16-1264,0,0.259883,"rence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09. 1 Introduction Machine reading comprehension (MRC) or question answering (QA) has been a long-standing goal in Natural Language Processing. There is a surge of interest in this area due to new end-to-end modeling techniques and the release of several largescale, open-domain datasets. In earlier datasets (Hermann et al., 2015; Hill et al., 2016; Yang et al., 2015; Rajpurkar et al., 2016), the questions did not arise from actual end users. Instead, they were constructed in cloze style or created by crowdworkers given a short passage from well-edited sources such as Wikipedia and CNN/Daily Mail. As a consequence, the questions ∗ Corresponding author: D. Lin (lindek@naturali.io). are usually well-formed and about simple facts, and the answers are guaranteed to exist as short spans in the given candidate passages. In MS-MARCO (Nguyen et al., 2016), the questions were sampled from actual search queries, which may have typos and may not be phrased as questions.1 Multiple short pass"
D18-1235,P16-1159,1,0.808107,"xtracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages, and Wang et al. (2018b) proposed Cross-Passage Answer Verification model for the same purpose. Neither of them involved multiple answers as in this work. Minimum Risk Training (MRT) has been widely used in various tasks in NLP. Shen et al. (2016) introduced MRT into Neural Machine Translation, and Ayana et al. (2016) applied it in Text Summarization. 3 Our Approach In this section we describe in details the architecture of our model which is depicted in Figure 2. 3.1 Passage Extraction Unlike most other datasets where the source of answers is a short passage with a few hundred words, the DuReader dataset provides up to 5 full documents, which may contain up to 100K words. This incurs exorbitant demand on memory and training time. To deal with this issue, previous approaches select a single representative paragraph for each document, o"
D18-1235,P17-1018,0,0.276629,"reading comprehension. The attention mechanism was first introduced by Hermann et al. (2015) into reading comprehension and soon became the dom2110 inating model. Wang and Jiang (2017) proposed to solve machine comprehension using MatchLSTM and answer pointer. Seo et al. (2017) and Xiong et al. (2017) applied different ways to match the question and the context with bidirectional attention. Hu et al. (2017) used iterative aligner to match the question and the passage with feature-rich encoder. Cui et al. (2017) employed one more layer of attention over the bi-directional attention mechanism. Wang et al. (2017) applied a self-matching mechanism to aggregate evidence from the context. Tan et al. (2018) proposed to generate answer from extracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages, and Wang et al. (2018b) proposed Cross-Passage Answer Verification model for the same purpose. Neithe"
D18-1235,P18-1178,0,0.0482647,"Missing"
D18-1235,K17-1028,0,0.0123283,"ted passages. 3.2 Representation of Word m Given a word sequence of question Q = {wtq }t=1 , and a word sequence of extracted passage P = n {wtp }t=1 , we combine different useful information to form the representation of each question word wtq and passage word wtp : 2111 • Word-level embedding: each word w in the question and passage is mapped to its corresponding n-dimensional embedding we. • POS tag embedding: we use a POS tagger to tag each word in the question and passage. Each POS tag is mapped to a m-dimensional embedding pe. • Word-in-question feature: following Chen et al. (2017) and Weissenborn et al. (2017), we use one additional binary feature wiq for each passage word, indicating whether this word occurs in the question. Figure 2: Model Architecture Each question word is represented as the concatenation of the word embedding we, and the POS tag embedding pe, denoted as xq = [we; pe]. Each passage word is additionally concatenated with the word-in-question feature wiq: xp = [we; pe; wiq]. It should be noted that, character-level embedding is an important part of word representation in English MRC models (Seo et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Tan et al., 2018). Character"
D18-1235,D15-1237,0,0.0634382,"Missing"
D18-1247,P11-1055,0,0.879452,"Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG F"
D18-1247,D17-1191,0,0.558533,"; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification models (McCallum et al., 1998; Rousu et al., 2005; Weinberger and Chapelle, 2009; Zhao et al., 2011; Bi and Kwok,"
D18-1247,P16-1200,1,0.84698,"to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Na"
D18-1247,D17-1189,0,0.645343,"wever, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22"
D18-1247,P09-1113,0,0.988836,"Parent Relation Parent Relation Tail Entity: Davos Ernst Haefliger died on Saturday in Davos. 0.8 0.6 0.4 Ernst Haefliger was born in Davos on July 6, 1919 0.1 0.2 0.3 Ernst Haefliger was born in Davos, Switzerland. 0.1 0.2 0.3 Attention Scores Figure 1: An example of hierarchical relation extraction. Relation extraction (RE) aims to predict relational facts from plain text. Conventional supervised RE models (Zelenko et al., 2003; Mooney and Bunescu, 2006) usually suffer from the lack of high-quality training data, because manual labeling of training data is time-consuming and humanintensive. Mintz et al. (2009) propose distant supervision to automatically label training instances by aligning existing knowledge graphs (KGs) and text: For an entity pair in KGs, those sentences containing both the entities will be labeled with † /people/deceased_person Parent Relation /people/deceased_person/place_of_death Introduction ∗ Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanie"
D18-1247,N13-1008,0,0.0863505,", especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for dista"
D18-1247,D15-1206,0,0.0488813,"beling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advan"
D18-1247,D15-1203,0,0.837594,"orresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an exam"
D18-1247,C14-1220,0,0.921179,"s with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al."
D18-1247,D17-1186,1,0.936619,"xplicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification mode"
D18-1247,P15-1061,0,0.0757283,"o alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategi"
D18-1247,D12-1042,0,0.778504,"es equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et a"
D18-1247,D17-1004,0,0.0499094,"tic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlat"
D18-1247,N16-1103,0,0.0621293,"nt supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017"
D18-1247,W16-1312,0,0.0321395,"el et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mecha"
D18-1247,P05-1053,0,0.487507,"ction. Since there are more sufficient data for training the top-layer attention, the whole hierarchical attention scheme can enhance RE models for solving those long-tail relations. We conduct experiments on a large-scale benchmark dataset for RE in this paper. The experimental results show that the proposed coarse-tofine grained attention scheme based on relation hierarchies significantly outperforms other baseline methods, even as compared to the recent stateof-the-art attention-based models, especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to"
D18-1247,D17-1187,0,0.700834,"tic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2236–2245 c Brussels"
D18-1353,D14-1179,0,0.00967642,"Missing"
D18-1353,D16-1126,0,0.349627,"Missing"
D18-1353,C08-1048,0,0.274585,"Missing"
D18-1353,N16-1014,0,0.121006,"Missing"
D18-1353,D15-1001,0,0.0256622,"Missing"
D18-1353,D16-1127,0,0.0868793,"Missing"
D18-1353,C16-1100,0,0.418218,"Missing"
D18-1353,D17-1230,0,0.0314926,"Missing"
D18-1353,D16-1137,0,0.0400278,"Missing"
D18-1353,P17-1125,0,0.546958,"Missing"
D18-1353,D14-1074,0,0.469248,"Missing"
D18-1390,N18-1172,0,0.0246302,"ommon approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T OP J UDGE. In this work, we introduce a topological learning framework TOPJUDGE to handle multiple subtasks in LJP. Different to conventional MTL models which focus on how to share parameters among relevant tasks, TOPJUDGE models the explicit dependencies among these subtasks with scalable DAG forms. 3 Method In the following parts, we will first give the essential definitions of LJP task. We then introduce the DAG dependen"
D18-1390,P15-1166,0,0.034693,"nge of areas, including NLP (Collobert and Weston, 2008), speech recognition (Deng et al., 2013), and computer vision (Girshick, 2015; Mao et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016)"
D18-1390,P15-2139,0,0.0142335,"l. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T"
D18-1390,N16-1101,0,0.0177558,"et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the a"
D18-1390,D17-1206,0,0.0212298,"ameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T OP J UDGE. In this work, we introduce a topological learning framework TOPJUDGE to handle multiple subtasks in LJP. Different to conventional MTL models which focus on how to share parameters among relevant tasks, TOPJUDGE models the explicit dependencies among these subtasks with scalable DAG forms. 3 Method In the following parts, we will first give the essential definit"
D18-1390,C18-1041,1,0.819815,"ical technique for the legal assistant system. On the one hand, LJP can provide low-cost but high-quality legal consulting services to the masses who are unfamiliar with legal terminology and the complex judgment procedures. On the other hand, it can serve as the handy reference for professionals (e.g., lawyers and judges) and improve their work efficiency. ∗ Indicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still confronted with two ma"
D18-1390,D14-1181,0,0.128909,"ore researchersformalize this task under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an attention-based neural network that jointly models charge prediction and relevant article extraction. Hu et al. (2018) incorporate 10 discriminative legal attributes to predict few-shot and confusing charges. Nevertheless, these models are designed for specific subtasks and thus non-trivial to be extended to more subtasks of LJP with complex dependencies. Besides, Ye et al. (2018) utilize a Seq2Seq model to generate co"
D18-1390,O12-5004,0,0.158723,"diction Employing automatic analysis techniques for legal judgment has drawn attention from researchers in the legal field for decades. Early works usually focus on analyzing existing legal cases in specific scenarios with mathematical and statistical algorithms (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012). With the development of machine learning and text mining techniques, more researchersformalize this task under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an at"
D18-1390,N15-1092,0,0.0247561,"solving them at the same time. It can transfer useful information among various tasks and has been applied to a wide range of areas, including NLP (Collobert and Weston, 2008), speech recognition (Deng et al., 2013), and computer vision (Girshick, 2015; Mao et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et"
D18-1390,D17-1289,0,0.460995,"dicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still confronted with two major challenges: Multiple Subtasks in Legal Judgment: Practically, legal judgment usually consists of detailed and complicated subclauses, such as charges, the term of penalty, and fines. Specifically, for those countries with the civil law system (e.g., China, France, and Germany), the prediction of relevant articles is also considered to be one of the fundamenta"
D18-1390,N18-1168,0,0.0807583,"ons. It is a critical technique for the legal assistant system. On the one hand, LJP can provide low-cost but high-quality legal consulting services to the masses who are unfamiliar with legal terminology and the complex judgment procedures. On the other hand, it can serve as the handy reference for professionals (e.g., lawyers and judges) and improve their work efficiency. ∗ Indicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still conf"
D18-1390,D15-1167,0,0.133013,"under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an attention-based neural network that jointly models charge prediction and relevant article extraction. Hu et al. (2018) incorporate 10 discriminative legal attributes to predict few-shot and confusing charges. Nevertheless, these models are designed for specific subtasks and thus non-trivial to be extended to more subtasks of LJP with complex dependencies. Besides, Ye et al. (2018) utilize a Seq2Seq model to generate court views with fact descriptions and predict"
D18-1430,D16-1116,0,0.0675134,"Missing"
D18-1430,W09-2005,0,0.0912738,"Missing"
D18-1430,C16-1100,0,0.369767,"out the Great Wall in the battlefield style or the sleepless feeling in the romantic style. Such ability to write stylistic poems under the same poetic imagery is an important characteristic of human poetries. Automatic poetry generation is one of the first attempts towards computer writing. Chinese quatrain generation has also attracted much atten∗ corresponding author: sms@tsinghua.edu.cn tion in recent years. Early works inspired by statistical machine translation explored rule-based and template-based methods (He et al., 2012; Yan et al., 2013), while recent works (Zhang and Lapata, 2014; Wang et al., 2016; Yan, 2016; Zhang et al., 2017; Yang et al., 2017; Yi et al., 2017) employed neural network based sequenceto-sequence approaches which have shown their effectiveness in neural machine translation for poem generation. Most works target on improving the coherency among all lines and the conformity between the theme and subsequent lines by planning (Wang et al., 2016), polishing schema (Yan, 2016), poem block (Yi et al., 2017) and conditional variational autoencoder (Yang et al., 2017). Different from these previous works, we aim to learn the ability of diverse stylistic poetry generation which"
D18-1430,P17-1125,0,0.409088,"lefield style or the sleepless feeling in the romantic style. Such ability to write stylistic poems under the same poetic imagery is an important characteristic of human poetries. Automatic poetry generation is one of the first attempts towards computer writing. Chinese quatrain generation has also attracted much atten∗ corresponding author: sms@tsinghua.edu.cn tion in recent years. Early works inspired by statistical machine translation explored rule-based and template-based methods (He et al., 2012; Yan et al., 2013), while recent works (Zhang and Lapata, 2014; Wang et al., 2016; Yan, 2016; Zhang et al., 2017; Yang et al., 2017; Yi et al., 2017) employed neural network based sequenceto-sequence approaches which have shown their effectiveness in neural machine translation for poem generation. Most works target on improving the coherency among all lines and the conformity between the theme and subsequent lines by planning (Wang et al., 2016), polishing schema (Yan, 2016), poem block (Yi et al., 2017) and conditional variational autoencoder (Yang et al., 2017). Different from these previous works, we aim to learn the ability of diverse stylistic poetry generation which can generate multiple outputs ("
D18-1430,D14-1074,0,0.364299,"on), she/he may write about the Great Wall in the battlefield style or the sleepless feeling in the romantic style. Such ability to write stylistic poems under the same poetic imagery is an important characteristic of human poetries. Automatic poetry generation is one of the first attempts towards computer writing. Chinese quatrain generation has also attracted much atten∗ corresponding author: sms@tsinghua.edu.cn tion in recent years. Early works inspired by statistical machine translation explored rule-based and template-based methods (He et al., 2012; Yan et al., 2013), while recent works (Zhang and Lapata, 2014; Wang et al., 2016; Yan, 2016; Zhang et al., 2017; Yang et al., 2017; Yi et al., 2017) employed neural network based sequenceto-sequence approaches which have shown their effectiveness in neural machine translation for poem generation. Most works target on improving the coherency among all lines and the conformity between the theme and subsequent lines by planning (Wang et al., 2016), polishing schema (Yan, 2016), poem block (Yi et al., 2017) and conditional variational autoencoder (Yang et al., 2017). Different from these previous works, we aim to learn the ability of diverse stylistic poetr"
D18-1493,P00-1041,0,0.0263717,"l. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jurafsky, 2000). Recent years have also witnessed the advances of Recurrent Neural Networks (RNNs) as the state-of-the-art approach for language modeling (Mikolov et al., 2010), in which the context is represented as a low-dimensional hidden state to predict the next word. Th"
D18-1493,D07-1090,0,0.05538,". 1 (b) context vector word distribution Conventional Decoder Sememe-Driven Decoder Sememe Predictor sememe distribution Sense Predictor sense distribution Word Predictor word distribution Figure 1: Decoder of (a) Conventional Language Model, (b) Sememe-Driven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jur"
D18-1493,J90-2002,0,0.377825,"/thunlp/SDLM-pytorch. 1 (b) context vector word distribution Conventional Decoder Sememe-Driven Decoder Sememe Predictor sememe distribution Sense Predictor sense distribution Word Predictor word distribution Figure 1: Decoder of (a) Conventional Language Model, (b) Sememe-Driven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood"
D18-1493,P16-1154,0,0.0156496,"nal distribution is a categorical distribution, each expert is only responsible for making predictions on a subset of the categories (usually less than 10), so we call it Sparse Product of Experts. Headline Generation. Headline generation is a kind of text summarization tasks. In recent years, with the advances of RNNs, a lot of works have been done in this domain. The encoderdecoder models (Sutskever et al., 2014; Cho et al., 2014) have achieved great success in sequenceto-sequence learning. Rush et al. (2015) propose a local attention-based model for abstractive 4649 sentence summarization. Gu et al. (2016) introduce the copying mechanism which is close to the rote memorization of the human being. Ayana et al. (2016) employ the minimum risk training strategy to optimize model parameters. Different from these works, we focus on the decoder of the sequence-to-sequence model, and adopt SDLM to utilize sememe knowledge for sentence generation. 6 Conclusion and Further Work In this paper, we propose an interpretable Sememe-Driven Language Model with a hierarchical sememe-sense-word decoder. Besides interpretability, our model also achieves stateof-the-art performance in the Chinese Language Modeling"
D18-1493,D15-1229,0,0.0130324,"able senses. (3) Finally, the distribution of words could be easily calculated by marginalizing out the distribution of senses. We evaluate the performance of SDLM on the language modeling task using a Chinese newsi Note that although sememes are defined as the minimum semantic units, there still exist several sememes for capturing syntactic information. For example, the word å “with” corresponds to one specific sememe ü˝Õ “FunctWord”. paper corpus People’s Daily ii (Renmin Ribao), and also on the headline generation task using the Large Scale Chinese Short Text Summarization (LCSTS) dataset (Hu et al., 2015). Experimental results show that SDLM outperforms all those data-driven baseline models. We also conduct case studies to show that our model can effectively predict relevant sememes given context, which can improve the interpretability and robustness of language models. 2 Background Language models target at learning the joint probability of a sequence of words P (w1 , w2 , · · · , wn ), which isQ usually factorized as P (w1 , w2 , · · · , wn ) = nt=1 P (wt |w&lt;t ). Bengio et al. (2003) propose the first Neural Language Model as a feed-forward neural network. Mikolov et al. (2010) use RNN and a"
D18-1493,P18-1227,1,0.412388,"are organized in a tree structure and the word probability is calculated as the probability of always choosing the correct child along the path from the root node to the word node. While Morin and Bengio (2005) utilize knowledge from WordSememe. Recently, there are a lot of works concentrating on utilizing sememe knowledge in traditional natural language processing tasks. For example, Niu et al. (2017) use sememe knowledge to improve the quality of word embeddings and cope with the problem of word sense disambiguation. Xie et al. (2017) apply matrix factorization to predict sememes for words. Jin et al. (2018) improve their work by incorporating character-level information. Our work extends the previous works and tries to combine word-sense-sememe hierarchy with the sequential model. To be specific, this is the first work to improve the performance and interpretability of Neural Language Modeling with sememe knowledge. Product of Experts. As Hinton (1999, 2002) propose, the final probability can be calculated as the product of probabilities given by experts.Gales and Airey (2006) apply PoE to the speech recognition where each expert is a Gaussian mixture model. Unlike their work, in our SDLM, each"
D18-1493,O02-2003,0,0.830586,"s). These atomic semantic units are named sememes (Dong and Dong, 2006).i Since sememes are naturally implicit in human languages, linguists have devoted much effort to explicitly annotate lexical sememes for words and build linguistic common-sense knowledge bases. HowNet (Dong and Dong, 2006) is one of the representative sememe knowledge bases, which annotates each Chinese word sense with its sememes. The philosophy of HowNet regards the parts and attributes of a concept can be well represented by sememes. HowNet has been widely utilized in many NLP tasks such as word similarity computation (Liu, 2002) and sentiment analysis (Fu et al., 2013). However, less effort has been devoted to exploring its effectiveness in language models, especially neural language models. It is non-trivial for neural language models to incorporate discrete sememe knowledge, as it is not compatible with continuous representations in neural models. In this paper, we propose a Sememe-Driven Language Model (SDLM) to leverage lexical sememe knowledge. In order to predict the next word, we design a novel sememesense-word generation process: (1) We first estimate sememes’ distribution according to the context. (2) Regard"
D18-1493,J93-2004,0,0.060876,"Missing"
D18-1493,P17-1187,1,0.530918,"he whole vocabulary into different classes and uses a hierarchical softmax decoder to model the probability as P(word) = P(word|class)P(class), which is similar to our model. For the tree-based models, all words are organized in a tree structure and the word probability is calculated as the probability of always choosing the correct child along the path from the root node to the word node. While Morin and Bengio (2005) utilize knowledge from WordSememe. Recently, there are a lot of works concentrating on utilizing sememe knowledge in traditional natural language processing tasks. For example, Niu et al. (2017) use sememe knowledge to improve the quality of word embeddings and cope with the problem of word sense disambiguation. Xie et al. (2017) apply matrix factorization to predict sememes for words. Jin et al. (2018) improve their work by incorporating character-level information. Our work extends the previous works and tries to combine word-sense-sememe hierarchy with the sequential model. To be specific, this is the first work to improve the performance and interpretability of Neural Language Modeling with sememe knowledge. Product of Experts. As Hinton (1999, 2002) propose, the final probabilit"
D18-1493,E17-2025,0,0.015615,"Sense Predictor, that is P (w|g) = X P (s|g). (9) s2S (w) 3.4 Implementation Details Basis Matrix Actually, HowNet contains K ⇡ 2000 sememes. In practice, we cannot directly introduce K ⇥ H1 ⇥ H2 parameters, which might be computationally infeasible and lead to overfitting. To address this problem, we apply a weightsharing trick called the basis matrix. We use R basis matrices and their weighted sum to estimate Uk : Uk = R X ↵k,r Qr , (10) r=1 where Qr 2 RH1 ⇥H2 , ↵k,r &gt; 0 are trainable paP rameters, and R r=1 ↵k,r = 1. Weight Tying To incorporate the weight tying strategy (Inan et al., 2017; Press and Wolf, 2017), we use the same output embedding for multiple 4645 senses of a word. To be specific, the sense output embedding ws for each s 2 S (w) is the same as the word input embedding xw . 4 Experiments We evaluate our SDLM on a Chinese language modeling dataset, namely People’s Daily based on perplexity.iii Furthermore, to show that our SDLM structure can be a generic Chinese word-level decoder for sequence-to-sequence learning, we conduct a Chinese headline generation experiment on the LCSTS dataset. Finally, we explore the interpretability of our model with cases, showing the effectiveness of utili"
D18-1493,D18-1033,1,0.722743,"for large LSTM. Employing the weight tying strategy, we get Tied LSTM with better performance. We set LSTM and Tied LSTM of medium and large size as our baseline models and use the code from PyTorch examplesiv as their implementations. AWD-LSTM Based on several strategies for regularizing and optimizing LSTM-based language models, Merity et al. (2018) propose AWD-LSTM iii Although we only conduct experiments on Chinese corpora, we argue that this model has the potential to be applied to other languages in the light of works on construction sememe knowledge bases for other languages, such as (Qi et al., 2018). iv https://github.com/pytorch/examples/ tree/master/word_language_model as a three-layer neural network, which serves as a very strong baseline for word-level language modeling. We build it with the code released by the authorsv . Variants of Softmax Meanwhile, to compare our SDLM with other language modeling decoders, we set cHSM (Class-based Hierarchical Softmax) (Goodman, 2001), tHSM (Tree-based Hierarchical Softmax) (Mikolov et al., 2013) and MoS (Mixture of Softmaxes) (Yang et al., 2018) as the baseline add-on structures to the architectures above. Experimental Settings We apply our SDL"
D18-1493,D15-1044,0,0.116532,"riven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jurafsky, 2000). Recent years have also witnessed the advances of Recurrent Neural Networks (RNNs) as the state-of-the-art approach for language modeling (Mikolov et al., 2010), in which the context is represented as a low-dimensional hidden state to pred"
D18-1514,D17-1189,0,0.0448817,"Missing"
D18-1514,P17-1040,0,0.0127205,"ion. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a n"
D18-1514,C16-1017,0,0.0297203,"to learn fast-learning abilities from previous experience and rapidly generalize to new concepts. Many meta-learning models (Ravi and Larochelle, 2017; Santoro et al., 2016; Finn et al., 2017; Munkhdalai and Yu, 2017) achieve the state-of-the-art results on several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-so"
D18-1514,D15-1205,0,0.0717884,"Missing"
D18-1514,W09-2415,0,0.370152,"Missing"
D18-1514,P11-1055,0,0.360509,"Missing"
D18-1514,D17-1191,0,0.0382899,"Missing"
D18-1514,P16-1200,1,0.932224,"Missing"
D18-1514,strassel-etal-2008-linguistic,0,0.0427918,"irater kappa (Randolph, 2005), and keep the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified int"
D18-1514,D12-1042,0,0.153708,"Missing"
D18-1514,D17-1187,0,0.0652808,"Missing"
D18-1514,D18-1121,1,0.796734,"et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a new large supervised"
D18-1514,N18-1109,0,0.0692706,"several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2"
D18-1514,W02-1010,0,0.201432,"Missing"
D18-1514,D15-1203,0,0.119209,"nder four different settings. In recent research on few-shot learning, N way K shot setting is widely adopted. We follow this setting for the few-shot relation classification problem. To be exact, for N way K shot learning N = m = |R|, K = n1 = . . . = nm 3.2 (2) Experiment Settings We consider four types of few-shot tasks in our experiments: 5 way 1 shot, 5 way 5 shot, 10 way 1 shot, 10 way 5 shot. Under this setting, we evaluate different few-shot training strategies and stateof-the-art few-shot learning methods built upon two widely used instance encoders, CNN (Zeng et al., 2014) and PCNN (Zeng et al., 2015). For both CNN and PCNN, the sentence is first represented to the input vectors by transforming each word into concatenation of word embeddings and position embeddings. In CNN, the input vectors pass a convolution layer, a max-pooling layer, and a non-linear activation layer to get the final output sentence embedding. PCNN is a variant of CNN, which replaces the max-pooling operation with a piecewise max-pooling operation. To evaluate this two vanilla models in few-shot RC task, we first consider two training strategies, namely Finetune and kNN. For the Finetune baseline, it learns to classify"
D18-1514,C14-1220,0,0.364541,"viet writer. Test Instance (A) or (B) or (C) Euler was elected a foreign member of the Royal Swedish Academy of Sciences. Table 1: An example for a 3 way 2 shot scenario. Different colors indicate different entities, blue for head entity, and red for tail entity. Introduction Relation classification (RC) is an important task in NLP, aiming to determine the correct relation between two entities in a given sentence. Many works have been proposed for this task, including kernel methods (Zelenko et al., 2002; Mooney and Bunescu, 2006), embedding methods (Gormley et al., 2015), and neural methods (Zeng et al., 2014). The performance of these conventional models heavily depends on time-consuming and labor-intensive annotated data, which make themselves hard to generalize well. Adopting distant supervision is a primary approach to alleviate this problem for RC (Mintz et al.; Riedel et al.; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng ∗ The first four authors contribute equally. The order is determined by dice rolling. † Z. Wang is now at New York University. ‡ Correspondence author. et al., 2015; Lin et al., 2016), which heuristically aligns knowledge bases (KBs) and text to automatically annotate ad"
D18-1514,D17-1186,1,0.886952,"Missing"
D18-1514,D17-1004,0,0.0706106,"the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified into. S is a support set S = {(x11 , r1"
D18-2024,P15-1067,0,0.573991,"nce and Technology, Beijing Normal University, Beijing, China Abstract knowledge embedding (KE) approaches have been proposed to embed both entities and relations in KGs into a continuous low-dimensional space, such as linear models (Bordes et al., 2011, 2012, 2014), latent factor models (Sutskever et al., 2009; Jenatton et al., 2012; Yang et al., 2015; Liu et al., 2017), neural models (Socher et al., 2013; Dong et al., 2014), matrix factorization models (Nickel et al., 2011, 2012, 2016; Trouillon et al., 2016), and translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). These models have achieved great performance on benchmark datasets. However, there exist two main issues which may lead to difficulty in full utilization and further development. On the one hand, the existing implementations are scattered and unsystematic to some extent. For example, the interfaces of these model implementations are inconsistent with each other. On the other hand, these model implementations mainly focus on model validation and are often time-consuming, which makes it difficult to apply them for realworld applications. Hence, it becomes urgent to develop an efficient and eff"
D19-1021,P08-1004,0,0.552062,"Missing"
D19-1021,P18-2065,0,0.125725,"-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clus"
D19-1021,D18-1514,1,0.907647,"01; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over linguistic features. In this paper, we focus on the clustering-based OpenRE methods, which have the advantage of discovering highly distinguishable relation types. Few-shot Learning. Few-shot learning aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC"
D19-1021,P16-1200,1,0.917398,"Missing"
D19-1021,P14-5010,0,0.00429393,"via unlabeled instances. It optimizes a relation classiﬁer by reconstructing entities from pairing entities and predicted relation types. Rich features including entity words, context words, trigger words, dependency paths, and context POS tags are used to predict the relation type. RW-HAC and VAE both rely on external linguistic tools to extract rich features from plain texts. Speciﬁcally, we ﬁrst align entities to Wikidata and get their KB types. Next, we preprocess the instances with part-of-speech (POS) tagging, named-entity recognition (NER), and dependency parsing with Stanford CoreNLP (Manning et al., 2014). It is worth noting that these features are only used by baseline models. Our models, in contrast, only use sentences and entity pairs as inputs. Evaluation Protocol. In evaluation, we use B 3 metric (Bagga and Baldwin, 1998) as the scoring function. B 3 metric is a standard measure to balance the precision and recall of clustering tasks, and is commonly used in previous OpenRE works (Marcheggiani and Titov, 2016; Elsahar et al., 2017). To be speciﬁc, we use F1 measure, the harmonic mean of precision and recall. First, we report the result of supervised RSN with different clustering methods."
D19-1021,Q16-1017,0,0.394598,"ht our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs learn relational similarity metrics fr"
D19-1021,P09-1113,0,0.147833,"relations. To the best of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for"
D19-1021,D14-1162,0,0.0822568,"for convenience. Experimental results show that the sample ratio decides RSN’s tendency to predict larger or smaller clusters. In other words, it controls the granularity of the predicted relation types. This phenomenon suggests a potential application of our model in hierarchical relation extraction. However, we leave any serious discussion to future work. Hyperparameter Settings. Following (Lin et al., 2016) and (Zeng et al., 2014), we ﬁx the less inﬂuencing hyperparameters for sentence encoding as their reported optimal values. For word embeddings, we use pre-trained 50-dimensional Glove (Pennington et al., 2014) word embeddings. For position embeddings, we use randominitialized 5-dimensional position embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually"
D19-1021,N18-1081,0,0.039658,"ew relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over lin"
D19-1021,D11-1135,0,0.870996,"(Unlabeled) 1 To highlight our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs le"
D19-1021,P12-1075,0,0.11007,"n embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually cluster instances by either clustering their linguistic features (Lin and Pantel, 2001; Yao et al., 2012; Elsahar et al., 2017) or imposing reconstruction constraints (Yao et al., 2011; Marcheggiani and Titov, 2016). To demonstrate 224 the effectiveness of our RSN models, we compare our models with two state-of-the-art models: (1) HAC with re-weighted word embeddings (RW-HAC) (Elsahar et al., 2017): RW-HAC is the state-of-the-art feature clustering model for OpenRE. The model ﬁrst extracts KB types and NER tags of entities as well as re-weighted word embeddings from sentences, then adopts principal component analysis (PCA) to reduce feature dimensionality, and ﬁnally uses HAC to cluster the conc"
D19-1021,I17-1086,0,0.0548203,"st of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extr"
D19-1021,C14-1220,0,0.139899,"g aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC layer with sigmoid activation maps features into a relational vector v. To summarize, we obtain a vector representation v for a relational sentence with our CNN module: 0.7 p vr word position embeddings embeddings Figure 2: The architecture of Relational Siamese Networks. The output is the similarity between two relational instances. v = CNN(s), in which we denote the joint information of a sentence x and two"
D19-1073,P17-2090,0,0.0506904,"systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation errors can be propag"
D19-1073,P11-1022,0,0.0271415,"neural machine translation and use uncertainty-based confidence measures to improve back-translation. The key idea is to use Monte Carlo Dropout to sample translation probabilities to calculate model uncertainty, without the need for manually labeled data. As our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of NMT such as post-editing and interactive MT in the future. Confidence Estimation Estimating the confidence or quality of the output of MT systems (Ueffing and Ney, 2007; Specia et al., 2009; Bach et al., 2011; Salehi et al., 2014; Rikters and Fishel, 2017; Kepler et al., 2019) is important for enabling downstream applications such as post-editing and interactive MT to better cope with translation mistakes. While existing methods rely on external models to estimate confidence, our approach leverages model uncertainty to derive confidence measures. The major benefit is that our approach does not need labeled data. 5.3 Conclusions Acknowledgments We thank all anonymous reviewers for their valuable comments. This work is supported by the National Key R&D Program of China (No. 2017YFB0202204), National"
D19-1073,D18-1040,0,0.465139,"oisy, translation errors can be propagated to subsequent steps and prone to hinder the ∗ Yang Liu is the corresponding author: liuyang2011@ tsinghua.edu.cn. 1 The source code is available at https://github. com/THUNLP-MT/UCE4BT 791 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 791–802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics The second step is to use the trained model θˆy→x to translate the monolingual corpus Dm : performance (Fadaee and Monz, 2018; Poncelas et al., 2018). In this work, we propose a method to quantify the confidence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Differe"
D19-1073,C04-1046,0,0.177503,"ence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 20"
D19-1073,P17-1176,1,0.842201,"in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation"
D19-1073,W18-2703,0,0.0307731,"atz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are inevitably erroneous. Translation errors can be propagated to subsequent steps and impair the performance of back-translation, espe˜ b is much larger than Db (Pinnis cially when D et al., 2017; Fadaee and Monz, 2018; Poncelas et al., 2018). Therefore, it is crucial to develop principled solutions to enable back-translation to better deal with the error propagation problem. 3 Approach This work aims to find solutions to the two following problems: 1. How to quantify the confidence of model predictions"
D19-1073,P16-1185,1,0.846769,"et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT m"
D19-1073,W18-2707,0,0.069184,"IST06 show that targeting difficult words improves over randomly selecting monolingual data (46.23 → 46.60 BLEU), confirming the finding of Fadaee and Monz (2018). Using uncertainty-based confidence can further imBack-translation Back-translation is a simple and effective approach to leveraging monolingual data for NMT (Sennrich et al., 2016a). There has been a growing body of literature that analyzes and extends back-translation recently. Currey et al. (2017) show that low-resource NMT can benefit from the synthetic data generated by simply copying target monolingual data to the source side. Imamura et al. (2018) and Edunov et al. (2018) demonstrate that it is more effective to generate source sentences via sampling rather than beam search. Cotterell and Kreutzer (2018) and Hoang et al. 798 target Man gewährt dem Sterbenden je nach Wunsch eine Mundpflege mit Brandy oder Pepsi . reference A person who is dying will accept being helped to drink brandy or Pepsi , whatever is their tipple . prediction The dying person is given oral care with brandy or Pepsi as desired . PTP EXP VAR CEV Figure 5: Example of confidence measures. terested in calculating uncertainty after the model has made the prediction rat"
D19-1073,W17-4715,0,0.0200581,"eriments, we used the same amount of monolingual data that was derived from a larger monolingual corpus using different data selection methods. Results on NIST06 show that targeting difficult words improves over randomly selecting monolingual data (46.23 → 46.60 BLEU), confirming the finding of Fadaee and Monz (2018). Using uncertainty-based confidence can further imBack-translation Back-translation is a simple and effective approach to leveraging monolingual data for NMT (Sennrich et al., 2016a). There has been a growing body of literature that analyzes and extends back-translation recently. Currey et al. (2017) show that low-resource NMT can benefit from the synthetic data generated by simply copying target monolingual data to the source side. Imamura et al. (2018) and Edunov et al. (2018) demonstrate that it is more effective to generate source sentences via sampling rather than beam search. Cotterell and Kreutzer (2018) and Hoang et al. 798 target Man gewährt dem Sterbenden je nach Wunsch eine Mundpflege mit Brandy oder Pepsi . reference A person who is dying will accept being helped to drink brandy or Pepsi , whatever is their tipple . prediction The dying person is given oral care with brandy or"
D19-1073,C18-1266,0,0.027334,"use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are inevitably erroneous."
D19-1073,P18-1069,0,0.406084,"Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 791–802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics The second step is to use the trained model θˆy→x to translate the monolingual corpus Dm : performance (Fadaee and Monz, 2018; Poncelas et al., 2018). In this work, we propose a method to quantify the confidence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT"
D19-1073,D18-1045,0,0.545914,"007). We used byte pair encoding (Sennrich et al., 2016b) to perform subword segmentation with 32k merge operations for Chinese-English and 35k merge operations for English-German. Sentence pairs are batched together by approximate length and each batch has roughly 25,000 source and target tokens. We distinguish between three kinds of translations of the monolingual corpus: 1. N ONE: there is no translation and only the authentic bilingual corpus is used; 2. S EARCH: the translations are generated by beam search (Sennrich et al., 2016a); 3. S AMPLE: the translations are generated by sampling (Edunov et al., 2018). As neural quality estimation (Kim et al., 2017; Wang et al., 2018) can also give word- and sentence-level confidences for the output of NMT models when labeled data is available, we distinguish between two kinds of confidence estimation methods: 1. N EURAL Q E: the confidences are given by an external neural quality estimator; 4.2 2. U NCERTAINTY: the proposed uncertaintybased confidence estimation method. Comparison of Confidence Measures Table 1 shows the comparison of confidence measures on the Chinese-English development set. We find that using either the translation probabilities output"
D19-1073,2001.mtsummit-papers.68,0,0.0301344,"ord-level Confidence As the source side instead of the target side of the synthetic bilingual corpus is noisy, wordlevel confidence cannot be integrated into backtranslation in a similar way to sentence-level confidence. This is because the word-level confidence associated with each source word does not get involved in backpropagation during training. Alternatively, we build a real-valued word-level confidence vector: n oI (n) ˆ &lt;i , x c = C y(n) , x ˆi , θˆy→x . (14) 4 4.1 Setup We evaluated our approach on Chinese-English and English-German translation tasks. The evaluation metric is BLEU (Papineni et al., 2001) as calculated by the multi-bleu.perl script. We use the paired bootstrap resampling (Koehn, 2004) for significance testing. For the Chinese-English task, the training set contains 1.25M sentence pairs from LDC5 with 27.8M Chinese words and 34.5M English words. To build the monolingual corpus for backtranslation, we extracted the English side of the training set of the WMT 2017 Chinese-English news translation task. After removing sentences longer than 256 words, we randomly selected 10M English sentences as the monolingual corpus. NIST06 is used as the development set and NIST02, 03, 04, 05,"
D19-1073,P19-3020,0,0.0433318,"Missing"
D19-1073,W17-4763,0,0.124017,"ation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model"
D19-1073,W04-3250,0,0.452125,"y, wordlevel confidence cannot be integrated into backtranslation in a similar way to sentence-level confidence. This is because the word-level confidence associated with each source word does not get involved in backpropagation during training. Alternatively, we build a real-valued word-level confidence vector: n oI (n) ˆ &lt;i , x c = C y(n) , x ˆi , θˆy→x . (14) 4 4.1 Setup We evaluated our approach on Chinese-English and English-German translation tasks. The evaluation metric is BLEU (Papineni et al., 2001) as calculated by the multi-bleu.perl script. We use the paired bootstrap resampling (Koehn, 2004) for significance testing. For the Chinese-English task, the training set contains 1.25M sentence pairs from LDC5 with 27.8M Chinese words and 34.5M English words. To build the monolingual corpus for backtranslation, we extracted the English side of the training set of the WMT 2017 Chinese-English news translation task. After removing sentences longer than 256 words, we randomly selected 10M English sentences as the monolingual corpus. NIST06 is used as the development set and NIST02, 03, 04, 05, and 08 datasets as test sets. For the English-German task, we used the dataset of the WMT 2014 Eng"
D19-1073,P18-1006,0,0.0164722,"ry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation errors can be propagated to subsequent"
D19-1073,P07-2045,0,0.0102164,"moothing was set as ls = 0.1 (Szegedy et al., 2016; Pereyra et al., 2017). During training and the Monte Carlo Dropout process, the hyper-parameter of dropout was set to 0.1 and 0.3 for Transformer base and big models, respectively. K was set to 20. Through experiments, we find our method works best when the α and β are set to 2. All experiments were conducted on 8 NVIDIA GTX 1080Ti GPUs. the development set and newstest 2012, 2014, and 2015 as test sets. Chinese sentences were segmented by an opensource toolkit THULAC6 . German and English sentences were tokenized by the tokenizer in Moses (Koehn et al., 2007). We used byte pair encoding (Sennrich et al., 2016b) to perform subword segmentation with 32k merge operations for Chinese-English and 35k merge operations for English-German. Sentence pairs are batched together by approximate length and each batch has roughly 25,000 source and target tokens. We distinguish between three kinds of translations of the monolingual corpus: 1. N ONE: there is no translation and only the authentic bilingual corpus is used; 2. S EARCH: the translations are generated by beam search (Sennrich et al., 2016a); 3. S AMPLE: the translations are generated by sampling (Edun"
D19-1073,N03-1017,0,0.0872106,"Missing"
D19-1073,D18-1549,0,0.052654,"Missing"
D19-1073,P16-1009,0,0.393142,"translation (SMT) (Koehn et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora g"
D19-1073,P16-1162,0,0.555111,"translation (SMT) (Koehn et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora g"
D19-1073,2009.mtsummit-papers.16,0,0.202609,"edictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kre"
D19-1073,J07-1003,0,0.0432043,"method for qualifying model uncertainty for neural machine translation and use uncertainty-based confidence measures to improve back-translation. The key idea is to use Monte Carlo Dropout to sample translation probabilities to calculate model uncertainty, without the need for manually labeled data. As our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of NMT such as post-editing and interactive MT in the future. Confidence Estimation Estimating the confidence or quality of the output of MT systems (Ueffing and Ney, 2007; Specia et al., 2009; Bach et al., 2011; Salehi et al., 2014; Rikters and Fishel, 2017; Kepler et al., 2019) is important for enabling downstream applications such as post-editing and interactive MT to better cope with translation mistakes. While existing methods rely on external models to estimate confidence, our approach leverages model uncertainty to derive confidence measures. The major benefit is that our approach does not need labeled data. 5.3 Conclusions Acknowledgments We thank all anonymous reviewers for their valuable comments. This work is supported by the National Key R&D Program"
D19-1073,W18-6465,0,0.172598,"central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are ine"
D19-1073,P17-4012,0,0.0769606,"e software officially recommended by the QE shared task of WMT. Following the guide of OpenKiwi, we used a GermanEnglish parallel corpus containing 2.09M sentence pairs to train the predictor and a post-edited corpus containing 25k sentence triples to train the estimator. All the data used to train QE models are provided by WMT. As there are no post-edited corpora for the Chinese-English task, N EURAL Q E can only be used in the English-German task in our experiments. For N EURAL Q E, both word- and sentence- level quality scores were considered. We implemented our method on the top of THUMT (Zhang et al., 2017). The NMT model we use is Transformer (Vaswani et al., 2017). We used the base model for the Chinese-English task and the big model for the English-German task. We used the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 to optimize model parameters. We used the same warmup strategy for learning rate as Vaswani et al. (2017) with warmup steps = 4, 000. During training, the hyper-parameter of label smoothing was set as ls = 0.1 (Szegedy et al., 2016; Pereyra et al., 2017). During training and the Monte Carlo Dropout process, the hyper-parameter of dropout was set to"
D19-1073,D16-1163,0,0.0432688,"een widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
D19-1634,W17-4773,0,0.218412,"Missing"
D19-1634,W18-6452,0,0.0281174,"Missing"
D19-1634,P15-2026,0,0.024013,"it the erroneous translation to obtain a correct translation (pe). Our work aims to explicitly model how to copy words from mt to pe (highlighted in bold), which is a common phenomenon in APE. Introduction Automatic post-editing (APE) is an important natural language processing (NLP) task that aims to automatically correct errors made by machine translation systems (Knight and Chander, 1994). It can be considered as an efficient way to modify translations to a specific domain or to incorporate additional information into translations rather than translating from scratch (McKeown et al., 2012; Chatterjee et al., 2015, 2018). Approaches to APE can be roughly divided into two broad categories: statistical and neural approaches. While early efforts focused on statistical approaches relying on manual feature engineering (Simard et al., 2007; B´echara et al., 2011), neural network based approaches capable of learning representations from data have ∗ Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/L2Copy4APE 1 I ate a cake yesterday Ich esse einen Hamburger Ich hatte gestern einen Kuchen gegessen shown remarkable superiority over their statistical counterparts (Varis"
D19-1634,P16-1154,0,0.362486,"e translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results. 1 1 Table 1: Example of automatic post-editing (APE). Given a source sentence (src) and a machine translation (mt), the goal of APE is to post-edit the erroneous translation to obtain a correct translation (pe). Our work aims to explicitly model how to copy words from mt to pe (highlighted in bold), which is a common phenomenon in APE. Introduction Automatic post-editing (APE) is an important natural language processing (NLP"
D19-1634,W16-2378,0,0.0514658,"MT Additional Dataset training set dev2016 test2016 test2017 training set dev2018 artificial-small artificial-big eSCAPE-PBSMT eSCAPE-NMT α 0.1 0.5 0.9 0.9 0.9 # Sent. 23,000 1,000 2,000 2,000 13,442 1,000 526,368 4,391,180 7,258,533 7,258,533 λ 1.0 1.0 0.1 0.5 1.0 TER↓ 18.83 18.45 18.89 18.47 18.38 BLEU↑ 72.59 72.83 72.27 72.83 72.99 Table 3: Effect of α and λ. The TER and BLEU scores are calculated on the WMT 2016 APE official development set. Table 2: Statistics of the English-German datasets in the WMT APE task. Note that the NMT official data only contains training and development sets. (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2014). We used the WMT official dataset for the PBSMT task and the NMT task separately. The artificial training data (Junczys-Dowmunt and Grundkiewicz, 2016) was also used for both tasks. More precisely, we used the concatenation of the official training data and the artificial-small data to learn a truecasing model (Koehn et al., 2007) and obtain sub-word units using byte-pair encoding (BPE) (Sennrich et al., 2015) with 32k merges. Then, we applied truecasing and BPE to all datasets. We oversampled the official training data 20 times and concatenated them with both artificial-"
D19-1634,I17-1013,0,0.0352878,"Missing"
D19-1634,W18-6467,0,0.415563,"x, y ˜, y&lt;j ; θ), (1) j=1 where yj is the j-th target word in pe, y&lt;j = y1 . . . yj−1 is a partial translation, θ is a set of model parameters, and P (yj |x, y ˜, y&lt;j ; θ) is a word-level translation probability. 6123 Output Prob. Encoder Encoder Output Prob. Original Prob. FFN Copy Prob. Self-Att CopyNet FFN Original Prob. Softmax × × Copy Prob. Self-Att Softmax Linear CopyNet Linear Sigmoid FFN Decoder Encoder × Src-Dec-Att FFN Mt-Dec-Att Self-Att Self-Att Predictor × × Linear FFN FFN Enc-Dec-Att Self-Att Self-Att (a) Decoder × (b) Figure 2: (a) The architecture of multi-source Transformer (Junczys-Dowmunt and Grundkiewicz, 2018) equipped with CopyNet (Gu et al., 2016) and (b) the architecture of our approach. While the existing work learns the representations of src and mt separately, our approach allows for learning the representations of src and mt in an interactive way by concatenating them as a single input. In addition, our approach introduces a Predictor module to explicitly indicate which words in mt should be copied. The word-level translation probability in Eq. (1) is computed as Hsrc = Encodersrc (x, θ), mt H hpe j (2) mt = Encoder (˜ y, θ), src (3) mt = Decoder(y&lt;j , H , H , θ), P (yj |x, y ˜, y&lt;j ; θ) ∝ e"
D19-1634,P07-2045,0,0.00591135,"and BLEU scores are calculated on the WMT 2016 APE official development set. Table 2: Statistics of the English-German datasets in the WMT APE task. Note that the NMT official data only contains training and development sets. (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2014). We used the WMT official dataset for the PBSMT task and the NMT task separately. The artificial training data (Junczys-Dowmunt and Grundkiewicz, 2016) was also used for both tasks. More precisely, we used the concatenation of the official training data and the artificial-small data to learn a truecasing model (Koehn et al., 2007) and obtain sub-word units using byte-pair encoding (BPE) (Sennrich et al., 2015) with 32k merges. Then, we applied truecasing and BPE to all datasets. We oversampled the official training data 20 times and concatenated them with both artificial-small and artificial-big datasets (JunczysDowmunt and Grundkiewicz, 2018). Finally, we obtained a dataset containing nearly 5M triplets for both tasks. To test our approach on a larger PBSMT dataset, we used the eSCAPE synthetic dataset (Negri et al., 2014), which contains 7.2M sentences. By including the eSCAPE dataset, the training set is enlarged to"
D19-1634,W16-2361,0,0.0526664,"Missing"
D19-1634,2012.eamt-1.34,0,0.062667,"Missing"
D19-1634,C16-1172,0,0.0550767,"Missing"
D19-1634,W18-6468,0,0.0235546,"Missing"
D19-1634,P17-1099,0,0.0647436,"other, which might lead to the inability to find which src word is untranslated and which mt word is incorrect. For example, the mt sentence in Figure 1 is fluent and meaningful. Without src, the APE system is unable to identify translation errors. In addition, the multisource Transformer does not explicitly model the copying between mt and pe in neither the Encoder nor the Decoder. 2.2 CopyNet CopyNet (Gu et al., 2016) is a widely used method for modeling copying in sequence-to-sequence learning. It has been successfully applied to single-turn dialogue (Gu et al., 2016), text summarization (See et al., 2017), and grammar error correction (Zhao et al., 2019). It is possible to extend the multi-source Transformer with CopyNet to explicitly model the copying mechanism, as shown in Figure 2(a). CopyNet defines the word-level translation probability in Eq. (1) as a linear interpolation of copying and generating probabilities: P (yj |x, y ˜, y&lt;j ; θ) = γj × P copy (yj ) + (1 − γj ) × P gen (yj ), (6) where P copy (yj ) is the copying probability for yj , P gen (yj ) is the generating probability for yj , and γj is a gating weight. They are defined as follows: P copy (yj ) ∝ exp(g(Hmt , hpe j )), P gen"
D19-1634,W18-6470,0,0.193069,"t-editing. Second, it is possible to predict which words in mt should be copied because it is easy to automatically construct labeled data by comparing mt and pe. Such predictions can be combined with CopyNet to better model copying for APE. Experiments show that our approach outperforms the best published results on the datasets of the WMT 20162017 APE shared tasks. 2 2.1 Background Multi-source Sequence-to-Sequence Learning Multi-source sequence-to-sequence learning has been widely used in APE in recent years (JunczysDowmunt and Grundkiewicz, 2018; Pal et al., 2018; Tebbifakhr et al., 2018; Shin and Lee, 2018). The architecture of multi-source Transformer is shown in Figure 2(a). It can be equipped with CopyNet (see Section 2.2) to serve as a baseline in our experiments. It is worth noting that src and mt are encoded separately. Let x = x1 . . . xI be a source sentence (i.e., src) with I words, y ˜ = y˜1 . . . y˜K be a translation output by a machine translation system (i.e., mt) with K words, and y = y1 . . . yJ be the postedited translation (i.e., pe) with J words. The APE model is given by P (y|x, y ˜; θ) = J Y P (yj |x, y ˜, y&lt;j ; θ), (1) j=1 where yj is the j-th target word in pe, y&lt;j = y1 . ."
D19-1634,N07-1064,0,0.0614393,"(APE) is an important natural language processing (NLP) task that aims to automatically correct errors made by machine translation systems (Knight and Chander, 1994). It can be considered as an efficient way to modify translations to a specific domain or to incorporate additional information into translations rather than translating from scratch (McKeown et al., 2012; Chatterjee et al., 2015, 2018). Approaches to APE can be roughly divided into two broad categories: statistical and neural approaches. While early efforts focused on statistical approaches relying on manual feature engineering (Simard et al., 2007; B´echara et al., 2011), neural network based approaches capable of learning representations from data have ∗ Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/L2Copy4APE 1 I ate a cake yesterday Ich esse einen Hamburger Ich hatte gestern einen Kuchen gegessen shown remarkable superiority over their statistical counterparts (Varis and Bojar, 2017; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Unanue et al., 2018). Most of them cast APE as a multi-source sequence-to-sequence learning problem (Zoph and Knight, 2016): given a source s"
D19-1634,W18-2702,0,0.036302,"Missing"
D19-1634,W17-4777,0,0.0338594,"Missing"
D19-1634,D18-1341,0,0.0186565,"to 2,048. The number of individual attention heads was set to 8 for multi-head attention. We set N = Ne = Nd = 6, Np = 3 and we tied all three src, mt, pe embeddings for saving memory. The embeddings and softmax weights were also tied. In training, we used Adam (Kingma and Ba, 2014) for optimization. Each mini-batch contains approximately 25K tokens. We used the learning rate decay policy described 6127 1. O RIGINAL: the original mt without any postediting. 2. C OPY N ET (Gu et al., 2016; Zhao et al., 2019): the multi-source Transformer equipped with CopyNet (see Figure 2(a)). 3. N PI -A PE (Vu and Haffari, 2018): a neural programmer-interpreter approach. 4. M S U EDIN (Junczys-Dowmunt and Grundkiewicz, 2018): a multi-source Transformerbased APE system that shares the encoders of src and mt. It is the champion of the WMT 2018 APE shared task. 5. U SAAR D FKI (Pal et al., 2018): a multisource Transformer-based APE system with a joint encoder that attends over a combination of two encoded sequences. It is a participant of the WMT 2018 APE shared task. 6 7 https://github.com/THUNLP-MT/THUMT http://www.cs.umd.edu/snover/tercom/ TEST16 TEST17 TEST16+17 TER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ (1) O RIGINAL 24.76 6"
D19-1634,D18-1475,0,0.0272315,"., “Kuchen” in Figure 1). This can be done by generating explicit labels using bilingual word alignment. We leave this for future work. 6125 0.8 0.6 where α and λ are hyper-parameters. The first part is the original log-likelihood loss function of APE: 5 0.2 Copying Scores Lape (θ) = − log P (y|x, y ˜; θ). (19) pe The second part is related to the CopyNet: src K 1 X Lcopy (θ) = (lk − ck )2 , K mt (20) k=1 Figure 4: Copying scores as scaling masks. The copying scores are used to modify the attention layers of the Encoder, Decoder, and CopyNet. Encoder, the Decoder, and the CopyNet. Inspired by Yang et al. (2018)’s strategy to integrate localness to self-attention, we propose to incorporate copying scores into our model by modifying attention weights involved in the aforementioned three modules. The original scaled dot-product attention (Vaswani et al., 2017) is defined as qK&gt; energy = √ , d Att(q, K) = softmax(energy), where lk is the ground-truth label (see Section 3.2) and ck is a quantity that measures how likely the k-th word in mt to be copied by CopyNet: ck = J X γj × P copy (˜ yk ). j=1 Note that γ × P copy (y) is the term related to copying the target word y in Eq. (6). The third part is a cr"
D19-1634,2020.amta-research.11,1,0.905528,"Missing"
D19-1634,N19-1014,0,0.334755,"which src word is untranslated and which mt word is incorrect. For example, the mt sentence in Figure 1 is fluent and meaningful. Without src, the APE system is unable to identify translation errors. In addition, the multisource Transformer does not explicitly model the copying between mt and pe in neither the Encoder nor the Decoder. 2.2 CopyNet CopyNet (Gu et al., 2016) is a widely used method for modeling copying in sequence-to-sequence learning. It has been successfully applied to single-turn dialogue (Gu et al., 2016), text summarization (See et al., 2017), and grammar error correction (Zhao et al., 2019). It is possible to extend the multi-source Transformer with CopyNet to explicitly model the copying mechanism, as shown in Figure 2(a). CopyNet defines the word-level translation probability in Eq. (1) as a linear interpolation of copying and generating probabilities: P (yj |x, y ˜, y&lt;j ; θ) = γj × P copy (yj ) + (1 − γj ) × P gen (yj ), (6) where P copy (yj ) is the copying probability for yj , P gen (yj ) is the generating probability for yj , and γj is a gating weight. They are defined as follows: P copy (yj ) ∝ exp(g(Hmt , hpe j )), P gen (yj ) ∝ γj = exp(hpe j Wg ), mt u(H , hpe j ), (7)"
D19-1634,N16-1004,0,0.124599,"Missing"
D19-1649,W06-1615,0,0.189413,"can sample instances from the training set as supporting instances for NOTA relation (this method is described explicitly in Section 4). Also note that to better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, w"
D19-1649,P07-1073,0,0.0597172,"mpled from the test set, each of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there"
D19-1649,N19-1423,0,0.0360439,"le NOTA is to regard it as an extra class in the N -way K-shot setting. To be more specific, we can sample instances outside the N relations as the supporting data of NOTA, and perform the (N + 1)-way K-shot learning. As compared to the current methods ignoring NOTA, this approach does not bring much improvements, since the supporting data for NOTA actually belong to several different relations and are scattered in the feature space, making it hard to perform classification. To better address few-shot NOTA, we propose a model named BERT-PAIR based on the sequence classification model in BERT (Devlin et al., 2019). We pair each query instance with all the supporting instances, concatenate each pair as one sequence, and send the concatenated sequence to the BERT sequence classification model to get the score of the two instances expressing the same relation. Denote the BERT model as B, the query instance as x and the paired supporting instance as xjr (the j-th supporting instance for the relation r), B(x, xjr ) outputs a two-element vector corresponding to scores of the pair sharing the same relation and not sharing the same relation. The probability over each relation in the few-shot scenario, includin"
D19-1649,P19-1279,0,0.106636,"Missing"
D19-1649,C18-1099,1,0.86151,"o better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, we feed the sentence encoder E and the discriminator D with the corpora from the training domain and the test domain, and optimize the min-max game, X"
D19-1649,D18-1514,1,0.42617,"Missing"
D19-1649,W09-2415,0,0.332326,"Missing"
D19-1649,P09-1113,0,0.589697,"ach of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there are wide disparities"
D19-3029,N16-1030,0,0.0461089,"ween the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect words or phrases (also named entity mentions) representing real-world objects. In OpenNRE, we provide two approaches for NER, one is 170 Architecture of OpenNRE a entity-pair bag. As shown in Figure 1, synthesizing the features of different sentences in a bag can provide more reliable information and result in more accurate predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Da"
D19-3029,P16-1200,1,0.949986,"ng, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open infor"
D19-3029,P09-1113,0,0.489525,"sentence-level RE, bag-level RE, document-level RE, and few-shot RE. For completing a full pipeline of extracting structured information, we also enable OpenNRE to have the capacity of entity-oriented applications to a certain extent, e.g., NER and EL. The examples of these application scenarios are all shown in Figure 1. 2.1 Sentence-Level Relation Extraction 2.3 Bag-Level Relation Extraction The supervised RE methods suffer from several problems, especially their requirements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting st"
D19-3029,W15-1506,0,0.0733895,"ao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linki"
D19-3029,P15-1034,0,0.0609609,"Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, we develop an open and extensible toolkit for designing and implementing RE models, especially for NRE models, which is named “OpenNRE”. The toolkit prioritizes operational efficiency based on TensorFlow and PyTorch, which support quick model training and validation. Meanwhile, the toolkit maintains sufficient system encapsulation and model extensibility, which can meet some individual requirements of incorporating new models. To keep t"
D19-3029,N19-1423,0,0.0171983,"bniz was a member of the Prussian Academy of Sciences Samuel Langhorne Clemens, better known by his pen name Mark Twain child of place of death [Olivia Langdon] [writer] Newton served as the president of the Royal Society member of birth name sibling of spouse of occupation Query … Supporting Set member of In 1921, Ernest Hemingway married Hadley Richardson, the first of his four wives [Jean Clemens] Euler was elected a foreign member of the Royal Swedish Academy of Sciences Figure 1: The examples of all application scenarios in OpenNRE. based on spaCy, the other is based on fine-tuning BERT (Devlin et al., 2019). The EL modules can align those entity mentions to the entities in Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) based on TagMe (Ferragina and Scaiella, 2010). models, they can quickly start up their RE system based on OpenNRE, without knowing too many technical details and writing tedious glue code. An online system is also available to extract structured relational facts from the text with friendly interactive interfaces and fast reaction speed. We will provide long-term maintenance to fix bugs and meet new requests for OpenNRE, and we think both researchers and industry developers can benefit"
D19-3029,P19-1279,0,0.172058,"gy and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit f"
D19-3029,D17-1187,0,0.633015,"ired to be capable of accurately capturing relation patterns of these small amounts of training instances. Considering few-shot RE is important for handling long-tail relations, OpenNRE also provides a custom platform for further research in this direction. 3 can maximize the reuse of code to avoid unnecessary redundant model implementations. For operational efficiency, OpenNRE is based on TensorFlow and PyTorch, which enables developers to train models on GPUs. For model extensibility, we systematically implement various neural modules and some special algorithms (e.g., adversarial training (Wu et al., 2017) and reinforcement learning (Feng et al., 2018)). Hence, it is easy to implement new RE models based on OpenNRE. We also implement some typical RE models so as to conveniently train custom models for specific application scenarios. More specifically, OpenNRE attains the above four design objects through implementing the following five components. 3.1 Tokenization The tokenization component is responsible for tokenizing input text into several input tokens. In OpenNRE, we implement both word-level tokenization and subword-level tokenization. These two operations satisfy most tokenization demand"
D19-3029,P19-1074,1,0.835257,"te predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Data Loader Model Train Method Eval Method Model Encoder Forward Bag-Level RE Few-Shot RE … Softmax Classifier Instance-Level Attention Prototypical Networks … Module Tokenizer Forward Encoder Tokenization CNN BERT … Tokenization Word Tokenization Word Piece Tokenization … 2.4 Framework Framework Module Convolutional NN Pooling … Document-Level Relation Extraction Example Code Yao et al. (2019) have pointed out that multiple entities in documents often exhibit complex intersentence relations rather than intra-sentence relations. Besides, as shown in Figure 1, a large number of relational facts are expressed in multiple sentences, e.g., Langdon is the sibling of Jean Clemens. Hence, it is hard to extract these intersentence relations with both the sentence-level and bag-level settings. Although the document-level RE setting is not widely explored by the current work, we argue that this scenario remains an open problem for future research, and still integrate document-level RE into Op"
D19-3029,P19-1277,0,0.217226,"ntelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient an"
D19-3029,D18-2024,1,0.917593,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,D15-1203,0,0.816632,"a University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 20"
D19-3029,D18-1247,1,0.904919,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,C14-1220,0,0.712368,"n Xu Han∗ , Tianyu Gao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scai"
D19-3029,D18-1514,1,0.864567,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,Y15-1009,0,0.292487,"hiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han"
D19-3029,W09-2415,0,0.128206,"Missing"
D19-3029,P19-1139,1,0.92994,"hua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, w"
D19-3029,P16-2034,0,0.322331,"g Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for"
D19-3029,P11-1055,0,0.138259,"ements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect"
I05-1004,I05-1004,1,0.0512826,"Missing"
I05-1004,J96-1002,0,0.013769,"Missing"
I05-1004,W02-2018,0,\N,Missing
I05-1022,M98-1027,0,0.0432489,"l the entities and relations into account, using a model called the Entity Relation Propagation Diagram; thirdly, recognizing a relation based on the results of the preceding stage. The experiments show that the proposed method can improve the entity and relation recognition in some degree. 1 Introduction The entity and relation recognition, i.e. assigning semantic classes (e.g., person, organization and location) to entities in a sentence and determining the relations (e.g., born-in and employee-of) that hold between entities, is an important task in areas such as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story comprehension [6]. In a QA system, many questions concern the specific entities in some relations. For example, the question that “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. In a typical IE task in constructing a job database from unstructured texts, the system are required to extract many meaningful entities like titles and salary from the texts and to determine how these entities are associated with job positions. The task of recognizing entity and relation is usually treated as two separate subtasks"
I05-1022,voorhees-tice-2000-trec,0,0.0746257,", using a model called the Entity Relation Propagation Diagram; thirdly, recognizing a relation based on the results of the preceding stage. The experiments show that the proposed method can improve the entity and relation recognition in some degree. 1 Introduction The entity and relation recognition, i.e. assigning semantic classes (e.g., person, organization and location) to entities in a sentence and determining the relations (e.g., born-in and employee-of) that hold between entities, is an important task in areas such as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story comprehension [6]. In a QA system, many questions concern the specific entities in some relations. For example, the question that “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. In a typical IE task in constructing a job database from unstructured texts, the system are required to extract many meaningful entities like titles and salary from the texts and to determine how these entities are associated with job positions. The task of recognizing entity and relation is usually treated as two separate subtasks carried out sequentially: (1) to recogniz"
I05-1022,P99-1042,0,0.0201748,"ntity Relation Propagation Diagram; thirdly, recognizing a relation based on the results of the preceding stage. The experiments show that the proposed method can improve the entity and relation recognition in some degree. 1 Introduction The entity and relation recognition, i.e. assigning semantic classes (e.g., person, organization and location) to entities in a sentence and determining the relations (e.g., born-in and employee-of) that hold between entities, is an important task in areas such as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story comprehension [6]. In a QA system, many questions concern the specific entities in some relations. For example, the question that “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. In a typical IE task in constructing a job database from unstructured texts, the system are required to extract many meaningful entities like titles and salary from the texts and to determine how these entities are associated with job positions. The task of recognizing entity and relation is usually treated as two separate subtasks carried out sequentially: (1) to recognize entities using an entity r"
I05-1022,P80-1024,0,0.192946,"the problem in a formal way, we first give some basic definitions as follows. A Method of Recognizing Entity and Relation 247 Definition 1 (Entity). An entity can be a single word or a set of consecutive words with a predefined boundary. A sentence is a linked list, which consists of words and entities. Entities in a sentence are denoted as E1, E2 … according to their order, with values ranging over a set of entity class CE. For example, the sentence in Fig. 2 has three entities: E1= “Dole”, E2= “Elizabeth” and E3= “Salisbury, N.C.”. Note that it is not easy to determine the entity boundaries [7]. Here we assume that it has been solved and its output serves as the input to our model. Fig. 2. A sentence that have three entities Definition 2 (Relation). In this paper, we only consider the relation between two entities. An entity pair (Ei, Ej) represents a relation Rij from entity Ei and Ej, where Ei is the first argument and Ej is the second argument. Relation Rij takes its value that ranges over a set of relation class CR. Note that (Ei, Ej) is an ordered pair, and there exist two relations Rij =(Ei, Ej) and Rji =(Ej, Ei) between entities Ei and Ej. Definition 3 (Class). The class of a"
I05-1022,C02-1151,0,\N,Missing
I11-1094,D08-1111,0,0.0207359,"he string in queshigh performance. It fails to segment many intion appears in. stances of UWs correctly. This makes them hard We define a restricted accessor pair as a pair of to be extracted based on WSR. words that has the ability to match the majority of In order to address this deficiency, we define a words. First, we define the matching between a notion of generalized word, and we use the Generpair of words (wl , wr ) and a string v. We say that alized Word-String Ratio (GWSR) to extract UWs (wl , wr ) and v match if: as a modified version of WSR. This idea is derived t(wl , v, wr ) from Liu et al. (2008) and Zhang et al. (2010). > th RAV (12) f (v) For convenience, the term “word” in the rest part of this subsection always refers to a string that is where th RAV is a threshold. f (v) is the numsegmented as a word by CWS model. ber of times that v appears as a word or a sequence We define a cost function of string v = ci · · · cj of words in the corpus segmented by our CWS based on the confidence function: model. t(w , v, w ) is the number of times that l Cost(v) = max{0 − Confo , Confi − 0} (10) If a string v is segmented as a single word, the cost function returns a non-positive value; other"
I11-1094,O97-4005,0,0.0833435,"as SMALL. Corpora with different sizes are used to investigate how the size of the corpus influences the performances of different UW extraction methods. It is difficult to evaluate the performance of UW extraction directly on such a large corpus. We used a partial evaluation method similar to the method used by Feng et al. (2004a). We sampled 2000 sentences from a balanced corpus (YUWEI corpus) consisting of news articles, academic articles, textbook articles, novels and other types of texts. Various UWs appear in these sentences. Since Chinese words commonly consist of 2, 3 or 4 characters (Chang and Su, 1997), only strings with length of 2, 3 or 4 in these sentences are considered as the UW candi-dates (strings in question). After filtering the strings that already appear in the dictionary, the remaining 111,536 strings are used as the test set. In order to annotate these strings in the test set, we used the record of a Chinese input method software as an auxiliary data . We selected strings that are frequently inputted by users and manually annotated 1,630 of them as UWs. The annotation may have bias because the low frequency words tend to be ignored for the manually annotation. But the annotatio"
I11-1094,W03-1705,0,0.0253413,"se sentences into words. A practical CWS model needs to handle UWs, which are also named as out-of-vocabulary (OOV) words. If a corpus is perfectly segmented, the UW extraction task is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morphological evidence, distributional evidence, or both (Chen and Ma, 2002; Ma and Chen, 2003; Feng et al., 2004a; Hong et al., 2009). Some methods can be used for both UW extraction and CWS (Sun et al., 1998; Feng et al., 2004b; Jin and Tanaka-Ishii, 2006; Zhao and Kit, 2008). But for CWS, these methods are not comparable with the character tagging based CWS methods (Zhao and Kit, 2008), because the character tagging based CWS methods can better capture the morphological information. We focus on a UW extraction method based on the distributional information, namely the accessor variety (AV) criterion (Feng et al., 2004a). Assuming that a string is likely a meaningful unit if it occur"
I11-1094,C02-1049,0,0.0265869,"ms to segment Chinese sentences into words. A practical CWS model needs to handle UWs, which are also named as out-of-vocabulary (OOV) words. If a corpus is perfectly segmented, the UW extraction task is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morphological evidence, distributional evidence, or both (Chen and Ma, 2002; Ma and Chen, 2003; Feng et al., 2004a; Hong et al., 2009). Some methods can be used for both UW extraction and CWS (Sun et al., 1998; Feng et al., 2004b; Jin and Tanaka-Ishii, 2006; Zhao and Kit, 2008). But for CWS, these methods are not comparable with the character tagging based CWS methods (Zhao and Kit, 2008), because the character tagging based CWS methods can better capture the morphological information. We focus on a UW extraction method based on the distributional information, namely the accessor variety (AV) criterion (Feng et al., 2004a). Assuming that a string is likely a meaningf"
I11-1094,W03-0430,0,0.0307646,"ution of this paper is twofold. First, we proposed two UW extraction methods which outperform the baselines based on morphological and distributional evidence. Second, our experiments were conducted on corpora with up to 200 billion Chinese characters and provided insights about the effect of corpus size on UW extraction. 2 Xue (2003) proposed a character sequence tagging framework for CWS. Comparing to other methods, it has better performance on dealing with the UWs (Ling et al., 2003; Peng et al., 2004). The sequence tagging framework is also used for named entity identification in English (McCallum and Li, 2003), which is related to Chinese UW extraction. In this framework, the input is a raw Chinese sentence s, denoted as a sequence of characters ci : s = c1 . . . cn (1) The output of the character sequence tagging is a sequence t of tags ti corresponding to the input characters: t = t 1 . . . tn (2) where ti ∈ {B, M, E, S}. The tags B / M / E indicate that the corresponding character is at the beginning / middle / end position of a multicharacter word. The tag S indicates that the corresponding character is a single character word. The segmentation result of this sentence can thus be determined by"
I11-1094,W02-1001,0,0.0322552,"0.5 0.6 Figure 2: The precision-recall curves for WSR and GWSR on three corpora. The curves of GWSR on the SMALL and MIDDLE corpora are not showed due to the limitation of space. season) and 官印 (official seal). Figure 1 is an overview of the frequencies of the UWs in the corpora we used. We can observe the differences between these corpora. A number of UWs even do not appear in the SMALL corpus, while most of the UWs appear with a frequency higher than 10,000 in the LARGE corpus. We use precision and recall as the evaluation measures: The CWS model we use is based on the averaged perceptron (Collins, 2002). The features templates are listed in Table 1, which are similar to the templates used for a CRF-based model (Tseng et al., 2005). The training set provided by Microsoft Research in SIGHAN bake-off 2005 (Emerson, 2005) is used to train our CWS model. The Fmeasure on the test set was 0.963. This is comparable with the reported best 0.964, which is from a CRF-based model (Tseng et al., 2005). Additional techniques were used to speed up the decoding of our CWS model. A modified doublearray trie (Aoe et al., 1992) data structure was implemented to store and retrieve the feature values. Fix-point"
I11-1094,W04-3236,0,0.0159844,"/ E indicate that the corresponding character is at the beginning / middle / end position of a multicharacter word. The tag S indicates that the corresponding character is a single character word. The segmentation result of this sentence can thus be determined by the tag sequence. Given an input sentence s, the output sequence of tags t is calculated as t = arg max W T Φ(s, t′ ) ′ t (3) where Φ returns a feature vector of the pair (s, t′ ), and W is a vector of feature weights. The decoding is to find a t that maximizes the objective function. Machine learning methods such as maximum entropy (Ng and Low, 2004), conditional random field model (Peng et al., 2004) and perceptron (Jiang et al., 2009) have been used for this framework. The features in this framework are mainly composed by character unigrams, character bigrams and tag bigrams. In Chinese, a character is usually a morpheme. Therefore the CWS model based on the character tagging framework can be regarded as a UW extraction method using morphological information. However, in contrast to CWS, UW extraction focuses on identifying substrings in a corpus that are potential words independent of the environments where they may occur. Though the p"
I11-1094,I05-3017,0,0.280797,"e features in this framework are mainly composed by character unigrams, character bigrams and tag bigrams. In Chinese, a character is usually a morpheme. Therefore the CWS model based on the character tagging framework can be regarded as a UW extraction method using morphological information. However, in contrast to CWS, UW extraction focuses on identifying substrings in a corpus that are potential words independent of the environments where they may occur. Though the performance of the CWS method is relatively high, the poor recall of UWs ’is still the Achilles heel of segmentation systems’ (Emerson, 2005). The Background 2.1 CWS as Character Tagging CWS aims to segment Chinese sentences into words. A practical CWS model needs to handle UWs, which are also named as out-of-vocabulary (OOV) words. If a corpus is perfectly segmented, the UW extraction task is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morpho"
I11-1094,C04-1081,0,0.0252923,"efficiently segment the corpus on the fly, which is practical for processing large-scale corpus. The contribution of this paper is twofold. First, we proposed two UW extraction methods which outperform the baselines based on morphological and distributional evidence. Second, our experiments were conducted on corpora with up to 200 billion Chinese characters and provided insights about the effect of corpus size on UW extraction. 2 Xue (2003) proposed a character sequence tagging framework for CWS. Comparing to other methods, it has better performance on dealing with the UWs (Ling et al., 2003; Peng et al., 2004). The sequence tagging framework is also used for named entity identification in English (McCallum and Li, 2003), which is related to Chinese UW extraction. In this framework, the input is a raw Chinese sentence s, denoted as a sequence of characters ci : s = c1 . . . cn (1) The output of the character sequence tagging is a sequence t of tags ti corresponding to the input characters: t = t 1 . . . tn (2) where ti ∈ {B, M, E, S}. The tags B / M / E indicate that the corresponding character is at the beginning / middle / end position of a multicharacter word. The tag S indicates that the corresp"
I11-1094,J04-1004,0,0.238159,"how that our methods outperform the baselines, and the combination of the two evidences can further improve the performance. Moreover, our methods can also efficiently segment the corpus on the fly, which is especially valuable for processing large-scale corpus. 1 The representative method of using morphological evidence is Chinese word segmentation (CWS) model. CWS is to identify every word token in a given sentence. Using the CWS model, we can define the word-string ratio (WSR) to extract UWs. The representative method of using distributional evidence is the accessor variety (AV) criterion (Feng et al., 2004a). WSR is directly derived from the CWS method based on character tagging (Xue, 2003). This CWS method is based on the morphological information of the strings in question and their context. Strings with high WSR are considered as words, for high WSR indicates that the corresponding string is segmented as a word by this CWS method with high probability. Though the performance of the CWS method is relatively high, it leaves a number of UWs unrecognized or incorrectly recognized due to erroneous segmentation. Introduction The AV criterion (Feng et al., 2004a) is based on the distributional info"
I11-1094,P98-2206,1,0.669682,"words. If a corpus is perfectly segmented, the UW extraction task is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morphological evidence, distributional evidence, or both (Chen and Ma, 2002; Ma and Chen, 2003; Feng et al., 2004a; Hong et al., 2009). Some methods can be used for both UW extraction and CWS (Sun et al., 1998; Feng et al., 2004b; Jin and Tanaka-Ishii, 2006; Zhao and Kit, 2008). But for CWS, these methods are not comparable with the character tagging based CWS methods (Zhao and Kit, 2008), because the character tagging based CWS methods can better capture the morphological information. We focus on a UW extraction method based on the distributional information, namely the accessor variety (AV) criterion (Feng et al., 2004a). Assuming that a string is likely a meaningful unit if it occurs in different linguistic environments (Feng et al., 2004a), AV is defined as: A character tagging based CWS model,"
I11-1094,I05-3027,0,0.133049,"corpora are not showed due to the limitation of space. season) and 官印 (official seal). Figure 1 is an overview of the frequencies of the UWs in the corpora we used. We can observe the differences between these corpora. A number of UWs even do not appear in the SMALL corpus, while most of the UWs appear with a frequency higher than 10,000 in the LARGE corpus. We use precision and recall as the evaluation measures: The CWS model we use is based on the averaged perceptron (Collins, 2002). The features templates are listed in Table 1, which are similar to the templates used for a CRF-based model (Tseng et al., 2005). The training set provided by Microsoft Research in SIGHAN bake-off 2005 (Emerson, 2005) is used to train our CWS model. The Fmeasure on the test set was 0.963. This is comparable with the reported best 0.964, which is from a CRF-based model (Tseng et al., 2005). Additional techniques were used to speed up the decoding of our CWS model. A modified doublearray trie (Aoe et al., 1992) data structure was implemented to store and retrieve the feature values. Fix-point numbers (integer) rather than floating point numbers are used for the calculation without losing accuracy. With some other minor i"
I11-1094,P09-1059,0,0.0643896,"ion of a multicharacter word. The tag S indicates that the corresponding character is a single character word. The segmentation result of this sentence can thus be determined by the tag sequence. Given an input sentence s, the output sequence of tags t is calculated as t = arg max W T Φ(s, t′ ) ′ t (3) where Φ returns a feature vector of the pair (s, t′ ), and W is a vector of feature weights. The decoding is to find a t that maximizes the objective function. Machine learning methods such as maximum entropy (Ng and Low, 2004), conditional random field model (Peng et al., 2004) and perceptron (Jiang et al., 2009) have been used for this framework. The features in this framework are mainly composed by character unigrams, character bigrams and tag bigrams. In Chinese, a character is usually a morpheme. Therefore the CWS model based on the character tagging framework can be regarded as a UW extraction method using morphological information. However, in contrast to CWS, UW extraction focuses on identifying substrings in a corpus that are potential words independent of the environments where they may occur. Though the performance of the CWS method is relatively high, the poor recall of UWs ’is still the Ac"
I11-1094,O03-4002,0,0.23119,"urther improve the performance. Moreover, our methods can also efficiently segment the corpus on the fly, which is especially valuable for processing large-scale corpus. 1 The representative method of using morphological evidence is Chinese word segmentation (CWS) model. CWS is to identify every word token in a given sentence. Using the CWS model, we can define the word-string ratio (WSR) to extract UWs. The representative method of using distributional evidence is the accessor variety (AV) criterion (Feng et al., 2004a). WSR is directly derived from the CWS method based on character tagging (Xue, 2003). This CWS method is based on the morphological information of the strings in question and their context. Strings with high WSR are considered as words, for high WSR indicates that the corresponding string is segmented as a word by this CWS method with high probability. Though the performance of the CWS method is relatively high, it leaves a number of UWs unrecognized or incorrectly recognized due to erroneous segmentation. Introduction The AV criterion (Feng et al., 2004a) is based on the distributional information. Strings that have various contexts can be considered as words. It is shown th"
I11-1094,P06-2056,0,0.0149977,"ted, the UW extraction task is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morphological evidence, distributional evidence, or both (Chen and Ma, 2002; Ma and Chen, 2003; Feng et al., 2004a; Hong et al., 2009). Some methods can be used for both UW extraction and CWS (Sun et al., 1998; Feng et al., 2004b; Jin and Tanaka-Ishii, 2006; Zhao and Kit, 2008). But for CWS, these methods are not comparable with the character tagging based CWS methods (Zhao and Kit, 2008), because the character tagging based CWS methods can better capture the morphological information. We focus on a UW extraction method based on the distributional information, namely the accessor variety (AV) criterion (Feng et al., 2004a). Assuming that a string is likely a meaningful unit if it occurs in different linguistic environments (Feng et al., 2004a), AV is defined as: A character tagging based CWS model, which is based on the morphological evidence, c"
I11-1094,I08-1002,0,0.0160061,"is also accomplished. 838 CWS methods also fails to capture distributional information of the strings in question. based on the distributional evidence. Finally, we discuss the combination of these methods. 2.2 UW Extraction and the Accessor Variety Criterion 3.1 Morphological Evidence There are methods proposed for UW extraction based on morphological evidence, distributional evidence, or both (Chen and Ma, 2002; Ma and Chen, 2003; Feng et al., 2004a; Hong et al., 2009). Some methods can be used for both UW extraction and CWS (Sun et al., 1998; Feng et al., 2004b; Jin and Tanaka-Ishii, 2006; Zhao and Kit, 2008). But for CWS, these methods are not comparable with the character tagging based CWS methods (Zhao and Kit, 2008), because the character tagging based CWS methods can better capture the morphological information. We focus on a UW extraction method based on the distributional information, namely the accessor variety (AV) criterion (Feng et al., 2004a). Assuming that a string is likely a meaningful unit if it occurs in different linguistic environments (Feng et al., 2004a), AV is defined as: A character tagging based CWS model, which is based on the morphological evidence, can be directly used t"
I11-1094,J09-4006,1,0.858405,"“飞地” (enclave, literally “flying territory”). Last but not least, as RAV only concerns with a small number of restricted accessor pairs, RAV is more effective and efficient than AV in a largescale corpus. propose a simple way of linear combination. 4 Experiments 4.1 Dataset and Evaluation Method A dictionary is needed to distinguish unknown words from known words. We used the same dictionary that Feng et al. (2004a) used. Totally 119,803 words in this downloaded dictionary are used as the known words. SogouT corpus is an open and free large-scale Web corpus . This Web corpus was also used by Li and Sun (2009) in their semi-supervised CWS model. After certain process to remove non-text content such as the HTML tags, we obtained 119 million web pages consisting of 203 billion Chinese characters. The whole corpus is denoted as LARGE. We sampled about one percent of these pages as a smaller corpus called MIDDLE, and further sampled about one percent of these pages in MIDDLE as SMALL. Corpora with different sizes are used to investigate how the size of the corpus influences the performances of different UW extraction methods. It is difficult to evaluate the performance of UW extraction directly on such"
I11-1094,P03-2039,0,0.0286347,"r methods can also efficiently segment the corpus on the fly, which is practical for processing large-scale corpus. The contribution of this paper is twofold. First, we proposed two UW extraction methods which outperform the baselines based on morphological and distributional evidence. Second, our experiments were conducted on corpora with up to 200 billion Chinese characters and provided insights about the effect of corpus size on UW extraction. 2 Xue (2003) proposed a character sequence tagging framework for CWS. Comparing to other methods, it has better performance on dealing with the UWs (Ling et al., 2003; Peng et al., 2004). The sequence tagging framework is also used for named entity identification in English (McCallum and Li, 2003), which is related to Chinese UW extraction. In this framework, the input is a raw Chinese sentence s, denoted as a sequence of characters ci : s = c1 . . . cn (1) The output of the character sequence tagging is a sequence t of tags ti corresponding to the input characters: t = t 1 . . . tn (2) where ti ∈ {B, M, E, S}. The tags B / M / E indicate that the corresponding character is at the beginning / middle / end position of a multicharacter word. The tag S indica"
I11-1094,C98-2201,1,\N,Missing
J09-4006,J96-1002,0,0.0525398,"Missing"
J09-4006,J04-1004,0,0.0729039,"olves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially be applied to thi"
J09-4006,J05-4005,0,0.0740456,"ent of our method is obvious. In addition, a model is of limited use if it fits the SIGHAN data well, but can’t maintain that accuracy elsewhere. Our model has a wider coverage through Table 7 Results of unknown word recognition in 24,470 sentences. our method the MSR model 510 P R F 96.2 88.3 97.9 84.5 97.1 86.3 Li and Sun Punctuation as Implicit Annotations mining the Web. It tends to segment long multiword expressions into their component words. This is not a disadvantage as long as the result is consistent. 5. Related Work Punctuation gives naturally occurring unambiguous word boundaries. Gao et al. (2005) described how to remove overlapping ambiguities in an annotated corpus to train a model for resolving these ambiguities. A raw corpus doesn’t play a role in that method, and the model involves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular w"
J09-4006,P06-1085,0,0.030229,"Missing"
J09-4006,P06-2056,0,0.117682,"d on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially be applied to this task as well. Appendix A: Input to the Training Algorithm We give"
J09-4006,I05-3025,0,0.114955,"Missing"
J09-4006,C04-1081,0,0.428998,"Missing"
J09-4006,H89-2048,0,0.266644,"ity We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation. 1. Introduction Paragraphs are composed of sentences. Hence when a paragraph begins, a sentence must begin, and as a paragraph closes, some sentence must finish. This observation is the basis of the sentence boundary detection method proposed by Riley (1989). Similarly, sentences consist of words. As a sentence begins or ends there must be word boundaries. Inspired by this notion, we invent a method to learn a Chinese word segmentation model with punctuation marks in a large raw corpus. The learning is guided by a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves notably the recognition of out-of-vocabulary (OOV) words with respect to approaches which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has practical implications in that the OOV problem has long been a big challenge for the research c"
J09-4006,N03-1028,0,0.0234667,"Missing"
J09-4006,P98-2206,1,0.515416,"Missing"
J09-4006,O03-4002,0,0.912696,"a paragraph closes, some sentence must finish. This observation is the basis of the sentence boundary detection method proposed by Riley (1989). Similarly, sentences consist of words. As a sentence begins or ends there must be word boundaries. Inspired by this notion, we invent a method to learn a Chinese word segmentation model with punctuation marks in a large raw corpus. The learning is guided by a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves notably the recognition of out-of-vocabulary (OOV) words with respect to approaches which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has practical implications in that the OOV problem has long been a big challenge for the research community. 2. Segmentation as Tagging We call the first character of a Chinese word its left boundary L, and the last character its right boundary R. If we regard L and R as random events, then we can derive four events (or tags) from them: b = L ·R, m = L · R, s = L · R, e=L·R ∗ Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: eemath@gmail.com. ∗∗ Department of Computer Science and Technology, Tsinghua Universit"
J09-4006,W03-3023,0,0.0135073,"Missing"
J09-4006,W06-0127,0,0.179937,"Missing"
J09-4006,C98-2201,1,\N,Missing
K18-1024,P17-4008,0,0.177166,"Missing"
K18-1024,P17-1016,0,0.042639,"Missing"
K18-1024,P17-1125,0,0.274281,"Missing"
K18-1024,P02-1040,0,0.106779,"Missing"
K18-1024,D14-1074,0,0.210415,"Missing"
K18-1024,P16-1159,1,0.898289,"Missing"
K18-1024,W17-4811,0,0.0431098,"Missing"
K18-1024,C16-1100,0,0.0733373,"Missing"
N19-1105,D15-1247,0,0.110478,"Missing"
N19-1105,C18-1075,0,0.479342,"Missing"
N19-1105,P98-1013,0,0.408429,"Missing"
N19-1105,D18-1021,1,0.869581,"Missing"
N19-1105,P18-1241,0,0.0663714,"Missing"
N19-1105,P17-1038,0,0.449592,"Missing"
N19-1105,P15-1017,0,0.704641,"Missing"
N19-1105,N18-1076,0,0.147721,"Missing"
N19-1105,I17-1036,0,0.128152,"Missing"
N19-1105,P16-2011,0,0.135941,"Missing"
N19-1105,N18-2058,0,0.476683,"Missing"
N19-1105,P16-2060,0,0.130043,"Missing"
N19-1105,P09-2093,0,0.207456,"Missing"
N19-1105,P11-1113,0,0.73024,"Missing"
N19-1105,P18-1048,0,0.155771,"Missing"
N19-1105,P16-1025,0,0.240411,"Missing"
N19-1105,C10-1077,0,0.711492,"Missing"
N19-1105,P10-1081,0,0.823295,"Missing"
N19-1105,P18-1145,0,0.0214274,"Missing"
N19-1105,D18-1127,0,0.406701,"Missing"
N19-1105,P16-1201,0,0.0323032,"Missing"
N19-1105,P17-1164,0,0.540835,"Missing"
N19-1105,P11-1163,0,0.257108,"Missing"
N19-1105,E12-1029,0,0.137736,"Missing"
N19-1105,P09-1113,0,0.272376,"Missing"
N19-1105,P08-1030,0,0.791729,"Missing"
N19-1105,P13-1008,0,0.756973,"Missing"
N19-1105,C18-1007,0,0.0627013,"Missing"
N19-1105,N16-1034,0,0.425474,"Missing"
N19-1105,N16-1033,0,0.379533,"Missing"
N19-1105,P15-2060,0,0.493355,"Missing"
N19-1105,P18-4009,0,0.10604,"Missing"
N19-1105,P18-1046,0,0.0622712,"Missing"
N19-1105,D15-1203,0,0.0776927,"Missing"
N19-1105,P18-2066,0,0.731835,"Missing"
N19-1105,C18-1099,1,0.90081,"Missing"
N19-1105,D17-1187,0,0.0611022,"Missing"
N19-1105,1983.tc-1.13,0,0.586136,"Missing"
P11-2085,P00-1031,0,0.724151,"ith English typos, we observe some language-specific properties in Chinese have impact on errors. All in all, user behaviors (Zheng et al., 2009; Zheng et al., 2010; Zheng et al., 2011b) in Chinese Pinyin input method provide novel perspectives for natural language processing tasks. Below we sketch three possible directions for the future work: (1) we should consider position features in analyzing Pinyin errors. For example, it is less likely that users make errors in the first letter of an input Pinyin. (2) we aim at designing a selfadaptive input method that provide error-tolerant features (Chen and Lee, 2000; Zheng et al., 2011a). (3) we want to build a Chinese spelling correction system based on extracted error-correction pairs. Acknowledgments This work is supported by a Tsinghua-Sogou joint research project and the National Natural Science Foundation of China under Grant No. 60873174. References F. Ahmad and G. Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 955–962. K. Atkinson. 2008. Gnu aspell 0.60.6. http://aspell.sourceforge.net. E. Brill and R.C"
P11-2085,D07-1019,0,0.0164549,"pos suggests that some language-specific properties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy ch"
P11-2085,W04-3238,0,0.103425,"t method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et al. (2000) propose an approximate word-matching algorithm for Chinese to solve Chinese spell detection and correction task. Zhang et al. (1999) present"
P11-2085,C10-1041,0,0.359851,"roperties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Rista"
P11-2085,D09-1129,0,0.0303874,"input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which rel"
P11-2085,C90-2036,0,0.751524,"Missing"
P11-2085,O04-2007,0,0.0643163,"Missing"
P11-2085,P09-2007,0,0.0668477,"Missing"
P11-2085,C10-2085,0,0.0425557,"Missing"
P11-2085,D10-1094,0,0.0221483,"thod. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et"
P11-2085,P02-1019,0,0.100427,"ors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et al. (2000) propose an approximate word-matching algorithm for Chinese to solve Chinese spell detection and correction task. Zhang et al. (1999) present a winnow-based approach for Chinese spelling correction which takes both loc"
P11-2085,D09-1093,0,0.240351,"ome language-specific properties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan"
P11-2085,P00-1032,0,0.0939944,"Missing"
P11-2085,P10-3009,1,0.773866,"study user input behaviors in Chinese Pinyin input method from backspace operations. We aim at analyzing the reasons that cause these errors. Users signal that they are very likely to make errors if they press backspace on the keyboard. Then they modify the errors and type in the correct words they want. Different from the previous research, we extract abundant Pinyin-correction and Chinese word-correction pairs from backspace operations. Compared with English typos, we observe some language-specific properties in Chinese have impact on errors. All in all, user behaviors (Zheng et al., 2009; Zheng et al., 2010; Zheng et al., 2011b) in Chinese Pinyin input method provide novel perspectives for natural language processing tasks. Below we sketch three possible directions for the future work: (1) we should consider position features in analyzing Pinyin errors. For example, it is less likely that users make errors in the first letter of an input Pinyin. (2) we aim at designing a selfadaptive input method that provide error-tolerant features (Chen and Lee, 2000; Zheng et al., 2011a). (3) we want to build a Chinese spelling correction system based on extracted error-correction pairs. Acknowledgments This"
P11-2085,P00-1037,0,\N,Missing
P11-2085,H05-1120,0,\N,Missing
P15-2093,P14-2037,0,0.339882,"Missing"
P15-2093,2005.mtsummit-papers.11,0,0.131041,"Missing"
P15-2093,D13-1167,0,0.0174992,"Missing"
P15-2093,E14-1049,0,0.229795,"zy, liuyang2011, sms}@tsinghua.edu.cn Abstract transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, f"
P15-2093,D14-1162,0,0.103768,"for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567–572, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Ltotal = li l1 l1 ?? ? ≈ ?? ⋅ ??? ? bilingual relations and constraints li li P · j,k Xjk Xjk P P li li , j Xjk · k Xjk where X li is the matrix of word-context co-occurrence counts. As Pennington et al. (2014), we add separate terms blwi j , blcik for each word and context to absorb the effect of any possible word-specific biases. We also add an additional matrix bias bli for the ease of sharing embeddings among matrices. The loss function is written as the sum of the weighted square error, i Llmono = X  2 li li f (Xjk ) wjli · clki + blwi j + blcik + bli − Mjk , j,k (2) where we choose the same weighting function as the GloVe model to place less confidence on those word-context pairs with rare occurrences, ( f (x) = (x/xmax )α 1 if x &lt; xmax . otherwise (3) li Notice that we only have to optimize"
P15-2093,D11-1014,0,0.0441177,"e cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to"
P15-2093,P14-1006,0,0.033965,"e low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities,"
P15-2093,P12-1092,0,0.0672644,"llel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual"
P15-2093,D13-1141,0,0.17614,"ly on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful. Koˇcisk`y et al. (2014) integrated word aligning process and word embedding in machine translation models. This method makes full use of parallel corpora and produces high-quality word alignments. However, it is unable to exploit the richer monolingual corpora. On the other hand, Zou et al. (2013) and Faruqui and Dyer (2014) learnt word embeddings of different languages in separate spaces with monolingual corpora and projected the embeddings into a joint space, but they can only capture linear transformation. In this paper, we address the above challenges with a framework of matrix co-factorization. We A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features. In this paper, we present a matrix cofactorization framework for learning cross-lingual word embeddings. We explicitly define monolingual training objectives in the form of"
P15-2093,C12-1089,0,0.655529,"tance, distance(wjl1 , wkl2 ) = ||wjl1 − wkl2 ||2 . Notice that similar to the monolingual objective, we may optimize for only those sim(j, k) 6= 0, which is efficient as the matrix of translation probabilities or dictionary is sparse. We call this method CLSim. 5 Experiments To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). Cross-lingual Similarities 5.1 An alternative way to set cross-lingual constraints is to minimize the distances between similar word pairs. Here the semantic similarities can be measured by equivalence in translation, sim(j, k), which is produced by a machine translation system. In this paper, we use the translation probabilities produced by a machine translation system. Minimizing the distances of related words in the two languages weighted by their similarities gives us the cross-lingual objective Data and Training For optimizing the monolingual objectives, We used exactly the same subset"
P15-2093,C14-1048,0,\N,Missing
P15-2093,D10-1025,0,\N,Missing
P15-2093,P14-2095,0,\N,Missing
P15-2093,D13-1168,0,\N,Missing
P16-1097,J93-2003,0,0.117603,"Missing"
P16-1097,2010.iwslt-papers.3,0,0.0182997,"alignment and machine translation evaluations. 2 Background Given a monolingual corpus of source language phrases E = {e(s) }Ss=1 and a monolingual corpus of target language phrases F = {f (t) }Tt=1 , we assume there exists a parallel corpus D = {he(s) , f (t) i|e(s) ↔ f (t) }, where e(s) ↔ f (t) denotes that e(s) and f (t) are translations of each other. As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F . To address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario: X P (F |E; θ) = P (F, m|E; θ) , (1) | {z } m phrase alignment where m is phrase alignment and θ is a set of model parameters. Each target phrase f (t) is re"
P16-1097,C14-1192,1,0.824319,"− −|f (t) , e(s) ; ← P (← a θ)× − −). δ(→ a ,← a (25) We experimented with collecting counts from both the unidirectional and agreement terms but obtained much worse results than counting only from the agreement term. (26) Counts of target-to-source length and translation models can be calculated in a similar way. The new length probabilities can be obtained by c(J|I; E, F ) . 0 J 0 c(J |I; E, F ) i=0 (s) δ(e, ei ) The new translation probabilities can be obtained by ˆ hs,ti∈m p(J|I) = P I X +σ(f, e, d). s∈{0,1,...,S} This can be cast as a translation retrieval problem (Zhang and Zong, 2013; Dong et al., 2014). Please refer to (Dong et al., 2015) for more details. The target-to-source Viterbi phrase alignment can be calculated similarly. (s) (t) δ(f, fj ) (28) For example, Figure 3 shows two examples of Chinese-to-English and English-to-Chinese word alignments. The shared links are highlighted in 1028 red. Our intuition is that a source phrase and a target phrase are more likely to be translations of each other if the two translation models also agree on word alignment within aligned phrases. 3.3.2 Training Objective and Algorithm The training objective for inner agreement is given by → − ← − Jinne"
P16-1097,D14-1061,0,0.0242113,"Missing"
P16-1097,W04-3208,0,0.103748,"Missing"
P16-1097,P04-1067,0,0.0994235,"Missing"
P16-1097,P08-1088,0,0.0752765,"Missing"
P16-1097,N03-1017,0,0.215573,"Missing"
P16-1097,P07-2045,0,0.0178646,"an extracted parallel corpus can be measured by F1 = 2|D ∩ G|/(|D |+ |G|). 4.1.2 o ← − P (hi, ji|f (t) , e(s) ; θ ) , (31) Alignment Evaluation Data Preparation Although it is appealing to apply our approach to dealing with real-world non-parallel corpora, it is time-consuming and labor-intensive to manually construct a ground truth parallel corpus. Therefore, we follow Dong et al. (2015) to build synthetic E, F , and G to facilitate the evaluation. We first extract a set of parallel phrases from a sentence-level parallel corpus using the stateof-the-art phrase-based translation system Moses (Koehn et al., 2007) and discard low-probability parallel phrases. Then, E and F can be constructed by corrupting the parallel phrase set by 1029 noise C E 0 0 0 10K 0 20K 10K 0 20K 0 10K 10K 20K 20K 0.40 0.35 inner outer no agreement agreement ratio 0.30 0.25 0.20 0.15 1 2 3 4 5 6 iteration 7 8 9 Outer Inner 58.5 41.0 28.3 54.7 50.4 34.9 22.4 61.2 54.4 48.3 43.1 31.4 34.4 23.1 86.5 83.6 80.1 84.9 83.8 80.0 73.6 86.1 83.8 81.2 84.3 83.6 79.7 74.3 10 Figure 4: Comparison of agreement ratios on the development set. seed 50 100 500 1,000 C→E 4.1 5.1 7.5 22.4 E→C 4.8 5.5 8.4 23.1 Outer 60.8 65.6 70.4 73.6 Inner 66.2"
P16-1097,P93-1003,0,0.628466,"Missing"
P16-1097,N06-1014,0,0.486435,"ter running for 70 iterations. In addition, their approach seems prone to be affected by noisy data in non-parallel corpora as the accuracy drops significantly with the increase of noise. Since asymmetric word alignment and phrase alignment models are usually complementary, it is natural to combine them to make more accurate predictions. In this work, we propose to in1024 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1024–1033, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics troduce agreement-based learning (Liang et al., 2006; Liang et al., 2008) into extracting parallel lexicons and phrases from non-parallel corpora. Based on the latent-variable model proposed by Dong et al. (2015), we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word alignment in two directions. As the inference is intractable, we resort to a Viterbi EM algorithm to train the two models efficiently. Experiments on the Chinese-English dataset show that agreementbased learning is more robust to noisy data and leads to substantial improvements in phrase alignment and machine translation ev"
P16-1097,W02-1018,0,0.132384,"Missing"
P16-1097,W97-0311,0,0.350534,"Missing"
P16-1097,P06-1011,0,0.0359141,"al improvements in phrase alignment and machine translation evaluations. 2 Background Given a monolingual corpus of source language phrases E = {e(s) }Ss=1 and a monolingual corpus of target language phrases F = {f (t) }Tt=1 , we assume there exists a parallel corpus D = {he(s) , f (t) i|e(s) ↔ f (t) }, where e(s) ↔ f (t) denotes that e(s) and f (t) are translations of each other. As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F . To address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario: X P (F |E; θ) = P (F, m|E; θ) , (1) | {z } m phrase alignment where m is phrase alignment and θ is a set of model parameters. Each tar"
P16-1097,P12-1017,0,0.0336177,"Missing"
P16-1097,J03-1002,0,0.0293945,"Missing"
P16-1097,P02-1040,0,0.0960556,"allel corpora consists of 2.65M Chinese phrases and 3.67M English phrases extracted from LDC news articles. We use a small out-domain parallel corpus extracted from financial news of FTChina which contains 10K phrase pairs. The task is to extract a parallel corpus from in-domain non-parallel corpora starting from a small out-domain parallel corpus. We use the state-of-the-art translation system Moses (Koehn et al., 2007) and evaluate the performance on Chinese-English NIST datasets. The development set is NIST 2006 and the test set is NIST 2005. The evaluation metric is caseinsensitive BLEU4 (Papineni et al., 2002). We use the SRILM toolkit (Stolcke, 2002) to train a 4-gram English language model on a monolingual corpus with 399M English words. Table 4 shows the results. At iteration 0, only the out-domain corpus is used and the BLEU score is 5.61. All methods iteratively extract parallel phrases from non-parallel corpora and enlarge the extracted parallel corpus. We find that agreementbased learning achieves much higher BLEU scores while obtains a smaller parallel corpus as compared with independent learning. One possible reason is that the agreement-based learning rules out most unlikely phrase pairs"
P16-1097,P11-1002,0,0.0291102,"Missing"
P16-1097,H94-1027,0,0.297624,"Missing"
P16-1097,C96-2141,0,0.528128,"irs by encouraging consensus between two models. 5 Conclusion We have presented agreement-based training for learning parallel lexicons and phrases from nonparallel corpora. By modeling the agreement on both phrase alignment and word alignment, our approach achieves significant improvements in both alignment and translation evaluations. In the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness. It is also interesting to extend the phrase translation model to more sophisticated models such as IBM models 2-5 (Brown et al., 1993) and HMM (Vogel and Ney, 1996). Acknowledgments We sincerely thank the reviewers for their valuable suggestions. We also thank Meng Zhang, Yankai Lin, Shiqi Shen, Meiping Dong and Congyu Fu for their insightful discussions. Yang Liu is sup1031 Iteration 0 1 2 3 4 5 6 7 8 9 10 E→ C 145k 195k 209k 214k 217k 219k 222k 224k 227k 229k Corpus Size C→ E Outer 10k 162k 59k 215k 69k 231k 88k 238k 106k 241k 123k 243k 137k 247k 140k 249k 153k 251k 159k 254k 163k Inner E→ C 73k 101k 132k 159k 181k 197k 207k 220k 233k 239k 8.65 8.82 8.42 8.46 8.87 8.52 8.81 8.71 8.92 8.33 BLEU C→ E Outer 5.61 8.90 13.53 9.47 15.26 9.29 16.88 9.27 17.15"
P16-1097,D13-1168,0,0.0421124,"Missing"
P16-1097,P15-2118,0,0.0311429,"Missing"
P16-1097,1994.amta-1.26,0,0.604428,"Missing"
P16-1097,P13-1140,0,0.303666,"rpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi ∗ Corresponding author: Yang Liu. et al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli´c and Moens, 2013; Mikolov et al., 2013; Vuli´c and Moens, 2015). Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. Dong et al. (2015) continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases. They introduce a corpus-level latentvariable translation model in a non-parallel scenario and develop a training algorithm that alternates between (1) using a parallel lexicon to extract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to enlarge the parallel lexico"
P16-1159,J93-2003,0,0.0998959,"Missing"
P16-1159,P05-1033,0,0.394114,"d-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a major drawback"
P16-1159,P14-1066,0,0.055646,"ages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , . . . , xm , . . . , xM and a target sentence y"
P16-1159,P12-1031,0,0.0465582,"the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , ."
P16-1159,P15-1001,0,0.46654,"missing. By optimizing model parameters directly with respect to sentence-level BLEU, RNN SEARCH -MRT seems to be able to generate translations more consistently at the sentence level. 4.7 Results on English-French Translation Table 7 shows the results on English-French translation. We list existing end-to-end NMT systems that are comparable to our system. All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion. They differ in network architectures and vocabulary sizes. Our RNN SEARCH -MLE system achieves a BLEU score comparable to that of Jean et al. (2015). RNN SEARCH -MRT achieves the highest BLEU score in this setting even with a vocabulary size smaller than Luong et al. (2015b) and Sutskever et al. (2014). Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems. 4.8 Results on English-German Translation Table 8 shows the results on English-German translation. Our approach still significantly out1689 Source Reference M OSES RNN SEARCH -MLE RNN SEARCH -MRT meiguo daibiao tuan baokuo laizi shidanfu daxue de yi wei zhongguo zhuanjia , liang ming canyuan waijiao zhengce zhuli yiji yi wei fu"
P16-1159,D15-1166,0,0.616781,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P15-1002,0,0.727498,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P03-1021,0,0.667288,"the expected loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 B"
P16-1159,P02-1040,0,0.11537,". . . , yN , end-to-end NMT directly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the"
P16-1159,P06-2101,0,0.836096,"ed loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source"
P16-1159,2006.amta-papers.25,0,0.0949686,"ectly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widel"
P16-1159,D13-1176,0,0.0837584,"on. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks. 1 Introduction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framewo"
P16-1159,D07-1091,0,0.0128786,"s with 91M English words and To build the subset, an alternative to sampling is computing top-k translations. We prefer sampling to comput87M German words. The concatenation of newsing top-k translations for efficiency: sampling is more effitest 2012 and news-test 2013 is used as the validacient and easy-to-implement than calculating k-best lists, estion set and news-test 2014 as the test set. pecially given the extremely parallel architectures of GPUs. 1686 40 35 35 30 30 BLEU score BLEU score 40 25 20 15 10 0 5 10 15 20 25 30 Training time (hours) k=100 k=50 k=25 5 α=1×10 35 0 40 1. M OSES (Koehn and Hoang, 2007): a phrasebased SMT system using minimum error rate training (Och, 2003). 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system using maximum likelihood estimation. M OSES uses the parallel corpus to train a phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit (Stolcke, 2002). 2 The log-linear model Moses uses is trained by the minimum error rate training (MERT) algorithm (Och, 2003) that directly optimizes model parameters with respect to evaluation metrics. RNN SEARCH uses the parallel corpus to train an attention-based ne"
P16-1159,N03-1017,0,0.112266,"duction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a"
P16-1159,W04-1013,0,0.0217352,"late reinforcement reward while MRT generates multiple samples to calculate the expected risk. Figure 2 indicates that multiple samples potentially increases MRT’s capability of discriminating between diverse candidates and thus benefit translation quality. Our experiments confirm their finding that taking evaluation metrics into account when optimizing model parameters does help to improve sentence-level text generation. More recently, our approach has been successfully applied to summarization (Ayana et al., 2016). They optimize neural networks for headline generation with respect to ROUGE (Lin, 2004) and also achieve significant improvements, confirming the effectiveness and applicability of our approach. 5 6 Related Work Our work originated from the minimum risk training algorithms in conventional statistical machine translation (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012). Och (2003) describes a smoothed error count to allow calculating gradients, which directly inspires us to use a parameter α to adjust the smoothness of the objective function. As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-bes"
P16-1159,D14-1179,0,\N,Missing
P16-1185,J93-2003,0,0.0781716,"Missing"
P16-1185,P05-1033,0,0.174897,"t show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT"
P16-1185,W14-4012,0,0.0289659,"Missing"
P16-1185,D14-1061,0,0.0330026,"Missing"
P16-1185,P15-1001,0,0.069953,"Missing"
P16-1185,D13-1176,0,0.0583835,"arget and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they h"
P16-1185,P13-1140,0,0.0205566,"Missing"
P16-1185,E12-1014,0,0.015191,"pora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning. 4.1 Exploiting Monolingual Corpora for Machine Translation Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques. Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014). 1972 Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora. The major advantages of our approach are the transparency to network arch"
P16-1185,N03-1017,0,0.145099,"hineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language"
P16-1185,P07-2045,0,0.0608607,"ts serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of tra"
P16-1185,P15-1002,0,0.100715,"N SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingual corpus, and λ1 = 0 and λ2 = 0.1 for source monolingual corpus incorporation. The threshold of gradient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus. 3.2 Effect of Sample Size k As the inference of our approach is intracta"
P16-1185,P03-1021,0,0.0280237,"sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingua"
P16-1185,P02-1040,0,0.119118,"nese validation set. For Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the validation set for hyper-parameter optimization and model selection. The NIST 2002, 2003, 2004, and 2005 datasets serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-base"
P16-1185,P11-1002,0,0.0466152,"Missing"
P16-1185,D11-1014,0,0.0792271,"icted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT. 2.2 the observed source sentence via a latent target sentence: → − ← − P (x0 |x; θ , θ ) X ← − = P (x0 , y|x; θ ) y Autoencoders on Monolingual Corpora It is appealing to explore the more readily available, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y(t) }Tt=1 ? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in Figure 1(b), given an observed English sentence “Bush held a talk with Sharon”, a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation “bushi yu shalong juxing le huitan” that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs t"
P16-1185,P07-1004,0,0.451948,"y of the observed source sentence x0 from the latent target sentence. As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs. In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations constitute an additional pseudo parallel corpus. Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs. In this paper, we propose semi-supervised learning for neural machine translation. Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models. The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder. In the autoencoder, the sou"
P16-1185,W09-0432,0,\N,Missing
P16-1200,P15-1061,0,0.768279,"o philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is t"
P16-1200,P05-1045,0,0.0389364,"of our model. And then we evaluate the effects of our selective attention and show its performance on the data with different set size. Finally, we compare the performance of our method to several state-of-the-art feature-based methods. 4.1 Dataset and Evaluation Metrics We evaluate our model on a widely used dataset1 which is developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012). This dataset was generated by aligning Freebase relations with the New York Times corpus (NYT). Entity mentions are found using the Stanford named entity tagger (Finkel et al., 2005), and are further matched to the names of Freebase entities. The Freebase relations are divided into two parts, one for training and one for testing. It aligns the the sentences from the corpus of the years 2005-2006 and regards them as training instances. And the testing instances are the aligned sentences from 2007. There are 53 possible relationships including a special relation NA which indicates there is no relation between head and tail entities. The training data contains 522,611 sentences, 281,270 entity pairs and 18,252 relational facts. The testing set contains 172,448 sentences, 96,"
P16-1200,P11-1055,0,0.982056,"or example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows:"
P16-1200,P09-1113,0,0.990881,"crosoft, founder, Bill Gates). Although existing KBs contain a ∗ Corresponding uzy@tsinghua.edu.cn). author: Zhiyuan Liu (limassive amount of facts, they are still far from complete compared to the infinite real-world facts. To enrich KBs, many efforts have been invested in automatically finding unknown relational facts. Therefore, relation extraction (RE), the process of generating relational data from plain text, is a crucial task in NLP. Most existing supervised RE systems require a large amount of labelled relation-specific training data, which is very time consuming and labor intensive. (Mintz et al., 2009) proposes distant supervision to automatically generate training data via aligning KBs and texts. They assume that if two entities have a relation in KBs, then all sentences that contain these two entities will express this relation. For example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill"
P16-1200,D12-1110,0,0.068894,"example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its c"
P16-1200,P13-1045,0,0.00975957,"h instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to incorporate the text informatio"
P16-1200,P07-1073,0,0.0221986,"nt supervision inevitably accompanies with the wrong labelling problem. To alleviate the wrong labelling problem, (Riedel et al., 2010) models distant supervision for relation extraction as a multiinstance single-label problem, and (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multiinstance multi-label learning in relation extraction. Multi-instance learning was originally proposed to address the issue of ambiguously-labelled training data when predicting the activity of drugs (Dietterich et al., 1997). Multi-instance learning considers the reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine tran"
P16-1200,D12-1042,0,0.957488,"founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compare"
P16-1200,C14-1008,0,0.0864291,"reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to"
P16-1200,C14-1220,0,0.886404,"e “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding senten"
P16-1200,D15-1203,0,0.676771,"ich cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is the weight given by sentence-level attention, and s indicates the representation of the sentence set. KBs due to the lack of human-annotated training data. Therefore, (Zeng et al., 2015) incorporates multi-instance learning with neural network model, which can build relation extractor based on distant supervision data. Although the method achieves significant improvement in relation extraction, it is still far from satisfactory. The method assumes that at least one sentence that mentions these two entities will express their relation, and only selects the most likely sentence for each entity pair in training and prediction. It’s apparent that the method will lose a large amount of rich information containing in neglected sentences. In this paper, we propose a sentence-level a"
P17-1004,C14-1192,1,0.776459,"Missing"
P17-1004,P15-1061,0,0.0328139,"Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015"
P17-1004,N15-1151,0,0.0855589,"ing English Sentence Representation Chinese 1 x1 n 2 x11 x1 English 1 x2 2 x2 n x22 Chinese Figure 1: Overall architecture of our multi-lingual attention which contains two languages including English and Chinese. The solid lines indicates mono-lingual attention and the dashed lines indicates cross-lingual attention. et al. (2016) further utilize sentence-level attention mechanism to consider all informative sentences jointly. Most existing RE systems are absorbed in extracting relations from mono-lingual data, ignoring massive information lying in texts from multiple languages. In this area, Faruqui and Kumar (2015) present a language independent open domain relation extraction system, and Verga et al. (2015) further employ Universal Schema to combine OpenIE and link-prediction perspective for multi-lingual relation extraction. Both the works focus on multi-lingual transfer learning and learn a predictive model on a new language for existing KBs, by leveraging uniﬁed representation learning for cross-lingual entities. Different from these works, our framework aims to jointly model the texts in multiple languages to enhance relation extraction with distant supervision. To the best of our knowledge, this i"
P17-1004,D15-1205,0,0.0230895,"Missing"
P17-1004,P11-1055,0,0.577715,"ence and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts fr"
P17-1004,P16-1200,1,0.43391,"ain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of human experiences and human cognitive systems. For instance, though New York and United States are expressed as 纽约 and 美国 respectively in Chinese, both Americans and Chinese share the fact th"
P17-1004,P09-1113,0,0.777584,"osong Sun1,2 1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems c"
P17-1004,D12-1110,0,0.0830015,"d on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015) attempt to connect neural networks with d"
P17-1004,D12-1042,0,0.239985,"ate Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In"
P17-1004,D15-1203,0,0.34937,"guage Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of"
P17-1004,C14-1220,0,0.352646,"in various lan2 Related Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. R"
P17-1106,P05-1033,0,0.0251887,"errors. 1 Introduction End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently (Sutskever et al., 2014; Bahdanau et al., 2015). NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems (Wu et al., 2016). However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT. In SMT (Koehn et al., 2003; Chiang, 2005), the translation process can be denoted as a derivation that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules). Defined on language structures with varying granularities, these translation rules are interpretable from a linguistic perspective. In contrast, NMT takes an end-to-end approach: all internal information is represented as real-valued vectors or ∗ Corresponding author. matrices. It is challenging to associate hidden states in neural networks with interpretable language structures. As a result, the lack of interpretability makes it very difficult"
P17-1106,E14-1049,0,0.00569481,"ord. (6) where g(·) is a non-linear function, yj−1 denotes the vector representation of the (j − 1)-th target word. Finally, the word-level translation probability is given by P (yj |x, y&lt;j ; θ) = ρ(yj−1 , sj , cj ), (7) where ρ(·) is a non-linear function. Although NMT proves to deliver state-of-theart translation performance with the capability to handle long-distance dependencies due to GRU and attention, it is hard to interpret the internal → − ← − information such as h i , h i , hi , cj , and sj in the encoder-decoder framework. Though projecting word embedding space into two dimensions (Faruqui and Dyer, 2014) and the attention matrix (Bahdanau et al., 2015) shed partial light on how NMT works, how to interpret the entire network still remains a challenge. Therefore, it is important to develop new methods for understanding the translation process and analyzing translation errors for NMT. 3 3.1 Approach Problem Statement Recent efforts on interpreting and visualizing neural models has focused on calculating the contribution of a unit at the input layer to the final decision at the output layer (Simonyan et al., 2014; Mahendran and Vedaldi, 2015; Nguyen et al., 2015; 1151 in New York 在 纽约 &lt;/s> in &lt;/s"
P17-1106,N03-1017,0,0.0722159,"analyze translation errors. 1 Introduction End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently (Sutskever et al., 2014; Bahdanau et al., 2015). NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems (Wu et al., 2016). However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT. In SMT (Koehn et al., 2003; Chiang, 2005), the translation process can be denoted as a derivation that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules). Defined on language structures with varying granularities, these translation rules are interpretable from a linguistic perspective. In contrast, NMT takes an end-to-end approach: all internal information is represented as real-valued vectors or ∗ Corresponding author. matrices. It is challenging to associate hidden states in neural networks with interpretable language structures. As a result, the lack of interpretability makes it"
P17-1106,N16-1082,0,0.563077,"ty makes it very difficult to understand translation process and debug NMT systems. Therefore, it is important to develop new methods for visualizing and understanding NMT. Existing work on visualizing and interpreting neural models has been extensively investigated in computer vision (Krizhevsky et al., 2012; Mahendran and Vedaldi, 2015; Szegedy et al., 2014; Simonyan et al., 2014; Nguyen et al., 2015; Girshick et al., 2014; Bach et al., 2015). Although visualizing and interpreting neural models for natural language processing has started to attract attention recently (Karpathy et al., 2016; Li et al., 2016), to the best of our knowledge, there is no existing work on visualizing NMT models. Note that the attention mechanism (Bahdanau et al., 2015) is restricted to demonstrate the connection between words in source and target languages and unable to offer more insights in interpreting how target words are generated (see Section 4.5). In this work, we propose to use layer-wise relevance propagation (LRP) (Bach et al., 2015) to visualize and interpret neural machine translation. Originally designed to compute the contributions of single pixels to predictions for image classifiers, LRP back-propagate"
P17-1106,P16-1159,1,0.694161,"Missing"
P17-1106,Q17-1007,1,0.326907,"at “history” repeats four times in the translation. Figure 8 visualizes the target hidden states of the 6-th target word “history”. According to the relevance of the target word embedding Ry6 , the first source word “meiguoren” (american), the second source word “lishi” (history) and the 5-th target word “the” are most relevant to the generation of “history”. Therefore, word repetition not only results from wrong attention but also is significantly influenced by target side context. This finding confirms the importance of controlling source and target contexts to improve fluency and adequacy (Tu et al., 2017). 4.3.2 Word Repetition Given a source sentence “meiguoren lishi shang you jiang chengxi de chuantong , you fancuo rencuo de chuantong” (in history , the people of america have the tradition of honesty and would not hesitate to admit their mistakes), the NMT model produces a wrong translation “in the history of the history of the history of the americans , there is a tradition of faith in the history of mistakes”. The 4.3.3 Unrelated Words Given a source sentence “ci ci huiyi de yi ge zhongyao yiti shi kuadaxiyang guanxi” (one the the top agendas of the meeting is to discuss the cross-atlantic"
P17-1106,P16-1008,1,0.894221,"Missing"
P17-1139,J93-2003,0,0.0876488,"Missing"
P17-1139,P16-1097,1,0.921497,"ge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary p"
P17-1139,P05-1033,0,0.0556812,"onstraint features to facilitate inference. As maximizing F (θ, q) involves minimizing the KL divergence, Ganchev et al. (2010) present a minorization-maximization algorithm akin to EM at sentence level:   E : q (t+1) = argmin KL q(y) P (y|x(n) ; θ (t) ) q h i M : θ (t+1) = argmax Eq(t+1) log P (y|x(n) ; θ) θ However, directly applying posterior regularization to neural machine translation faces a major difficulty: it is hard to specify the hyper-parameter b to effectively bound the expectation of features, which are usually real-valued in translation (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2005). For example, the coverage penalty constraint (Wu et al., 2016) proves to be an essential feature for controlling the length of a translation in NMT. As the value of coverage penalty varies significantly over different sentences, it is difficult to set an appropriate bound for all sentences on the training data. In addition, the minorization-maximization algorithm involves an additional step to find q (t+1) as compared with standard NMT, which increases training time significantly. 3 3.1 = λ1 L(θ) − N   X (n) (n) λ2 KL Q(y|x ; γ) P (y|x ; θ) , (7)   min KL q(y) P (y|x(n) ; θ), (5) Posteri"
P17-1139,C16-1290,0,0.117642,"idden state in neural networks from a linguistic perspective. On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benef"
P17-1139,D07-1091,0,0.0111553,"ds RNN SEARCH by incorporating prior knowledge. For each source sentence, we sample 80 candidate translations to ap˜ distributions. The hyperproximate the P˜ and Q parameter α is set to 0.2. The batch size is 1. The hyper-parameters λ1 and λ2 are set to 8×10−5 and 2.5 × 10−4 . Note that they not only balance the preference between likelihood and posterior regularization, but also control the values of gradients to fall in a reasonable range for optimization. We construct bilingual dictionary and phrase table in an automatic way. First, we run the statistical machine translation system M OSES (Koehn and Hoang, 2007) to obtain probabilistic bilingual dictionary and phrase table. For the bilingual dictionary, we retain entries with probabilities higher than 0.1 in both source-to-target and 1519 Feature BD PT CP LR BD+PT BD+PT+CP BD+PT+CP+LR Rerank w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ MT02 36.06 36.61 34.98 35.07 34.68 34.68 34.60 34.57 35.76 36.30 35.71 36.11 36.06 36.10 MT03 32.99 33.47 32.01 32.11 31.99 31.99 31.89 31.89 33.27 33.83 33.15 33.64 33.01 33.64 MT04 35.62 36.04 34.71 34.73 34.67 34.67 34.79 34.95 35.64 36.02 35.81 36.36 35.86 36.48 MT05 32.59 32.96 31.77 31.84 31.37 31.37 31.72 31"
P17-1139,N03-1017,0,0.18717,"s required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary prior knowledge sources still remains a major challenge. It is difficult to achieve this end by directly modifying model architectures because neural networks usually impose strong independence assumptions between hidden states. As a result, extending a neural model requires th"
P17-1139,D16-1162,0,0.108937,"Bahdanau et al., 2015). We will introduce a number of features used in our experiments as follows. number of authors have proposed to model the fertility (Brown et al., 1993) and converge constraint (Koehn et al., 2003) to improve the adequacy of translation (Tu et al., 2016; Cohn et al., 2016; Feng et al., 2016; Wu et al., 2016; Mi et al., 2016). We follow Wu et al. (2016) to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2 3.2.1 Bilingual Dictionary It is natural to leverage a bilingual dictionary D to improve neural machine translation. Arthur et al. (2016) propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused. In our work, for each entry hx, yi ∈ D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:  1 if x ∈ x ∧ y ∈ y . (9) φBDhx,yi (x, y) = 0 otherwise where ai,j is the attention probability of the j-th target word on the i-th source word. Note that the value of coverage penalty feature varies significantly over sentences of different lengths. Note that number of bilingual dictionary features depends on the vocabu"
P17-1139,D16-1096,0,0.048484,"ed on latent structures such as phrase pairs and synchronous CFG rules, which are not accessible to the decoding process of NMT. Fortunately, we can still leverage internal information in neural models that is linguistically meaningful such as the attention matrix a (Bahdanau et al., 2015). We will introduce a number of features used in our experiments as follows. number of authors have proposed to model the fertility (Brown et al., 1993) and converge constraint (Koehn et al., 2003) to improve the adequacy of translation (Tu et al., 2016; Cohn et al., 2016; Feng et al., 2016; Wu et al., 2016; Mi et al., 2016). We follow Wu et al. (2016) to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2 3.2.1 Bilingual Dictionary It is natural to leverage a bilingual dictionary D to improve neural machine translation. Arthur et al. (2016) propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused. In our work, for each entry hx, yi ∈ D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:  1 if x ∈ x ∧ y ∈ y . (9) φBDhx,yi (x, y) = 0 othe"
P17-1139,P02-1038,0,0.0611375,"training objective is to maximize the logization for incorporating indirect supervision via likelihood of the training set: constraints on posterior distributions of structured n o latent-variable models. The basic idea is to penalθˆMLE = argmax L(θ) , (3) ize the log-likelihood of a neural translation model θ 1515 with the KL divergence between a desired distribution that incorporates prior knowledge and the model posteriors. The posterior regularized likelihood is defined as F (θ, q) chine translation. The major difference is that we represent the desired distribution as a log-linear model (Och and Ney, 2002) rather than a constrained posterior set as described in (Ganchev et al., 2010): J (θ, γ) = λ1 L(θ) − λ2 N X n=1 q∈Q where λ1 and λ2 are hyper-parameters to balance the preference between likelihood and posterior regularization, Q is a set of constrained posteriors: Q = {q(y) : Eq [φ(x, y)] ≤ b}, (6) where φ(x, y) is constraint feature and b is the bound of constraint feature expectations. Ganchev et al. (2010) use constraint features to encode structural bias and define the set of valid distributions with respect to the expectations of constraint features to facilitate inference. As maximizin"
P17-1139,P16-1159,1,0.781961,"table P:  1 if x ˜ ∈ x ∧ y˜ ∈ y φPTh˜x,˜yi (x, y) = . (10) 0 otherwise The number of phrase table features also depends on the vocabulary of the neural translation model. 3.2.3 Coverage Penalty To overcome the over-translation and undertranslation problems widely observed in NMT, a φCP (x, y) = |x| X i=1 3.2.4 log  min |y| X j=1  ai,j , 1.0 , (11) Length Ratio Controlling the length of translations is very important in NMT as neural models tend to generate short translations for long sentences, which deteriorates the translation performance of NMT for long sentences as compared with SMT (Shen et al., 2016). Therefore, we define the length ratio (LR) feature to encourage the length of a translation to fall in a reasonable range:  (β|x|)/|y |if β|x |< |y| φLR (x, y) = , (12) |y|/(β|x|) otherwise where β is a hyper-parameter for penalizing too long or too short translations. For example, to convey the same meaning, an English sentence is usually about 1.2 times longer than a Chinese sentence. As a result, we can set β = 1.2. If the length of a Chinese sentence |x |is 10 and the length of an English sentence |y |is 12, then, φLR (x, y) = 1. If the translation is too long (e.g., |y |= 100), then th"
P17-1139,P16-1008,1,0.865444,"Yang Liu. et al., 2014), it is hard to interpret each hidden state in neural networks from a linguistic perspective. On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objective"
P17-1139,P16-5005,0,0.0348747,"arameters θˆ and γ sion rule for translating an unseen source sentence x is given by n o ˆ . ˆ = argmax P (y|x; θ) y (19) Y(x) The search process can be factorized at the word level: n o ˆ , ˆ <j ; θ) yˆj = argmax P (y|x, y (20) y∈Vy where Vy is the target language vocabulary. Although this decision rule shares the same efficiency and simplicity with standard NMT (Bahdanau et al., 2015), it does not involve prior knowledge in decoding. Previous studies reveal that incorporating prior knowledge in decoding also significantly boosts translation performance (Arthur et al., 2016; He et al., 2016; Wang et al., 2016). As directly incorporating prior knowledge into the decoding process of NMT depends on both model structure and the locality of features, we resort to a coarse-to-fine approach instead to keep the architecture transparency of our approach. Given a source sentence x in the test set, we first ˆ to genuse the neural translation model P (y|x; θ) erate a k-best list of candidate translation C(x). Then, the algorithm decides on the most probable candidate translation using the following decision rule: n o ˆ +γ ˆ · φ(x, y) . (21) ˆ = argmax log P (y|x; θ) y y∈C(x) 4 Experiments 4.1 Setup We evaluate"
P17-1158,P14-1062,0,0.245708,"assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together v"
P17-1158,D14-1181,0,0.00592123,"ng to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE me"
P17-1158,P15-1150,0,0.082723,"aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 201"
P17-1179,D16-1250,0,0.277787,"(1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target language space, its transpos"
P17-1179,W16-1614,0,0.330476,"ingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. Our approach does not make any assumptions and directly matches the mapped source embedding distribution with the target distribution by adversarial training. A recent work also attempts adversarial training for cross-lingual embedding transformation (Barone, 2016). The model architectures are similar to ours, but the reported results are not positive. We tried the publicly available code on our data, but the results were not positive, either. Therefore, we attribute the outcome to the difference in the loss and training techniques, but not the model architectures or data. 5.2 Adversarial Training Generative adversarial networks are originally proposed for generating realistic images as an implicit generative model, but the adversarial training technique for matching distributions is generalizable to much more tasks, including natural language processin"
P17-1179,C16-1171,0,0.142232,"4; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. Our approach does not make any assumptions and directly matches the mapped source embeddi"
P17-1179,D15-1131,0,0.00893287,"ns, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word emb"
P17-1179,D12-1025,0,0.0126253,"e words. We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4. Table 1: Statistics of the non-parallel corpora. Language codes: zh = Chinese, en = English, es = Spanish, it = Italian, ja = Japanese, tr = Turkish. • Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation. We use a publicly available implementation.3 Baselines Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica. An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline. The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language. We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach. Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to li"
P17-1179,D13-1173,0,0.0874179,"Missing"
P17-1179,P15-1081,0,0.0134005,"report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4. Table 1: Statistics of the non-parallel corpora. Language codes: zh = Chinese, en = English, es = Spanish, it = Italian, ja = Japanese, tr = Turkish. • Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation. We use a publicly available implementation.3 Baselines Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica. An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline. The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language. We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach. Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to link different langua"
P17-1179,E14-1049,0,0.413993,"ed interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encou"
P17-1179,W04-3208,0,0.0280075,"ts tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann"
P17-1179,P04-1067,0,0.0800296,"for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇc"
P17-1179,P11-2071,0,0.016233,"Missing"
P17-1179,N15-1157,0,0.0434591,"016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach use"
P17-1179,W13-2233,0,0.0233917,"v et al., 2013a). Although trained independently, the two sets of embeddings exhibit approximate isomorphism. Introduction ∗ horse et al. (2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illustrated in Figure 1. This interesting finding is in line with research on human cognition (Youn et al., 2016). It also means a linear transformation may be established to connect word embedding spaces, allowing word feature transfer. This has far-reaching implication on low-resource scenarios (Daum´e III and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013), because word embeddings only require plain text to train, which is the most abundant form of linguistic resource. However, connecting separate word embedding spaces typically requires supervision from crosslingual signals. For example, Mikolov et al. (2013a) use five thousand seed word translation pairs to train the linear transformation. In a recent study, Vuli´c and Korhonen (2016) show that at least hundreds of seed word translation pairs are needed for the model to generalize. This is unfortunate for low-resource languages and domains, 1959 Proceedings of the 55th Annual Meeting of the A"
P17-1179,W02-0902,0,0.051305,"uage pairs; it often lists tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P"
P17-1179,P16-2080,0,0.0145854,"es in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the"
P17-1179,P14-2037,0,0.0423755,"Missing"
P17-1179,P15-1027,0,0.420011,"[log (1 − D (G (x)))] . Theoretical analysis reveals that adversarial training tries to minimize the Jensen-Shannon divergence JSD py ||pf (x) (Goodfellow et al., 2014). Importantly, the minimization happens at the distribution level, without requiring word 1960 2.2 translation pairs to supervise training. 2.1 Model 1: Unidirectional Transformation The first model directly implements the adversarial game, as shown in Figure 2(a). As hinted by the isomorphism shown in Figure 1, previous works typically choose the mapping function f to be a linear map (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015). We therefore parametrize the generator as a transformation matrix G ∈ Rd×d . We also tried non-linear maps parametrized by neural networks, without success. In fact, if the generator is given sufficient capacity, it can in principle learn a constant mapping function to a target word embedding, which makes the discriminator impossible to distinguish, much like the “mode collapse” problem widely observed in the image domain (Radford et al., 2015; Salimans et al., 2016). We therefore believe it is crucial to grant the generator with suitable capacity. As a generic binary classifier, a standard"
P17-1179,N15-1028,0,0.090077,"lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embedd"
P17-1179,W15-1521,0,0.245132,"Missing"
P17-1179,P99-1067,0,0.436112,"e other language pairs; it often lists tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou e"
P17-1179,P15-2093,1,0.793518,"his setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixe"
P17-1179,N15-1104,0,0.348617,"lue function (1): LD = − log D (y) − log (1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space"
P17-1179,L16-1521,0,0.0110972,"Missing"
P17-1179,P16-1157,0,0.0374091,"Missing"
P17-1179,C16-1300,1,0.877671,"Missing"
P17-1179,P16-1024,0,0.258895,"Missing"
P17-1179,N13-1011,0,0.0392668,"Missing"
P17-1179,P15-2118,0,0.0717992,"Missing"
P17-1179,N16-1156,0,0.306912,"D = − log D (y) − log (1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target lang"
P17-1179,D13-1141,0,0.0617117,"1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel t"
P17-1179,P11-2084,0,0.0121872,"Missing"
P17-1187,N15-1070,0,0.017728,"external knowledge resources such as knowledge bases or dictionaries to suggest possible senses for a word. (Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning. (Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations. Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies. (Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words. In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning. 3 word “apple”. The word “apple” actually has two main senses sho"
P17-1187,D14-1082,0,0.0252926,"ds using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic relations between words. To address these issues, (Rumelhart et al., 1988) proposes the idea of distributed representation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector. Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al., 2015). Word distributed representations are capable of encoding semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks. There are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and lear"
P17-1187,D14-1110,1,0.839273,"Missing"
P17-1187,P12-1092,0,0.0574238,"to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014). There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice balance between effectiveness and efficiency. In word2vec, each word corresponds to one single embedding, ignoring the polysemy of most words. To address this issue, (Huang et al., 2012) introduces a multiprototype model for WRL, conducting unsupervised word sense induction and embeddings according to context clusters. (Chen et al., 2014) further utilizes the synset information in WordNet to instruct word sense representation learning. From these previous studies, we conclude that word sense disambiguation are critical for WRL, and we believe that the sememe annotation of word senses in HowNet can provide necessary semantic regularization for the both tasks. To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning 2049 Proceedings of the 55th"
P17-1187,W04-0834,0,0.0419152,"provides useful semantic regularization for WRL. Moreover, the unified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings. 2.2 Word Sense Disambiguation and Representation Learning Word sense disambiguation (WSD) aims to identify word senses or meanings in a certain context computationally. There are mainly two approaches for WSD, namely the supervised methods and the knowledge-based methods. Supervised methods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation (Lee et al., 2004), which are intensively limited to the time-consuming human annotation of training data. On contrary, knowledge-based methods utilize 2050 large external knowledge resources such as knowledge bases or dictionaries to suggest possible senses for a word. (Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation an"
P17-1187,O02-2003,0,0.0779094,"of concepts (i.e. word sense). However, sememes are not explicit ∗ † indicates equal contribution Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) for each word. Hence, people manually annotate word sememes and build linguistic common-sense knowledge bases. HowNet (Dong and Dong, 2003) is one of such knowledge bases, which annotates each concept in Chinese with one or more relevant sememes. Different from WordNet (Miller, 1995), the philosophy of HowNet emphasizes the significance of part and attribute represented by sememes. HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al., 2013), and in section 3.2 we will give a detailed introduction to sememes, senses and words in HowNet. In this paper, we aim to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014). There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice b"
P17-1187,D14-1162,0,0.120776,"ng semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks. There are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. (Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. (Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations and build distinct vectors for each word sense. (Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. (Rothe and Sch¨utze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset"
P17-1187,D16-1174,0,0.0124902,"dersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning. (Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations. Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies. (Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words. In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning. 3 word “apple”. The word “apple” actually has two main senses shown on the second layer: one is a sort of juicy fruit (apple), and another is a famous computer brand (Apple brand). The third and f"
P17-1187,P15-1173,0,0.0675097,"Missing"
P17-1187,C14-1016,0,0.0195536,"t issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. (Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. (Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations and build distinct vectors for each word sense. (Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. (Rothe and Sch¨utze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space. This paper, for the first time, jointly learns representations of sememes, senses and words. The sememe annotation in HowNet provides useful semantic regularization for WRL. Moreover, the unified representations incor"
P17-1187,D14-1113,0,\N,Missing
P18-1161,D13-1160,0,0.532208,"Missing"
P18-1161,P17-1171,0,0.576465,"ow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open domain QA. In recent years, researchers attempt to answer opendomain questions with a large-scale unlabeled corpus. Chen et al. (2017) propose a distantly supervised open-domain question answering (DS-QA) system which uses information retrieval technique to obtain relevant text from Wikipedia, and then applies reading comprehension technique to extract the answer. Although DS-QA proposes an effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retriev"
P18-1161,E17-2114,0,0.0552077,"Missing"
P18-1161,P16-1046,0,0.00746877,"hes, which can aggregate the results extracted from each paragraph by existing DS-QA system to better determine the answer. However, the method relies on the pre-extracted answers of existing DS-QA models and still suffers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrie"
P18-1161,P17-1020,0,0.26701,"effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retrieved paragraph ‘Dublin is the capital of Ireland. Besides, Dublin is one of the famous tourist cities in Ireland and ...” is not the correct token of the answer. These noisy paragraphs and tokens are regarded as valid instances in DS-QA. To address this issue, Choi et al. (2017) separate the answer generation in DS-QA into two modules including selecting a target paragraph in document and extracting the correct answer from the target paragraph by reading comprehension. Further, Wang et al. (2018a) use reinforcement learning to train target paragraph selection and answer extraction jointly. These methods only extract the answer according to the most related paragraph, which will lose a large amount of rich information contained in 1736 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1736–1745 c Melbourne, Au"
P18-1161,P17-1055,0,0.171124,"noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot b"
P18-1161,P17-1168,0,0.544434,"tor to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension"
P18-1161,N16-1174,0,0.00903545,"erent from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting answer a through a multi-layer long short-term memory network. Overall, the probability Pr(a|q, P ) of extracting answer a g"
P18-1161,P17-1147,0,0.166145,"Missing"
P18-1161,P16-1200,1,0.0376917,"ers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting an"
P18-1161,D14-1162,0,0.103004,"512}, the number of LSTM layers for document and question encoder among {1, 2, 3, 4}, regularization weight α among {0.1, 0.5, 1.0, 2.0} and the batch size among {4, 8, 16, 32, 64, 128}. The optimal parameters are highlighted with bold faces. For other 1740 parameters, since they have little effect on the results, we simply follow the settings used in (Chen et al., 2017). For training, our Our+FULL model is first initialized by pre-training using Our+AVG model, and we set the iteration number over all the training data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe6 (Pennington et al., 2014) word embeddings learned from 840B Web crawl data. 4.4 Effect of Different Paragraph Selectors As our model incorporates different types of neural networks including MLP and RNN as our paragraph selector, we investigate the effect of different paragraph selector on the Quasar-T and SearchQA development set. As shown in Table 3, our RNN paragraph selector leads to statistically significant improvements on both Quasar-T and SearchQA. Note that Our+FULL which uses MLP paragraph selector even performs worse on Quasar-T dataset as compared to Our+AVG. It indicates that MLP paragraph selector is ins"
P18-1161,P17-1018,0,0.124425,"der to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open"
P18-1223,D17-1284,0,0.031617,"query to documents through related entities. Latent Entity Space (LES) builds an unsupervised model using latent entities’ descriptions (Liu and Fang, 2015). EsdRank uses related entities as a latent space, and performs learning to rank with various information retrieval features (Xiong and Callan, 2015). AttR-Duet develops a four-way interaction to involve cross matches between entity and word representations to catch more semantic relevance patterns (Xiong et al., 2017a). 2396 There are many other attempts to integrate knowledge graphs in neural models in related tasks (Miller et al., 2016; Gupta et al., 2017; Ghazvininejad et al., 2018). Our work shares a similar spirit and focuses on exploring the effectiveness of knowledge graph semantics in neuralIR. 3 Entity-Duet Neural Ranking Model ~veemb = Embe (e). This section first describes the standard architecture in current interaction based neural ranking models. Then it presents our Entity-Duet Neural Ranking Model, including the semantic entity representation which integrates the knowledge graph semantics, and then the entity-duet ranking framework. The overall architecture of EDRM is shown in Figure 1. 3.1 Given a query q and a document d, inter"
P18-1223,D17-1110,0,0.0567975,"Missing"
P18-1223,D16-1147,0,0.0160732,"nal connections from query to documents through related entities. Latent Entity Space (LES) builds an unsupervised model using latent entities’ descriptions (Liu and Fang, 2015). EsdRank uses related entities as a latent space, and performs learning to rank with various information retrieval features (Xiong and Callan, 2015). AttR-Duet develops a four-way interaction to involve cross matches between entity and word representations to catch more semantic relevance patterns (Xiong et al., 2017a). 2396 There are many other attempts to integrate knowledge graphs in neural models in related tasks (Miller et al., 2016; Gupta et al., 2017; Ghazvininejad et al., 2018). Our work shares a similar spirit and focuses on exploring the effectiveness of knowledge graph semantics in neuralIR. 3 Entity-Duet Neural Ranking Model ~veemb = Embe (e). This section first describes the standard architecture in current interaction based neural ranking models. Then it presents our Entity-Duet Neural Ranking Model, including the semantic entity representation which integrates the knowledge graph semantics, and then the entity-duet ranking framework. The overall architecture of EDRM is shown in Figure 1. 3.1 Given a query q and"
P18-1223,W03-1730,0,0.204162,"Missing"
P18-1227,Q17-1010,0,0.25932,"formation of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes"
P18-1227,P16-1156,0,0.0575657,"Missing"
P18-1227,P10-4002,0,0.0157426,"s, and fail to handle low-frequency words and outof-vocabulary words. In this paper, we propose to incorporate internal information for lexical sememe prediction. 3.1 Subword and Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The succ"
P18-1227,N15-1184,0,0.120806,"Missing"
P18-1227,P14-1050,0,0.138855,"finite number of sememes. However, the sememe set of a word is not explicit, which is why linguists build knowledge bases (KBs) to annotate words with sememes manually. HowNet is a classical widely-used sememe KB (Dong and Dong, 2006). In HowNet, linguists manually define approximately 2, 000 sememes, and annotate more than 100, 000 common words in Chinese and English with their relevant sememes in hierarchical structures. HowNet is well developed and has a wide range of applications in many NLP tasks, such as word sense disambiguation (Duan et al., 2007), sentiment analysis (Fu et al., 2013; Huang et al., 2014) and cross-lingual word similarity (Xia et al., 2011). Since new words and phrases are emerging every day and the semantic meanings of existing concepts keep changing, it is time-consuming and work-intensive for human experts to annotate new ∗ Work done while doing internship at Tsinghua University. † Equal contribution. Huiming Jin proposed the overall idea, designed the first experiment, conducted both experiments, and wrote the paper; Hao Zhu made suggestions on ensembling, proposed the second experiment, and spent a lot of time on proofreading the paper and making revisions. All authors he"
P18-1227,W17-4110,1,0.843902,"Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods"
P18-1227,P17-1182,0,0.0363986,"Missing"
P18-1227,D14-1095,0,0.0255867,"13) on large-scale text corpus. These methods do not exploit internal information of words, and fail to handle low-frequency words and outof-vocabulary words. In this paper, we propose to incorporate internal information for lexical sememe prediction. 3.1 Subword and Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings"
P18-1227,P17-1187,1,0.477596,"2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representation learning (Niu et al., 2017; Faruqui et al., 2015), and sentiment analysis (Fu et al., 2013) inter alia. Hence, lexical sememe prediction is an important task to construct sememe KBs. Automatic Sememe Prediction. Automatic sememe prediction is proposed by Xie et al. (2017). 2440 For this task, they propose SPWE and SPSE, which are inspired by collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) respectively. SPWE recommends the sememes of those words that are close to the unlabelled word in the embedding space. SPSE learns sememe embeddings by matrix factorization (Koren et al., 20"
P18-1227,D14-1162,0,0.0892683,"hunlp/Character-enhanced-Sememe-Prediction 2439 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2439–2449 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics concepts and maintain consistency for large-scale sememe KBs. To address this issue, Xie et al. (2017) propose an automatic sememe prediction framework to assist linguist annotation. They assumed that words which have similar semantic meanings are likely to share similar sememes. Thus, they propose to represent word meanings as embeddings (Pennington et al., 2014; Mikolov et al., 2013) learned from a large-scale text corpus, and they adopt collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) for sememe prediction, which are concluded as Sememe Prediction with Word Embeddings (SPWE) and Sememe Prediction with Sememe Embeddings (SPSE) respectively. However, those methods ignore the internal information within words (e.g., the characters in Chinese words), which is also significant for word understanding, especially for words which are of lowfrequency or do not appear in the corpus at all. In this paper, we take Chi"
P18-1227,W00-1213,0,0.812936,"7), automatic completion and alignment (Bordes et al., 2013; Toutanova et al., 2015; Zhu et al., 2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representation learning (Niu et al., 2017; Faruqui et al., 2015), and sentiment analysis (Fu et al., 2013) inter alia. Hence, lexical sememe prediction is an important task to construct sememe KBs. Automatic Sememe Prediction. Automatic sememe prediction is proposed by Xie et al. (2017). 2440 For this task, they propose SPWE and SPSE, which are inspired by collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) respectively. SPWE recommends the sememes of those words that are close to the unlabelled word i"
P18-1227,Q15-1026,0,0.0648466,"Missing"
P18-1227,D15-1174,0,0.0268145,"sememe prediction framework considering both external and internal information, and show the effectiveness and robustness of our models on a real-world dataset. 2 Related Work Knowledge Bases. Knowledge Bases (KBs), aiming to organize human knowledge in structural forms, are playing an increasingly important role as infrastructural facilities of artificial intelligence and natural language processing. KBs rely on manual efforts (Bollacker et al., 2008), automatic extraction (Auer et al., 2007), manual evaluation (Suchanek et al., 2007), automatic completion and alignment (Bordes et al., 2013; Toutanova et al., 2015; Zhu et al., 2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representat"
P18-1227,D16-1157,0,0.0211799,"vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes. Following the settings of Xie et al. (2017), we simply ignore th"
P18-1227,D16-1100,0,0.0311036,"em. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes. Following the settings of Xie et al. (2017), we simply ignore the senses and the hi"
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1085,W13-3819,0,0.0691858,"Missing"
P19-1085,W17-5307,0,0.125125,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,D17-1070,0,0.0306419,"guage Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselo"
P19-1085,N19-1423,0,0.49966,"r is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESI"
P19-1085,W18-2501,0,0.0172141,"ose our Graph-based Evidence Aggregating and Reasoning (GEAR) framework in the final claim verification stage. The full pipeline of our method is illustrated in Figure 1. 3.1 Document Retrieval and Sentence Selection In this section, we describe our document retrieval and sentence selection components. Additionally, we add a threshold filter after the sentence selection component to filter out those noisy evidence. In the document retrieval step, we adopt the entity linking approach from Hanselowski et al. (2018). Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim. Then it uses the entities as search queries and finds relevant Wikipedia documents via the online MediaWiki API2 . The seven highest-ranked results for each query are stored to form a candidate article set. Finally, the method drops the articles which are not in the offline Wikipedia dump and filters the articles by the word overlap between their titles and the claim. The sentence selection component selects the most relevant evidence for the claim from all sentences in the retrieved documents. Hanselowski et al. (2018) modify the ESIM 3.2 Claim V"
P19-1085,J84-3009,0,0.693828,"Missing"
P19-1085,W18-5516,0,0.229796,"ounty in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competiti"
P19-1085,W18-5525,0,0.0407684,"tive pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018)"
P19-1085,D14-1059,0,0.0998272,"twork is an American basic cable and satellite television channel. Table 1: Some examples of reasoning over several pieces of evidence together for verification. The italic words are the key information to verify the claim. Both of the claims require to reason and aggregate multiple evidence sentences for verification. More specifically, given a claim, an FV system is asked to label it as “SUPPORTED”, “REFUTED”, or “NOT ENOUGH INFO”, which indicate that the evidence can support, refute, or is not sufficient for the claim. Existing FV methods formulate FV as a natural language inference (NLI) (Angeli and Manning, 2014) task. However, they utilize simple evidence combination methods such as concatenating the evidence or just dealing with each evidence-claim pair. These methods are unable to grasp sufficient relational and logical information among the evidence. In fact, many claims require to simultaneously integrate and reason over several pieces of evidence for verification. As shown in Table 1, for both of the “SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple"
P19-1085,D15-1075,0,0.0499913,"cially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task"
P19-1085,W18-5526,0,0.190196,"Missing"
P19-1085,P17-1171,0,0.224451,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,W18-5517,0,0.0467547,"hat the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis"
P19-1085,E17-1002,0,0.0154565,"m verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et"
P19-1085,W17-5308,0,0.0203664,"intly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et"
P19-1085,D18-1010,0,0.0874661,"dey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results ("
P19-1085,W18-5515,0,0.449838,"ore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these metho"
P19-1085,D16-1244,0,0.131708,"Missing"
P19-1085,N18-1202,0,0.0381169,"nd test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams. Existing methods mainly formulate FV as an NLI task. Thorne et al. (2018a) simply concatenate all evidence together, and then feed the concatenated evidence and the given claim into the NLI model. Luken et al. (2018) adopt the decomposable attention model (DAM) (Parikh et al., 2016) to generate NLI predictions for each claimevidence pair individually and then aggregate all 2.3 Pre-trained Language Models Pre-trained language representation models such as ELMo (Peters et al., 2018) and OpenAI GPT (Radford et al., 2018) are proven to be effective on many NLP tasks. BERT (Devlin et al., 2019) employs bidirectional transformer and welldesigned pre-training tasks to fuse bidirectional context information and obtains the state-of-theart results on the NLI task. In our experiments, we find the fine-tuned BERT model outperforms other NLI-based models on the claim verification subtask of FEVER. Hence, we use BERT as the sentence encoder in our framework to better encoding semantic information of evidence and claims. 1 https://competitions.codalab.org/ competitions/18814 893 Cla"
P19-1085,C16-1270,0,0.0227379,"ins the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competi"
P19-1085,D18-1185,0,0.0219262,"Missing"
P19-1085,N18-1074,0,0.605773,"the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great result"
P19-1085,W18-5501,0,0.12184,"Missing"
P19-1128,P18-2014,0,0.0429986,"en and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·)"
P19-1128,N19-1240,0,0.0430003,"Missing"
P19-1128,P18-1148,0,0.0240849,"15; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. T"
P19-1128,D17-1188,0,0.624311,"y an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·) indicates an embedding function, and θen denotes the parameters of the encoding module of n-th layer. 3.2 Pr"
P19-1128,P16-1200,1,0.891879,"and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat re"
P19-1128,D17-1159,0,0.0382641,"ning as compared to those models which extract relationships separately. Moreover, we also present three datasets, which could help future researchers compare their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the"
P19-1128,P16-1105,0,0.0569637,"that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · ,"
P19-1128,W15-1506,0,0.261678,"bjects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. ("
P19-1128,D15-1203,0,0.647955,"7) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochre"
P19-1128,C14-1220,0,0.816506,"e generic backpropagation and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le"
P19-1128,D17-1186,1,0.883827,"entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pai"
P19-1128,D18-1244,0,0.158096,"e their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the relationship during the learning process. In sharp contrast, this paper focuses on extracting relations from real-world relation datasets. 2.2 Rel"
P19-1128,D14-1162,0,0.0823489,"e n denotes the index of layer 1 , [·] means reshaping a vector as a matrix, BiLSTM encodes a sequence by concatenating tail hidden states of the forward LSTM and head hidden states of the backward LSTM together and MLP denotes a multilayer perceptron with non-linear activation σ. Word Representations We first map each token xt of sentence {x0 , x1 , . . . , xl−1 } to a kdimensional embedding vector xt using a word embedding matrix We ∈ R|V |×dw , where |V |is the size of the vocabulary. Throughout this paper, we stick to 50-dimensional GloVe embeddings pre-trained on a 6-billion-word corpus (Pennington et al., 2014). 1 Adding index to neural models means their parameters are different among layers. Position Embedding In this work, we consider a simple entity marking scheme2 : we mark each token in the sentence as either belonging to the first entity vi , the second entity vj or to neither of those. Each position marker is also mapped to a dp -dimensional vector by a position embedding matrix P ∈ R3×dp . We use notation pi,j t to represent the position embedding for xt corresponding to entity pair (vi , vj ). 4.2 Propagation Module Next, we use Eq. (2) to propagate information among nodes where the initia"
P19-1139,D15-1075,0,0.0415293,"nington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and autho"
P19-1139,D18-1021,1,0.795851,"Missing"
P19-1139,P17-1149,0,0.0267538,"token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in"
P19-1139,P18-1224,0,0.0426992,"low ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this"
P19-1139,P18-1009,0,0.0439758,"to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various n"
P19-1139,N19-1423,0,0.626127,"e Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results"
P19-1139,I05-5002,0,0.0118207,"ovide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE"
P19-1139,D18-1247,1,0.937706,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,D18-1514,1,0.936476,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,P18-1031,0,0.153256,"olume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language represent"
P19-1139,P16-1200,1,0.845415,"from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to"
P19-1139,Q15-1023,0,0.119712,"Missing"
P19-1139,P18-1136,0,0.0322388,"oken Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language represen"
P19-1139,W03-0419,0,0.686759,"Missing"
P19-1139,W16-1313,0,0.0292746,"nd its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work. UFET. For Open Entity, we add a new hybrid model UFET (Choi et al., 2018) for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC. Besides NFGEC and UFET, we also report th"
P19-1139,D13-1170,0,0.0050581,"anguage models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small"
P19-1139,speer-havasi-2012-representing,0,0.0818577,"Missing"
P19-1139,D15-1174,0,0.032581,"token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most"
P19-1139,P18-1076,0,0.0381481,"T). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962"
P19-1139,P10-1040,0,0.0598698,"cific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015)"
P19-1139,D14-1162,0,0.102807,"rresponding author: Z.Liu(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015)"
P19-1139,P17-1161,0,0.0313441,"(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classifica"
P19-1139,N18-1202,0,0.102763,"ture-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. How"
P19-1139,D16-1264,0,0.274955,"e representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly imp"
P19-1139,W18-5446,0,0.172541,") and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For th"
P19-1139,D14-1167,0,0.0268165,"input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m i"
P19-1139,N18-1101,0,0.0178587,"ion than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-"
P19-1139,D17-1187,0,0.0321162,"or the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three base"
P19-1139,D18-1121,1,0.825023,"Missing"
P19-1139,E17-1055,0,0.0423233,"Missing"
P19-1139,K16-1025,0,0.0384676,"concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be align"
P19-1139,P18-2104,0,0.0250317,"er (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token"
P19-1139,D18-1009,0,0.0262803,"including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the"
P19-1139,D15-1203,0,0.231948,"rom Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss. to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention informa"
P19-1139,D18-1244,0,0.0362932,"n classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. 1447 Model MNLI-(m/mm) 392k QQP 363k QNLI 104k SST-2 67k BERTBASE 84.6/83.4 71.2 - 93.5 ERNIE 84.0/83.2 71.2 91.3 93.5 Model CoLA 8.5k STS-B 5.7k MRPC 3.5k RTE 2.5k BERTB"
P19-1139,D17-1004,0,0.0498373,"labels more precisely. In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. Relation Classification Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling lay"
P19-1139,W07-1401,0,\N,Missing
P19-1227,P17-1171,0,0.241113,"guages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA. 1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks"
P19-1227,P17-1020,0,0.170533,"n questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual Ope"
P19-1227,P18-1078,0,0.399956,"aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build"
P19-1227,D18-1269,0,0.190208,"ly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 Proceedings of the 57th Annual Meeting of the Association for Com"
P19-1227,P17-1055,0,0.0333341,"h the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and"
P19-1227,N19-1423,0,0.63948,"nglish questionanswer pairs along with relevant documents. The development and test sets contain a total amount of 17, 358 and 16, 973 question-answer pairs respectively. All questions are naturally produced by native speakers, and potentially reflect cultural differences in different languages. Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in almost all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how diffic"
P19-1227,P17-1168,0,0.0200036,"ine systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Li"
P19-1227,W18-2413,0,0.0219766,"lated as “Fuyang Palace” in the question, while correctly translated in the retrieved document. In addition, as we can see from the underlined parts, highly similar expressions in the question and the retrieved document are translated into largely different ones. Compared to other words or phrases which occur more frequently in the training corpus, name entities are more flexible and various, and thus have worse translation results from prevailing Neural Machine Translation systems (Li et al., 2018). While some work has focused on solving this problem (Hassan et al., 2007; Jiang et al., 2007; Grundkiewicz and Heafield, 2018; Li et al., 2018), it remains largely underresearched. With a translation system that handles name entities better, we can potentially obtain better results from translation-based methods. 6.3 Zero-shot Cross-lingual Method Trained on pure English data without the involvement of machine translation systems, much effort has been saved using zero-shot cross-lingual methods. Moreover, a single model could be applied directly to various languages. Thus, compared to Origin Question: &lt;Query&gt;位于汉长安城外西南侧，与未央宫 之间曾有跨越城墙的复道相连？ Retrieved Text: ...在长安城外修建了建章宫...并且与未 央宫之间有跨越宫墙和城墙的复道相通... Answer: 建章宫 Transla"
P19-1227,C12-1089,0,0.0596235,"ith vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiven"
P19-1227,P18-1160,0,0.019114,"t al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and Wang et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, p"
P19-1227,N19-1380,0,0.035498,"tence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is still no existing benchmark for cross-lingual OpenQA. In addition, another line of research attempts to answer questions in one language using documents in other languages (Magnini et al., 2004; Vallin et al., 2005; Magnini et al., 2006). Different from their setting, we emphasize on building question answering systems for other languages using labeled data from a rich source language such as English, while the documents are in the same language as the questions. 3 Cross-lingual Open-domain Question Answering Existing OpenQA models usually first retrieve documents related t"
P19-1227,W17-2619,0,0.0173788,"l corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is sti"
P19-1227,L18-1560,0,0.0418847,"low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 P"
P19-1227,P18-1161,1,0.910792,"1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to"
P19-1227,W15-1521,0,0.0275638,"et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 201"
P19-1227,2020.amta-research.11,1,0.825841,"Missing"
P19-1278,D15-1082,1,0.727498,"Missing"
P19-1278,N13-1008,0,0.0278094,"on of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so"
P19-1278,S12-1055,0,0.0259867,", researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy"
P19-1278,P17-2050,0,0.0462859,"Missing"
P19-1278,P16-1200,1,0.832915,"er defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms o"
P19-1278,P15-1061,0,0.0277445,"Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, a"
P19-1278,Q16-1017,0,0.018265,"ever, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to remove redundant relations in"
P19-1278,J06-3003,0,0.156138,"Missing"
P19-1278,S12-1070,0,0.033295,"1; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2"
P19-1278,D15-1206,0,0.0592151,"Missing"
P19-1278,D11-1135,0,0.0282773,"et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to"
P19-1278,N07-4013,0,0.0157421,". On the other hand, our model shows a stronger correlation (0.63) with human judgment, indicating that considering the probability over whole entity pair space helps to gain a similarity closer to human judgments. These results provide evidence for our claim raised in §3.2. 6 Wikidata ReVerb Extractions FB15K TACRED Open IE extracts concise token patterns from plain text to represent various relations between entities, e.g.„ (Mark Twain, was born in, Florida). As Open IE is significant for constructing KBs, many effective extractors have been proposed to extract triples, such as Text-Runner (Yates et al., 2007), ReVerb (Fader et al., 2011), and Standford Open IE (Angeli et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted"
P19-1278,D15-1203,0,0.0346458,"relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those dist"
P19-1278,C14-1220,0,0.0489211,"cker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between"
P19-1278,D17-1004,0,0.316785,"11) is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia9. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset. 4.1 4.3 4 Dataset Construction Wikidata In Wikidata (Vrandeˇci´c and Krötzsch, 2014), facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head FB15K and TACRED FB15K (Bordes et al., 2013) is a subset of freebase. TACRED (Zhang et al., 2017) is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. 8Embeddings used in this graph are from a trained TransE model. 2885 9http://reverb.cs.washington.edu/ 5 Triple Set Human Judgments Following Miller and Charles (1991); Resnik (1999) and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata (Vrandeˇci´c and Krötzsch, 2014)10 that are chosen to cover from high to low levels of sim"
P19-1278,N13-1120,0,0.0183395,"ler and Charles, 1991; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtei"
P19-1571,D10-1115,0,0.327112,"2016). In the field of NLP, SC has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of hu"
P19-1571,D12-1050,0,0.364815,"p at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily exte"
P19-1571,W00-1213,0,0.185735,"ole of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) further incorporate Chinese character information"
P19-1571,W13-0112,0,0.0153263,"the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Seme"
P19-1571,D11-1129,0,0.357139,"C has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926)."
P19-1571,D18-1493,1,0.890044,"es of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation learning. We propose two sememe-incor"
P19-1571,P18-1227,1,0.920458,"MWE p, its training loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chin"
P19-1571,D16-1175,0,0.405979,"the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC"
P19-1571,O02-2003,1,0.5746,"eir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) fu"
P19-1571,D11-1014,0,0.0408112,"-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive and elementwise multiplicative models (Mitchell and Lapata, 2008); (2) RAE, the recursive autoencoder model (Socher et al., 2011); (3) RNTN, the recursive neural tensor network (Socher et al., 2013b); (4) TIM, the tensor index model (Zhao et al., 2015); and (5) SCAS-S, the ablated version of our SCAS model which removes sememe knowledge6 . These baseline methods range from the simplest additive model to complicated tensor-based model, all of which take no knowledge into consideration. (11) ˆ p ∈ R|S |, Ws ∈ R|S|×d and σ is the sigwhere y ˆ p , demoid function. [ˆ yp ]i , the i-th element of y notes the predicted score of i-th sememe, where the higher the score is, the more probable the sememe is selected. And Ws = [s1 ,"
P19-1571,D13-1170,0,0.606441,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N13-1134,0,0.0453151,"Missing"
P19-1571,J16-4006,0,0.170424,"hor (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also"
P19-1571,W13-3512,0,0.121261,"Missing"
P19-1571,P11-1015,0,0.0702493,"eme Knowledge Fanchao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositiona"
P19-1571,P08-1028,0,0.646777,"and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe kn"
P19-1571,D11-1016,0,0.0347472,"del can take advantage of sememes. 5 5.1 Related Work Semantic Compositionality Based on the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general lin"
P19-1571,D09-1045,0,0.0970384,"Missing"
P19-1571,P17-1187,1,0.86977,"tion formulae and examples. Bold sememes of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation"
P19-1571,D14-1162,0,0.0942188,") θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive a"
P19-1571,D18-1033,1,0.906717,"ng loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of"
P19-1571,P13-1045,0,0.687064,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N16-1106,0,0.372405,"WE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC for two-word MWEs in this paper because they are the most co"
P19-1571,D12-1110,0,0.539642,"ks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that t"
P19-1623,P18-1094,0,0.217946,"egy is to impose constraints on the decoding process (Wu et al., 2016). Our work differs from prior studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output sentences as contrastive examples. While they focu"
P19-1623,D18-1150,0,0.039177,"r studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output sentences as contrastive examples. While they focus on improving the capability of Seq2Seq model to capture global dependencies, we focus on reducing"
P19-1623,N03-1017,0,0.103674,"ther quantify to what extent our approach reduces word omission errors, we asked human evaluators to manually count word omission errors on the test sets of all the translation tasks. Table 3 shows the error counts. We find that CLone achieves significant error reduction as compared with MLE, MLE + CP, and WordDropout for all the three language pairs. 4 Related Work Our work is related to two lines of research: modeling coverage for NMT and contrastive learning in NLP. 4.1 Modeling Coverage for NMT The notion of coverage dates back to conventional phrase-based statistical machine translation (Koehn et al., 2003). A coverage vector, which is used to indicate whether a source phrase is translated or not during the decoding process, ensures that each source phrase is translated exactly once. As there are no latent variables defined on language structures in neural networks, it is hard to directly introduce coverage into NMT. As a result, there are two strategies. The first strategy is to modify the model architectures to incorporate coverage (Tu et al., 2016; Mi et al., 2016), which requires considerable expertise. The second strategy is to impose constraints on the decoding process (Wu et al., 2016). O"
P19-1623,P14-5010,0,0.00241709,"ds randomly from the ground-truth translations in D; ˜ is constructed via omitting the • CLlow/high : D word with the lowest/highest frequency from each ground-truth translation in D; Figure 1: Visualization of margin differences between CLone and MLE on 500 sampled sentence pairs. We use red to highlight sentence pairs on which CLone achieves a larger margin than MLE. Blue points denote MLE achieves a higher margin. ˜ is constructed via omitting • CLV/IN : D one verb or preposition randomly from the ground-truth translation in D. The part-ofspeech information is given by the Stanford Parser (Manning et al., 2014). 3.2 Comparison of Margins To find out whether CL increases the margin compared with MLE, we calculate the following margin difference for a ground-truth sentence pair ˜ i: hx, yi and an erroneous sentence pair hx, y ∆M = log P (y|x; θˆCL )−log P (˜ y|x; θˆCL ) − log P (y|x; θˆMLE )+log P (˜ y|x; θˆMLE ) (6) Figure 1 shows the margin difference between CLone and MLE on 500 sampled sentence pairs from the training set for the Chinese-to-English task. “Sentence length” denotes the sum of the lengths of the source and target sentences (i.e., |x |+ |y|). Red points denote sentence pairs on which"
P19-1623,D16-1096,0,0.159341,"eline methods. 1 Introduction While neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of languages and can be ap"
P19-1623,P02-1040,0,0.106626,"Missing"
P19-1623,P16-1009,0,0.0378869,"newstest2017 datasets are used as the development set and test set, respectively. For the German-to-English translation task, we use the WMT 2017 dataset as the training set, which consists of 6M preprocessed sentence pairs. The newstest2014 and newstest2017 datasets are used as the development set and test set, respectively. For the Russian-to-English translation task, we use the WMT 2017 preprocessed dataset as the training set, which consists of 25M preprocessed sentence pairs. The newstest2015 and newstest2016 datasets are used as the development set and test set, respectively. Following Sennrich et al. (2016b), we split words into sub-word units. The numbers of merge operations in byte pair encoding (BPE) for both language pairs are set to 32K. After performing BPE, the training set of the Chinese-to-English task contains 550M Chinese sub-word units and 615M English sub-word units, the training set of the German-to-English task consists of 157M German sub-word units and 153M English subword units, and the training set of the Russian-toEnglish task consists of 653M Russian sub-word units and 629M English sub-word units. We used three baselines in our experiments: • MLE: Maximum likelihood estimati"
P19-1623,P16-1008,1,0.929386,"ce than three baseline methods. 1 Introduction While neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of langua"
P19-1623,D13-1140,0,0.0377863,"requires considerable expertise. The second strategy is to impose constraints on the decoding process (Wu et al., 2016). Our work differs from prior studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output se"
P19-1623,N16-1113,0,0.0142537,"e neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of languages and can be applied to arbitrary languages. • Fast"
P19-1623,D16-1137,0,0.115061,"Missing"
P19-1623,P16-1162,0,0.0618026,"newstest2017 datasets are used as the development set and test set, respectively. For the German-to-English translation task, we use the WMT 2017 dataset as the training set, which consists of 6M preprocessed sentence pairs. The newstest2014 and newstest2017 datasets are used as the development set and test set, respectively. For the Russian-to-English translation task, we use the WMT 2017 preprocessed dataset as the training set, which consists of 25M preprocessed sentence pairs. The newstest2015 and newstest2016 datasets are used as the development set and test set, respectively. Following Sennrich et al. (2016b), we split words into sub-word units. The numbers of merge operations in byte pair encoding (BPE) for both language pairs are set to 32K. After performing BPE, the training set of the Chinese-to-English task contains 550M Chinese sub-word units and 615M English sub-word units, the training set of the German-to-English task consists of 157M German sub-word units and 153M English subword units, and the training set of the Russian-toEnglish task consists of 653M Russian sub-word units and 629M English sub-word units. We used three baselines in our experiments: • MLE: Maximum likelihood estimati"
P19-3005,D18-1430,1,0.747332,"Re-Ranking. Our preliminary experiments show that the best candidate may not be ranked as the top 1 because the training objective is Maximum Likelihood Estimation (MLE), which tends to give the generic and meaningless candidates lower costs (Yi et al., 2018a). To automatically select the best candidate, we adopt the automatic rewarders we proposed in (Yi et al., 2018a), including a fluency rewarder, a context coherence rewarder, and a meaningfulness rewarder. Then the candidate with the highest weighted-average rewards given by them will be selected. our proposed style disentanglement model (Yang et al., 2018) to achieve unsupervised style control. This method disentangles the style space into M different sub-spaces by maximizing the mutual information between the style distribution and the generated poetry distribution. It is noteworthy that this method is transparent to model structures which can be applied to any generation model. In this stage, we employ it for the generation of Chinese quatrain poetry (Jueju), which will be extended to more genres in the future. We set the number of styles M = 10. After training, we manually annotate each style with some descriptive phrases, such as sorrow dur"
P19-3005,J09-4006,1,0.714169,"reate a satisfying poem. We detail each module in the following parts. modal input such as keywords, plain text, and even images. For modern concepts in the input, Jiuge utilizes a knowledge graph to map them into relevant keywords in classical Chinese poetry. • Various styles and genres. Unlike previous systems, Jiuge provides more than twenty options of genre and ten options of style, and can generate more diverse poems. 2.2 Keyword Extraction. Jiuge allows multi-modal input to meet the needs of generating poetry according to keywords, tweets or photos. For plain text, we first use THULAC4 (Li and Sun, 2009) to conduct Chinese word segmentation and compute the importance r(w) of each word w: • Human-machine collaboration. Jiuge supports human-machine collaborative and interactive generation. The user can revise the unsatisfied parts of a generated poem. In terms of the revision, Jiuge will dynamically update and re-generate the poem. During this process, Jiuge also offers candidate words and human-authored poetry as references for beginners. 2 2.1 Input Preprocessing Module r(w) = [α∗ti(w)+(1−α)∗tr(w)], (1) where ti(w) and tr(w) are the TF-IDF (Term Frequency-Inverse Document Frequency) value and"
P19-3005,D18-1353,1,0.904193,", 360 entities and 5, 102, 192 relations. 40, 276 of these entities occur in our poetry corpus. Before keywords extension and selection, we first use PKG to map the modern concepts to its most relevant entities in poetry, to guarantee both quality and relevance of generated poems. For a modern concept word wi , we score its each neighbor word wj by: g(wj ) = tfwiki (wj |wi ) · log( Figure 3: The simplified structure of the working memory model, which mainly comprise an encoder, a decoder and there memory components. xi is the i-th line and xi,j is the j-word in the i-th line. Please refer to (Yi et al., 2018b) for more details. ory model (Yi et al., 2018b), which takes at most K preprocessed keywords as input. The encoder maps each word or line into vector representations, and the decoder generates each line wordby-word. The topic memory stores keywords explicitly and independently, which can learn a flexible order and form of keywords expression. The history memory and local memory are dynamically read and written to improve the context coherence of generated poems. Genere Control. Chinese classical poetry involves various genres, and each genre strictly defines the structural and phonological p"
P19-3005,W04-3252,0,0.00729971,"Chinese word segmentation and compute the importance r(w) of each word w: • Human-machine collaboration. Jiuge supports human-machine collaborative and interactive generation. The user can revise the unsatisfied parts of a generated poem. In terms of the revision, Jiuge will dynamically update and re-generate the poem. During this process, Jiuge also offers candidate words and human-authored poetry as references for beginners. 2 2.1 Input Preprocessing Module r(w) = [α∗ti(w)+(1−α)∗tr(w)], (1) where ti(w) and tr(w) are the TF-IDF (Term Frequency-Inverse Document Frequency) value and TextRank (Mihalcea and Tarau, 2004) score calculated with the whole poetry corpus respectively. α is a hyper-parameter to balance the weights of ti(w) and tr(w). Afterwards, we select top K words with the highest scores. For each image, we use the Aliyun image recognition tool5 , which gives the names of five recognized objects with corresponding probability s(w). Then we select top K words with the highest s(w) · r(w). Keyword Mapping. The extracted or recognized keywords could be some modern concepts, such as airplane and refrigerator. Since these words never occur in the classical poetry corpus, the generation module will ta"
P19-3005,P17-1125,0,0.0246014,"Missing"
P19-3005,D14-1074,0,0.0429991,"rofessional assistant, allows constant and active participation of users in poetic creation. 1 Introduction Language is one of the most important forms of human intelligence, among different genres, poetry is a beautiful, poetic and artistic genre which expresses one’s emotions and ideas with relatively fewer words. Across various countries, nationalities, and cultures, poetry is always fascinating, impacting profoundly on the development of human civilization. Recently, researchers have worked on automatic poetry generation. Meanwhile, neural networks have proven to be powerful on this task (Zhang and Lapata, 2014; Wang et al., 2016; Yan, 2016; ∗ † • Multi-modal input. Jiuge can accept multi1 http://www.poeming.com/web/ https://crl.ptopenlab.com:8800/poem/index 3 http://duilian.msra.cn/jueju/ 2 indicates equal contribution Corresponding author: M.Sun(sms@tsinghua.edu.cn) 25 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25–30 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collaborative Revision Module Keywords: plane, blue sky Plain Text: There is a plane in the blue sky. Automatic Refere"
P19-3005,C16-1100,0,0.0886937,"llows constant and active participation of users in poetic creation. 1 Introduction Language is one of the most important forms of human intelligence, among different genres, poetry is a beautiful, poetic and artistic genre which expresses one’s emotions and ideas with relatively fewer words. Across various countries, nationalities, and cultures, poetry is always fascinating, impacting profoundly on the development of human civilization. Recently, researchers have worked on automatic poetry generation. Meanwhile, neural networks have proven to be powerful on this task (Zhang and Lapata, 2014; Wang et al., 2016; Yan, 2016; ∗ † • Multi-modal input. Jiuge can accept multi1 http://www.poeming.com/web/ https://crl.ptopenlab.com:8800/poem/index 3 http://duilian.msra.cn/jueju/ 2 indicates equal contribution Corresponding author: M.Sun(sms@tsinghua.edu.cn) 25 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25–30 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collaborative Revision Module Keywords: plane, blue sky Plain Text: There is a plane in the blue sky. Automatic Reference Recommendation"
P98-2206,C92-1019,0,0.0114873,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,P94-1010,0,0.0356739,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,P97-1041,0,0.0394095,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,A97-1018,1,0.722729,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,J96-3004,0,\N,Missing
S17-1005,I05-1047,0,0.0551181,"Another type treats new word detection as a separate task. This line of methods can be mainly divided into three genres. The first genre is usually preceded by part-of-speech tagging, and treats the new word detection task as a classification problem or directly extracts new words by semantic rules. For example, Argamon et al. (1998) segments the POS sequence of a multi-word into small POS tiles, and then counts tile frequency in both new words and non-new words on training sets, then uses these counts to extract new word. Chen and Ma (2002) uses statistical rules to extract new Chinese word. GuoDong (2005) proposes a discriminative Markov Model to detect new words by chunking one or more separated words. However, these supervised models usually need expert knowledge to design linguistic features and lots of annotated data which are expensive and unavailable in the new arising domains. • We propose a novel Domain TopWords model that can extract both two types of domain-specific words jointly. Experimental results demonstrate the effectiveness of our model. The second genre employs user behavior data to detect new words. User typing behavior in Sogou Chinese Pinyin input method which is the most"
S17-1005,P98-1010,0,0.148661,"opose a flexible domain score function to take the external information into consideration, such as word frequencies in common background corpus. Therefore, the proposed model can extract these two types of words jointly. The main contributions of this paper are summarized as follows: Another type treats new word detection as a separate task. This line of methods can be mainly divided into three genres. The first genre is usually preceded by part-of-speech tagging, and treats the new word detection task as a classification problem or directly extracts new words by semantic rules. For example, Argamon et al. (1998) segments the POS sequence of a multi-word into small POS tiles, and then counts tile frequency in both new words and non-new words on training sets, then uses these counts to extract new word. Chen and Ma (2002) uses statistical rules to extract new Chinese word. GuoDong (2005) proposes a discriminative Markov Model to detect new words by chunking one or more separated words. However, these supervised models usually need expert knowledge to design linguistic features and lots of annotated data which are expensive and unavailable in the new arising domains. • We propose a novel Domain TopWords"
S17-1005,P14-1050,0,0.153605,"Czech data(Pecina, 2005). To measure arbitrary of n-grams, some works separate n-grams into two parts and adopt the existing bi-gram based measurements directly. Some other n-gram based measures are also proposed, such as Enhanced Mutual Information (EMI) Zhang et al. (2009). And Multi-word Expression Distance (MED) was proposed by Bu et al. (2010) which based on the information distance theory. The MED measure was reported superior performance to EMI, SCP and other measures. And a pattern based framework which integrates these statistical features together to detect new words was proposed by Huang et al. (2014). 3 TopWords algorithm based on WDM is introduced in Deng et al. (2016), and is used as an unsupervised Chinese text segmentation and new word discovery method. In English texts, words are split by spacing, but in Chinese, there is no spacing between words in a sentence. For unsegemented Chinese text T , let CT denote the set of all possible segmentations under the dictionary D. Then, under WDM, we have the probability of a Chinese text T : P (T |D, θ) = P (Si |D, θ) (2) Then the likelihood of the parameter θ under the given corpus G is: Methodology L(θ|D, G) = P (G|D, θ) Y = P (Tj |D, θ) Tj ∈"
S17-1005,C10-1014,0,0.0880812,"dels, typical models include conditional random fields proposed by Peng et al. (2004). These supervised models cannot be used in domain-specific words detection directly, due to the lack of annotated domain-specific data. In addition, there are also some unsupervised models, such as TopWords proposed by Deng et al. (2016). However, it needs time-consuming post-processing to extract the second type of domain-specific words. pear constantly. Such methods like Pointwise Mutual Information(Church and Hanks, 1990), Enhanced Mutual Information(Zhang et al., 2009), and Multi-word Expression Distance(Bu et al., 2010). These methods focus on extracting the first type of domain-specific words and conduct postprocessing to discover the second type of words. Deng et al. proposed a statistical model TopWords(Deng et al., 2016) to extract the first type of words, it can imply some of these statistical measures into the model itself. Besides, it designs a feature called relative frequency to extract the second type of domain-specific words. TopWords is based on a Word Dictionary Model(WDM)(Ge et al., 1999; Chang and Su, 1997; Cohen et al., 2007) in which a sentence is sampled from a word dictionary. To extract t"
S17-1005,P05-2003,0,0.0540429,"rd expression extraction. The measurements of multi-word association are crucial in this type of work. Traditional measurements include: Pointwise Mutual Information (PMI) (Church and Hanks, 1990) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Both these two measures are proposed to measure bi-gram association. Among all 84 biRelated work New word detection as a superset of new domainspecific word detection has been investigated for a long time. New word detection methods mainly 45 3.2 gram association measurements, PMI has been reported to be the best in Czech data(Pecina, 2005). To measure arbitrary of n-grams, some works separate n-grams into two parts and adopt the existing bi-gram based measurements directly. Some other n-gram based measures are also proposed, such as Enhanced Mutual Information (EMI) Zhang et al. (2009). And Multi-word Expression Distance (MED) was proposed by Bu et al. (2010) which based on the information distance theory. The MED measure was reported superior performance to EMI, SCP and other measures. And a pattern based framework which integrates these statistical features together to detect new words was proposed by Huang et al. (2014). 3 T"
S17-1005,O97-4005,0,0.461229,"1990), Enhanced Mutual Information(Zhang et al., 2009), and Multi-word Expression Distance(Bu et al., 2010). These methods focus on extracting the first type of domain-specific words and conduct postprocessing to discover the second type of words. Deng et al. proposed a statistical model TopWords(Deng et al., 2016) to extract the first type of words, it can imply some of these statistical measures into the model itself. Besides, it designs a feature called relative frequency to extract the second type of domain-specific words. TopWords is based on a Word Dictionary Model(WDM)(Ge et al., 1999; Chang and Su, 1997; Cohen et al., 2007) in which a sentence is sampled from a word dictionary. To extract the second type of words, it needs to train its model on a common background corpus which is expensive and time-consuming. To address these issues, we propose a Domain TopWords model by assuming that a sentence is sampled from two word dictionaries, one for common words and the other for domain-specific words. Besides, we propose a flexible domain score function to take the external information into consideration, such as word frequencies in common background corpus. Therefore, the proposed model can extrac"
S17-1005,C04-1081,0,0.0143096,"example, if we do not recognize “栈顶”(stack top) as a word, the segmentation “栈 顶 运算符 是 乘号”(the operator at stack top is multiplication sign) will be like “栈 顶运 算符 是 乘号”. In this case, “栈顶” means “stack top” 44 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 44–53, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics contain two directions: the first ones conduct the word segmentation and new word detection jointly. Most of them are supervised models, typical models include conditional random fields proposed by Peng et al. (2004). These supervised models cannot be used in domain-specific words detection directly, due to the lack of annotated domain-specific data. In addition, there are also some unsupervised models, such as TopWords proposed by Deng et al. (2016). However, it needs time-consuming post-processing to extract the second type of domain-specific words. pear constantly. Such methods like Pointwise Mutual Information(Church and Hanks, 1990), Enhanced Mutual Information(Zhang et al., 2009), and Multi-word Expression Distance(Bu et al., 2010). These methods focus on extracting the first type of domain-specific"
S17-1005,C02-1049,0,0.0862666,"intly. The main contributions of this paper are summarized as follows: Another type treats new word detection as a separate task. This line of methods can be mainly divided into three genres. The first genre is usually preceded by part-of-speech tagging, and treats the new word detection task as a classification problem or directly extracts new words by semantic rules. For example, Argamon et al. (1998) segments the POS sequence of a multi-word into small POS tiles, and then counts tile frequency in both new words and non-new words on training sets, then uses these counts to extract new word. Chen and Ma (2002) uses statistical rules to extract new Chinese word. GuoDong (2005) proposes a discriminative Markov Model to detect new words by chunking one or more separated words. However, these supervised models usually need expert knowledge to design linguistic features and lots of annotated data which are expensive and unavailable in the new arising domains. • We propose a novel Domain TopWords model that can extract both two types of domain-specific words jointly. Experimental results demonstrate the effectiveness of our model. The second genre employs user behavior data to detect new words. User typi"
S17-1005,J90-1003,0,0.610238,": the first ones conduct the word segmentation and new word detection jointly. Most of them are supervised models, typical models include conditional random fields proposed by Peng et al. (2004). These supervised models cannot be used in domain-specific words detection directly, due to the lack of annotated domain-specific data. In addition, there are also some unsupervised models, such as TopWords proposed by Deng et al. (2016). However, it needs time-consuming post-processing to extract the second type of domain-specific words. pear constantly. Such methods like Pointwise Mutual Information(Church and Hanks, 1990), Enhanced Mutual Information(Zhang et al., 2009), and Multi-word Expression Distance(Bu et al., 2010). These methods focus on extracting the first type of domain-specific words and conduct postprocessing to discover the second type of words. Deng et al. proposed a statistical model TopWords(Deng et al., 2016) to extract the first type of words, it can imply some of these statistical measures into the model itself. Besides, it designs a feature called relative frequency to extract the second type of domain-specific words. TopWords is based on a Word Dictionary Model(WDM)(Ge et al., 1999; Chang"
sun-etal-2000-hua,P98-2206,1,\N,Missing
sun-etal-2000-hua,C98-2201,1,\N,Missing
sun-etal-2000-hua,hu-yu-2000-multi,0,\N,Missing
sun-etal-2000-hua,A97-1018,1,\N,Missing
W03-1704,C00-2116,0,0.209572,"Missing"
W03-1704,W01-0513,0,0.105131,"Missing"
W09-3426,W09-3426,1,0.0512755,"Missing"
W11-0316,P00-1041,0,0.116716,"g appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summar"
W11-0316,J93-2003,0,0.0857214,"5 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135–144, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics documents and their keyphrases: each document and its keyphrases are descriptions to the same object, but the document is written using one language, while keyphrases are written using another language. Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases. Based on the idea of translation, we use word alignment models (WAM) (Brown et al., 1993) in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Be"
W11-0316,P03-1003,0,0.0261329,"on pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-"
W11-0316,W00-0405,0,0.011384,"forms than other existing methods. Moreover, the second method will improve much if we use more effective measures to identify the most important sentence. Method First Importance Precision 0.290 0.260 Recall 0.410 0.367 F-measure 0.327±0.013 0.293±0.010 Table 2: Precision, recall and F-measure of keyphrase extraction when Md = 2 by extracting one sentence to construct translation pairs. 4.1.3 When Titles/Summaries Are Unavailable Suppose in some special cases, the titles or summaries are unavailable, how can we construct translation pairs? Inspired by extraction-based document summarization (Goldstein et al., 2000; Mihalcea and Tarau, 2004), we can extract one or more important sentences from the given document to construct translation pairs. Unsupervised sentence extraction 141 4.2 Beyond Extraction: Keyphrase Generation In Section 4.1, we evaluate our method on keyphrase extraction by suggesting keyphrases from documents. In fact, our method is also able to suggest keyphrases that have not appeared in the content of given document. The ability is important especially when the length of each document is short, which itself may not contain appropriate keyphrases. We name the new task keyphrase generati"
W11-0316,W03-1028,0,0.629116,"t and the latter as Prt2d . We define Pr⟨D,T ⟩ (t|w) in Eq.(1) as the harmonic mean of the two models: ( )−1 λ) Pr⟨D,T ⟩ (t|w) ∝ Prd2tλ(t|w) + Pr(1− (3) t2d (t|w) where λ is the harmonic factor to combine the two models. When λ = 1.0 or λ = 0.0, it simply uses model Prd2t or Prt2d , correspondingly. Using the translation probabilities Pr(t|w) we can bridge the vocabulary gap between documents and keyphrases. 3.3 Keyphrase Extraction Given a document d, we rank candidate keyphrases by computing their likelihood Pr(p|d). Each candidate keyphrase p may be composed of multiple words. As shown in (Hulth, 2003), most keyphrases are noun phrases. Following (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b), we simply select noun phrases from the given document as candidate keyphrases with the help of POS tags. For each word t, we compute its likelihood given d, Pr(t|d) = ∑w∈d Pr(t|w) Pr(w|d), where Pr(w|d) is the weight of the word w in d, which is measured using normalized TFIDF scores. Pr(t|w) is the translation probabilities obtained from WAM training. Using the scores of all words in candidate keyphrases, we compute the ranking score of each candidate keyphrase by summing up the scores of each word"
W11-0316,J10-4005,0,0.0159254,"ural Language Learning, pages 135–144, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics documents and their keyphrases: each document and its keyphrases are descriptions to the same object, but the document is written using one language, while keyphrases are written using another language. Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases. Based on the idea of translation, we use word alignment models (WAM) (Brown et al., 1993) in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010),"
W11-0316,D09-1027,1,0.724961,"sarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with docume"
W11-0316,D09-1051,0,0.0397125,"sarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with docume"
W11-0316,D10-1036,1,0.737756,"their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and sum"
W11-0316,P10-1085,0,0.171686,"their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and sum"
W11-0316,J03-1002,0,0.0357929,"ticles are manually annotated with keyphrases by website editors, and all these keyphrases come from the corresponding documents. Each news article is also provided with a title and a short summary. In this dataset, there are 72, 900 unique words in documents, and 12, 405 unique words in keyphrases. The average lengths of documents, titles and summaries are 971.7 words, 11.6 words, and 45.8 words, respectively. The average number of keyphrases for each document is 2.4. In experiments, we use the annotated titles and summaries to construct translation pairs. In experiments, we select GIZA++ 3 (Och and Ney, 2003) to train IBM Model-1 using translation pairs. GIZA++, widely used in various applications of statistical machine translation, implements IBM Models 1-5 and an HMM word alignment model. To evaluate methods, we use the annotated keyphrases by www.163.com as the standard keyphrases. If one suggested keyphrase exactly matches one of the standard keyphrases, it is a correct keyphrase. We use precision p = ccorrect /cmethod , recall r = ccorrect /cstandard and Fmeasure f = 2pr/(p + r) for evaluation, where ccorrect is the number of keyphrases correctly suggested by the given method, cmethod is the"
W11-0316,W04-3219,0,0.0225548,"romising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and summaries are usually accompanying with th"
W11-0316,J10-3010,0,0.0338656,"vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve t"
W11-0316,P07-1059,0,0.0105715,"ranslation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases fo"
W11-0316,C08-1093,0,0.0246522,"re able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM train"
W11-0316,C08-1122,0,0.416422,"is alleviates the problem of vocabulary gap to some extent. TextRank, however, still tends to extract high-frequency words as keyphrases because these words have more opportunities to get linked with other words and obtain higher PageRank scores. Moreover, TextRank usually constructs a word graph simply according to word co-occurrences as an approximation of the semantic relations between words. This will introduce much noise because of connecting semantically unrelated words and highly influence extraction performance. Some methods have been proposed to improve TextRank, of which ExpandRank (Wan and Xiao, 2008b; Wan and Xiao, 2008a) uses a small number, namely k, of neighbor documents to provide more information of word relatedness for the construction of word graphs. Compared to TextRank, ExpandRank performs better when facing the vocabulary gap by borrowing the information on document level. However, the finding of neighbor documents are usually arbitrary. This process may introduce much noise and result in topic drift when the document and its so-called neighbor documents are not exactly talking about the same topics. Another potential approach to alleviate vocabulary gap is latent topic models"
W11-0316,C10-1148,0,0.0499411,"solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and summaries are usually accompanying with the corresponding docu"
W11-0316,W04-3252,0,\N,Missing
W12-6308,I05-3019,0,0.0802317,"Missing"
W12-6308,P04-1015,0,0.0808984,"hm used by Huang and Sagae (2010). Beam search is used in the decoding algorithm, while different hypotheses with the same status at a certain step will be merged in a dynamic programming manner. This decoding algorithm can efficiently search exponentially many hypotheses in linear-time (O(nb) where b is the width of the beam). Comparatively, the time complexity of the decoding algorithm using fully dynamic programming is O(n3 ) ( or O(nL2 ) if the max length of words L is specified). The parameter Λ is trained using an average perceptron algorithm (Collins, 2002). We also tried early update (Collins and Roark, 2004) in the learning algorithm. Although it is reported that early update helps the learning of parsers, we do not observe that early update helps the learning of word segmentation models. So we do not implement early update in our experiments. 3 Preprocessing 3.2 Character-Type-Based Features Since there are more non-standard uses of nonChinese characters in micro-blog data than in news data and adding character-type-based features can improve the performance of generalWord Segmentation for Micro-Blog Data 1 Word-based feature templates in Table 1 are also modified slightly for the word segmentat"
W12-6308,I05-3017,0,0.264578,"i is a Arabic numeric character for the word segmentation model for the micro   x otherwise blog data. i Three templates htb-1, a0i i, htb-2, a0i , POSi−1 i (4) and htb-3, a0i , POSi i are added. a0i is the action The additional feature templates that we based on the results of the tagger, and POSi is the use are hct-1, type(xi )i, hct-2, type(xi−1 )i, part-of-speech tag of the word that xi belongs to. hct-3, type(xi+1 )i, hct-4, type(xi−1 ), type(xi )i and hct-5, type(xi ), type(xi+1 )i. 4 3.3 Experiments Lexical Features We report the performances of our model on four SIGHAN05 datasets (Emerson, 2005). Then we report the performance our model on the microblog data. We use 5-fold cross validation for the development and use the whole dataset to train the final model for the test. The F-score is used to evaluate the performance, which is the harmonic mean of precision (percentage of words that are correctly segmented in the results) and recall (percentage of words that are correctly segmented in the gold standard). The results of our model and related work on the SIGHAN05 datasets are listed in Table 3. The results of the micro-blog data are listed in Table 4. The first row is the final perf"
W12-6308,P10-1110,0,0.226138,"d is still far from perfect. The lack of segmented micro-blog data is one of the bottlenecks of our model. If more training data is provided, our model can reach better performance. Introduction Chinese word segmentation is an important and fundamental task for Chinese language processing. General-purpose word segmentation is widely studied. Micro-blog-related topic emergences and becomes a new research topic in recent years. Therefore researchers pay more and more attention to the word segmentation model for Chinese micro-blog data. Motivated by the linear-time incremental parser proposed by Huang and Sagae (2010) and the word-based word segmentation model proposed by Zhang and Clark (2011), first we presented a linear-time incremental word segmentation model. Various features including character-based features and word-based features can be employed while exponentially many segmented results can be tested in linear-time. We report the performances of our model on four datasets in the SIGHAN bake-off 2005. One of the difficulties of training word segmentation model on micro-blog data is the lack of an2 2.1 The Linear-Time Incremental Word Segmentation Model Word Segmentation Definition First, we give a"
W12-6308,P09-1059,0,0.107785,"Missing"
W12-6308,P12-1027,0,0.203774,"Missing"
W12-6308,P11-1139,0,0.0513623,"late hlex-k, lexk (w0 )i is based on a function whose variable is a word. Since we have various lexical resources, we can define several functions lexk to create different lexical feature templates. If the lexical resource is just a word list, the lexk (w0 ) could just return a binary value to indicate whether this word w0 is in the word list or not. If the lexical resource is about the frequencies of words, lexk (w0 ) could return log2 (freq(w0 ) + 1) where freq(w0 ) is the frequency of word w0 . We use several word lists to add lexical feature templates, including a word list of idioms from Sun (2011), word lists based on People’s Daily corpus, Yuwei Corpus and Tsinghua Treebank. We also use words with frequencies counted from the three mentioned segmented corpora. Additionally, we add another lexical feature template based on whether these four characters xui , xui +1 , xui +2 and xui +3 form an idiom. 2 The code we use is a part of the tool THULAC (Tsinghua University - Lexical Analyzer for Chinese) http://nlp. csai.tsinghua.edu.cn/thulac/. 44 All features for test All features for cross validation w/o character-type-based features w/o lexical features w/o tagger-based features F-score A"
W12-6308,I05-3027,0,0.197699,"Missing"
W12-6308,C10-1132,0,0.230187,"rom perfect. The lack of segmented micro-blog data is one of the bottlenecks of our model. If more training data is provided, our model can reach better performance. Introduction Chinese word segmentation is an important and fundamental task for Chinese language processing. General-purpose word segmentation is widely studied. Micro-blog-related topic emergences and becomes a new research topic in recent years. Therefore researchers pay more and more attention to the word segmentation model for Chinese micro-blog data. Motivated by the linear-time incremental parser proposed by Huang and Sagae (2010) and the word-based word segmentation model proposed by Zhang and Clark (2011), first we presented a linear-time incremental word segmentation model. Various features including character-based features and word-based features can be employed while exponentially many segmented results can be tested in linear-time. We report the performances of our model on four datasets in the SIGHAN bake-off 2005. One of the difficulties of training word segmentation model on micro-blog data is the lack of an2 2.1 The Linear-Time Incremental Word Segmentation Model Word Segmentation Definition First, we give a"
W12-6308,J11-1005,0,0.148501,"he bottlenecks of our model. If more training data is provided, our model can reach better performance. Introduction Chinese word segmentation is an important and fundamental task for Chinese language processing. General-purpose word segmentation is widely studied. Micro-blog-related topic emergences and becomes a new research topic in recent years. Therefore researchers pay more and more attention to the word segmentation model for Chinese micro-blog data. Motivated by the linear-time incremental parser proposed by Huang and Sagae (2010) and the word-based word segmentation model proposed by Zhang and Clark (2011), first we presented a linear-time incremental word segmentation model. Various features including character-based features and word-based features can be employed while exponentially many segmented results can be tested in linear-time. We report the performances of our model on four datasets in the SIGHAN bake-off 2005. One of the difficulties of training word segmentation model on micro-blog data is the lack of an2 2.1 The Linear-Time Incremental Word Segmentation Model Word Segmentation Definition First, we give a formal general definition of word segmentation. A raw sentence X is a Chinese"
W12-6308,I11-1094,1,0.806818,"], w0 [−1]i hw-6, w−1 [−1], w0 [−1]i, hw-7, |w−1 |, w0 i, hw-8, w−1 , |w0 |i hw-9, w0 [0], xi i, hw-10, w0 [−1], xi i Table 1: Feature templates of word w, respectively. Each tuple is corresponding to one dimension of the feature vector and the value of that dimension will be set to 1 if this corresponding feature was generated. There are action-based, character-based and word-based templates. Note that when only action-based and character-based templates are used, these feature templates are equivalent to the templates used by conventional word segmentation models based on character tagging (Zhang et al., 2011). And the word-based features are mainly based on the work by Zhang and Clark (2011). 2.4 in the last section by adding a preprocessing and more features. We just perform feature engineering manually for the development to decide which feature is useful for segmenting micro-blog data 1 . 3.1 A rule-based preprocessing is conducted before the statistical model. This preprocessing is mainly used to reduce the search space of the statistical model by assigning the action ai of certain position before the decoding algorithm. Thus the decoding algorithm will only search either hypotheses that ai ="
W12-6308,W06-0127,0,0.0582986,"e 1 are also modified slightly for the word segmentation model for micro-blog data. In order to segment the micro-blog data better, we modified the word segmentation model described 43 Method Best05 (Wang et al., 2010) (Zhang and Clark, 2011) (Sun et al., 2012) Our model AS Dataset CityU MSR PKU 0.952 (Asahara et al., 2005) 0.956 0.954 NA 0.953 0.943 0.964 (Tseng et al., 2005) 0.956 0.972 0.951 0.973 0.948 0.974 0.948 0.973 0.950 (Chen et al., 2005) 0.957 0.944 0.954 0.952 Table 3: F-scores of our model and models in related work on SIGHAN 05 bake-off data 3.4 purpose word segmentation model (Zhao et al., 2006), we employ character-type-based features. We define a function type(xi ) that returns the type of the characters Tagger-Based Features The annotated micro-blog data contains only 500 micro-blogs. So more annotated data are required. We train a character-based joint word segmenta tion and part-of-speech tagging model using the  C if x is a Chinese character  i  People’s Daily corpus (Zhang, 2012)2 , and then  L if x is a Latin letter i use the decoding results of this model as features type(xi ) =  A if xi is a Arabic numeric character for the word segmentation model for the micro"
W12-6308,W02-1001,0,\N,Missing
W12-6308,I05-3018,0,\N,Missing
Y02-1003,A97-1018,1,0.891396,"Missing"
Y02-1003,C92-4199,0,0.0253304,"on in names across different Chinese communities constitutes a critical factor in designing Chinese personal name identification algorithm. Keywords: Chinese personal name identification, Chinese word segmentation, Chinese IT applications, Chinese linguistic differences 1 Introduction Automatic identification of Chinese personal names in unrestricted texts plays a key role in Chinese NLP tasks such as word segmentation. Personal name identification in Chinese involves many different issues that are not found in similar tasks in English. Despite extensive research in the issue in recent years (Wang et al. 1992, Song et al. 1993, Sun et al. 1995), many NLP applications still suffer from the weakness of current available performance. The paper has four sections: Section 1 illustrates some IT applications in which Chinese personal name identification plays important roles, to promote an appreciation of the significance of the research. Section 2 introduces the structure of Chinese personal names, and Section 3 discusses the relevant processing strategies. Lastly, we will highlight the significant differences of Chinese personal names between Beijing and Hong Kong in Section 4, showing the added diffic"
Y09-2003,I05-3017,0,0.0354544,"strings: For two segmented strings S1 and S2 the segmentation distance is defined as the minimum number of boundary insertions and boundary deletions to transform one segmentation way to the other which can be represented as: SD(S1 , S2 ) = min{Σ(Insertion(S1 → S2 ) + Deletion(S1 → S2 ))} Dynamic programming algorithm is used to calculate SD value. 4 Experimental results In experiment part, we firstly determine the number of candidates and rules for lattice construction. Then, performance evaluation for the proposed framework will be performed on five corpora provided by SIGHAN-bakeoff 2005 (Emerson, 2005) and SIGHAN-bakeoff 2006 (Levow, 2006). Statistics of the five datasets4 is listed in Table 3. The performance is evaluated by F1-Measure and OOV recall rate (R OOV). 4.1 Determine the number of candidates for lattice construction If all the 10-best candidates are used to construct lattice, there will be many small segments in Su (see section 3.1) which only contains one character. In order to bring down the complexity of lattice, experiment is done to determine the number of candidates we adopted to construct the 4 Here, PKU05 dataset is not included the representation of personal names is di"
Y09-2003,J05-4005,0,0.028648,"Missing"
Y09-2003,O05-4005,0,0.030241,"Missing"
Y09-2003,P06-1085,0,0.0245868,"Missing"
Y09-2003,I05-3025,0,0.0124172,"the specific implementation of CRF-based word segmenter uses the CRF++ toolkit version 0.53 provided by Taku Kudo1 . Four tags, denoted as S(single-character word), L(the most left character of a word), M(middle character of a word) and R(the most right character of a word), are used to distinguish the position of a character in a word. The window size is set as five to extract features to train the model. This means when we consider current character, the adjacent four characters (the two ahead of it and the two after it) are taken as local features. The basic feature template adopted from (Low et al., 2005) is used, here we restate them to make the paper self-contained: (a) Cn , n = −2, −1, 0, 1, 2 1 http://chasen.org/taku/software/CRF++/ 455 (b) Cn Cn+1 , n = −2, −1, 0, 1 (c) C−1 C1 (d) P u(C0 ) (e) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) Where Cn refers to a Chinese character, here, n indicates the relative distance to current character C0 . For example, C1 indicates the character next to C0 while C− 1 refers to the character previous to C0 . P u(C0 ) represents whether current character is a punctuation. T (Cn ) represents what type the character Cn belongs to. Here, four types are defined as N"
Y09-2003,P97-1041,0,0.0413101,"Missing"
Y09-2003,C04-1081,0,0.0169098,"ch-based segmentation with the CRF segmentation candidates. The rest of the paper is organized as follows. We introduce our specific implementation of linear-chain CRF model based word segmenter in Section 2. In Section 3, we propose the new segmentation framework which combines using search technology and supervised machine learning method. Experimental results are given in Section 4 and in Section 5, we conclude our work. 2 CRF-based Chinese word segmenter Recent studies show that linear-chain structured CRF model(Lafferty et al., 2001), which was first applied to CWS task in the year 2004 (Peng et al., 2004), has been proved to be the most effective one for sequence labeling problem. In this paper, CRF-based word segmenter is selected as our basic word segmenter. In subsection 2.1, we introduce the specific implementation of our CRFbased Chinese word segmenter. Error analysis and performance evaluation is done in subsection 2.2 and 2.3 separately. 2.1 Implementation of CRF-based word segmenter In this paper, the specific implementation of CRF-based word segmenter uses the CRF++ toolkit version 0.53 provided by Taku Kudo1 . Four tags, denoted as S(single-character word), L(the most left character"
Y09-2003,J96-3004,0,0.13408,"Missing"
Y09-2003,P98-2206,1,0.675327,"Missing"
Y09-2003,O03-4002,0,0.0630716,"Missing"
Y09-2003,I08-1002,0,0.0357428,"Missing"
Y09-2003,W06-0115,0,\N,Missing
