2020.coling-main.198,D16-1216,0,0.286197,"identify the different contexts in which the word appears and how they correlate with the categories of the task being studied. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 2181 Proceedings of the 28th International Conference on Computational Linguistics, pages 2181–2190 Barcelona, Spain (Online), December 8-13, 2020 We use politeness as case study of a linguistic phenomenon. Existing computational work on politeness developed feature-based (Danescu-Niculescu-Mizil et al., 2013) and neural (Aubakirova & Bansal, 2016, Niu & Bansal, 2018) models which detect if a natural language request (e.g. Can you please tell me how to do that?) is polite or impolite. Danescu-Niculescu-Mizil et al. (2013) developed a computational tool driven by existing theories in the literature on politeness (Brown & Levinson, 1987). These theories highlighted linguistic constructions that speakers use to reduce the burden on the addressee by sounding indirect (e.g. Could you please [...]). Danescu-Niculescu-Mizil et al. (2013) showed further that, for some words, their position in the request plays a role in whether the request wil"
2020.coling-main.198,P13-1025,0,0.149633,"Our model further discovers novel finer-grained patterns associated with (im)polite language. For example, the word please can occur in impolite contexts that are predictable from BERT clustering. The approach proposed here is validated by showing that features based on fine-grained patterns inferred from the clustering improve over politeness-word baselines. 1 Introduction Linguistic analyses have often been computationally performed around the static notion of words or word categorization methods (e.g. LIWC) where the context of words is not taken into account. Previous work on politeness (Danescu-Niculescu-Mizil et al., 2013) and gender (Bamman et al., 2014) have focused on comparing the use of non-contextual broad word categories such as personal pronouns across different categories/groups. For example, Bamman et al. (2014) found that women use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed"
2020.coling-main.198,Q18-1027,0,0.0179448,"exts in which the word appears and how they correlate with the categories of the task being studied. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 2181 Proceedings of the 28th International Conference on Computational Linguistics, pages 2181–2190 Barcelona, Spain (Online), December 8-13, 2020 We use politeness as case study of a linguistic phenomenon. Existing computational work on politeness developed feature-based (Danescu-Niculescu-Mizil et al., 2013) and neural (Aubakirova & Bansal, 2016, Niu & Bansal, 2018) models which detect if a natural language request (e.g. Can you please tell me how to do that?) is polite or impolite. Danescu-Niculescu-Mizil et al. (2013) developed a computational tool driven by existing theories in the literature on politeness (Brown & Levinson, 1987). These theories highlighted linguistic constructions that speakers use to reduce the burden on the addressee by sounding indirect (e.g. Could you please [...]). Danescu-Niculescu-Mizil et al. (2013) showed further that, for some words, their position in the request plays a role in whether the request will be perceived as pol"
2020.coling-main.198,D14-1162,0,0.0923898,"tegories/groups. For example, Bamman et al. (2014) found that women use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed to static word embeddings which provide the same representation for a word regardless of its context (i.a. Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014), the BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models provide methods for extracting pre-trained contextualized word representations. By leveraging contextualized representations, linguistic theories can be validated and enriched. Given a dataset annotated for a downstream task, we build a model which automatically discovers fine-grained context patterns of words. Discovering such patterns provides insight into the phenomenon the task attempts to model. We use pre-trained BERT embeddings and exploit the fact that words which occur in similar contexts tend to have similar repre"
2020.coling-main.198,N18-1202,0,0.0520276,"use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed to static word embeddings which provide the same representation for a word regardless of its context (i.a. Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014), the BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models provide methods for extracting pre-trained contextualized word representations. By leveraging contextualized representations, linguistic theories can be validated and enriched. Given a dataset annotated for a downstream task, we build a model which automatically discovers fine-grained context patterns of words. Discovering such patterns provides insight into the phenomenon the task attempts to model. We use pre-trained BERT embeddings and exploit the fact that words which occur in similar contexts tend to have similar representations. Our model takes as input contextualized representat"
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2021.naacl-main.390,N18-1171,0,0.0465164,"Missing"
2021.naacl-main.390,2020.scil-1.45,0,0.0431226,"W Heuristic Vanilla BERT Joint BERT 55.00 55.25 65.00 63.71 64.47 39.03 40.54 62.08 63.54 64.28 45.42 45.09 54.17 62.50 62.61 28.37 28.37 50.60 61.93 62.07 0.00 0.00 22.54 59.26 59.77 0.00 0.00 52.94 49.64 47.27 0.00 0.69 64.46 69.09 67.36 62.46 62.17 58.20 61.93 63.21 AAs (ours) 65.15 64.41 65.60* 64.97* 61.07 51.27 70.89 66.49* Table 3: Baselines and AAs overall performance on CB dev and test sets, and F1 scores of each class on the test set (average of 10 runs). * indicates a statistically significant difference (t-test, p ≤ 0.01). identifier improves performance. Essentially, Gantt et al. (2020) used a mixed-effect model to learn a mapping from an item and the associated annotator identifier to a NLI label. However, annotator identifiers are not always accessible, especially in many datasets that have been there for a while. Thus, we decide to simulate the annotation process instead of learning from real identifiers. As shown by Pavlick and Kwiatkowski (2019), if annotations of an item follow unimodal distributions, then it is suitable to use aggregation (i.e., take an average) to obtain a inference label; but such an aggregation is not appropriate when annotations follow multi-modal"
2021.naacl-main.390,D15-1075,0,0.0311844,"a 0/speaker is not certain whether the CC is true or and Sick (2017), annotated labels are subject to false, -3/speaker is certain the CC is false). Follow- uncertainty. Annotations are indeed influenced by ing Jiang and de Marneffe (2019b), we recast CB several factors: workers’ past experience and conby taking the context and target as the premise and centration level, cognition complexities of items, the embedded clause in the target as the hypothesis. etc. They proposed to simulate the annotation proCommon NLI benchmark datasets are SNLI cess in an active learning paradigm to make use of (Bowman et al., 2015) and MultiNLI (Williams the annotations that contribute to uncertainty. Likeet al., 2018), but these datasets have only one an- wise, for NLI, Gantt et al. (2020) observed that dinotation per item in the training set. CB has at rectly training on raw annotations using annotator least 8 annotations per item, which permits to iden1 Compared with the labeling scheme in Jiang and de Marntify items on which annotators disagree. Jiang and effe (2019b), our labeling scheme results in 59 fewer Disagreement items, 48 of which are labeled as Neutral. de Marneffe (2019b) discarded items if less than 2 We"
2021.naacl-main.390,N19-1423,0,0.276662,"6–11, 2021. ©2021 Association for Computational Linguistics Entailment Neutral Contradiction Disagreement Total Train Dev Test 177 23 58 57 9 19 196 22 54 410 66 109 840 120 240 Total 258 85 272 585 1,200 MLP ????? Entailment -biased ????? ????? Neutral -biased ????? Contradiction -biased Table 2: Number of items in each class in train/dev/test. PREMISE [SEP] HYPOTHESIS for NLI: teasing disagreement items, labeled “Disagreement”, from systematic inferences, which can be “Contradiction”, “Neutral” or “Entailment”. To this end, we propose Artificial Annotators (AAs), an ensemble of BERT models (Devlin et al., 2019), which simulate the uncertainty in the annotation process by capturing modes in annotations. That is, we expect to utilize simulated modes of annotations to enhance finer-grained NLI label prediction. Our results, on the CommitmentBank, show that AAs perform statistically significantly better than all baselines (including BERT baselines) by a large margin in terms of both F1 and accuracy. We also show that AAs manage to learn linguistic patterns and context-dependent reasoning. 2 Data: The CommitmentBank Figure 1: Artificial Annotators setup. 3 in Table 1 would thus be “Disagreement”. However"
2021.naacl-main.390,2020.starsem-1.9,0,0.0258755,"Always 0 CBOW Heuristic Vanilla BERT Joint BERT 55.00 55.25 65.00 63.71 64.47 39.03 40.54 62.08 63.54 64.28 45.42 45.09 54.17 62.50 62.61 28.37 28.37 50.60 61.93 62.07 0.00 0.00 22.54 59.26 59.77 0.00 0.00 52.94 49.64 47.27 0.00 0.69 64.46 69.09 67.36 62.46 62.17 58.20 61.93 63.21 AAs (ours) 65.15 64.41 65.60* 64.97* 61.07 51.27 70.89 66.49* Table 3: Baselines and AAs overall performance on CB dev and test sets, and F1 scores of each class on the test set (average of 10 runs). * indicates a statistically significant difference (t-test, p ≤ 0.01). identifier improves performance. Essentially, Gantt et al. (2020) used a mixed-effect model to learn a mapping from an item and the associated annotator identifier to a NLI label. However, annotator identifiers are not always accessible, especially in many datasets that have been there for a while. Thus, we decide to simulate the annotation process instead of learning from real identifiers. As shown by Pavlick and Kwiatkowski (2019), if annotations of an item follow unimodal distributions, then it is suitable to use aggregation (i.e., take an average) to obtain a inference label; but such an aggregation is not appropriate when annotations follow multi-modal"
2021.naacl-main.390,P19-1412,1,0.869708,"Missing"
2021.naacl-main.390,D19-1630,1,0.861947,"Missing"
2021.naacl-main.390,W09-3714,0,0.124808,"Missing"
2021.naacl-main.390,P19-1334,0,0.0415966,"Missing"
2021.naacl-main.390,P16-1204,0,0.038963,"Missing"
2021.naacl-main.390,Q19-1043,0,0.0787678,"). Scores in brackas question answering, semantic textual similarity ets are the raw human annotations. and sentiment analysis. Natural language inference (NLI), an increasingly important benchmark while others view it as a contradiction (-3). A comtask for NLU research, is the task of determining mon practice to generate an inference label from whether a piece of text is entailed, contradicted by annotations is to take the average (i.a., Pavlick and or unrelated to another piece of text (i.a., Dagan Callison-Burch, 2016). In this case, the average et al., 2005; MacCartney and Manning, 2009). Pavlick and Kwiatkowski (2019) observed inher- of the annotations is 0.25 and the gold label for this item would thus be “Neutral”, but such label is ent disagreements among annotators in several NLI not accurately capturing the annotation distribution. datasets, which cannot be smoothed out by hiring Alternatively, some work simply ignores items on more people. They pointed out that to achieve rowhich annotators disagree and only studies sysbust NLU, we need to be able to tease apart systemtematic inference items (Jiang and de Marneffe, atic inferences (i.e., items for which most people 2019a,b; Raffel et al., 2019). agre"
2021.naacl-main.390,D14-1162,0,0.0933346,"Missing"
2021.naacl-main.390,W19-4302,0,0.0284061,"Missing"
2021.naacl-main.390,N18-1101,0,0.0675017,"Missing"
cer-etal-2010-parsing,de-marneffe-etal-2006-generating,1,\N,Missing
cer-etal-2010-parsing,A00-2018,0,\N,Missing
cer-etal-2010-parsing,nivre-etal-2006-maltparser,0,\N,Missing
cer-etal-2010-parsing,W07-1420,0,\N,Missing
cer-etal-2010-parsing,N03-1033,0,\N,Missing
cer-etal-2010-parsing,W09-1419,0,\N,Missing
cer-etal-2010-parsing,W07-1423,0,\N,Missing
cer-etal-2010-parsing,C96-1058,0,\N,Missing
cer-etal-2010-parsing,W09-1402,0,\N,Missing
cer-etal-2010-parsing,W08-1301,1,\N,Missing
cer-etal-2010-parsing,W07-1004,0,\N,Missing
cer-etal-2010-parsing,P03-1054,0,\N,Missing
cer-etal-2010-parsing,H05-1066,0,\N,Missing
cer-etal-2010-parsing,P05-1022,0,\N,Missing
cer-etal-2010-parsing,W07-1417,0,\N,Missing
cer-etal-2010-parsing,P06-1055,0,\N,Missing
cer-etal-2010-parsing,P08-1006,0,\N,Missing
cer-etal-2010-parsing,W03-3017,0,\N,Missing
cer-etal-2010-parsing,W04-3224,0,\N,Missing
cer-etal-2010-parsing,P08-1000,0,\N,Missing
D11-1067,E89-1001,0,0.645467,"e FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) w"
D11-1067,C88-1002,0,0.433647,"hould be an MWE. The FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To"
D11-1067,P05-1038,0,0.343069,"s used in the WSJ—did not imof MWEs occurring in the language. For exam- prove accuracy. Enriched tag sets like that of Crabbé and Canple, Gross (1986) shows that dictionaries contain dito (2008) could also be investigated and compared to our reabout 1,500 single-word adverbs but that French con- sults since Evalb is insensitive to POS tags. 726 labels shown in Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank, and then assigning the most frequent MWE phrasal category for the resulting POS sequence.2 FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs we"
D11-1067,P10-1112,0,0.0175887,"se the collapsed, block Gibbs sampler of Liang et al. (2010). We sample binary variables bs associated with each non-terminal node/site in the treebank. The key idea is to select a block of exchangeable sites S of the same type that do not conflict (Figure 1). Since the sites in S are exchangeable, we can set bS randomly so long as we know m, the number of sites with bs = 1. Because this algorithm is a not a contribution of this paper, we refer the reader to Liang et al. (2010). 6 The Stanford parser is a product model, so the results in §5.1 include the contribution of a dependency parser. 7 Bansal and Klein (2010) also experimented with symbol refinement in an all-fragments (parametric) TSG for English. After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: θc,e = nc,e (z) + αc P0 (e|c) nc,· (z) + αc (5) To make the grammar more robust, we also include all CFG rules in P0 with zero counts in n. Scores for these rules follow from (5) with nc,e (z) = 0. For decoding, we note th"
D11-1067,C92-3126,0,0.0465504,"atic usage. For example, consider the two utterances: (5) a. b. He [ MWV kicked the bucket] . He [ VP kicked [ NP the pail]] . The examples in (5) may be equally probable and receive the same analysis under a PCFG; words are generated independently. However, recall that in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features ar"
D11-1067,W09-3821,0,0.017284,"Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank, and then assigning the most frequent MWE phrasal category for the resulting POS sequence.2 FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs were concatenated into single tokens. For example, nouns followed by adjectives, such as loi agraire ‘land law’ or Union monétaire et économique ‘monetary and economic Union’ were considered syntactically regular. They are MWEs because the choice of adjective is arbitrary (loi agraire and not * loi agricole, similarly to ‘coal black’ but not * ‘crow black’ for exampl"
D11-1067,candito-etal-2010-statistical,0,0.0177664,"Missing"
D11-1067,N10-1029,0,0.208887,"Missing"
D11-1067,N09-1062,0,0.0216909,"t in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits did not lead to any improve4 ment. Parent annotation of POS tags (tagPA) capSimilar models were developed independently by tures information about the external context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0"
D11-1067,P10-4002,0,0.00556612,"nals and non-terminals. We encode this fragment as an SCFG rule of the form [X → γ , X → i, Y1 , . . . , Yn ] (6) where Y1 , . . . , Yn is the sequence of non-terminal nodes in γ.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k, . . . }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically recover the monolingual parse with top-down re-writes of ♦. The SCFG formulation has a practical benefit: we can take advantage of the heavily-optimized SCFG decoders for machine translation. We use cdec (Dyer et al., 2010) to recover the Viterbi derivation under a DP-TSG grammar sample. 5 Experiments 5.1 Standard Parsing Experiments We evaluate parsing accuracy of the Stanford and DP-TSG models (Table 6). For comparison, we also include the Berkeley parser (Petrov et al., 2006).9 For the DP-TSG, we initialized all bs with fair coin tosses and ran for 400 iterations, after which likelihood stopped improving. 8 This formulation is due to Chris Dyer. Training settings: right binarization, no parent annotation, six split-merge cycles, and random initialization. 9 731 Leaf Ancestor Corpus Sent PA-PCFG DP-TSG Stanfor"
D11-1067,C86-1001,0,0.577847,"in à l’insu de ‘to the ignorance of’. Labels We augmented the basic FTB label set— which contains 14 POS tags and 19 phrasal tags—in two ways. First, we added 16 finer-grained POS tags for punctuation.1 Second, we added the 11 MWE The corpus used in our experiments is the French Treebank (Abeillé et al. (2003), version from June 2010, hereafter FTB). In French, there is a linguistic tradition of lexicography which compiles lists 1 Punctuation tag clusters—as used in the WSJ—did not imof MWEs occurring in the language. For exam- prove accuracy. Enriched tag sets like that of Crabbé and Canple, Gross (1986) shows that dictionaries contain dito (2008) could also be investigated and compared to our reabout 1,500 single-word adverbs but that French con- sults since Evalb is insensitive to POS tags. 726 labels shown in Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacke"
D11-1067,D07-1028,0,0.0808741,"Missing"
D11-1067,P03-1054,1,0.0130339,"Missing"
D11-1067,levy-andrew-2006-tregex,0,0.0166486,"ey mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and Genabith, 2007). MFT (Schluter and Genabith, 2007): Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the 2 73 of the unlabeled word types did not appear elsewhere in the treebank. All but 11 of these were nouns. We manually assigned the correct tags, but we would not expect a negative effect by deterministically labeling all of them as nouns. 3 We automate tree manipulation with Tregex/Tsurgeon (Levy and Andrew, 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. 727 Almost all work on the FTB has followed ArunCont and used gold MWE pre-grouping. As a result, most results for French parsing are analogous to early results for Chinese, which used gold word segmentation, and Arabic, which used gold clitic segmentation. Candito et al. (2010) were the first to acknowledge and address this issue, but they still used FTBUC (with some pre-grouped MWEs). Since the syntax and definition of MWEs is a contentious issue, we take a more agnostic view—which is consistent w"
D11-1067,N10-1082,0,0.0870239,"xternal context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ∆nS:m DP concentration parameter for each c ∈ V CFG base distribution Set of non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 → frontier node) Latent state of the segmented treebank Number of sites s ∈ S s.t. bS = 1 Sufficient statistics of z Change in counts by setting m sites in S Table 5: DP-TSG model notation. For consistency, we largely follow the notation of Liang et al. (2010). Note that z = (b, x), and as such z = hc, ei. t ∈ Σ are terminals; e ∈ R are elementary trees;5 ♦ ∈ V is a unique start symbol; and θc,e ∈ θ are parameters for each tree fragment. A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node (denoted by c+ ). All other nodes are internal (denoted by c− ). In the supervised setting, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that we model with Bayes’ rule: p(R |T ) ∝ p(T |R) p(R) (1) Since the tree fragments complet"
D11-1067,P99-1041,0,0.0626924,"Missing"
D11-1067,P06-1055,0,0.0129047,"icting sites of the same def type. Define the type of a site t(z, s) = (∆ns:0 , ∆ns:1 ). Sites (1) and (2) above have the same type since t(z, s1 ) = t(z, s2 ). However, the two sites conflict since the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP. Consequently, sites (1) and (2) are not exchangeable: the probabilities of their assignments depend on the order in which they are sampled. ford parser.6,7 After applying the manual state splits, we perform simple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al., 2006). For each non-terminal type c, we learn a stop probability sc ∼ Beta(1, 1). Under P0 , the probability of generating a rule A+ → B − C + composed of nonterminals is P0 (A+ → B − C + ) = pMLE (A → B C)sB (1−sC ) (3) For lexical insertion rules, we add a penalty proportional to the frequency of the lexical item: P0 (c → t) = pMLE (c → t)p(t) (4) where p(t) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-based In"
D11-1067,P09-2012,0,0.0401802,"c PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits did not lead to any improve4 ment. Parent annotation of POS tags (tagPA) capSimilar models were developed independently by tures information about the external context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ∆nS:m DP concentration parameter for each c ∈ V CFG base distribution Set of non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 → frontier node) Latent state of the segmented treebank Number of sites s ∈ S s.t. bS = 1 Sufficient statistics of z Change in counts by setting m sites in S Table 5: DP-TSG model notation. For consistency, we largely follow the notation of Liang et al. (2010). Note that z = (b, x), and as such z = hc, ei. t ∈ Σ"
D11-1067,ramisch-etal-2010-mwetoolkit,0,0.0869479,"Missing"
D11-1067,seddah-2010-exploring,0,0.0838383,"Missing"
D11-1067,J93-1007,0,0.431093,"the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the r"
D11-1067,E93-1045,0,0.0586717,"fragments (parametric) TSG for English. After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: θc,e = nc,e (z) + αc P0 (e|c) nc,· (z) + αc (5) To make the grammar more robust, we also include all CFG rules in P0 with zero counts in n. Scores for these rules follow from (5) with nc,e (z) = 0. For decoding, we note that the derivations of a TSG are a CFG parse forest (Vijay-Shanker and Weir, 1993). As such, we can use a Synchronous Context Free Grammar (SCFG) to translate the 1-best parse to its derivation. Consider a unique tree fragment ei rooted at X with frontier γ, which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form [X → γ , X → i, Y1 , . . . , Yn ] (6) where Y1 , . . . , Yn is the sequence of non-terminal nodes in γ.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k, . . . }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically"
D13-1169,P10-1018,1,0.641954,"Missing"
D13-1169,P12-1092,0,0.014172,"Missing"
D13-1169,N13-1090,0,0.66395,"to an n-dimensional feature space. One of the advantages put forth for such distributed representations compared to traditional n-gram models is that similar words are likely to have similar vector representations in a continuous space model, whereas the discrete units of an n-gram model do not exhibit any inherent relation with one another. It has been shown that the continuous space representations improve performance in a variety of NLP tasks, such as POS tagging, semantic role labeling, named entity resolution, parsing (Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012). Mikolov et al. (2013) show that there are some syntactic and semantic regularities in the word representations learned, such as the singular/plural relation (the difference of singular and plural word vectors are equivalent: apple − apples ≈ car − cars ≈ family − families) or the gender relation (a masculine noun can be transformed into the feminine form: king − man + woman ≈ queen). We extend Mikolov et al. (2013)’s approach and explore further the interpretation of the vector space. We show that the word vectors learned by NNLMs are meaningful: we can extract scalar relationships between adjectives (e.g., bad &lt;"
D13-1169,P10-1040,0,0.0612086,"Missing"
D13-1169,P10-4014,0,0.0146411,"f good. Because the absolute cosine similarity of goodexcellent to good-bad is higher than to good-evil, we choose bad as the antonym in this case. evil bad excellent good Figure 4: An example of antonym selection. 4.2 Results and discussion Table 3 compares our results with previous ones where adjectival scales are considered: de Marneffe et al. (2010) propose an unsupervised approach where scales are learned from distributional information in a Web corpus; Mohtarami et al. (2011)’s model is similar to ours but uses word representations obtained by LSA and a word sense disambiguation system (Zhong and Ng, 2010) to choose antonyms. To compare with Mohtarami et al. (2011), we use macro-averaged precision and recall for yes and no. For the given metrics, our model significantly outperforms the previous ones (p &lt; 0.05, McNemar’s test). Mohtarami et al. (2011) present higher numbers obtained by replacing the answer words with their synonyms in WordNet. However, that approach fails to capture orderings. Two words of different degree are often regarded as synonyms: even though furious means extremely angry, furious and angry are synonyms in WordNet. Therefore using synonyms, the system will output the same"
D15-1132,C12-1163,0,0.664812,"and the Chinese Discourse (Zhou and Xue, 2015) Treebanks. 1 Introduction Discourse relations between units of text are crucial for the production and understanding of discourse. Different taxonomies of discourse relations have been proposed (i.a. Hobbs (1985), Lascarides and Asher (1993) and Knott and Sanders (1998)). One taxonomy is based on deictic continuity (Segal et al., 1991; Murray, 1997): continuity in the sense of Segal et al. (1991) means that the same frame of reference is maintained, for example by subsequent sentences talking about the same event, without a shift in perspective (Asr and Demberg, 2012). For instance, a causal relation such as I was tired, so I drank a cup of coffee. is continuous, and adversatives show discontinuous relations: I drank a cup of coffee but I was still tired. Other continuous relations include temporal succession, topic succession and so on. The continuity hypothesis predicts that sentences connected by continuous relations are easier to understand than ones connected by discontinuous relations. Previous work on continuity hypothesis (Maury and Teisserenc, 2005; Cain and Nash, 2011; Hoek and Zufferey, 2015) suggests that discourse connectives are indicators of"
D15-1132,W13-2610,0,0.22357,"but, are the marked ones because they indicate harder-to-comprehend content. Asr and Demberg (2012, 2013) extend this idea to discourse relations, proposing that discourse relations which are discontinuous, or posing a conceptual difficulty (Haspelmath, 2006), may be less explicitly conveyed in text, or more explicitly marked by a connective which unambiguously conveys that specific relation than continuous ones. They propose a new measure called markedness to capture this, but when computed on the Penn Discourse Treebank, results do not fit the continuity theory well. This paper improves on Asr and Demberg (2013)’s measure and shows that the results on the Penn Discourse and the Chinese Discourse Treebanks fit the continuity hypothesis very well. 2 Discourse Treebanks Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a corpus of Wall Street Journal articles annotated with discourse relations (Prasad et al., 2008). The discourse relations are organized in a hierarchical structure with three levels: a level 1 (e.g. TEMPORAL), level 1/level 2 (e.g. TEMPORAL .Asynchronous) or level 1/level 2/level 3 (e.g. TEMPORAL.Asynchronous.succession) relation can appear between two clauses within a senten"
D15-1132,W15-0117,0,0.573848,"ectives, whereas if one predicts an explicit relation, then one predicts the next word using a distribution of all the connectives as in M-exp. However surprisal should not be a model of deterministic decision making. It is more likely the case that given CONTEXT, one assigns probabilities to all words given the preceding context, which includes the case where no connective, in other words a zero or null connective, is predicted. The null connective may also be viewed as the probability mass for all the non-connective words predicted by CONTEXT where the connective is predicted to be omitted (Asr and Demberg, 2015). The markedness measure can be analyzed in terms of point-wise mutual information (pmi), indicating the amount of information one relation has for the distribution of the words that follow it (Hume, 2011). Because pmi is proportionate to npmi, we can rewrite (2) as X markedness(r) ∝ p(c|r)pmi(r; c) (4) to get the markedness of a discourse relation: markedness(r) = X c c We also have the mutual information measure: XX I(X; Y ) = p(x, y)pmi(x; y) (5) npmi(r; c) + 1 p(c|r) 2 (2) where r is a relation and c is a discourse connective. Asr and Demberg (2013) propose this measure in the surprisal fr"
D15-1132,W15-0205,0,0.404996,"g about the same event, without a shift in perspective (Asr and Demberg, 2012). For instance, a causal relation such as I was tired, so I drank a cup of coffee. is continuous, and adversatives show discontinuous relations: I drank a cup of coffee but I was still tired. Other continuous relations include temporal succession, topic succession and so on. The continuity hypothesis predicts that sentences connected by continuous relations are easier to understand than ones connected by discontinuous relations. Previous work on continuity hypothesis (Maury and Teisserenc, 2005; Cain and Nash, 2011; Hoek and Zufferey, 2015) suggests that discourse connectives are indicators of the continuity of discourse and help the interlocutors predict the level of continuity of upcoming sentences. Segal et al. (1991) propose that connectives which signal discontinuous discourse relations, such as but, are the marked ones because they indicate harder-to-comprehend content. Asr and Demberg (2012, 2013) extend this idea to discourse relations, proposing that discourse relations which are discontinuous, or posing a conceptual difficulty (Haspelmath, 2006), may be less explicitly conveyed in text, or more explicitly marked by a c"
D15-1132,prasad-etal-2008-penn,0,0.387244,"scontinuous in the hypothesis of continuity (Murray, 1997), with continuous relations expressing normal succession of events in discourse such as temporal, spatial or causal. Asr and Demberg (2013) propose a markedness measure to test the prediction that discontinuous relations may have more unambiguous connectives, but restrict the markedness calculation to relations with explicit connectives only. This paper extends their measure to explicit and implicit relations and shows that results from this extension better fit the continuity hypothesis predictions both for the English Penn Discourse (Prasad et al., 2008) and the Chinese Discourse (Zhou and Xue, 2015) Treebanks. 1 Introduction Discourse relations between units of text are crucial for the production and understanding of discourse. Different taxonomies of discourse relations have been proposed (i.a. Hobbs (1985), Lascarides and Asher (1993) and Knott and Sanders (1998)). One taxonomy is based on deictic continuity (Segal et al., 1991; Murray, 1997): continuity in the sense of Segal et al. (1991) means that the same frame of reference is maintained, for example by subsequent sentences talking about the same event, without a shift in perspective ("
D17-1166,W15-1201,0,0.059731,"Missing"
D17-1166,P11-2102,0,0.113136,"Missing"
D17-1166,N13-1132,0,0.0335281,"nt Leonardo to win at the Oscars!” asserts the author’s desire toward Leonardo winning, but remains agnostic about the likelihood of this outcome, whereas “Leonardo DiCaprio will win the Oscars” is predicting with confidence that the event will happen. Figure 1 shows the annotation interface presented to Turkers. Each HIT contained 10 tweets to be annotated. We gathered annotations for 1, 841 tweets for winners and 1, 702 tweets for losers, giving us a total of 3, 543 tweets. We paid $0.30 per HIT. The total cost for our dataset was $1,000. Each tweet was annotated by 7 Turkers. We used MACE (Hovy et al., 2013) to resolve differences between annotators and produce a single gold label for each tweet. Figures 2a and 2c show heatmaps of the distribution of annotations for the winners for the Oscars in addition to all categories. In both instances, most of the data is annotated with “Definitely Yes” and “Probably Yes” labels for veridicality. Figures 2b and 2d show that the distribution is more diverse for the losers. Such distributions indicate that the veridicality of crowds’ statements could indeed be predictive of outcomes. We provide additional evidence for this hypothesis using automatic veridical"
D17-1166,N10-1038,0,0.0345519,"mes that were not expected according to popular belief (Section 4.5). We also show how TwiVer can be used to measure the number of correct and incorrect predictions made by individual accounts. This provides an intuitive measurement of the reliability of an information source (Section 4.6). 2 Related Work In this section we summarize related work on textdriven forecasting and computational models of veridicality. Text-driven forecasting models (Smith, 2010) predict future response variables using text written in the present: e.g., forecasting films’ box-office revenues using critics’ reviews (Joshi et al., 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017). These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014). In contrast"
D17-1166,D14-1108,0,0.0286733,"o entities: e.g., t1 will win t2 . Distance to keyword. We also compute the distance of TARGET and OPPONENT entities to the keyword. Punctuation. We introduce two binary features for the presence of exclamation marks and question marks in the tweet. We also have features which check whether a tweet ends with an exclamation mark, a question mark or a period. Punctuation, especially question marks, could indicate how certain authors are of their claims. Dependency paths. We retrieve dependency paths between the two TARGET entities and between the TARGET and keyword (win) using the TweeboParser (Kong et al., 2014) after applying rules to normalize paths in the tree (e.g., “doesn’t” → “does not”). Negated keyword. We check whether the keyword is negated (e.g., “not win”, “never win”), using the normalized dependency paths. We randomly divided the annotated tweets into a training set of 2,480 tweets, a development set of 354 tweets and a test set of 709 tweets. MAP parameters were fit using LBFGS-B (Zhu et al., 1997). Table 6 provides examples of high-weight features for positive and negative veridicality. 3.4 Evaluation We evaluated TwiVer’s precision and recall on our held-out test set of 709 tweets. F"
D17-1166,P12-3005,0,0.0382806,"r the Eurovision contest,5 52 for Tennis Grand Slams,6 6 for the Rugby World Cup,7 18 for the Cricket World Cup,8 12 for the Football World Cup,9 76 for the 2016 US presidential elections,10 and 68 queries for the 2014 Indian general elections.11 We added an event prefix (e.g., “Oscars” or the state for presidential primaries), a keyword (“win”), and the relevant date range for the event. For example, “Oscars Leonardo DiCaprio win since:2016-2-22 until:2016-2-28” would be the query generated for the first entry in Table 2. We restricted the data to English tweets only, as tagged by langid.py (Lui and Baldwin, 2012). Jaccard similarity was computed between messages to identify and remove duplicates.12 We removed URLs and preserved only tweets that mention contenders in the text. This automatic postprocessing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table 4 gives the data distribution across event categories. Event 2016 US Presidential primaries Oscars (2009 – 2016) Tennis Grand Slams (2011 – 2016) Ballon d’Or Award (2010 – 2016) Eurovision (2010 – 2016) 2016 US Presidential elections 2014 Indian general elections Rugby World C"
D17-1166,J12-2003,1,0.894104,"Missing"
D17-1166,S13-2053,0,0.0314073,"ries 2016 US presidential elections Indian elections Table 9: F1 scores for each event when training on all events vs. holding out that event from training. |Tt |is the number of tweets of that event category present in the test dataset. where |Tc |is the set of tweets mentioning positive veridicality predictions toward candidate c, and |TO |is the set of all tweets predicting any opponent will win. For each contest, we simply predict as winner the contender whose score is highest. 4.2 Sentiment Baseline We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O’Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al. (2010) use political sentiment to make predictions about outcomes in German elections. We use a re-implementation of (Mohammad et al., 2013)’s system14 to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation"
D17-1166,P15-1159,0,0.0282846,"elated work on textdriven forecasting and computational models of veridicality. Text-driven forecasting models (Smith, 2010) predict future response variables using text written in the present: e.g., forecasting films’ box-office revenues using critics’ reviews (Joshi et al., 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017). These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014). In contrast, our approach does not rely on historical data for training; instead we forecast outcomes of future events by directly extracting users’ explicit predictions from text. Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O’Connor et al., 2010) and m"
D17-1166,P14-2068,0,0.0239631,"for training; instead we forecast outcomes of future events by directly extracting users’ explicit predictions from text. Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O’Connor et al., 2010) and movie revenues (Mishne and Glance, 2006). In this paper, we empirically demonstrate that veridicality can often be more predictive than sentiment (Section 4.1). Also related is prior work on detecting veridicality (de Marneffe et al., 2012; Søgaard et al., 2015) and sarcasm (Gonz´alez-Ib´anez et al., 2011). Soni et al. (2014) investigate how journalists frame quoted content on Twitter using predicates such as think, claim or admit. In contrast, our system TwiVer, focuses on the author’s belief toward a claim and direct predictions of future events as opposed to quoted content. Our approach, which aggregates predictions extracted from user-generated text is related to prior work that leverages explicit, positive veridicality, statements to make inferences about users’ demographics. For example, Coppersmith et al. (2014; 2015) exploit users’ self-reported statements of diagnosis on Twitter. 3 Measuring the Veridical"
D17-1166,D11-1055,0,0.060319,"Missing"
D17-1166,D11-1141,1,0.738866,"els for veridicality into three: positive veridicality (“Definitely Yes” and “Probably Yes”), neutral (“Uncertain about the outcome”) and negative veridicality (“Definitely No” and “Probably No”). We model the conditional distribution over a tweet’s veridicality toward a candidate c winning a contest against a set of opponents, O, using a log-linear model: P (y = v|c, tweet) ∝ exp (θv · f (c, O, tweet)) where v is the veridicality (positive, negative or neutral). To extract features f (c, O, tweet), we first preprocessed tweets retrieved for a specific event to identify named entities, using (Ritter et al., 2011)’s Twitter NER system. Candidate (c) and opponent entities were identified in the tweet as follows: - TARGET (t). A target is a named entity that matches a contender name from our queries. - OPPONENT (O). For every event, along with the current TARGET entity, we also keep track of other contenders for the same event. If a named entity in the tweet matches with one of other contenders, it is labeled as opponent. - ENTITY (e): Any named entity which does not match the list of contenders. Figure 3 illustrates the named entity labeling for a tweet obtained from the query “Oscars Leonardo DiCaprio"
D17-1166,D13-1181,0,\N,Missing
D19-1630,D15-1075,0,0.135431,"Missing"
D19-1630,N19-1423,0,0.104895,"Missing"
D19-1630,N18-2017,0,0.0217684,"remise, has become one of the standard benchmark tasks for natural language understanding. NLI datasets, such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), are typically built by asking annotators to compose sentences based on premises extracted from corpora, so that the composed sentences stand in entailment/contradiction/neutral relationship to the premise. The hypotheses collected this way have been found to contain annotation artifacts – clues allowing systems to identify the relationship between a premise and a hypothesis without observing the premise. For instance, Gururangan et al. (2018) found that in SNLI and MultiNLI, negation is highly indicative of contradiction and generic nouns (e.g., animal, something) of entailment. To address this issue, we recast the CommitmentBank (CB henceforth) (de Marneffe et al., 2019), an English dataset of speaker commitment/event factuality, for NLI.1 The original CommitmentBank includes naturally occurring discourses annotated with speaker commitment towards the content of complements of clauseembedding verbs under entailment-canceling environments (negation, modal, question and conditional). CB does not suffer from the drawback of annotati"
D19-1630,N19-1225,0,0.0157186,"ch subset in parentheses). Table 4: Performance on the CB test set and the MultiNLI dev set. data, the heuristics based on linguistic generalizations is a strong baseline, performing better than MNLIB . We gain a lot of performance with supervision from CB only (CBB ), which aligns with McCoy et al.’s observation that BERT performs very well when trained with in-domain data. The best results are obtained by MNLI+CBB on CB and by MNLIB on MultiNLI, but still lag behind human performance. While MNLI+CBB gives the best performance on CB, it does not perform well on MultiNLI. This is in line with Liu et al. (2019) who found that fine-tuning on datasets that test for a specific linguistic phenomenon decrease the performance on the original dataset. 4 Yes (203) Analysis Figure 1 shows the precision, recall and F1 scores of each class on the CB test set for the three BERT variants and the Heuristics baseline. Heuristics performs similarly as CBB on all classes. Compared with CBB , MNLI+CBB improves the overall performance of contradiction and the recall of neutral. MNLIB identifies contradictions with perfect precision but poor recall. All models do poorly on the neutral class, which has very few items in"
D19-1630,P19-1449,0,0.0117636,"rs; a Heuristics baseline, only applicable to the CB dataset, which uses five rules based on the observations in Section 2.2: 1. items under modals are entailments, 2. neg-raising items of the form I don’t think/know/believe are contradictions, 3. items with factive verbs are entailments, 4. items under negation are contradictions, 5. all other items are neutral. Human Performance We included human performance on CB from Wang et al. (2019a) obtained by asking crowdworkers to re-annotate a part of the test set. The MultiNLI human performance accuracy is for the matched/mismatched test set from Nangia and Bowman (2019). Results Table 4 shows the results. CBOW does not perform well on either datasets. On the CB 6 Superscript B denotes BERT model tuned on the corresponding dataset to distinguish them from the actual datasets. 6088 Figure 1: Precision, recall and F1 of each class on the CB test set for three BERT variants and Heuristics. CB Acc. F1 MultiNLI Acc. F1 Correct label with Heuristics? CBOW MNLIB Heuristics CBB MNLI+CBB 69.2 77.6 81.2 85.2 91.2 47.6 66.7 71.3 81.2 85.3 71.2 83.6 41.1 72.3 71.2 83.6 30.6 74.4 MNLIB MNLI+CBB Human 98.9 95.8 92.0/92.8 - CBB No (47) 88.7 68.9 89.4 55.8 55.7 68.5 Table 5:"
D19-1630,D14-1162,0,0.0830568,"into a softmax layer for a three-way classification. For all experiments, we used the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 1e-5, a batch size of 8, and fine-tuned with at most 10 epochs on each dataset. We finetuned BERT with three different sets of training data: CB only (CBB ), MultiNLI only (MNLIB ), and MultiNLI first then CB (MNLI+CBB ).6 For comparison, we also included the models’ performance on the MultiNLI dev set. Baselines We included two baselines: a bag-ofwords baseline (CBOW) in which each item is represented as the average of its tokens’ GloVe (Pennington et al., 2014) vectors; a Heuristics baseline, only applicable to the CB dataset, which uses five rules based on the observations in Section 2.2: 1. items under modals are entailments, 2. neg-raising items of the form I don’t think/know/believe are contradictions, 3. items with factive verbs are entailments, 4. items under negation are contradictions, 5. all other items are neutral. Human Performance We included human performance on CB from Wang et al. (2019a) obtained by asking crowdworkers to re-annotate a part of the test set. The MultiNLI human performance accuracy is for the matched/mismatched test set"
D19-1630,P19-1452,0,0.0570746,"Missing"
D19-1630,N18-1101,0,0.0379347,"k of Britain for $110 million. Still, analysts said the accord doesn’t suggest Japan is opening up to more foreign takeovers. Hypothesis: Japan is opening up to more foreign takeovers. Contradiction (-1.2) Table 1: Examples from the CommitmentBank, with NLI class and original annotation mean. Introduction Natural language inference (NLI), the task of identifying whether a hypothesis can be inferred from, contradicted by, or not related to a premise, has become one of the standard benchmark tasks for natural language understanding. NLI datasets, such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), are typically built by asking annotators to compose sentences based on premises extracted from corpora, so that the composed sentences stand in entailment/contradiction/neutral relationship to the premise. The hypotheses collected this way have been found to contain annotation artifacts – clues allowing systems to identify the relationship between a premise and a hypothesis without observing the premise. For instance, Gururangan et al. (2018) found that in SNLI and MultiNLI, negation is highly indicative of contradiction and generic nouns (e.g., animal, something) of entailment. To address t"
D19-1630,P19-1334,0,0.0269351,"chmark.com. 6086 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6086–6091, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics inferences involving a particular kind of syntactic construction and contains no annotation artifacts in the hypothesis, making it suitable to test for robust language understanding. CB has many challenging aspects which are highlighted in various adversarial NLI datasets. It can be thought of as a variant of HANS (McCoy et al., 2019), which contains examples where the hypothesis is a subsequence or a constituent of the premise. It contains several phenomena in the “stress tests” (Naik et al., 2018) including word overlap, negation, and length mismatch. However these datasets are artificially constructed while CB data are naturally occurring. Here we evaluate BERT, the state-of-the-art model in NLI, on CB. While BERT models achieve good performance with supervision from both CB and MultiNLI, they still struggle with items involving pragmatic reasoning and lag behind human performance. Experiments show that BERT does not us"
D19-1630,C18-1198,0,0.021551,"ocessing, pages 6086–6091, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics inferences involving a particular kind of syntactic construction and contains no annotation artifacts in the hypothesis, making it suitable to test for robust language understanding. CB has many challenging aspects which are highlighted in various adversarial NLI datasets. It can be thought of as a variant of HANS (McCoy et al., 2019), which contains examples where the hypothesis is a subsequence or a constituent of the premise. It contains several phenomena in the “stress tests” (Naik et al., 2018) including word overlap, negation, and length mismatch. However these datasets are artificially constructed while CB data are naturally occurring. Here we evaluate BERT, the state-of-the-art model in NLI, on CB. While BERT models achieve good performance with supervision from both CB and MultiNLI, they still struggle with items involving pragmatic reasoning and lag behind human performance. Experiments show that BERT does not use the linguistic generalizations for speaker commitment to make predictions, although BERT can learn them with direct supervision. CB is thus a useful benchmark for mea"
de-marneffe-etal-2006-generating,levy-andrew-2006-tregex,0,\N,Missing
de-marneffe-etal-2006-generating,W03-2401,0,\N,Missing
de-marneffe-etal-2006-generating,A00-2018,0,\N,Missing
de-marneffe-etal-2006-generating,N03-1022,0,\N,Missing
de-marneffe-etal-2006-generating,J03-4003,0,\N,Missing
de-marneffe-etal-2006-generating,P03-1054,1,\N,Missing
de-marneffe-etal-2014-universal,de-marneffe-etal-2006-generating,1,\N,Missing
de-marneffe-etal-2014-universal,W09-2307,1,\N,Missing
de-marneffe-etal-2014-universal,J03-4003,0,\N,Missing
de-marneffe-etal-2014-universal,W08-1301,1,\N,Missing
de-marneffe-etal-2014-universal,P03-1054,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1033,1,\N,Missing
de-marneffe-etal-2014-universal,W13-3721,1,\N,Missing
de-marneffe-etal-2014-universal,P08-1109,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1055,0,\N,Missing
de-marneffe-etal-2014-universal,C12-1147,0,\N,Missing
de-marneffe-etal-2014-universal,petrov-etal-2012-universal,0,\N,Missing
de-marneffe-etal-2014-universal,J05-1004,0,\N,Missing
de-marneffe-etal-2014-universal,P05-1013,1,\N,Missing
de-marneffe-etal-2014-universal,N13-1070,0,\N,Missing
de-marneffe-etal-2014-universal,W13-2308,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2103,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2017,1,\N,Missing
J12-2003,J96-1002,0,0.0251094,"Missing"
J12-2003,de-marneffe-etal-2006-generating,1,0.142503,"Missing"
J12-2003,W09-3012,0,0.274288,"Missing"
J12-2003,W09-1401,0,0.0233968,"e new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems work at roughly the c"
J12-2003,P03-1054,1,0.0286283,"on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t know if X. The classes have names like ANNOUNCE, CONFIRM, CONJECTURE, and SAY. Like Saur´ı, we used dependency graphs produced by the Stanford parser (Klein and Manning 2003; de Marneffe, MacCartney, and Manning 2006) to follow the path from the target event to the root of the sentence. If a predicate in the path was contained in one of the classes and the grammatical relation matched, we added both the lemma of the predicate as a feature and a feature marking the predicate class. 6 The maximum entropy formulation differs from the standard multi-class logistic regression model by having a parameter value for each class giving logit terms for how a feature’s value affects the outcome probability relative to a zero feature, whereas in the standard multi-class logis"
J12-2003,N03-5008,1,0.65703,"o give the log-likelihood for the exact distribution from the Turkers (which thus gives an upper-bound) as well as a log-likelihood for a baseline model which uses only the overall distribution of classes in the training data. A maximum entropy classiﬁer is an instance of a generalized linear model with a logit link function. It is almost exactly equivalent to the standard multi-class (also called polytomous or multinomial) logistic regression model from statistics, and readers more familiar with this presentation can think of it as such. In all our experiments, we use the Stanford Classiﬁer (Manning and Klein 2003) with a Gaussian prior (also known as L2 regularization) set to N (0, 1).6 4.1 Features The features were selected through 10-fold cross-validation on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t kno"
J12-2003,W09-1105,0,0.0316776,"ghlights the sort of vagueness and ambiguity that can affect veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the v"
J12-2003,C10-2117,0,0.178904,"Missing"
J12-2003,N07-2036,0,0.011388,"ilding 460, Stanford CA 94305, USA. E-mail: cgpotts@stanford.edu. Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication: 30 November 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 call this event veridicality, building on logical, linguistic, and computational insights about the relationship between language and reader commitment (Montague 1969; Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides 2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur´ı 2008). The central goal of this article is to begin to identify the linguistic and contextual factors that shape readers’ veridicality judgments.1 There is a long tradition of tracing veridicality to ﬁxed properties of lexical items (Kiparsky and Kiparsky 1970; Karttunen 1973). On this view, a lexical item L is veridical if the meaning of L applied to argument p entails the truth of p. For example, because both true and false things can be believed, one should not infer directly from A believes that S that S is true, making believe non-veridical. Conversely, realize appears to be veri"
J12-2003,D08-1027,0,0.120747,"Missing"
J12-2003,W08-0606,0,0.0142166,"ct veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems wor"
J12-2003,W05-1206,0,0.155297,"Missing"
J12-2003,W10-3001,0,\N,Missing
J12-2003,W07-1401,0,\N,Missing
J12-2003,W10-3100,0,\N,Missing
J13-1009,C88-1002,0,0.722196,"Missing"
J13-1009,E89-1001,0,0.607443,"cal lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental results. Siham Boulaknadel and Aboutajdine (2008) evaluated several lexical association measures in isolation for MWE identification in newswire. More recently, Attia et al. (2010b) compared crosslingual projection methods (using Wikipedia and English Wordnet) with standard n-gram classification methods. French Statistical Constituency Parsing. Abeillé (1988) and Abeillé and Schabes (1989) identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP. They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items 219 Computational Linguistics Volume 39, Number 1 that included MWEs. Recent statistical parsing work on French has included stochastic tree insertion grammars (STIG), which are related to TAGs, but with a restricted adjunction operation.23 Both Seddah, Candito, and Crabbé (2009) and Seddah (2010) showed that STIGs underperform CFG-based pa"
J13-1009,P05-1038,0,0.651979,"(Scha 1990, page 13). Most DOP work, however, has focused on parameter estimation issues with a view to improving overall parsing performance rather than explicit modeling of idioms. Given developments in linguistics, and to a lesser degree DOP, in modeling MWEs, it is curious that most NLP work on MWE identification has not utilized syntax. Moreover, the words-with-spaces idea, which Sag et al. (2002) dismissed as unattractive on both theoretical and computational grounds,22 has continued to appear in NLP evaluations such as dependency parsing (Nivre and Nilsson 2004), constituency parsing (Arun and Keller 2005), and shallow parsing (Korkontzelos and Manandhar 2010). In all cases, the conclusion was drawn that pre-grouping MWEs improves task accuracy. Because the yields (and thus the labelings) of the evaluation sentences were modified, however, the experiments were not strictly comparable. Moreover, gold pre-grouping was usually assumed, as was the case in most FTB parsing evaluations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic p"
J13-1009,W10-1408,0,0.0530816,"Missing"
J13-1009,P10-1112,0,0.018634,"f t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-Based Inference Algorithm. To learn the parameters θ we use the collapsed, block Gibbs sampler of Liang, Jordan, and Klein (2010). We sample binary variables bs associated with each sampling site s in the treebank. The key idea is to select a block 6 The Stanford parser is a product model which scores parses with both a dependency grammar and a PCFG. We extract the TSG from the manually split PCFG only. Bansal and Klein (2010) also experimented with manual grammar features in an all-fragments (parametric) TSG for English. 206 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions NP+ PUNC− (1) N+ N− PUNC+ (2) “ Jacques Chirac “ Figure 2 Example of two conflicting sites of the same type in a training tree. Define the type of a def site t(z, s) = (Δns:0 , Δns:1 ). Sites (1) and (2) have the same type because t(z, s1 ) = t(z, s2 ). The two sites conflict, however, because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP. Consequently, sites"
J13-1009,J04-4004,0,0.0265214,"cy arcs or bracketed spans), thus the results are not strictly comparable. From an application perspective, pre-grouping assumes high accuracy identification, which may not be available for all languages. Our goal differs considerably from these two studies, which attempt to improve parsing via MWE information. In contrast, we tune statistical parsers for MWE identification. 8.3 Related Experiments on Arabic and French Arabic Statistical Constituency Parsing. Kulick, Gabbard, and Marcus (2006) were the first to parse the sections of the ATB used in this article. They adapted the Bikel parser (Bikel 2004) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set. The ATB was subsequently revised (Maamouri, Bies, and Kulick 2008), and Maamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for"
J13-1009,N03-2002,0,0.0134091,"ntence), or consists entirely of capital letters  If none of the above, deterministically extract one- and two-character suffixes 203 Computational Linguistics Volume 39, Number 1 3.4 Factored Lexicon with Morphological Features We will apply our models to Arabic and French, yet we have not dealt with the lexical sparsity induced by rich morphology (see Table 5 for a comparison to English). One way to combat sparsity is to parse a factored representation of the terminals, where factors might be the word form, the lemma, or grammatical features such as gender, number, and person (φ features) (Bilmes and Kirchoff 2003; Koehn and Hoang 2007, inter alia). The basic parser lexicon estimates the generative probability of a word given a tag p(w|t) from word/tag pairs observed in the training set. Additionally, the lexicon includes parameter estimates p(t|s) for unknown word signatures s produced by the unknown word models (see Section 3.3). At parsing time, the lexicon scores each input word type w according to its observed count in the training set c(w). We define the unsmoothed and smoothed parameter estimates: p(t|w) = psmooth (t|w) = c(t, w) c(w) c(t, w) + αp(t|s) c(w) + α (1) (2) We then compute the desire"
J13-1009,W06-1620,0,0.159429,"of Computer Science and Linguistics. E-mail: manning@stanford.edu. Submission received: October 1, 2011; revised submission received: June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variant"
J13-1009,C92-3126,0,0.262763,"is of the transitive verb kicked and its two arguments. TSGs are weakly equivalent to CFGs, but with greater strong generative capacity (Joshi and Schabes 1997). TSGs can store lexicalized tree fragments as rules. Consequently, if we have seen [MWV kicked the bucket] several times before, we can store that whole lexicalized fragment in the grammar. We consider the non-parametric probabilistic TSG (PTSG) model of Cohn, Goldwater, and Blunsom (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a data-oriented parsing (DOP) model (Scha 1990; Bod 1992) with Bayesian parameter estimation. A PTSG is a 5-tuple V, Σ, R, ♦, θ where c ∈ V are non-terminals; t ∈ Σ are terminals; e ∈ R are elementary trees;5 ♦ ∈ V is a unique start symbol; and θc,e ∈ θ are parameters for each tree fragment. A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node (denoted by c+ ). All other nodes are internal (denoted by c− ). In the supervised setting, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that is modeled with Bayes’ rule: p("
J13-1009,W09-3821,0,0.0245326,"(2) E XP, in which MWEs were marked with a flat structure. For both representations, they also gave results in which coordinated phrase structures were flattened. In the published experiments, they mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and van Genabith 2007). MFT. (Schluter and van Genabith 2007) Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the correction of annotation errors. Like A RUN -C ONT, MFT contains concatenated MWEs. FTB-UC. (Candito and Crabbé 2009) A version of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, whereas all other MWEs were grouped. For example, nouns followed by adjectives, such as loi agraire (‘land law’) or Union monétaire et économique (‘monetary and economic Union’) were considered syntactically regular. They are MWEs because the choice of adjective is arbitrary (loi agraire and not ∗ loi agricole, similarly to (‘coal black’) but not (∗ ‘crow black’), for example), but their syntact"
J13-1009,candito-etal-2010-statistical,0,0.0732072,"Missing"
J13-1009,W10-1409,0,0.0379731,"Missing"
J13-1009,N10-1029,0,0.0192466,"October 1, 2011; revised submission received: June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variants of this technique. Broadly speaking, n-gram classification methods measure word co-o"
J13-1009,N09-1062,0,0.0399445,"Missing"
J13-1009,P12-1022,0,0.505035,"Missing"
J13-1009,constant-tellier-2012-evaluating,0,0.131555,"Missing"
J13-1009,P10-4002,0,0.0452481,"Missing"
J13-1009,N09-1037,1,0.544537,"Missing"
J13-1009,D11-1067,1,0.823799,"Missing"
J13-1009,C10-1045,1,0.134693,"the first to parse the sections of the ATB used in this article. They adapted the Bikel parser (Bikel 2004) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set. The ATB was subsequently revised (Maamouri, Bies, and Kulick 2008), and Maamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for non-gold tokenization. Attia et al. (2010a) developed an Arabic unknown word model for the Berkeley parser based on signatures, much like those in Table 3. More recently, Huang and Harper (2011) presented a discriminative lexical model for Arabic that can encode arbitrary local lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental result"
J13-1009,P84-1058,0,0.718277,"bank. Dependency representations may be more appropriate for these idiom classes. 2.3 French MWEs In French, there is a lexicographic tradition of compiling MWE lists. For example, Gross (1986) shows that whereas French dictionaries contain about 1,500 single-word adverbs there are over 5,000 multiword adverbs. MWEs occur in every part of speech (POS) category (e.g., noun trousse de secours (‘first-aid kit’); verb faire main-basse [do hand-low] (‘seize’); adverb comme dans du beurre [as in butter] (‘easily’); adjective à part entière (‘wholly’)). Motivated by the prevalence of MWEs in French, Gross (1984) developed a linguistic theory known as Lexicon-Grammar. In this theory, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in terms of the sequence of the POS tags of the words that constitute the MWE (e.g., “N de N” garde d’enfant [guard of child] (‘daycare’), pied de guerre [foot of war] (‘at the ready’)) (Gross 1986). In other words, MWEs are represented by a flat structure. The Lexicon-Grammar distinguishes between units that are fixed and have to appear as is (en tout et pour tout [in all and for all] (‘in total’)) and units that accept"
J13-1009,C86-1001,0,0.800946,"iqa (‘neighborly’) D+A C+D+A:  !1  .   D+A C+D+A:  1  3 al-bariia w-al-baHariia (‘land and sea’) A D+N: These idiom types usually do not cross constituent boundaries, so constituency parsers are well suited for modeling them. The other three classes of Ashraf (2012)—verbsubject, verbal, and adverbial—tend to cross constituent boundaries, so they are difficult to represent in a PTB-style treebank. Dependency representations may be more appropriate for these idiom classes. 2.3 French MWEs In French, there is a lexicographic tradition of compiling MWE lists. For example, Gross (1986) shows that whereas French dictionaries contain about 1,500 single-word adverbs there are over 5,000 multiword adverbs. MWEs occur in every part of speech (POS) category (e.g., noun trousse de secours (‘first-aid kit’); verb faire main-basse [do hand-low] (‘seize’); adverb comme dans du beurre [as in butter] (‘easily’); adjective à part entière (‘wholly’)). Motivated by the prevalence of MWEs in French, Gross (1984) developed a linguistic theory known as Lexicon-Grammar. In this theory, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in ter"
J13-1009,P05-1071,0,0.0380349,"Missing"
J13-1009,D07-1028,0,0.110528,"Missing"
J13-1009,I11-1025,0,0.0176747,"aamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for non-gold tokenization. Attia et al. (2010a) developed an Arabic unknown word model for the Berkeley parser based on signatures, much like those in Table 3. More recently, Huang and Harper (2011) presented a discriminative lexical model for Arabic that can encode arbitrary local lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental results. Siham Boulaknadel and Aboutajdine (2008) evaluated several lexical association measures in isolation for MWE identification in newswire. More recently, Attia et al. (2010b) compared crosslingual projection methods (using Wikipedia and English Wordnet) with standard n-gram cla"
J13-1009,J98-4004,0,0.124524,"supervised parsing models for Arabic and French. Now we show how to construct MWE-aware training resources for them. The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al. 2004) and the French Treebank (FTB) (Abeillé, Clément, and Kinyon 2003). Prior to parsing, both treebanks require significant pre-processing, which we perform automatically.8 Because parsing evaluation metrics are sensitive to the terminal/non-terminal ratio (Rehbein and van Genabith 2007), we only remove nonterminal labels in the case of unary rewrites of the same category (e.g., NP → NP) (Johnson 1998). Table 5 compares the pre-processed corpora with the WSJ section of the PTB. Appendix C compares the annotation consistency of the ATB, FTB, and WSJ. 5.1 Arabic Treebank We work with parts 1–3 (newswire) of the ATB,9 which contain documents from three different news agencies. In addition to phrase structure markers, each syntactic tree also contains per-token morphological analyses. 8 Tree manipulation is automated with Tregex/Tsurgeon (Levy and Andrew 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. 9 LDC catalog numbers: LDC2008E61, LDC200"
J13-1009,P03-1054,1,0.0692462,"es of part of speech than parts of speech. Surface statistics may erroneously predict that only the former is an MWE and the latter is not. More worrisome is that the statistics for the two n-grams are separate, thus missing an obvious generalization. In this article, we show that statistical parsing models generalize more effectively over arbitrary-length multiword expressions. This approach has not been previously demonstrated. To show its effectiveness, we build two parsing models for MWE identification. The first model is based on a context-free grammar (CFG) with manual rule refinements (Klein and Manning 2003). This parser also includes a novel lexical model—the factored lexicon—that incorporates morphological features. The second model is based on tree substitution grammar (TSG), a formalism with greater strong generative capacity that can store larger structural tree fragments, some of which are lexicalized. We apply the models to Modern Standard Arabic (henceforth MSA, or simply “Arabic”) and French, two morphologically rich languages (MRLs). The lexical sparsity (in finite corpora) induced by rich morphology poses a particular challenge for n-gram classification. Relative to English, French has"
J13-1009,D07-1091,0,0.0174518,"ely of capital letters  If none of the above, deterministically extract one- and two-character suffixes 203 Computational Linguistics Volume 39, Number 1 3.4 Factored Lexicon with Morphological Features We will apply our models to Arabic and French, yet we have not dealt with the lexical sparsity induced by rich morphology (see Table 5 for a comparison to English). One way to combat sparsity is to parse a factored representation of the terminals, where factors might be the word form, the lemma, or grammatical features such as gender, number, and person (φ features) (Bilmes and Kirchoff 2003; Koehn and Hoang 2007, inter alia). The basic parser lexicon estimates the generative probability of a word given a tag p(w|t) from word/tag pairs observed in the training set. Additionally, the lexicon includes parameter estimates p(t|s) for unknown word signatures s produced by the unknown word models (see Section 3.3). At parsing time, the lexicon scores each input word type w according to its observed count in the training set c(w). We define the unsmoothed and smoothed parameter estimates: p(t|w) = psmooth (t|w) = c(t, w) c(w) c(t, w) + αp(t|s) c(w) + α (1) (2) We then compute the desired parameter p(w|t) as"
J13-1009,N10-1089,0,0.434284,"June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variants of this technique. Broadly speaking, n-gram classification methods measure word co-occurrence. Suppose that a corpus contains more occurren"
J13-1009,P11-2122,0,0.0448892,"Missing"
J13-1009,levy-andrew-2006-tregex,0,0.0089729,"Missing"
J13-1009,P03-1056,1,0.271894,"s a particular challenge for n-gram classification. Relative to English, French has a richer array of morphological features— such as grammatical gender and verbal conjugation for aspect and voice. Arabic also has richer morphology including gender and dual number. It has pervasive verbinitial matrix clauses, although preposed subjects are also possible. For languages like these it is well known that constituency parsing models designed for English often do not generalize well. Therefore, we focus on the interplay among language, annotation choices, and parsing model design for each language (Levy and Manning 2003; Kübler 2005, inter alia), although our methods are ultimately very general. Our modeling strategy for MWEs is simple: We mark them with flat bracketings in phrase structure trees. This representation implicitly assumes a locality constraint on idioms, an assumption with a precedent in linguistics (Marantz 1997, inter alia). Of course, it is easy to find non-local idioms that do not correspond to surface constituents or even contiguous strings (O’Grady 1998). Utterances such as All hell seemed to break loose and The cat got Mary’s tongue are clearly idiomatic, yet the idiomatic elements are d"
J13-1009,N10-1082,0,0.0233521,"Missing"
J13-1009,P99-1041,0,0.034676,"uations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic processes such as inflection and phrasal expansion. 8.2 Syntactic Methods for MWE Identification There is a voluminous literature on MWE identification, so we focus on syntaxbased methods. The classic statistical approach to MWE identification, Xtract (Smadja 1993), used an incremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically extracted dependency relationships to find MWEs. To our knowledge, 22 Sag et al. (2002) showed how to integrate MWE information into a non-probabilistic head-driven phrase structure grammar for English. 218 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions Wehrli (2000) was the first to propose the use of a syntactic parser for multiword expression identification. No empirical results were provided, however, and the MWEaugmented scoring function for the output of his symbolic parser was left to future r"
J13-1009,maamouri-etal-2008-enhancing,0,0.0513029,"Missing"
J13-1009,J93-2004,0,0.0557089,"Missing"
J13-1009,P06-1055,0,0.215711,"s of z Change in counts by setting m sites in S Table 4 defines notation. The data likelihood is given by the latent state z and the  n (z) parameters θ: p(z|θ ) = z∈z θc,ec,e . Integrating out the parameters, we have: p(z) =   (αc P0 (e|c))nc,e (z) e c∈ V n (z) αc c,· (6) where xn = x(x + 1) . . . (x + n − 1) is the rising factorial. Base Distribution. The base distribution P0 is the same maximum likelihood PCFG used in the Stanford parser.6 After applying the manual grammar features, we perform simple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al. 2006). For each non-terminal type c, we learn a stop probability qc ∼ Beta(1, 1). Under P0 , the probability of generating a tree fragment A+ → B− C+ composed of nonterminals is P0 (A+ → B− C+ ) = pMLE (A → B C)qB (1 − qC ) (7) Unlike Cohn, Goldwater, and Blunsom (2009), we penalize lexical insertion: P0 (c → t) = pMLE (c → t)p(t) (8) where p(t) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-Based Inference Algorit"
J13-1009,N07-1051,0,0.060662,"Missing"
J13-1009,P09-2012,0,0.0247629,"g, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that is modeled with Bayes’ rule: p(R |T) ∝ p(T |R) p(R) (5) Because the tree fragments completely specify each tree, p(T |R) is either 0 or 1, so all work is performed by the prior over the set of elementary trees. The DP-TSG contains a DP prior for each c ∈ V and generates a tree fragment e rooted at non-terminal c according to: θc |c, αc , P0 (·|c) ∼ DP(αc , P0 ) e|θc ∼ θc 4 Similar models were developed independently by O’Donnell, Tenenbaum, and Goodman (2009) and Post and Gildea (2009). 5 We use the terms tree fragment and elementary tree interchangeably. 205 Computational Linguistics Volume 39, Number 1 Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ΔnS:m DP concentration parameter for each non-terminal type c ∈ V CFG base distribution Set of all non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 for frontier nodes) Latent state of the segmented treebank Number o"
J13-1009,ramisch-etal-2010-mwetoolkit,0,0.0156972,"Missing"
J13-1009,D07-1066,0,0.0769972,"Missing"
J13-1009,seddah-2010-exploring,0,0.128531,"cy Parsing. Abeillé (1988) and Abeillé and Schabes (1989) identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP. They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items 219 Computational Linguistics Volume 39, Number 1 that included MWEs. Recent statistical parsing work on French has included stochastic tree insertion grammars (STIG), which are related to TAGs, but with a restricted adjunction operation.23 Both Seddah, Candito, and Crabbé (2009) and Seddah (2010) showed that STIGs underperform CFG-based parsers on the FTB. In their experiments, MWEs were grouped. Appendix B describes additional prior work on CFG-based FTB parsing. French MWE Identification. Statistical French MWE identification has only been investigated recently. We previously reported the first results on the FTB using a parser for MWE identification (Green et al. 2011). Contemporaneously, Watrin and Francois (2011) applied n-gram methods to a French corpus of multiword adverbs (Laporte, Nakamura, and Voyatzi 2008). Constant and Tellier (2012) used a linear chain conditional random"
J13-1009,W10-1410,0,0.118491,"Missing"
J13-1009,boulaknadel-etal-2008-multi,0,0.106692,"Missing"
J13-1009,J93-1007,0,0.0327018,"ts were not strictly comparable. Moreover, gold pre-grouping was usually assumed, as was the case in most FTB parsing evaluations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic processes such as inflection and phrasal expansion. 8.2 Syntactic Methods for MWE Identification There is a voluminous literature on MWE identification, so we focus on syntaxbased methods. The classic statistical approach to MWE identification, Xtract (Smadja 1993), used an incremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically extracted dependency relationships to find MWEs. To our knowledge, 22 Sag et al. (2002) showed how to integrate MWE information into a non-probabilistic head-driven phrase structure grammar for English. 218 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions Wehrli (2000) was the first to propose the use of a syntactic parser for multiword expression identification. No empirical results"
J13-1009,N03-1033,1,0.0158233,"Missing"
J13-1009,E93-1045,0,0.190071,"ree fragments have fixed estimates according to a single sample from the DP-TSG: θc,e = nc,e (z) + αc P0 (e|c) nc, (z) + αc (9) This MAP grammar has an infinite rule set, however, because elementary trees with zero count in n have some residual probability under P0 . We discard all zero-count trees except for the zero-count CFG rules in P0 . Scores for these rules follow from Equation (9) with nc,e (z) = 0. This grammar represents most of the probability mass and permits inference using dynamic programming (Cohn, Goldwater, and Blunsom 2009). Because the derivations of a TSG are context-free (Vijay-Shanker and Weir 1993), we can form a CFG of the derivation sequences and use a synchronous CFG to translate the most probable CFG parse to its TSG derivation. Consider a unique tree fragment ei rooted at cj with frontier γ, which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form [cj → γ , cj → i, ck , cl , . . . ] (10) where ck , cl , . . . is a finite-length sequence of the non-terminal frontier nodes in γ.7 The SCFG translates the input string to a sequence of tree fragment indices. Because the TSG substitution operator applies to the leftmost frontier node, the be"
J13-1009,W11-0813,0,0.0232652,"Missing"
J13-1009,chrupala-etal-2008-learning,0,\N,Missing
J13-1009,E06-1047,0,\N,Missing
J13-1009,W10-3704,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
N06-1006,H05-1079,0,0.0893309,"the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al. (2005). Then the theorem prover may generate intermediate forms in the proof, but, nevertheless, individual terms are resolved locally without reference to global context. Finally, a few efforts (Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. While in principle this approach does not suffer from the limitations we describe below, in practice it has not borne much fruit. Because few problem sentences can be accurately translated to logical form, and because logical entailment is a strict standard, recall tends to be poor. The simple graph matching formulation of the problem belies three important issues. First, the above systems assume a form of upward monotonicity: if a good match is found with a part of"
N06-1006,de-marneffe-etal-2006-generating,1,0.369258,"Missing"
N06-1006,H05-1049,1,0.379173,"e. The simplest approach is to base the entailment prediction on the degree of semantic overlap between the text and hypothesis using models based on bags of words, bags of n-grams, TF-IDF scores, or something similar (Jijkoun and de Rijke, 2005). Such models have serious limitations: semantic overlap is typically a symmetric relation, whereas entailment is clearly not, and, because overlap models do not account for syntactic or semantic structure, they are easily fooled by examples like ID 2081. A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005). In this formulation, sentences are represented as normalized syntactic dependency graphs (like the one shown in figure 1) and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph(s) representing the text. Each possible alignment of the graphs has an associated score, and the score of the best alignment is used as an approximation to the strength of the entailment: a betteraligned hypothesis is assumed to be more likely to be entailed. To enable incremental search, alignment scores are"
N06-1006,P88-1012,0,0.0610457,"variety of approximate search techniques. Haghighi et al. (2005) 42 divide the search into two steps: in the first step they consider node scores only, which relaxes the problem to a weighted bipartite graph matching that can be solved in polynomial time, and in the second step they add the edges scores and hillclimb the alignment via an approximate local search. A third approach, exemplified by Moldovan et al. (2003) and Raina et al. (2005), is to translate dependency parses into neo-Davidsonian-style quasilogical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al., 1988). Unless supplemented with a knowledge base, this approach is actually isomorphic to the graph matching approach. For example, the graph in figure 1 might generate the quasi-LF rose(e1), nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2), dobj(e1, x3), percent(x3), num(x3, x4), 46(x4). There is a term corresponding to each node and arc, and the resolution steps at the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become disti"
N06-1006,P03-1054,1,0.0200203,"ble about their semantic content. We use typed dependency graphs, which contain a node for each word and labeled edges representing the grammatical relations between words. Figure 1 gives the typed dependency graph for ID 971. This representation contains much of the information about words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modi"
N06-1006,levy-andrew-2006-tregex,0,0.0121484,"Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then annotated with their associated word, part-of-speech (given by the parser), lemma (given by a finite-state transducer described by Minnen et al. (2001)) and named-entity tag. 3.2 Alignment graphs representing the hypothesis and the text. An alignment consists of a mapping from each node (word) in the hypothesis graph to a single node in the text graph, or to null.3 Figure 1 gives the alignment for ID 971. The space of alignments is large: there are O((m + 1)n ) possible alignments for a hypothesis graph with n"
N06-1006,W05-1201,0,0.108372,"of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect alignment is the subject arc of were is mismatched in the two. A robust inference guesser will still likely conclude that there is entailment. We propose that all three problems can be resolved in a two-stage architecture, where the alignment phase is followed by a separate phase of entailment determination. Although developed independently, the same division between alignment and classification has also been proposed by Marsi and Krahmer (2005), whose textual system is developed and evaluated on parallel translations into Dutch. Their classification phase features an output space of five semantic relations, and performs well at distinguishing entailing sentence pairs. Finding aligned content can be done by any search procedure. Compared to previous work, we emphasize structural alignment, and seek to ignore issues like polarity and quantity, which can be left to a subsequent entailment decision. For example, the scoring function is designed to encourage antonym matches, and ignore the negation of verb predicates. The ideas clearly g"
N06-1006,W03-0430,0,0.00852042,"ut words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then anno"
N06-1006,N03-1022,0,0.1205,"e extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. 1 Introduction During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise. The best-known such work has occurred within the field of question answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003); more recently, such work has continued with greater focus in addressing the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005) and within the U.S. Government AQUAINT program. Substantive progress on this task is key to many text and natural language applications. If one could tell that Protestors chanted slogans opposing a free trade agreement was a match for people demonstrating against free trade, then one could offer a form of semantic search not available with current keywordbased search. Even greater benefits would flow to richer and more semantically complex NLP"
N06-1006,H05-1047,0,0.0223103,"with a factored alignment score. The last issue arising in the graph matching approaches is the inherent confounding of alignment and entailment determination. The way to show that one graph element does not follow from another is to make the cost of aligning them high. However, since we are embedded in a search for the lowest cost alignment, this will just cause the system to choose an alternate alignment rather than recognizing a non-entailment. In ID 152, we would like the hypothesis to align with the first part of the text, to 1 This is the same problem labeled and addressed as context in Tatu and Moldovan (2005). 43 be able to prove that civilians are not members of law enforcement agencies and conclude that the hypothesis does not follow from the text. But a graphmatching system will to try to get non-entailment by making the matching cost between civilians and members of law enforcement agencies be very high. However, the likely result of that is that the final part of the hypothesis will align with were civilians at the end of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect al"
N13-1071,W10-4305,0,0.132984,"77* 66.40 73.14* 69.61 58.31 58.83* R CEAF-φ4 P F1 45.49 47.55* 46.50 47.77* 46.38 47.07* CoNLL F1 60.65 61.13* Table 6: Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically predicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05). System Baseline w/ Lifespan R B3 P F1 58.53* 71.58 64.40 58.14 73.14* 64.78* R CEAF-φ3 P F1 63.71* 58.31 60.89 63.38 58.83* 61.02 CoNLL F1 58.86 59.52* Table 7: B3 , CEAF-φ3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows Cai and Strube (2010). Scores are on automatically predicted mentions. 4 Application to coreference resolution To assess the usefulness of the lifespan model in an NLP application, we incorporate it into the Stanford coreference resolution system (Lee et al., 2011), which we take as our baseline. This was the highestscoring system in the CoNLL-2011 Shared Task, and was also part of the highest-scoring system in the CoNLL-2012 Shared Task (Fernandes et al., 2012). It is a rule-based system that includes a total of ten rules (or “sieves”) for entity coreference, such as exact string match and pronominal resolution."
N13-1071,de-marneffe-etal-2006-generating,1,0.0313242,"Missing"
N13-1071,J12-2003,1,0.227058,"Missing"
N13-1071,W12-4502,0,0.115124,"Missing"
N13-1071,J95-2003,0,0.424826,"Missing"
N13-1071,N06-2015,0,0.141562,"effective at predicting whether a given mention is singleton or coreferent. We then provide an initial assessment of the engineering value of making the singleton/coreferent distinction by incorporating our lifespan model into the Stanford coreference resolution system (Lee et al., 2011). This addition results in a significant improvement on the CoNLL-2012 Shared Task data, across the MUC, B3 , CEAF, and CoNLL scoring algorithms. 2 Data All the data used throughout the paper come from the CoNLL-2012 Shared Task (Pradhan et al., 2012), which included the 1.6M English words from OntoNotes v5.0 (Hovy et al., 2006) that have been annotated with different layers of annotation (coreference, parse trees, etc.). We used the training, development (dev), and test splits as defined in the shared task (Table 1). Since the OntoNotes coreference annotations do not contain singleton mentions, we automatically marked as singletons all the NPs 627 Proceedings of NAACL-HLT 2013, pages 627–633, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Dataset Docs Tokens Training Dev Test 2,802 343 348 1.3M 160K 170K M ENTIONS Coreferent Singletons 152,828 18,815 19,392 192,248 24,170 24,921 T"
N13-1071,W11-1902,0,0.277586,"Missing"
N13-1071,H05-1004,0,0.174329,"the S TANDARD model. Results and discussion To evaluate the coreference system with and without the lifespan model, we used the English dev and test sets from the CoNLL2012 Shared Task, presented in Section 2. Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-singleton) entities, by stopping singletons from being linked to multi-mention entities, we expected the lifespan model to increase the system’s precision. Our evaluation uses five of the measures given by the CoNLL-2012 scorer: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF-φ3 and CEAF-φ4 (Luo, 2005), and the CoNLL official score (Denis and Baldridge, 2009). We do not include BLANC (Recasens and Hovy, 2011) because it assumes gold mentions and so is not suited for the scenario considered in this paper, which uses automatically predicted mentions. Table 6 summarizes the test set performance. All the scores are on automatically predicted mentions. We use gold POS, parse trees, and NEs. The baseline is the Stanford system, and ‘w/ Lifespan’ is the same system extended with our lifespan model to discard singletons, as explained above. As expected, the lifespan model increases precision but de"
N13-1071,W12-4501,0,0.11096,"s the existing literature leads us to expect, and that the model itself is highly effective at predicting whether a given mention is singleton or coreferent. We then provide an initial assessment of the engineering value of making the singleton/coreferent distinction by incorporating our lifespan model into the Stanford coreference resolution system (Lee et al., 2011). This addition results in a significant improvement on the CoNLL-2012 Shared Task data, across the MUC, B3 , CEAF, and CoNLL scoring algorithms. 2 Data All the data used throughout the paper come from the CoNLL-2012 Shared Task (Pradhan et al., 2012), which included the 1.6M English words from OntoNotes v5.0 (Hovy et al., 2006) that have been annotated with different layers of annotation (coreference, parse trees, etc.). We used the training, development (dev), and test splits as defined in the shared task (Table 1). Since the OntoNotes coreference annotations do not contain singleton mentions, we automatically marked as singletons all the NPs 627 Proceedings of NAACL-HLT 2013, pages 627–633, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Dataset Docs Tokens Training Dev Test 2,802 343 348 1.3M 160K 170"
N13-1071,M95-1005,0,0.825852,"reference (on the dev set) was higher with the C ONFIDENT model than with the S TANDARD model. Results and discussion To evaluate the coreference system with and without the lifespan model, we used the English dev and test sets from the CoNLL2012 Shared Task, presented in Section 2. Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-singleton) entities, by stopping singletons from being linked to multi-mention entities, we expected the lifespan model to increase the system’s precision. Our evaluation uses five of the measures given by the CoNLL-2012 scorer: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF-φ3 and CEAF-φ4 (Luo, 2005), and the CoNLL official score (Denis and Baldridge, 2009). We do not include BLANC (Recasens and Hovy, 2011) because it assumes gold mentions and so is not suited for the scenario considered in this paper, which uses automatically predicted mentions. Table 6 summarizes the test set performance. All the scores are on automatically predicted mentions. We use gold POS, parse trees, and NEs. The baseline is the Stanford system, and ‘w/ Lifespan’ is the same system extended with our lifespan model to discard singletons, as explained ab"
N13-1071,C69-7001,0,\N,Missing
N13-1071,C69-6902,0,\N,Missing
N15-1051,P98-1013,0,0.0598898,"Sheinman et al., 2013; Schulam and Fellbaum, 2010). de Melo and Bansal (2013) propose a novel Mixed Integer Linear Programming (MILP) based approach, publish a gold standard dataset and report the best performance on ordering scalar adjectives on this dataset. However, these approaches are limited in two ways. First, they depend on a manually created resource, such 483 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 483–493, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics as WordNet or FrameNet (Baker et al., 1998). Lexical patterns (e.g., ‘not just x but y’) are used both to extract words that belong to the same scale and to determine the direction of the ordering (e.g., in the above pattern, x is weaker than y). However, this extraction process gives noisy results that require filtering using an electronic thesaurus. The domain of application is thus restricted to words that exist in an electronic thesaurus. Second, previous work is limited to the study of adjectives. In this paper, we propose a fully automated pipeline that uses structural patterns to extract gradable terms from a corpus, cluster the"
N15-1051,P14-2131,0,0.0153494,"t work shows promise for context vectors embedded in a compressed semantic space that are derived using neural networks: Baroni et al. (2014) compare standard context vectors with embedded vectors for a wide range of lexical semantic tasks and found embedded vectors to yield better results. We therefore generate context vectors and compare the utility of both skip-gram and continuous bag of words (CBOW) representations using the word2vec tool (Mikolov et al., 2013) for our task. These two representations have demonstrated varying degrees of success in different NLP tasks (Baroni et al., 2014; Bansal et al., 2014). Given a 2 The choice of a hard-clustering algorithm was mostly for implementational convenience, but carries with it the issue that polysemous words can only appear in one semantic cluster. We leave the issue of deriving a soft clustering approach that works with context vectors, a separate research problem in its own right, to future work. window size w, the CBOW model predicts the current word given the neighboring words as context. In contrast, the skip-gram model predicts the neighboring words given the current word. We used w = 5 and found CBOW to yield better results for our task. Thus"
N15-1051,P14-1023,0,0.225147,". As the clustering algorithm, we use the Matlab (2014) implementation of K-means++ (Arthur and Vassilvitskii, 2007), a hard clustering algorithm2 with cosine similarity as a distance metric. Following Hatzivassiloglou and McKeown (1993), we use context vectors to represent the words to cluster. They make use of standard context vectors for clustering adjectives, where context for every adjective comprises of nouns it modifies across all sentences in a corpus. However, recent work shows promise for context vectors embedded in a compressed semantic space that are derived using neural networks: Baroni et al. (2014) compare standard context vectors with embedded vectors for a wide range of lexical semantic tasks and found embedded vectors to yield better results. We therefore generate context vectors and compare the utility of both skip-gram and continuous bag of words (CBOW) representations using the word2vec tool (Mikolov et al., 2013) for our task. These two representations have demonstrated varying degrees of success in different NLP tasks (Baroni et al., 2014; Bansal et al., 2014). Given a 2 The choice of a hard-clustering algorithm was mostly for implementational convenience, but carries with it th"
N15-1051,P10-1018,1,0.761133,"Missing"
N15-1051,Q13-1023,0,0.252435,"Missing"
N15-1051,D13-1169,1,0.771925,"Missing"
N15-1051,P03-1054,0,0.00437118,"t)$ (ADJP<JJ)).’ Similarly, a structural pattern for adverbs can be written as ‘ADVP< ((ADVP<RB) $ (CC<but)$(RB<not)$ (ADVP<RB)).’ These patterns are available for download1 . 1 http://web.cse.ohio-state.edu/˜shivade/naacl2015 485 Introducing tree patterns requires parsing a corpus: while this additional step in the pipeline might lead to error propagation, the advantages of the structural patterns are that (i) they are more robust than the lexical ones and (ii) restricting results to a desired part-of-speech comes for free. In the experiments reported here, we use the Stanford parser v3.3.1 (Klein and Manning, 2003). 3.2 Automatic clustering In order to determine a ranking of words based on their semantic intensity, the first step is to determine words that belong to the same scale of meaning. As pointed out earlier, previous work (de Melo and Bansal, 2013; Sheinman et al., 2013) use WordNet dumbbells, and this restricts the utility of these approaches to the scope of a manually created lexical resource. We overcome this limitation by automatically clustering words that belong to the same scale. As the clustering algorithm, we use the Matlab (2014) implementation of K-means++ (Arthur and Vassilvitskii, 2"
N15-1051,levy-andrew-2006-tregex,0,0.0128554,"proaches suffer from a coverage issue. This is because these patterns consist of longer n-grams, which are sparsely found in a small dataset. Therefore, Sheinman et al. (2013) use the Web as their corpus, and de Melo & Bansal use Google N-grams (Brants and Franz, 2006). However, this results in a large number of instances where satisfied lexical patterns do not correspond to adjectives (e.g., sometimes but not always). Moreover, since the Google N-grams corpus is limited to 5-grams, adjective pairs of interest beyond a five-word window are lost. To deal with these shortcomings, we use Tregex (Levy and Andrew, 2006), which enables pattern matching on parse trees based on syntactic relationships and regular expression matches on nodes. Using Tregex, we transform de Melo and Bansal’s weak-strong and strong-weak lexical patterns into structural patterns. For example, one way of expanding the lexical pattern ‘∗ but not ∗’ into a structural Tregex pattern for adjectives is ‘ADJP< ((ADJP<JJ) $ (CC<but)$(RB<not)$ (ADJP<JJ)).’ Similarly, a structural pattern for adverbs can be written as ‘ADVP< ((ADVP<RB) $ (CC<but)$(RB<not)$ (ADVP<RB)).’ These patterns are available for download1 . 1 http://web.cse.ohio-state.e"
N15-1051,W14-1618,0,0.0123674,"ty can be successful established between regular terms, doing so for domain-specific terms requires knowledge of context. We plan to expand the structure patterns derived from the lexical patterns of de Melo and Bansal (2013), looking for new patterns that could be more suited for adverbs. We also plan to investigate soft clustering algorithms such as (Pereira et al., 1993) that may allow us to model polysemous words better. Furthermore, recent studies have compared traditional vectors against embedded vectors (such as the CBOW vectors used in this study) for different lexical semantic tasks (Levy and Goldberg, 2014; Baroni et al., 2014), which suggests that such a comparison for our clustering task could be insightful. Our experimental results show that automatic clustering of gradable words produces promising results. However, we also observe that with domainspecific words, context is important for establishing a ranking between words that is based on semantic intensity. Thus, rather than clustering adjectives or adverbs in isolation, a joint with the clustering of nouns or verbs with which they occur is a possible direction of research. Finally, studies deriving a ranking based on semantic intensities"
N15-1051,P93-1024,0,0.559226,"aces lexical patterns with structural patterns, and show that the approach has utility for not only discovering adjective patterns but also adverb patterns in biomedical text. We observe that while automatic ranking based 491 on semantic intensity can be successful established between regular terms, doing so for domain-specific terms requires knowledge of context. We plan to expand the structure patterns derived from the lexical patterns of de Melo and Bansal (2013), looking for new patterns that could be more suited for adverbs. We also plan to investigate soft clustering algorithms such as (Pereira et al., 1993) that may allow us to model polysemous words better. Furthermore, recent studies have compared traditional vectors against embedded vectors (such as the CBOW vectors used in this study) for different lexical semantic tasks (Levy and Goldberg, 2014; Baroni et al., 2014), which suggests that such a comparison for our clustering task could be insightful. Our experimental results show that automatic clustering of gradable words produces promising results. However, we also observe that with domainspecific words, context is important for establishing a ranking between words that is based on semantic"
N15-1051,E14-4023,0,0.208288,"ity: although words such as small and minuscule illustrate varying degrees of size, they are listed as synonyms in WordNet. Introduction Gradability (Sapir, 1944) is a property of words that identifies different degrees of the quality the word denotes. For example, adjectives such as large, huge and gigantic present different degrees of size or volume. Similarly, adverbs such as approximately, almost and roughly present different degrees of how Recently, there has been a lot of interest in exploring different approaches to derive an ordering among gradable adjectives based on their semantics (Ruppenhofer et al., 2014; Sheinman et al., 2013; Schulam and Fellbaum, 2010). de Melo and Bansal (2013) propose a novel Mixed Integer Linear Programming (MILP) based approach, publish a gold standard dataset and report the best performance on ordering scalar adjectives on this dataset. However, these approaches are limited in two ways. First, they depend on a manually created resource, such 483 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 483–493, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics as WordNet or FrameNet"
N15-1051,D08-1027,0,0.0330722,"Missing"
N15-1051,C98-1013,0,\N,Missing
N15-1051,P93-1023,0,\N,Missing
N19-1231,D16-1153,0,0.0176387,"ctures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning p"
N19-1231,Q17-1010,0,0.0231081,"ining data is reported as percentage of the entire corpus for inclusive evaluations, and as tokens actively annotated (i.e., not counting the random seed sentences) for exclusive evaluations. For consistency, following seed annotation, we always fetch additional annotation batches at the following intervals, in tokens: 1K, 4K, 5K, 10K, 20K until we reach 100K total tokens, 50K until 300K total, 100K until 500K total, and 250K from there. For all experiments leveraging neural taggers, we use freely available pretrained embeddings (Grave et al., 2018), except for Latin, where we train fasttext (Bojanowski et al., 2017) embeddings on the Perseus (Smith et al., 2000) and Latin Library collections with default parameters (using pretrained embeddings yield small performance boosts that decrease with additional training data). We conclude this section with a direct comparison to the recently proposed active learning pipeline of Shen et al. (2017) and their MNLP ranking algorithm. 4.1 Consistency of Non-deterministic Results Because the active learning pipeline involves taking a random seed and many of the experiments on larger corpora could not be averaged over several runs, we first measure performance variatio"
N19-1231,I17-2016,0,0.0181024,"input data structure and noisy optical character recognition (Van Hooland et al., 2013; Kettunen et al., 2017). Low Resource NER Language agnostic NER is highly desirable, yet limited by the data available in the least resourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use diction"
N19-1231,W03-0424,0,0.149449,"fine grained NER for English. Erdmann et al. (2016) and Sprugnoli (2018), among others, have shown that such off-the-shelf models can be substantially improved on DH-relevant data. Work such as Smith and Crane (2001) and Simon et al. (2016) represent a large community mining such data for geospatial entities. Additional DH work on NER concerns the impact of input data structure and noisy optical character recognition (Van Hooland et al., 2013; Kettunen et al., 2017). Low Resource NER Language agnostic NER is highly desirable, yet limited by the data available in the least resourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER"
N19-1231,W16-4012,1,0.905616,"Missing"
N19-1231,P05-1045,0,0.0310344,"frameworks when evaluating an active learning approach, as the intended application determines which framework is more relevant and thus, which model should be employed. Controlling for the NER model, HER’s active learning sentence ranking component achieves significant improvement over a competitive baseline (Shen et al., 2017). Because HER does not reference the inference model during sentence ranking, this provides counter evidence to Lowell et al. (2018)’s hypothesis that non-native active learning is suboptimal. 2 Related Work The best known NER systems among humanists are Stanford NER (Finkel et al., 2005), with pretrained models in several languages and an interface for building new models, and among researchers interested in NER for spatial research, the Edinburgh Geoparser (Grover et al., 2010), with fine grained NER for English. Erdmann et al. (2016) and Sprugnoli (2018), among others, have shown that such off-the-shelf models can be substantially improved on DH-relevant data. Work such as Smith and Crane (2001) and Simon et al. (2016) represent a large community mining such data for geospatial entities. Additional DH work on NER concerns the impact of input data structure and noisy optical"
N19-1231,L18-1550,0,0.0162553,"OOVs occurring in non-sentence initial position. Quantity of training data is reported as percentage of the entire corpus for inclusive evaluations, and as tokens actively annotated (i.e., not counting the random seed sentences) for exclusive evaluations. For consistency, following seed annotation, we always fetch additional annotation batches at the following intervals, in tokens: 1K, 4K, 5K, 10K, 20K until we reach 100K total tokens, 50K until 300K total, 100K until 500K total, and 250K from there. For all experiments leveraging neural taggers, we use freely available pretrained embeddings (Grave et al., 2018), except for Latin, where we train fasttext (Bojanowski et al., 2017) embeddings on the Perseus (Smith et al., 2000) and Latin Library collections with default parameters (using pretrained embeddings yield small performance boosts that decrease with additional training data). We conclude this section with a direct comparison to the recently proposed active learning pipeline of Shen et al. (2017) and their MNLP ranking algorithm. 4.1 Consistency of Non-deterministic Results Because the active learning pipeline involves taking a random seed and many of the experiments on larger corpora could not"
N19-1231,N16-1030,0,0.21778,"ework is relevant to the user who wants to build an NER tool that can generalize well to other corpora. We conduct extensive experiments comparing several combinations of active learning algorithms and NER model architectures in both frameworks across many typologically diverse languages and domains. The systematic differences between inclusive and exclusive results demonstrate that while deep NER model architectures 1 github.com/alexerdmann/HER. 2223 Proceedings of NAACL-HLT 2019, pages 2223–2234 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (Lample et al., 2016) are highly preferable for tagging held out sentences, shallow models (Lafferty et al., 2001) perform better on sentences that could have been chosen for manual annotation but were not selected by the active learning algorithm. We argue for the importance of considering both frameworks when evaluating an active learning approach, as the intended application determines which framework is more relevant and thus, which model should be employed. Controlling for the NER model, HER’s active learning sentence ranking component achieves significant improvement over a competitive baseline (Shen et al.,"
N19-1231,D18-1226,0,0.0269341,"sourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government co"
N19-1231,D17-1269,0,0.0611551,"Missing"
N19-1231,I17-2050,0,0.0174354,"nvironments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct mea"
N19-1231,D08-1112,0,0.0662843,"st informative to the model if annotated: uncertainty, where instances which confuse the model are given priority; diversity, where instances that would expand the model’s coverage are prioritized; and representativeness, prioritizing instances that best approximate the true distribution over all instances. Uncertainty-based approaches outperform other single-criterion approaches, though many works, primarily in Computer Vision, demonstrate that considering diversity reduces repetitive training examples and representativeness reduces outlier sampling (Roy and McCallum, 2001; Zhu et al., 2003; Settles and Craven, 2008; Zhu et al., 2008; Olsson, 2009; Gu et al., 2014; He et al., 2014; Yang et al., 2015; Wang et al., 2018b). For active learning in NER, Shen et al. (2017) propose the uncertainty-based metric maximized normalized log-probability (MNLP). It prioritizes sentences based on the length normalized log probability of the model’s predicted label sequence. To make neural active learning tractable, they shift workload to lighter convolutional neural networks (CNN) and update weights after each manual annotation batch instead of retraining from scratch. They demonstrate state-of-the-art performance with"
N19-1231,D18-1230,0,0.02038,"2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct means of improving NER quality. Active Learning Active learning seeks to"
N19-1231,P04-1075,0,0.428456,"ical sources, often require extensive funding, relying on considerable manual annotation (Simon et al., 2017). To this end, we introduce the Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotatio"
N19-1231,W17-2630,0,0.228707,"end, we introduce the Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotation. The standard, exclusive evaluation framework, by contrast, only measures the accuracy of the final trained model’s"
N19-1231,D18-1309,0,0.0455514,"Missing"
N19-1231,W03-0419,0,0.28682,"Missing"
N19-1231,D18-1124,0,0.054593,"ty; diversity, where instances that would expand the model’s coverage are prioritized; and representativeness, prioritizing instances that best approximate the true distribution over all instances. Uncertainty-based approaches outperform other single-criterion approaches, though many works, primarily in Computer Vision, demonstrate that considering diversity reduces repetitive training examples and representativeness reduces outlier sampling (Roy and McCallum, 2001; Zhu et al., 2003; Settles and Craven, 2008; Zhu et al., 2008; Olsson, 2009; Gu et al., 2014; He et al., 2014; Yang et al., 2015; Wang et al., 2018b). For active learning in NER, Shen et al. (2017) propose the uncertainty-based metric maximized normalized log-probability (MNLP). It prioritizes sentences based on the length normalized log probability of the model’s predicted label sequence. To make neural active learning tractable, they shift workload to lighter convolutional neural networks (CNN) and update weights after each manual annotation batch instead of retraining from scratch. They demonstrate state-of-the-art performance with MNLP, though Lowell et al. (2018) show its improvement above random sampling to be less dramatic, as do"
N19-1231,D18-1034,0,0.0126689,"hes leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct means of improving NER"
N19-1231,C08-1143,0,0.262902,"he Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotation. The standard, exclusive evaluation framework, by contrast, only measures the accuracy of the final trained model’s predictions on a h"
P08-1118,W07-1427,1,0.171238,"Missing"
P08-1118,W04-3205,0,0.0679853,"well as world knowledge (WK). We consider contradictions in category (1) ‘easy’ because they can often be automatically detected without full sentence comprehension. For example, if words in the two passages are antonyms and the sentences are reasonably similar, especially in polarity, a contradiction occurs. Additionally, little external information is needed to gain broad coverage of antonymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, 1041 to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even"
P08-1118,de-marneffe-etal-2006-generating,1,0.0221103,"Missing"
P08-1118,W07-1401,0,0.101463,"Missing"
P08-1118,W97-1311,0,0.0472477,"sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 Proceedings of ACL-08: HLT, pages 1039–1047, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics refer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entail"
P08-1118,P03-1054,1,0.00685307,"http://nlp.stanford.edu/projects/contradiction. of an event is acquired over time (e.g., a rising death toll) or various parties have divergent views of an event (e.g., example 9 in table 1). 4 System overview Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al., 2006), but adds a stage for event coreference decision. 4.1 Linguistic analysis The first stage computes linguistic representations containing information about the semantic content of the passages. The text and hypothesis are converted to typed dependency graphs produced by the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). To improve the dependency graph as a pseudo-semantic representation, collocations in WordNet and named entities are collapsed, causing entities and multiword relations to become single nodes. 4.2 Alignment between graphs The second stage provides an alignment between text and hypothesis graphs, consisting of a mapping from each node in the hypothesis to a unique node in the text or to null. The scoring measure uses node similarity (irrespective of polarity) and structural information based on the dependency graphs. Similarity measures and structural information are"
P08-1118,N06-1006,1,0.801024,"Missing"
P08-1118,P02-1047,0,0.0619565,"ymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, 1041 to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even more difficult instance of contradiction created by a lexical discrepancy. Structural issues also create contradictions (examples 6 and 7). Lexical complexities and variations in the function of arguments across verbs can make recognizing these contradictions complicated. Even when similar verbs are used and argument differences exist, structural differences may indicate non-entailment or"
P08-1118,W06-3907,0,0.0192452,"ivity features. The context in which a verb phrase is embedded may give rise to contradiction, as in example 5 (table 1). Negation influences some factivity patterns: Bill forgot to take his wallet contradicts Bill took his wallet while Bill did not forget to take his wallet does not contradict Bill took his wallet. For each text/hypothesis pair, we check the (grand)parent of the text word aligned to the hypothesis verb, and generate a feature based on its factivity class. Factivity classes are formed by clustering our expansion of the PARC lists of factive, implicative and non-factive verbs (Nairn et al., 2006) according to how they create contradiction. Modality features. Simple patterns of modal reasoning are captured by mapping the text and hypothesis to one of six modalities ((not )possible, (not )actual, (not )necessary), according to the presence of predefined modality markers such as can or maybe. A feature is produced if the text/hypothesis modality pair gives rise to a contradiction. For instance, the following pair will be mapped to the contradiction judgment (possible, not possible): T: The trial court may allow the prevailing party reasonable attorney fees as part of costs. H: The prevai"
P08-1118,W07-1029,0,0.0356958,"Missing"
P08-1118,P08-1008,0,0.164003,"Missing"
P08-1118,W05-1206,0,0.0893975,"ch turned violent when a woman stabbed her partner because she didn’t want to watch the game. (2) A woman passionately wanted to watch the game. We also mark as contradictions pairs reporting contradictory statements. The following sentences refer to the same event (de Menezes in a subway station), and display incompatible views of this event: (1) Eyewitnesses said de Menezes had jumped over the turnstile at Stockwell subway station. (2) The documents leaked to ITV News suggest that Menezes walked casually into the subway station. This example contains an “embedded contradiction.” Contrary to Zaenen et al. (2005), we argue that recognizing embedded contradictions is important for the application of a contradiction detection system: if John thinks that he is incompetent, and his boss believes that John is not being given a chance, one would like to detect that the targeted information in the two sentences is contradictory, even though the two sentences can be true simultaneously. 3.2 Typology of contradictions Contradictions may arise from a number of different constructions, some overt and others that are comID 1 Type Antonym Text Capital punishment is a catalyst for more crime. 2 Negation 3 Numeric 4"
P08-1118,W07-1412,0,0.0152219,"the amount of available information. Contradiction detection could also be applied to intelligence reports, demonstrating which information may need further verification. In bioinforThis pair is contradictory: defused rockets cannot go off, and thus cannot injure anyone. Detecting contradictions appears to be a harder task than detecting entailments. Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 Proceedings of ACL-08: HLT, pages 1039–1047, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics refer to the same event. The importance of event coreference was recognized in th"
P08-1118,W03-0906,0,\N,Missing
P08-1118,W07-1415,0,\N,Missing
P08-1118,N03-1022,0,\N,Missing
P08-1118,W07-1400,0,\N,Missing
P10-1018,de-marneffe-etal-2006-generating,1,0.0316939,"Missing"
P10-1018,W09-3920,1,0.905684,"Missing"
P10-1018,W10-0719,1,0.812257,"Missing"
P10-1018,P94-1009,0,0.125376,"no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inherent in some answers needs to be captured (de Marneffe et al., 2009). While a straightforward ‘yes’ or ‘no’ response is clear in some indirect answers, such as in (1), the intended answer is less certain in other cases (2):1 (1) A: Do you think that’s a good idea, that we just begin to ignore these"
P10-1018,J99-3004,0,0.191727,"Missing"
P10-1018,J80-3003,0,0.513184,"to learn meanings that can drive pragmatic inference in dialogue. This paper demonstrates to some extent that meaning can be grounded from text in this way. 2 a. The speaker is certain of ‘yes’ or ‘no’ and conveys that directly and successfully to the hearer. b. The speaker is certain of ‘yes’ or ‘no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inher"
P10-1018,D08-1027,0,0.0518347,"Missing"
P10-1018,P03-1054,1,0.00969769,"Missing"
P10-1018,D08-1103,0,\N,Missing
P15-2016,W11-0702,0,0.261501,"e event of Jesus walking on water and thus does not take it as a fact, whereas the quote’s author does take it as a fact. Full Data Set Balanced Training Set Disagree Neutral Agree 5741 3125 1113 779 0 779 Total 9980 1158 Table 2: Category counts in the training set. 2 Data We used the Internet Argument Corpus (IAC), a corpus of quote-response pairs annotated for agreement via Mechanical Turk (Walker et al., 2012). Agreement scores span from -5 (strong disagreement) and +5 (strong agreement). The distribution is shown in Figure 2. Because the original data skews toward disagreement, following Abbott et al. (2011), we created a balanced set, discarding “neutral” pairs between -1 and +1. We split the data into training, development and test sets. 1 Table 2 shows the category counts in the training set. Our experiments demonstrate that our linguistic model based on alignment significantly outperforms a baseline bag-of-words model in the recall of disagreeing quote-response (QR) pairs. Such linguistic models will transfer more easily to any debate dialogue, independent of the structural information of post threads and author’s stance which might not always be recoverable. 1 We could not obtain the trainin"
P15-2016,H05-2018,0,0.0123886,"entence-final question mark means a sentence is considered interrogative; if a sentence’s root is labeled VB and has no subject relation, it is deemed an imperative. The features in the classifier are counts of the instances of interrogatives and imperatives in the response. Discourse Markers. In lieu of tracking discourse markers such as oh and so really, Abbott et al. (2011) tracked response-initial unigrams, bigrams, and trigrams. Typed Dependencies and MPQA. In addition to all dependencies from the response being used as features, dependencies were supplemented with MPQA sentiment values (Wilson et al., 2005). A dependency like (agree,I) would also yield the sentiment-dependency feature (positive,I), whereas (wrong, you) would also yield (negative,you). 96 Accuracy Baseline Alignment+ 71.85 75.45 P Agreement R F1 70.64 76.04 74.77 74.32 72.65 75.17 P Disagreement R F1 73.21 74.89 68.92 76.58 71.00 75.73 Table 3: Accuracy, precision (P), recall (R) and F1 scores for both categories (agreement and disagreement) on the test set. 0.6 0.4 0.2 sensitivity 0.8 1.0 Personal Pronouns. The presence of first, second, and third person pronouns in the response are each tracked as binary features. The inclusion"
P15-2016,P13-2123,0,0.0706147,"Missing"
P15-2016,de-marneffe-etal-2006-generating,1,0.0633341,"Missing"
P15-2016,W07-1401,0,0.0362844,"Missing"
P15-2016,P13-2142,0,0.0694,"Missing"
P15-2016,P13-1048,0,0.0743867,"Missing"
P15-2016,N03-5008,0,0.00873916,"efore and after balancing. -5 is high disagreement, +5 is high agreement. 3 Post Length. Following Misra and Walker (2013), we track various length features such as word count, sentence count, and average sentence length, including differentials of these measures between quote and response. Short responses (relative to both word-wise and sentence-wise counts) tend to correlate with agreement, while longer responses tend to correlate with disagreement. Features In this section, we detail the features of our model. We use the maximum entropy model as implemented in the Stanford CoreNLP toolset (Manning and Klein, 2003). Many of the features make use of the typed dependencies from the CoreNLP toolset (de Marneffe et al., 2006). For comparison, the baseline features attempt to replicate Abbott et al. (2011). 3.1 Alignment+ Features Emoticons. Emoticons are a popular way of communicating sentiment in internet text. Many emoticons in the corpus are in forum-specific code, such as emoticon rolleyes. We also detect a wider array of common emoticons as regular expressions beginning with colons, semicolons, or equals signs, such as :-D, ;), and =). Baseline Features from Abbott et al. 2011 N-Grams. All unigrams, bi"
P15-2016,W13-4006,0,0.104755,"evelopment set are then analyzed for feature extraction. The sentence pair with the maximum alignment score for each post pair is also analyzed regardless of its meeting the threshold. 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) Agreement Score Range (2,3) (3,4) (4,5) (2,3) (3,4) (4,5) (a) Full dataset. Number of QR Pairs 400 300 200 100 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) Agreement Score Range (b) Balanced training set. Figure 1: Agreement score distribution of the dataset, before and after balancing. -5 is high disagreement, +5 is high agreement. 3 Post Length. Following Misra and Walker (2013), we track various length features such as word count, sentence count, and average sentence length, including differentials of these measures between quote and response. Short responses (relative to both word-wise and sentence-wise counts) tend to correlate with agreement, while longer responses tend to correlate with disagreement. Features In this section, we detail the features of our model. We use the maximum entropy model as implemented in the Stanford CoreNLP toolset (Manning and Klein, 2003). Many of the features make use of the typed dependencies from the CoreNLP toolset (de Marneffe et"
P15-2016,Q14-1018,0,0.035113,"Missing"
P15-2016,C12-2120,0,0.0716645,"Missing"
P15-2016,walker-etal-2012-corpus,0,0.0328678,"es us to identify the polarity mismatch (doesn’t vs. does). In example (3), the italicized sentences are the most well-aligned, enabling us to identify that the response’s author embeds under modality the event of Jesus walking on water and thus does not take it as a fact, whereas the quote’s author does take it as a fact. Full Data Set Balanced Training Set Disagree Neutral Agree 5741 3125 1113 779 0 779 Total 9980 1158 Table 2: Category counts in the training set. 2 Data We used the Internet Argument Corpus (IAC), a corpus of quote-response pairs annotated for agreement via Mechanical Turk (Walker et al., 2012). Agreement scores span from -5 (strong disagreement) and +5 (strong agreement). The distribution is shown in Figure 2. Because the original data skews toward disagreement, following Abbott et al. (2011), we created a balanced set, discarding “neutral” pairs between -1 and +1. We split the data into training, development and test sets. 1 Table 2 shows the category counts in the training set. Our experiments demonstrate that our linguistic model based on alignment significantly outperforms a baseline bag-of-words model in the recall of disagreeing quote-response (QR) pairs. Such linguistic mode"
P19-1412,W09-3012,0,0.0309378,"ker commitment Nanjiang Jiang and Marie-Catherine de Marneffe Department of Linguistics The Ohio State University {jiang.1879, demarneffe.1}@osu.edu Abstract or question answering: for instance, we should extract from example (1) in Table 1 that the speaker could wish someone dead, but from (3) that people should not be allowed to carry guns in their vehicles, even though both events are embedded under believe and negation. There has been work on factors leading to speaker commitment in theoretical linguistics (i.a., Karttunen (1971); Simons et al. (2010)) and computational linguistics (i.a., Diab et al. (2009); Saur´ı and Pustejovsky (2012); Prabhakaran et al. (2015)), but mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances. de Marneffe et al. (2019) introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment. In this paper, we use it to eval"
P19-1412,S12-1020,0,0.0328144,"which uses a top-down approach on a de4209 # Predicate r FactBank MEANTIME UW UDS 9,761 1,395 13,644 27,289 SoA MAE 0.86 0.61 0.75 0.77 0.31 0.23 0.42 0.96 Figure 2: Number of items with different features in the full and restricted sets of the CommitmentBank. Table 2: The number of annotated predicates in each dataset, and previous state-of-the-art performance. The score on UW with MAE was obtained by Stanovsky et al. (2017), while the other scores were obtained by Rudinger et al. (2018). pendency tree and predicts speaker commitment score in [−3, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers. For example, refuse p entails ¬p, so the factuality of its complement p gets flipped if encountered. and Pearson’s r correlation, measuring how well the model captures variability in the data. Pearson’s r is considered more informative than MAE because the reference sets are biased towards +3. Neural-based model Rudinger et al. (2018) introduced three neural models for speaker commitment: a linear biLSTM, a dependency tree biLSTM, a hybrid model that ensembles the two. Rudinger et al. (201"
P19-1412,D15-1189,0,0.0207758,"factuality of its complement p gets flipped if encountered. and Pearson’s r correlation, measuring how well the model captures variability in the data. Pearson’s r is considered more informative than MAE because the reference sets are biased towards +3. Neural-based model Rudinger et al. (2018) introduced three neural models for speaker commitment: a linear biLSTM, a dependency tree biLSTM, a hybrid model that ensembles the two. Rudinger et al. (2018) also proposed a multitask training scheme in which a model is trained on four factuality datasets: FactBank (Saur´ı and Pustejovsky, 2009), UW (Lee et al., 2015), MEAN TIME (Minard et al., 2016) and UDS (Rudinger et al., 2018), all with annotations on a [−3, 3] scale. Each dataset has shared biLSTM weights but specific regression parameters. Reference datasets The FactBank, UW, and MEANTIME datasets all consist of sentences from news articles. Each event in FactBank was annotated by 2 annotators, with 0.81 Cohen’s κ. UW has 5 annotations for each event, and MEANTIME has 6. UDS contains sentences from the English Web Treebank (Bies et al., 2012), which contains weblogs, newsgroups, emails, reviews, and question-answers. It has 2 annotations for each pr"
P19-1412,P17-2056,0,0.121794,"ut mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances. de Marneffe et al. (2019) introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment. In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitment: Stanovsky et al. (2017) and Rudinger et al. (2018). The CommitmentBank, restricted to specific linguistic constructions, is a good test case. It allows us to evaluate whether current speaker commitment models achieve robust language understanding, by analyzing their performance on specific challenging linguistic constructions. When a speaker, Mary, asks Do you know that Florence is packed with visitors?, we take her to believe that Florence is packed with visitors, but not if she asks Do you think that Florence is packed with visitors? Inferring speaker commitment (aka event factuality) is crucial for information ex"
P19-1412,D18-1501,0,0.100786,"Missing"
P19-1412,N13-1091,0,0.0525399,"Missing"
P19-1412,L16-1699,0,0.0273151,"Missing"
P19-1412,S15-1009,0,0.0192286,"e Marneffe Department of Linguistics The Ohio State University {jiang.1879, demarneffe.1}@osu.edu Abstract or question answering: for instance, we should extract from example (1) in Table 1 that the speaker could wish someone dead, but from (3) that people should not be allowed to carry guns in their vehicles, even though both events are embedded under believe and negation. There has been work on factors leading to speaker commitment in theoretical linguistics (i.a., Karttunen (1971); Simons et al. (2010)) and computational linguistics (i.a., Diab et al. (2009); Saur´ı and Pustejovsky (2012); Prabhakaran et al. (2015)), but mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances. de Marneffe et al. (2019) introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment. In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitme"
P19-1412,N18-1067,0,0.0876378,"Missing"
P19-1412,J12-2002,0,0.0365396,"Missing"
silveira-etal-2014-gold,levy-andrew-2006-tregex,0,\N,Missing
silveira-etal-2014-gold,de-marneffe-etal-2006-generating,1,\N,Missing
silveira-etal-2014-gold,de-marneffe-etal-2014-universal,1,\N,Missing
silveira-etal-2014-gold,N06-2015,0,\N,Missing
silveira-etal-2014-gold,W07-1004,0,\N,Missing
silveira-etal-2014-gold,W13-3721,1,\N,Missing
silveira-etal-2014-gold,W03-3017,0,\N,Missing
W07-1427,W02-1001,0,0.0111527,"Missing"
W07-1427,de-marneffe-etal-2006-generating,1,0.0621763,"Missing"
W07-1427,levy-andrew-2006-tregex,0,0.0313803,"age A core part of an entailment system is the ability to find semantically equivalent patterns in text. Previously, we wrote tedious graph traversal code by hand for each desired pattern. As a remedy, we wrote Semgrex, a pattern language for dependency graphs. We use Semgrex atop the typed dependencies from the Stanford Parser (de Marneffe et al., 2006b), as aligned in the alignment phase, to identify both semantic patterns in a single text and over two aligned pieces of text. The syntax of the language was modeled after tgrep/Tregex, query languages used to find syntactic patterns in trees (Levy and Andrew, 2006). This speeds up the process of graph search and reduces errors that occur in complicated traversal code. 5.1 Semgrex Features Rather than providing regular expression matching of atomic tree labels, as in most tree pattern languages, Semgrex represents nodes as a (nonrecursive) attribute-value matrix. It then uses regular expressions for subsets of attribute values. For example, {word:run;tag:/ˆNN/} refers to any node that has a value run for the attribute word and a tag that starts with NN, while {} refers to any node in the graph. However, the most important part of Semgrex is that it allow"
W07-1427,W07-1431,1,0.380995,"Missing"
W07-1427,N06-1006,1,\N,Missing
W08-1301,W08-0601,0,0.00981273,"Missing"
W08-1301,H91-1060,0,0.040933,"e question of the suitability of the Stanford scheme for parser evaluation. 1 Introduction The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that could easily be understood and effectively used by people without linguistic expertise who wanted to extract textual relations. The representation was not designed for the purpose of parser evaluation. Nevertheless, we agree with the widespread sentiment that dependency-based evaluation of parsers avoids many of the problems of the traditional Parseval measures (Black et al., 1991), and to the extent that the Stanford dependency representation is an effective representation for the tasks envisioned, it is perhaps closer to an appropriate taskbased evaluation than some of the alternative dependency representations available. In this paper c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 On the other hand, evaluation seems less important; to the best of our knowledge there has never been a convincing and thorough evaluation of either MiniPar o"
W08-1301,de-marneffe-etal-2006-generating,1,0.223848,"Missing"
W08-1301,D07-1024,0,0.011106,"Missing"
W08-1301,P01-1052,0,0.00581586,"onal) linguists and suitability for relation extraction applications led SD to try to adhere to the following design principles (DPs): This paper advocates for the Stanford typed dependencies representation (henceforth SD) being a promising vehicle for bringing the breakthroughs of the last 15 years of parsing research to this broad potential user community. The representation aims to provide a simple, habitable design. All information is represented as binary relations. This maps straightforwardly on to common representations of potential users, including the logic forms of Moldovan and Rus (Moldovan and Rus, 2001),2 semantic web Resource Description Framework (RDF) triples (http://www.w3.org/RDF/), and graph representations (with labeled edges and nodes). Unlike many linguistic formalisms, excessive detail is viewed as a defect: information that users do not understand or wish to process detracts from uptake and usability. The user-centered design process saw the key goal as representing semantically contentful relations suitable for relation extraction and more general information extraction uses. The design supports this use by favoring relations between content words, by maintaining semantically use"
W08-1301,W03-2806,0,0.0424603,"Missing"
W08-1301,W07-1401,0,0.0125064,"Missing"
W08-1301,W07-1004,0,0.0543531,"endency representations for shallow text understanding tasks has become salient, we would argue, following Clegg and Shepherd (2007), that dependency-based evaluation is close to typical user tasks. Moreover, it avoids some of the known deficiencies of other parser evaluation measures such as Parseval (Carroll et al., 1999). Recent work on parser evaluation using dependency graphs in the biomedical domain confirms 6 reviewers for their helpful comments. that researchers regard dependency-based evaluation as a more useful surrogate for extrinsic task-based evaluation (Clegg and Shepherd, 2007; Pyysalo et al., 2007a). In their evaluation, Clegg and Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked par"
W08-1301,P03-1054,1,0.0398737,"d Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked parsers (e.g., the Stanford parser (Klein and Manning, 2003)). Their parser evaluation accommodates user needs: they used the collapsed version of the dependency graphs offered by the SD scheme, arguing that this is the kind of graph one would find most useful in an information extraction project. Although Clegg and Shepherd (2007) also favor dependency graph representations for parser evaluation, they advocate retention of parse trees so information lost in the dependency structures can be accessed. In essence, any existing dependency scheme could be adopted as the gold-standard for evaluation. However if one believes in ultimately valuing extrinsic t"
W08-1301,I05-1006,0,0.00949667,"(Clegg and Shepherd, 2007; Pyysalo et al., 2007a). In their evaluation, Clegg and Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked parsers (e.g., the Stanford parser (Klein and Manning, 2003)). Their parser evaluation accommodates user needs: they used the collapsed version of the dependency graphs offered by the SD scheme, arguing that this is the kind of graph one would find most useful in an information extraction project. Although Clegg and Shepherd (2007) also favor dependency graph representations for parser evaluation, they advocate retention of parse trees so information lost in the dependency structures can be accessed. In essence, any existing dependency scheme could be adopted as the gold-st"
W08-1301,1993.iwpt-1.22,0,0.0293606,", etc. Thinking about this issue, we were struck by two facts. First, we noted how frequently WordNet (Fellbaum, 1998) gets used compared to other resources, such as FrameNet (Fillmore et al., 2003) or the Penn Treebank (Marcus et al., 1993). We believe that much of the explanation for this fact lies in the difference of complexity of the representation used by the resources. It is easy for users not necessarily versed in linguistics to see how to use and to get value from the straightforward structure of WordNet. Second, we noted the widespread use of MiniPar (Lin, 1998) and the Link Parser (Sleator and Temperley, 1993). This clearly shows that (i) it is very easy for a non-linguist thinking in relation extraction terms to see how to make use of a dependency representation (whereas a phrase structure representation seems much more foreign and forbidding), and (ii) the availability of high quality, easy-to-use (and preferably free) tools is essential for driving broader use of NLP tools.1 This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For s"
W08-1301,levy-andrew-2006-tregex,0,0.264916,"ndent of the head of the adverbial 3 Without word position, the representation is deficient if the same word occurs more than once in a sentence. 4 depend on this head. To retrieve adequate heads from a semantic point of view, heuristics are used to inject more structure when the Penn Treebank gives only flat constituents, as is often the case for conjuncts, e.g., (NP the new phone book and tour guide), and QP constituents, e.g., (QP more than 300). Then for each grammatical relation, patterns are defined over the phrase structure parse tree using the tree-expression syntax defined by tregex (Levy and Andrew, 2006). Conceptually, each pattern is matched against every tree node, and the matching pattern with the most specific grammatical relation is taken as the type of the dependency. The automatic extraction of the relations is not infallible. For instance, in the sentence Behind their perimeter walls lie freshly laundered flowers, verdant grass still sparkling from the last shower, yew hedges in an ecstasy of precision clipping (BNC), the system will erroneously retrieve apposition relations between flowers and grass, as well as between flowers and hedges whereas these should be conj and relations. Th"
W08-1301,P04-1042,1,0.0622189,"ubj(think-4, he-3) However in a sentence such as Who the hell does he think he’s kidding? (BNC), the automatic extraction will fail to find that who is the direct object of kidding. Here, it is vital to distinguish between SD as a representation versus the extant conversion tool. Long-distance dependencies are not absent from the formalism, but the tool does not accurately deal with them.5 4 Stanford dependencies in practice SD has been successfully used by researchers in different domains. In the PASCAL Recognizing 5 As possible future work, we have thought of using a tool such as the one of Levy and Manning (2004) to correctly determine long distance dependencies, as input to the current dependency conversion system. This would presumably be effective, but would make the conversion process much heavier weight. 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 traction in the biomedical domain and originally followed the Link Grammar scheme, Pyysalo et al. (2007a) developed a version of the corpus annotated with the SD scheme. They also made available a program and conversion rules that they used to transform Link Grammar relations into SD graphs, which were then hand-corrected (Pyysalo et al., 2007"
W08-1301,J93-2004,0,0.0561261,"has been developed over the last two decades approachable to and usable by everyone who has text understanding needs. That is, usable not only by computational linguists, but also by the computer science community more generally and by all sorts of information professionals including biologists, medical researchers, political scientists, law firms, business and market analysts, etc. Thinking about this issue, we were struck by two facts. First, we noted how frequently WordNet (Fellbaum, 1998) gets used compared to other resources, such as FrameNet (Fillmore et al., 2003) or the Penn Treebank (Marcus et al., 1993). We believe that much of the explanation for this fact lies in the difference of complexity of the representation used by the resources. It is easy for users not necessarily versed in linguistics to see how to use and to get value from the straightforward structure of WordNet. Second, we noted the widespread use of MiniPar (Lin, 1998) and the Link Parser (Sleator and Temperley, 1993). This clearly shows that (i) it is very easy for a non-linguist thinking in relation extraction terms to see how to make use of a dependency representation (whereas a phrase structure representation seems much mo"
W08-1301,W03-2401,0,\N,Missing
W09-2501,P05-1074,0,0.0313752,"stitute a major factor in determining entailment. However, we have argued that about half of the true MWEs are decomposable, that is, the part of the alignment that is crucial for entailment can be recovered with a one-to-one alignment link that can be identified even by very limited alignment models. 2 4 We thank Patrick Pantel for granting us access to DIRT. poorly represented represented 0.42 poorly 0.07 rarely 0.06 good 0.05 representatives 0.04 very few 0.04 well 0.02 representative 0.01 Parallel corpora-based paraphrases. An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). It exploits the variance inherent in translation to extract paraphrases from bilingual parallel corpora. Concretely, it observes translational relationships between a source and a target language and pairs up source language phrases with other source language phrases that translate into the same target language phrases. We applied this method to the large Chinese-English GALE MT evaluation P3/P3.5 corpus (∼2 GB text per language, mostly newswire). The large number of translations makes it impractical to store all observed paraphrases. We therefore filtered the list of paraphrases against the"
W09-2501,W07-1428,0,0.0161426,"of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on pr"
W09-2501,W05-1210,0,0.127926,"g et al., 2002). The importance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedin"
W09-2501,W07-1421,0,0.0212269,"to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 C"
W09-2501,P98-2127,0,0.0298403,"→ executive director military → naval forces Before we come to actual experiments on the automatic recognition of MWEs in a practical RTE system, we need to consider the prerequisites for this task. As mentioned in Section 2, if an RTE system is to establish multi-word alignments, it requires a knowledge source that provides accurate semantic similarity judgments for “many-to-many” alignments (capital punishment – death penalty) as well as for “one-to-many” alignments (vote – cast ballots). Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to be paraphrase resources. In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. In particular when light verbs are involved (file lawsuits) or when modification adds just minor meaning aspects (executive director), we argue that it is su"
W09-2501,N06-1006,1,0.880616,"Missing"
W09-2501,J93-2003,0,0.00952629,"Word Expressions in Alignment Almost all textual entailment recognition models incorporate an alignment procedure that establishes correspondences between the premise and the hypothesis. The computation of word alignments is usually phrased as an optimization task. The search space is based on lexical similarities, but usually extended with structural biases in order to obtain alignments with desirable properties, such as the contiguous alignment of adjacent words, or the mapping of different source words on to different target words. One prominent constraint of the IBM word alignment models (Brown et al., 1993) is functional alignment, that is each target word is mapped onto at most one source word. Other models produce only one-to-one alignments, where both alignment directions must be functional. MWEs that involve many-to-many or one-tomany alignments like Ex. (1) present a problem for such constrained word alignment models. A functional alignment model can still handle cases like Ex. (1) correctly in one direction (from bottom to top), but not in the other one. One-to-one alignments manage neither. Various workarounds have been proposed in the MT literature, such as computing word alignments in b"
W09-2501,D08-1084,1,0.88192,"Missing"
W09-2501,W07-1402,0,0.0213983,"he bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs"
W09-2501,W07-1414,0,0.0204608,"portance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Works"
W09-2501,W07-1412,0,0.0230942,"ch can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 CARDINALITY DECOM - POSAB"
W09-2501,E09-1025,0,0.0138437,"to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual"
W09-2501,W02-1001,0,\N,Missing
W09-2501,W07-1401,0,\N,Missing
W09-2501,C98-2122,0,\N,Missing
W09-3920,J80-3003,0,0.886264,"y will be based on some combination of this disparate, partially conflicting, uncertain evidence. The plan and logical inference model of Green and Carberry falters in the face of such collections of uncertain evidence. However, natural dialogues are often interpreted in the midst of uncertain and conflicting signals. We therefore propose to enrich a logical inference model with probabilistic methods to deal with such cases. This study addresses the phenomenon of indirect question–answer pairs (IQAP), such as in (1), from both empirical and engineering perspectives. Introduction Clark (1979), Perrault and Allen (1980), and Allen and Perrault (1980) study indirect speech acts, identifying a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages. Prior discourse conditions, the relationship between the literal meaning and the common ground, and specific lexical, constructional, and intonational cues Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 136–143, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 136 First, we underta"
W09-3920,P92-1009,0,0.852988,"Missing"
W09-3920,P94-1009,0,0.383451,"Missing"
W13-3721,D11-1037,0,0.0975334,"The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set o"
W13-3721,W07-2416,0,0.0184095,"staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For ex"
W13-3721,W06-2920,0,0.0547907,"ructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotatio"
W13-3721,de-marneffe-etal-2006-generating,1,0.178631,"Missing"
W13-3721,P80-1024,0,0.417167,"Missing"
W13-3721,W07-1004,0,0.0560843,"imell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For example, in (1a), the object of find can be “raised” to subject position in the main clause to form a tough adjective construction, as in (1b). One of the difficulties for generative grammar in modeling this construction is that the object being raised can be embedded arbitrarily deeply in the sentence, as in (1c). introduces a PP experiencer, the PP can “move” separately (4b), both supporting the hypothesis that the experience"
W13-3721,D09-1085,0,0.405499,"erent constructions The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 20"
W13-3721,W08-1301,1,\N,Missing
W15-1305,D13-1099,0,0.0210921,"lts without the need for annotated data. This is an important result especially in the clinical domain where available annotated data is sparse and extremely costly to generate. 6 Dependency Tree Kernels Dependency tree kernels have been showed to be effective for NLP tasks in the past. Culotta et al. (2004) showed that although tree kernels by themselves may not be effective for relation extraction, combining a tree kernel with a bag of words kernel showed promising results. Dependency tree kernels have also been explored in the context of negation extraction in the medical domain. Recently, Bowei et al. (2013) demonstrated the use of tree kernel based approaches in detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dep"
W15-1305,P04-1054,0,0.770769,"detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dependency tree kernel similar to the approach described in Culotta and Sorensen (2004) to automatically capture the structural patterns in negated assertions. We used the Stanford dependencies parser (version 2.0.4) (de Marneffe et al., 2006) to get the dependency parse for every assertion. As per their representation (de Marneffe and Manning, 2008) every dependency is a triple, consisting of a governor, a dependent and a dependency relation. In this triple, the governor and dependent are words from the input sentence. Thus, the tree kernel comprised of nodes corresponding to every word and every dependency relation in the parse. Node similarity was computed based on features s"
W15-1305,W08-1301,1,0.7412,"Missing"
W15-1305,de-marneffe-etal-2006-generating,1,0.112784,"Missing"
W15-1305,W08-0606,0,0.0340525,"ly to generate. 6 Dependency Tree Kernels Dependency tree kernels have been showed to be effective for NLP tasks in the past. Culotta et al. (2004) showed that although tree kernels by themselves may not be effective for relation extraction, combining a tree kernel with a bag of words kernel showed promising results. Dependency tree kernels have also been explored in the context of negation extraction in the medical domain. Recently, Bowei et al. (2013) demonstrated the use of tree kernel based approaches in detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dependency tree kernel similar to the approach described in Culotta and Sorensen (2004) to automatically capture the structural patterns in negated assertions. We used"
W15-1520,W14-1505,0,0.012627,"itchboard corpus (Godfrey et al., 1992). Switchboard is a collection of about 2400 telephone dialogs among 543 speakers in the United States. Each utterance is assigned one of 42 dialog-act tags, which summarize syntactic, semantic and pragmatic information about the turns (e.g., yes/no question, yes answer, agree).8 The minimum, mean, and maximum lengths of the phrases in the training set are 0, 34.1, and 549, respectively. Zero length phrases exist because of the preprocessing, and they are ignored. The task in this section is identifying the dialog act tags from given utterances. Following Milajevs and Purver (2014); Milajevs et al. (2014), we used the first 1115 utterances as the training set and the 8 The tags are described in http://web.stanford. edu/˜jurafsky/ws97/manual.august1.html. Task Method (Milajevs et al., 2014) mean mult mean + mult concat {mean,mult} mean + nbr prj nbr outer prj mean + nbr outer prj Paraphrase detection Addition Multiplication 0.73 0.42 0.686 0.393 0.665 0.652 0.690 0.388 0.688 0.587 0.689 0.387 0.684 0.412 0.688 0.371 Dialog act tagging Addition Multiplication 0.63 0.58 0.638 0.522 0.636 0.606 0.633 0.593 0.636 0.515 0.636 0.581 0.565 0.573 0.626 0.598 Table 4: Accuracies"
W15-1520,D14-1079,0,0.058763,"g, ‡ Department of Linguistics, The Ohio State University, Columbus, OH 43210, USA kimjook@cse.ohio-state.edu, mcdm@ling.ohio-state.edu, fosler@cse.ohio-state.edu Abstract their efficient but still effective architectures. The CBOW model takes the mean vector of projections of the context words and use it to predict the target word as the following objective function:2 Categorical compositional distributional models unify compositional formal semantic models and distributional models by composing phrases with tensor-based methods from vector representations. For the tensor-based compositions, Milajevs et al. (2014) showed that word vectors obtained from the continuous bag-of-words (CBOW) model are competitive with those from co-occurrence based models. However, because word vectors from the CBOW model are trained assuming additive interactions between context words, the word composition used for the training mismatches to the tensor-based methods used for evaluating the actual compositions including pointwise multiplication and tensor product of context vectors. In this work, we show whether the word embeddings from extended CBOW models using multiplication or tensor product between context words, refle"
W15-1520,P08-1028,0,0.120063,"ative interactions between word projections to obtain word embeddings more suitable for the tensor-based compositions. For four datasets, evaluating different types of compositions, we show that those extensions of the CBOW model improve the performance of the actual composition tasks with multiplication or tensor product operations. 2 Tensor-based compositions Prior to discussing the modification to the CBOW algorithm, we review different composition methods used in the literature (Table 1). Addition and Multiplication are compositions by point-wise addition and multiplication, respectively (Mitchell and Lapata, 2008). They can be done simply without any other information, but they cannot reflect word orders and grammatical structures. 144 Mitchell and Lapata (2008, 2009) showed that composition by multiplication can be more effective than composition by addition because additive models compose by considering the content altogether whereas multiplicative models focus on the content relevant to the composition by scaling each element of one with the strength of the corresponding element of the other. Using multiplication as the composition method could be unstable in the previous work because multiplication"
W15-1520,D09-1045,0,0.0258645,"ut they cannot reflect word orders and grammatical structures. 144 Mitchell and Lapata (2008, 2009) showed that composition by multiplication can be more effective than composition by addition because additive models compose by considering the content altogether whereas multiplicative models focus on the content relevant to the composition by scaling each element of one with the strength of the corresponding element of the other. Using multiplication as the composition method could be unstable in the previous work because multiplication with zero or negative values changes the value abruptly (Mitchell and Lapata, 2009). In our models, however, these instability issues could be alleviated since the training model adapt the constituent word vectors to be proper for the composition by multiplication. Mitchell and Lapata (2010) also showed that the tensor product is effective to represent composition because it allows the interactions between different features in different vectors whereas point-wise multiplication can interact with only the same feature in different vectors. Therefore, we also examine an extension of the CBOW model using tensor product for modeling local context. There are neural network model"
W15-1520,D11-1129,0,0.0624369,"Missing"
W15-1520,W11-2507,0,0.0407099,"Missing"
W15-1520,C12-2054,0,0.06082,"l number of words in a corpus, wt is the tth word, pt is the tth word vector, and c is the half window size. Milajevs et al. (2014) showed that the word vectors generated from the CBOW model are competitive with those from co-occurrence based models for both simple arithmetic compositions and tensorbased compositions for categorical compositional distributional models (Coecke et al., 2010).3 Categorical compositional distributional models represent compositional semantics with algebra of Pregroup by representing each grammatical reduction as a linear map in vector spaces (Coecke et al., 2010; Kartsaklis et al., 2012). For example, cats like milk consists of a subject noun, a transitive verb requiring a subject and an object, and an object noun, respectively. In Pregroup grammar,(the types ) of the r l three words in this example are n, n sn , and n, respectively, where n is a noun, nr can be combined with a n in the left, nl can be combined with a n in the right, and s is a declarative statement. Then, 2 Although sum is used in Mikolov et al. (2013a), the current version of word2vec implementation uses mean. 3 Although Milajevs et al. (2014) described that the skipgram model was used to generate the word"
W15-1520,J00-3003,0,0.0137673,"two word sentences in the training corpus for the training. To deal with these issues, in the last model, we combine the mean with the projection of the tensor product. 4 Experiment results To evaluate the five different CBOW-based models proposed in Section 3, we use the following datasets: similarity of transitive verbs with multi146 ple senses from Grefenstette and Sadrzadeh (2011a), three-word sentence similarity from Kartsaklis and Sadrzadeh (2014), paraphrase detection from Dolan et al. (2013), and dialog act tagging for the Switchboard corpus (Godfrey et al., 1992) from Stolcke et al. (2000). These are all the datasets evaluated in Milajevs et al. (2014)’s work as well. Each phrase in the first two datasets is fixed as a subject, a transitive verb, and an object whereas the length of each phrase in the last two datasets is arbitrary. There are several differences between our word vectors and the ones used in Milajevs et al. (2014). First, we use BNC as the training set while Milajevs et al. (2014) use pretrained word vectors from word2vec that are trained using GoogleNews dataset. To reduce the size of projection matrices, all the words are lower-cased and words occurring 20 time"
W15-1520,D13-1170,0,\N,Missing
W16-1607,P15-1011,0,0.0196327,"formation, inhibiting adjustment of vector spaces to better represent semantic intensity scales. In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces."
W16-1607,P10-1018,1,0.891855,"Missing"
W16-1607,N15-1184,0,0.200006,"Missing"
W16-1607,N15-1100,0,0.377801,"ract because they can have similar contexts in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors"
W16-1607,P15-2070,0,0.0476149,"Missing"
W16-1607,D14-1162,0,0.0781683,"tensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces. For example, in a vector space, a word and its antonym should be sufficiently far apart, but they can be quite close 62 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 62–69, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics word a"
W16-1607,J15-4004,0,0.113272,"ds that are adjusted with semantic intensity orders are small, not all the cases comparing the adjustments using just synonyms and antonyms to the adjustments including semantic intensity orders were significant for p-value &lt; 0.05, as shown in Table 4. However, since many of them are slightly insignificant (like p-value=0.07) and the scores noticeably increased in many cases, using semantic intensity orders for the adjustments seem promising. In addition, to show that the adjustments are not harmful for the representation of the general semantics of the words, we also evaluated on SimLex-999 (Hill et al., 2015), where 999 word For the WordNet synset pair dataset and de Melo and Bansal (2013)’s dataset, where the subtle semantic intensity differences are more critical, using synonyms, antonyms, and semantic intensity orders altogether (“syn&ant,same_ord,diff_ord”) showed significantly higher scores than “syn&ant” in many settings. Here, “diff_ord” corresponds to equation 6. Table 5 shows the adjective pairs whose intensity judgements were changed by including adjustments with semantic intensity orders. The pairs are from the WordNet synset pairs and 67 baseline syn&ant same_ord (kmeans only) same_ord"
W16-1607,P15-2004,0,0.0463164,"an have similar contexts in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of m"
W16-1607,N15-1051,1,0.914173,"Missing"
W16-1607,D13-1169,1,0.925372,"Missing"
W16-1607,levy-andrew-2006-tregex,0,0.00928147,"e of word vectors which are wrongly ordered. To reduce wrong orderings, we formulate a Inferring intensity ordering We follow de Melo and Bansal (2013)’s approach to order the adjectives in each cluster. For every possible pair of adjectives in the cluster, we search for regular expressions like “h∗i but not h∗i” in Google N -gram (Brants and Franz, 2006). These patterns give us the direction of the ordering between the adjectives. For example, if “good but not great” appears frequently in Google N -gram, we infer that “great&quot; is semantically stronger than 1 Shivade et al. (2015) used Tregex (Levy and Andrew, 2006) to extract patterns including more words but it is not necessary when we extract patterns from phrases consisting of less or equal to five words. 64 bad Max # Turkers agreeing great 10 9 8 7 6 5 4 good Figure 1: An example of incoherent word vector positions, where “bad” should be closer to “good” than “great” but the similarity between “bad” and “good” is lower than the similarity between “bad” and “great”. annotator agreement (Fleiss’ kappa) of this dataset is 0.359. Note that Fleiss’ kappa is a very conservative measure given the partial order in the annotation, which is not taken into acc"
W16-1607,D15-1174,0,0.0290489,"r spaces to better represent semantic intensity scales. In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces. For example, in a vector space, a word and i"
W16-1607,P15-1145,0,0.124901,"in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of models, which can eventu"
W16-1607,Q15-1025,0,0.129688,"N (i) is the set of the initial neighbors of the i-th word. Word pairs with cosine similarities equal to or higher than 0.8 are regarded as neighbors. The objective function for the word vector adjustment is represented as the sum of the three terms:   C V, V 0 = AF (V ) + SC (V ) + KN V, V 0 (4) This function is minimized with stochastic gradient descent with learning rate 0.1 for 20 iterations. Adjusting word embeddings with semantic lexicons In this study, we start from one of three different off-the-shelf word vector types as a baseline for our studies: GloVe, CBOW, and Paragram-SL999 (Wieting et al., 2015); we adjust each of these sets of vectors with a variety of contrastive methods. Our first contrastive system is a baseline using synonyms and antonyms (“syn&ant”) following Mrkši´c et al. (2016)’s approach, which adjusts word vectors so that the sum of the following three max-margin objective functions are minimized. 3 Adjusting word embeddings with semantic intensity orders In order to better model semantic intensity ordering, we augment the synonym and antonym adjusted model with semantic intensity information to adjust word vectors. We first cluster semantically related words, infer semant"
W16-1607,N13-1090,0,0.0982564,"oaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of models, which can eventually perform better than the joint training models (Mrkši´c et al., 20"
W16-1607,N16-1018,0,0.109011,"Missing"
W16-1607,Q13-1023,0,\N,Missing
W16-2903,S07-1103,0,0.0335092,". Adjectives have been studied extensively in computational linguistics. WordNet (Fellbaum, 1998) classifies adjectives into two broad categories: descriptive and relational. Descriptive adjectives (e.g., big house, heavy bag) ascribe the value of an attribute to a noun, while relational adjectives (e.g., atomic bomb, dental hygiene) do not. Among the various distinctions between descriptive and relational adjectives, relational adjectives are typically not gradable (Fellbaum, 1998). Although association between adjectives and numerical quantities has been a topic of research in some studies (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010), very few studies have investigated grounding the meaning of adjectives to numerical quantities. de Marneffe et al. (2010) investigated the problem of interpreting implied answers to yes/no questions when the response is not explicit. Specifically, they investigated question-answer pairs in which the question contains an adjective and the answer contains a numerical measure. For example, predicting the correct yes/no answer in (1) involves interpreting a numerical quantity (age) with respect to the gradable adjective little. 1. 8 Conclusio"
W16-2903,C00-1044,0,0.297509,"and Plan”). The beginning of a section is formatted as distinct text with the section name in capital letters followed by a newline characted. We used a simple rule-based system to identify section headers and map the contents of a note to these sections. As with note types, section names also had multiple lexical variations (e.g., “Physical Examination” can be “Physical Exam” or “Physical Assessment” or simply “Exam”). Our corpus had 587 Identification of gradable adjectives First, we want to automatically identifiy gradable adjectives in our corpus. We reimplemented the method described in (Hatzivassiloglou and Wiebe, 2000), a log linear regression model that learns the weights associated with two features: 1) Number of times an adjective is used in comparative and superlative constructs, and 2) Number of times an adjective is modified by terms that intensify or diminish the semantic meaning of adjectives (mostly adverbs such as very, little, somewhat, etc. and a few nouns such as bit, etc.). Hatzivassiloglou and Wiebe (2000) manually created a list of 73 such terms. Their model was generated using the 1987 Wall Street Journal Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453"
W16-2903,W03-1314,0,0.107498,"Missing"
W16-2903,P10-1133,0,0.0260639,"studied extensively in computational linguistics. WordNet (Fellbaum, 1998) classifies adjectives into two broad categories: descriptive and relational. Descriptive adjectives (e.g., big house, heavy bag) ascribe the value of an attribute to a noun, while relational adjectives (e.g., atomic bomb, dental hygiene) do not. Among the various distinctions between descriptive and relational adjectives, relational adjectives are typically not gradable (Fellbaum, 1998). Although association between adjectives and numerical quantities has been a topic of research in some studies (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010), very few studies have investigated grounding the meaning of adjectives to numerical quantities. de Marneffe et al. (2010) investigated the problem of interpreting implied answers to yes/no questions when the response is not explicit. Specifically, they investigated question-answer pairs in which the question contains an adjective and the answer contains a numerical measure. For example, predicting the correct yes/no answer in (1) involves interpreting a numerical quantity (age) with respect to the gradable adjective little. 1. 8 Conclusion We empirically evaluated us"
W16-2903,de-marneffe-etal-2006-generating,1,0.11017,"Missing"
W16-2903,P10-1018,1,0.899156,"Missing"
W16-2903,Q13-1023,0,0.143111,"Missing"
W16-2903,J93-2004,0,0.0538098,"s. We reimplemented the method described in (Hatzivassiloglou and Wiebe, 2000), a log linear regression model that learns the weights associated with two features: 1) Number of times an adjective is used in comparative and superlative constructs, and 2) Number of times an adjective is modified by terms that intensify or diminish the semantic meaning of adjectives (mostly adverbs such as very, little, somewhat, etc. and a few nouns such as bit, etc.). Hatzivassiloglou and Wiebe (2000) manually created a list of 73 such terms. Their model was generated using the 1987 Wall Street Journal Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453 adjectives (235 gradable and 218 non-gradable) created using the Collins Birmingham University International Language Database dictionary, which is annotated for gradable and non-gradable adjectives. We developed a logistic regression model with the two features described above. For the first feature, a morphology analysis component was developed to identify inflections of adjectives from their base form. This consisted of identifying adjectives in their comparative form using simple parts-ofspeech tagging (Toutanova et al., 2003) and"
W16-2903,P13-1038,0,0.330314,"n in males is 11.7 while that for females is 13.2. These variations are small in magnitude. However, this is a problem in cases where the dependency on other variables is much more pronounced. We illustrate this through an example. Bone Marrow Cellularity (BMC) is the volume ratio of hematopoietic cells (blood cells that give rise to other blood cells) and fat. Pathologists perform a bone marrow analysis and use the three adjectives hypocellular, normocellular, and hypercellular to describe the sample. However, BMC 23 little kids”), so that both positive and negative instances can be learned. Narisawa et al. (2013) explore a closely related problem of learning numerical common sense for the task of RTE in Japanese text. They study a broad set of cases that require semantic inference over numerical expressions. They query the web to gather instances of pairs of numerical quantities and corresponding contexts and propose two approaches. The distribution based approach concludes the numerical quantity to be large or small if it appears in the top or bottom five percent of the distribution generated for the numerical quantity and normal if it is in between. The cue-based approach relies on explicit textual"
W16-2903,E14-4023,0,0.0403275,"Missing"
W16-2903,R15-1071,0,0.0217434,"Missing"
W16-2903,N15-1051,1,0.85109,"Missing"
W16-2903,N03-1033,0,0.0116011,"Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453 adjectives (235 gradable and 218 non-gradable) created using the Collins Birmingham University International Language Database dictionary, which is annotated for gradable and non-gradable adjectives. We developed a logistic regression model with the two features described above. For the first feature, a morphology analysis component was developed to identify inflections of adjectives from their base form. This consisted of identifying adjectives in their comparative form using simple parts-ofspeech tagging (Toutanova et al., 2003) and regular expression based rules. Although the test set used in (Hatzivassiloglou and Wiebe, 2000) is available, the list of 73 noun phrases and adverbial modifications is not. We therefore compiled this list using ten fold cross validation to capture the second feature. In each fold of training, we found all the adverbs and nouns modifying the gradable adjectives using the Stanford Dependency Parser (version 2.0.4) (de Marneffe et al., 2006). We determined the best subset by choosing an optimal threshold for the (k = 81) most frequent modifiers through cross validation. This gave us the se"
W16-3919,W15-4319,1,0.519066,"Missing"
W16-3919,D10-1057,0,0.0761634,"n when applied to noisy Twitter data. But tweets often contain more up-to-date information than news, in addition the increased volume of text offers opportunities to exploit redundancy of information which is very beneficial for information extraction (Downey et al., 2005). To exploit the opportunities for information extraction on top of social media, there is a crucial need for in-domain annotated data to train and evaluate named entity recognition systems on this noisy style of text. Twitter processing has the additional challenge that the language people use on Twitter changes over time (Dredze et al., 2010; Fromreide et al., 2014). The previous edition of this task (Baldwin et al., 2015) addressed this issue by evaluating on a test set collected from a later time period then the training and development data. This year we take a similar approach, providing a new test dataset of tweets gathered from 2016. In addition to enabling research on adapting named entity recognition to new language over time, we hope this new dataset will be useful for adapting future Twitter named entity recognition systems, improving their performance on up-to-date data. Additionally, this year we address the issue of"
W16-3919,W16-3924,0,0.0132806,"features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses automatically constructed lexicons with a partial matching algorithm and text normalization to handle the large vocabulary problem in Web texts. 141 CambridgeLTL Talos akora NTNU ASU DeepNNNER DeepER hjpwhu UQAM-NTL LIOX Precision Recall F1 60.77 58.51 51.70 53.19 40.58 54.97 45.40 48.90 40.73 40.15 46.07 38.12 39.48 32.13 37.58 28.16 31.15 28.76 23.52 12.69 52.41 46.16 44.77 40.06 39.02 37.24 36.95 36.22 29.82 19.26 CambridgeLTL NTNU Talos akora ASU DeepER Dee"
W16-3919,P05-1045,0,0.0488289,"the location of the shooting event. The shooting domain contains 8,963 tokens with 751 phrases. Computer hacking events were found by searching for tweets including the keyword “breach”. The breach domain contains 5,537 tokens with 603 phrases. The additional data annotated this year was completed by a single annotator instructed to follow the annotation guidelines of the prior annotations. The annotator was presented with a set of simple guidelines2 that cover common ambiguous cases and was also instructed to refer to the September 2010 data 1 2 For example, the Stanford named entity tagger (Finkel et al., 2005) achieves an F1 score of 0.86 on the CoNLL data set. http://bit.ly/1FSP6i2 139 for reference (Ritter et al., 2011). The BRAT tool3 was used for annotation. Figure 1 is a screenshot of the interface presented to the annotators. To ensure that the new annotations were consistent with the earlier annotations, 100 tweets were annotated in both tasks to calculate agreement. The new annotator proved a high agreement with the old data set with a F1 score of 67.67. Table 1 presents the count of each of the 10 named entity types labeled by the annotators in the training, development and test sets creat"
W16-3919,W16-3923,0,0.0295917,"Missing"
W16-3919,W16-3920,0,0.153678,"ML – – – – GloVe Multiple – – CRFsuite LSTM LSTM CRF L2S LSTM-CNN LSTM CRF Table 3: Features and machine learning approach taken by each team. 2.3 System Descriptions This section briefly describes the approach taken by each team. Overall we noticed different trends between the types of systems submitted this year and last year. The most notable change is the use of LSTM-based systems. Four of the seven submissions were LSTM-based as opposed to zero submissions last year. The previous year Conditional Random Fields was the most popular ML technique for extracting named entities. CambridgeLTL (Limsopatham and Collier, 2016) The system uses bidirectional LSTM to automatically induce and leverage orthographic features for performing Named Entity Recognition in Twitter messages. akora (Kurt Junshean Espinosa and Ananiadou, 2016) This system uses bidirectional LSTM networks and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters a"
W16-3919,Q14-1019,0,0.00717094,"and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters and token internal data, five types of features built through context and chunk information, and five types of features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses a"
W16-3919,I11-1108,0,0.0110506,"presents the results of the Twitter Named Entity Recognition shared task associated with W-NUT 2016: a named entity tagging task with 10 teams participating. We outline the shared task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task. 1 Introduction The increasing flood of user-generated text on social media has created an enormous opportunity for new data analysis techniques to extract and aggregate information about breaking news (Ritter et al., 2012), disease outbreaks (Paul and Dredze, 2011), natural disasters (Neubig et al., 2011), cyber-attacks (Ritter et al., 2015) and more. Named entity recognition is an important first step in most information extraction pipelines. However, performance of state-of-the-art NER on social media still lags behind well edited text genres. This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. In this paper, we present the development and evaluation of a shared task on named entity recognition in Twitter, which was held at the 2nd Workshop on Noisy User-generated Text (W-NUT 2016) and attracted 10 participating teams, 7 of"
W16-3919,W16-3926,0,0.0197598,"n segmenting and categorizing entities into 10 types. ASU (Michel Naim Gerguis and Gerguis, 2016) The system shows an experimental study on using word embeddings, Brown clusters, part-of-speech tags, shape features, gazetteers, and local context to create a feature representation along with a set of experiments for the network design. A Wikipedia-based classifier framework was adopted to extract lists of fine-grained entities out of few input examples to be used as gazetteers. The model uses the LSTM algorithm to learn a NE classifier from the feature representation. UQAM-NTL (Ngoc Tan LE and Sadat, 2016) The system is based on supervised machine learning and trained with a sequential labeling algorithm, using Conditional Random Fields to learn a classifier for Twitter NE extraction. The model uses 6 different categories of features including (1) orthographic, (2) lexical and (3) syntactic features as well as (4) part-of-speech tags, (5) polysemy count and (6) longest n-gram length in order to create a feature representation. 3 Summary In this paper, we presented a shared task for Named Entity Recognition in Twitter data. We detailed the task setup and datasets used in the respective shared ta"
W16-3919,D11-1141,1,0.92682,"with general domain data. Both the time period and topic selection of the evaluation data were not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. Teams had 7 days to submit their results on the test data, which were subsequently scored and gold annotations were released to participants. Evaluating the NER systems on these domains specific Twitter data provides information about possible system weakness. 2.1 Training and Development Data The training and development data for our task was taken from prior work on Twitter NER (Ritter et al., 2011; Baldwin et al., 2015), which distinguishes 10 different named entity types (see Table 1 for the set of types). The training data was created from the union of the training and development data from the 2015 task (Baldwin et al., 2015). The data was split into 2,394 annotated tweets for training and 1,000 as a development set. We also provided an additional 425 annotated tweets from the 2015 development data set (Baldwin et al., 2015). 2.2 Test Data Annotation The data set we created for testing is new for this shared task. We collected general Twitter data and domain specific Twitter data. I"
W16-3919,W16-3922,0,0.121077,"Missing"
W16-3919,W03-0419,0,0.420022,"Missing"
W16-4012,2011.mtsummit-papers.12,0,0.0303415,"n of Blum and Mitchell’s (1998) co-training (the output of one tagger is used to train another) could be implemented without seed rules, yet, Pierce and Cardie’s (2001) assessment of the limitations of co-training shows that when “all the classes are [not] represented according to their prior probabilities in every region in the feature space”, as when we adapt to new domains in Perseus, we get Charniak’s (1997) result where mistakes are magnified, not smoothed. Active-learning, as opposed to self-training, allows the learner to query the user for additional annotation. Lynn et al. (2012) and Ambati et al. (2011) suggest that this is an effective solution for low-resource languages when self-training fails. Following Cohn et al. (1994), we modify the Query by Uncertainty tactic, where the tagger selects informative sentences based on how uncertain it is of the correct tag sequence and sends these to be annotated. Our modifications ensure that the most useful sentences are 89 Figure 1: Learning Curves for our 3 folds demonstrate a strong negative relationship between Remaining UNK’s and Accuracy. Remaining UNK’s is the number of unknown words in what is left of the test set normalized by the total numb"
W16-4012,W16-2110,0,0.026837,"filtering out any components that the analyzer considers impossible. Furthermore, we combine the output of the tagger and analyzer with our own set of rules, thereby deducing a lemma (if one failed to be identified by the tagger or analyzer), component morphemes, and number (singular/plural), all to be used as features. Like Farber et al. (2008), which uses a similar process to leverage morpho-syntactic information in morphologically rich, low-resource Arabic, we too find that filtering POS tag output cuts down on noise, boosting accuracy. By additionally leveraging the newly enhanced LEMLAT (Budassi and Passarotti, 2016) morphological analyzer, or even substituting it for Whitaker’s (1993), which has a smaller lexical base and struggles with graphical 87 Table 1: Fold 1 tests on Ep, trains on the remaining annotated data. 2 tests on AA, and 3 on books 2 and 7 of BG which, when concatenated, resemble the lengths of the other 2 test sets. 3 tests “in-domain” as the training set is mostly from the same domain, i.e. the other 6 books of BG, the historiography domain. Table 2: UNK’s are words unseen in training, F, F-score, Prec, precision, Rec, recall, and GAZ, gazetteer. variants, we expect even further gains in"
W16-4012,W99-0613,0,0.245566,"ity 1 words. Forms like Video, “I see”, which also do not appear in training but do show uncapitalized variants, are still challenging but much less so as most are non-NE’s, only capitalized when sentence initial. We refer to these as Priority 2 words; however, these tend to be more frequent than priority 1’s. We consider this tradeoff between difficulty and frequency as we tailor our pipeline to better handle capitalized UNK’s. 4 Semi-Supervised Model Semi-supervised learning involves supplementing with unannotated data during training. Liao and Veeramachaneni (2009), Rani et al. (2014), and Collins and Singer (1999) show that self-training, where unannotated data is used for training without querying the user, can overcome data sparsity or gaps between training and testing domains. However, the first two identify high precision unannotated sentences by relying on seed rules which are difficult to develop for Latin. While Liao and Veeramachaneni (2009) can rely on any capitalized word following a PRS to be part of the same NE, Latin’s free word order frequently allows NE’s from entirely different syntactic constituents to appear adjacent to one another, as in Caesar [PRS] Haeduos [GRP] frumentum ... flagi"
W16-4012,P05-1045,0,0.0134187,"NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple occurrences of the same word type (Finkel et al., 2005). 3.2 Our Model Our model, like Stanford’s, is a CRF using similar features, though we alter ours to suit our language and corpus. We employ a POS tagger (Schmid, 1999) to leverage the highly informative morphological complexity of Latin. Finkel et al. (2005) claim only a negligible boost from using POS features in English NER and thus do not include them in the off-the-shelf version, yet we find that when implemented with creativity, the output of even a low accuracy POS tagger can be beneficial for Latin NER. For each token, we deconstruct the fine-grained POS tag into component parts rangin"
W16-4012,farber-etal-2008-improving,0,0.0325477,"each token, we deconstruct the fine-grained POS tag into component parts ranging from case and mood distinctions to coarser distinctions between syntactic categories like nouns and verbs. We then run each token through a rule-based morphological analyzer (Whitaker, 1993), filtering out any components that the analyzer considers impossible. Furthermore, we combine the output of the tagger and analyzer with our own set of rules, thereby deducing a lemma (if one failed to be identified by the tagger or analyzer), component morphemes, and number (singular/plural), all to be used as features. Like Farber et al. (2008), which uses a similar process to leverage morpho-syntactic information in morphologically rich, low-resource Arabic, we too find that filtering POS tag output cuts down on noise, boosting accuracy. By additionally leveraging the newly enhanced LEMLAT (Budassi and Passarotti, 2016) morphological analyzer, or even substituting it for Whitaker’s (1993), which has a smaller lexical base and struggles with graphical 87 Table 1: Fold 1 tests on Ep, trains on the remaining annotated data. 2 tests on AA, and 3 on books 2 and 7 of BG which, when concatenated, resemble the lengths of the other 2 test s"
W16-4012,W09-2208,0,0.0843949,"es Institute Latin Libraries corpus (Packard Humanities Institute, 1992). Because no gold data was annotated to train or evaluate this system, our small annotated corpus presents the first opportunity to gauge its performance. Stanford’s NER system is a more sophisticated conditional random field (CRF) model, although, being fully supervised in its off-theshelf implementation, it lacks the extensive vocabulary that the CLTK system has access to. CRF’s are undirected graphical models trained to maximize the conditional probability of a sequence of labels given the corresponding input sequence (Liao and Veeramachaneni, 2009), and are especially effective in NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple o"
W16-4012,U12-1005,0,0.0224543,"Missing"
W16-4012,W03-0430,0,0.0441248,"ore sophisticated conditional random field (CRF) model, although, being fully supervised in its off-theshelf implementation, it lacks the extensive vocabulary that the CLTK system has access to. CRF’s are undirected graphical models trained to maximize the conditional probability of a sequence of labels given the corresponding input sequence (Liao and Veeramachaneni, 2009), and are especially effective in NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple occurrences of the same word type (Finkel et al., 2005). 3.2 Our Model Our model, like Stanford’s, is a CRF using similar features, though we alter ours to suit our language and corpus. We employ a POS tagger (Schmid, 1999) to leverage the highly i"
W16-4012,W01-0501,0,0.100693,"Missing"
W16-4012,L16-1108,0,0.014621,"es fine-grained distinctions among NE’s, and Perseus’ digital gazetteers, mark-ups of Smith (1854; 1870; 1890), cover just individual persons (PRS) and geographical place names (GEO), not group names (GRP), which we want also to be able to recognize.1 Additionally, the reliability of part-of-speech taggers (e.g. Schmid (1999) trained by Gabriele Brandolini (http://www.cis.uni-muenchen.de/˜schmid/tools/TreeTagger/data/ latin-par-linux-3.2.bin.gz) and Johnson et al. (2014)), dependency parsers (Bamman and Crane, 2009), and semantic models (Johnson et al., 2014) is very low due to data-sparsity. Ponti and Passarotti (2016) actually demonstrate reliable syntactic dependency parsing on Medieval Latin using the Index Thomisticus Treebank (McGillivray et al., 2009), but find that more work is needed to successfully adapt their model to handle other varieties of Latin. In broad terms, generalizing from linguistic patterns recognized in one Latin text when processing another is difficult because “a series of particular historical, geographical and cultural circumstances [led] to an inhomogeneous linguistic system where elements from different areas and registers met and were only partially transmitted by the sources”"
W17-5405,D12-1096,0,0.0134449,"ight the distance which even sophisticated, modern sentiment analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete"
W17-5405,P14-2084,0,0.022055,"analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete metaphors), and it was difficult to create many instances"
W17-6514,W11-3405,0,0.61958,"), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potential dependency annotation inconsistencies, and evaluate it on three of the UD v2 corpora (English, French and Finnish). The original Boyd et al. method finds pairs of words i"
W17-6514,W11-0108,0,0.0180377,"rucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potential dependency annotation incons"
W17-6514,E03-1068,0,0.538508,"linked differently. Boyd et al. also experimented with a “dependency context heuristic” requiring the governors of the dependency pairs to have the same incoming dependency relation. They also considered the case of pairs of words which are linked by a dependency relation in some instances and not linked by any relation in other instances, but required for those cases that the internal context between the two words be exactly the same. advmod cop nummod (1) a. Here ’s two examples : nsubj cop Boyd et al. (2008) extend, to dependency representation, the concept of variation nuclei developed by Dickinson and Meurers (2003b; 2005) for identifying inconsistent annotations in phrasestructure trees. Variation nuclei are elements which occur multiple times in a corpus with varying annotation. For phrase-structure trees, a variation nucleus is any n-gram for which bracketing or labeling varies, with one shared word of context on each side of the n-gram. Figure 1, from Boyd et al. (2008), shows an example of a 5-gram, its biggest jolt last month, which receives two different analyses in the Penn TreeBank. For dependency representation, the basic elements are dependencies, i.e. pairs of words linked by a labeled depen"
W17-6514,P05-1040,0,0.66456,"Missing"
W17-6514,A00-2020,0,0.132573,"are followed, and crucially that similar phenomena do receive a consistent analysis within and across corpora. Given the recent success of the Universal Dependencies (UD) project1 which aims at building cross-linguistically consistent treebanks for many languages and the rapid creation of 74 corpora for 51 languages supposedly following the UD scheme, investigating the quality of the dependency annotations and improving their consistency is, more than ever, of crucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of A"
W17-6514,W04-1905,0,0.0982428,"Missing"
W17-6514,P13-4010,0,0.0655167,"Missing"
W17-6514,N15-3011,1,0.839952,"istency is, more than ever, of crucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potentia"
W17-6514,nivre-etal-2006-talbanken05,0,0.0580503,"Missing"
W17-6514,L16-1262,1,0.901229,"Missing"
W17-6514,stepanek-pajas-2010-querying,0,0.0586947,"Missing"
W17-6514,L16-1680,0,0.0410951,"Missing"
W17-6514,E14-2015,0,0.0383523,"Missing"
W17-6514,W00-1907,0,\N,Missing
W18-5526,N06-2015,0,0.104217,"Missing"
W18-5526,P14-5010,0,0.00290508,"ith a parenthesis description mapped to its root title without parenthesis. These are used as “backup” documents to be searched if no evidence is found in documents returned with the two other maps. 2.2 3 Once all potential documents are collected by the Document Finder, each sentence within each document is compared against the claim to see if it is similar enough to be considered relevant evidence. Key Phrase Identification The key phrases aim at capturing the “topic” of the claim. We used capitalization, named entity, phrasal and part-of-speech tags, and dependency from the CoreNLP system (Manning et al., 2014) to identify key phrases. Subject, direct object, and their modifier/complement dependencies are marked as “topics”. Noun phrases containing those topic words are considered key phrases. Consecutive capitalized terms, allowing for lowercase words not capitalized in titles such as prepositions and determiners, are also considered key phrases. For instance, the key phrases for the claims in Figure 1 are: Ann Richards, politics; Andrew Kevin Walker. Once all possible key phrases in a claim are found, each key phrase is checked against the maps of Wikipedia titles: if there is a full match between"
W18-5526,D16-1244,0,0.235789,"Missing"
W18-5526,D14-1162,0,0.0796704,"Missing"
W18-5526,N18-1074,0,0.128245,"Missing"
W18-5526,D15-1075,0,0.0363538,"ent sentence contains that same verb (or a synonym of the verb). When the claim contains reference to either a birth or a death, the document sentence needs only to have a date encompassed within a set of parenthesis to be considered a valid piece of evidence. 3.3 4 Inference Once evidence sentences are retrieved, we used one of the state-of-the-art inference systems to assess whether the sentences verify the claim or not. We chose the decomposable attention model of (Parikh et al., 2016) because it is one of the highest-scored systems on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) that has a lightweight architecture with relatively few parameters. 4.1 Preprocessing Most of the evidence sentences are often in the middle of a paragraph in the document, and the entity the document is about is referred to with a pronoun or a definite description. For instance, The Southwest Golf Classic, in its Wikipedia article, is referred to with the pronoun it or the noun phrase the event. We thus made the simplifying assumption that each pronoun is used to refer to the entity the page is about, and perform a deterministic coreference resolution by replacing the first pronoun in the se"
