2021.naacl-main.154,Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment,2021,-1,-1,3,0,1875,ethan chi,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose *iterative realignment*, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup; on LibriSpeech, we reach an LM-free test-other WER of 9.0{\%} (19{\%} relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine."
2021.ecnlp-1.3,{ASR} Adaptation for {E}-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling,2021,-1,-1,3,0,10424,ashish shenoy,Proceedings of The 4th Workshop on e-Commerce and NLP,0,"Automatic Speech Recognition (ASR) robustness toward slot entities are critical in e-commerce voice assistants that involve monetary transactions and purchases. Along with effective domain adaptation, it is intuitive that cross utterance contextual cues play an important role in disambiguating domain specific content words from speech. In this paper, we investigate various techniques to improve contextualization, content word robustness and domain adaptation of a Transformer-XL neural language model (NLM) to rescore ASR N-best hypotheses. To improve contextualization, we utilize turn level dialogue acts along with cross utterance context carry over. Additionally, to adapt our domain-general NLM towards e-commerce on-the-fly, we use embeddings derived from a finetuned masked LM on in-domain data. Finally, to improve robustness towards in-domain content words, we propose a multi-task model that can jointly perform content word detection and language modeling tasks. Compared to a non-contextual LSTM LM baseline, our best performing NLM rescorer results in a content WER reduction of 19.2{\%} on e-commerce audio test set and a slot labeling F1 improvement of 6.4{\%}."
2020.nlpmc-1.8,Robust Prediction of Punctuation and Truecasing for Medical {ASR},2020,-1,-1,5,0,16097,monica sunkara,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,0,"Automatic speech recognition (ASR) systems in the medical domain that focus on transcribing clinical dictations and doctor-patient conversations often pose many challenges due to the complexity of the domain. ASR output typically undergoes automatic punctuation to enable users to speak naturally, without having to vocalize awkward and explicit punctuation commands, such as {``}period{''}, {``}add comma{''} or {``}exclamation point{''}, while truecasing enhances user readability and improves the performance of downstream NLP tasks. This paper proposes a conditional joint modeling framework for prediction of punctuation and truecasing using pretrained masked language models such as BERT, BioBERT and RoBERTa. We also present techniques for domain and task specific adaptation by fine-tuning masked language models with medical domain data. Finally, we improve the robustness of the model against common errors made in ASR by performing data augmentation. Experiments performed on dictation and conversational style corpora show that our proposed model achieves 5{\%} absolute improvement on ground truth text and 10{\%} improvement on ASR outputs over baseline models under F1 metric."
2020.acl-main.240,Masked Language Model Scoring,2020,52,0,4,1,3722,julian salazar,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model{'}s WER by 30{\%} relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL{'}s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring."
W19-5906,"Simple, Fast, Accurate Intent Classification and Slot Labeling for Goal-Oriented Dialogue Systems",2019,27,0,3,0,23734,arshit gupta,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"With the advent of conversational assistants, like Amazon Alexa, Google Now, etc., dialogue systems are gaining a lot of traction, especially in industrial setting. These systems typically consist of Spoken Language understanding component which, in turn, consists of two tasks - Intent Classification (IC) and Slot Labeling (SL). Generally, these two tasks are modeled together jointly to achieve best performance. However, this joint modeling adds to model obfuscation. In this work, we first design framework for a modularization of joint IC-SL task to enhance architecture transparency. Then, we explore a number of self-attention, convolutional, and recurrent models, contributing a large-scale analysis of modeling paradigms for IC+SL across two datasets. Finally, using this framework, we propose a class of {`}label-recurrent{'} models that otherwise non-recurrent, with a 10-dimensional representation of the label history, and show that our proposed systems are easy to interpret, highly accurate (achieving over 30{\%} error reduction in SL over the state-of-the-art on the Snips dataset), as well as fast, at 2x the inference and 2/3 to 1/2 the training time of comparable recurrent models, thus giving an edge in critical real-world systems."
W18-1806,Context Models for {OOV} Word Translation in Low-Resource Languages,2018,23,2,2,0,26127,angli liu,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track),0,"Out-of-vocabulary word translation is a major problem for the translation of low-resource languages that suffer from a lack of parallel training data. This paper evaluates the contributions of target-language context models towards the translation of OOV words, specifically in those cases where OOV translations are derived from external knowledge sources, such as dictionaries. We develop both neural and non-neural context models and evaluate them within both phrase-based and self-attention based neural machine translation systems. Our results show that neural language models that integrate additional context beyond the current sentence are the most effective in disambiguating possible OOV word translations. We present an efficient second-pass lattice-rescoring method for wide-context neural language models and demonstrate performance improvements over state-of-the-art self-attention based neural MT systems in five out of six low-resource language pairs."
W16-6107,Unsupervised Resolution of Acronyms and Abbreviations in Nursing Notes Using Document-Level Context Models,2016,0,2,1,1,3723,katrin kirchhoff,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
N15-1102,Morphological Modeling for Machine Translation of {E}nglish-Iraqi {A}rabic Spoken Dialogs,2015,24,3,1,1,3723,katrin kirchhoff,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper addresses the problem of morphological modeling in statistical speech-tospeech translation for English to Iraqi Arabic. An analysis of user data from a real-time MT-based dialog system showed that generating correct verbal inflections is a key problem for this language pair. We approach this problem by enriching the training data with morphological information derived from sourceside dependency parses. We analyze the performance of several parsers as well as the effect on different types of translation models. Our method achieves an improvement of more than a full BLEU point and a significant increase in verbal inflection accuracy; at the same time, it is computationally inexpensive and does not rely on target-language linguistic tools."
D14-1014,Submodularity for Data Selection in Machine Translation,2014,32,29,1,1,3723,katrin kirchhoff,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,None
N13-1086,Using Document Summarization Techniques for Speech Data Subset Selection,2013,12,46,3,0,10449,kai wei,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,In this paper we leverage methods from submodular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data. We evaluate our results on data subset selection for a phone recognition task. Our framework shows significant improvements over random selection and previously proposed methods using a similar amount of resources.
2013.mtsummit-wptp.4,Integrated post-editing and translation management for lay user communities,2013,-1,-1,4,0,41864,adrian laurenzi,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2012.eamt-1.35,Evaluating User Preferences in Machine Translation Using Conjoint Analysis,2012,19,6,1,1,3723,katrin kirchhoff,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"In spite of much ongoing research on machine translation evaluation there is little quantitative work that directly measures usersxe2x80x99 intuitive or emotional preferences regarding different types of machine translation errors. However, the elicitation and modeling of user preferences is an important prerequisite for future research on user adaptation and customization of machine translation engines. In this paper we explore the use of conjoint analysis as a formal quantitative framework to gain insight into usersxe2x80x99 relative preferences for different translation error types. Using English-Spanish as the translation direction we conduct a crowd-sourced conjoint analysis study and obtain utility values for individual error types. Our results indicate that word order errors are clearly the most dispreferred error type, followed by word sense, morphological, and function word errors."
2012.amta-papers.29,Unsupervised Translation Disambiguation for Cross-Domain Statistical Machine Translation,2012,-1,-1,2,1,43881,mei yang,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Most attempts at integrating word sense disambiguation with statistical machine translation have focused on supervised disambiguation approaches. These approaches are of limited use when the distribution of the test data differs strongly from that of the training data; however, word sense errors tend to be especially common under these conditions. In this paper we present different approaches to unsupervised word translation disambiguation and apply them to the problem of translating conversational speech under resource-poor training conditions. Both human and automatic evaluation metrics demonstrate significant improvements resulting from our technique."
W10-4357,Hand Gestures in Disambiguating Types of You Expressions in Multiparty Meetings,2010,13,0,3,0,12160,tyler baldwin,Proceedings of the {SIGDIAL} 2010 Conference,0,"The second person pronoun you serves different functions in English. Each of these different types often corresponds to a different term when translated into another language. Correctly identifying different types of you can be beneficial to machine translation systems. To address this issue, we investigate disambiguation of different types of you occurrences in multiparty meetings with a new focus on the role of hand gesture. Our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic use of you (e.g., refer to people in general) and the referential use of you (e.g., refer to a specific person or a group of people). Incorporation of gesture can also compensate for limitations in automated language processing (e.g., reliable recognition of dialogue acts) and achieve comparable results."
C10-1138,Contextual Modeling for Meeting Translation Using Unsupervised Word Sense Disambiguation,2010,29,3,2,1,43881,mei yang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"In this paper we investigate the challenges of applying statistical machine translation to meeting conversations, with a particular view towards analyzing the importance of modeling contextual factors such as the larger discourse context and topic/domain information on translation performance. We describe the collection of a small corpus of parallel meeting data, the development of a statistical machine translation system in the absence of genre-matched training data, and we present a quantitative analysis of translation errors resulting from the lack of contextual modeling inherent in standard statistical machine translation systems. Finally, we demonstrate how the largest source of translation errors (lack of topic/domain knowledge) can be addressed by applying document-level, unsupervised word sense disambiguation, resulting in performance improvements over the baseline system."
N09-1014,Graph-based Learning for Statistical Machine Translation,2009,23,35,2,1,47333,andrei alexandrescu,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Current phrase-based statistical machine translation systems process each test sentence in isolation and do not enforce global consistency constraints, even though the test data is often internally consistent with respect to topic or style. We propose a new consistency model for machine translation in the form of a graph-based semi-supervised learning algorithm that exploits similarities between training and test data and also similarities between different test sentences. The algorithm learns a regression function jointly over training and test data and uses the resulting scores to rerank translation hypotheses. Evaluation on two travel expression translation tasks demonstrates improvements of up to 2.6 BLEU points absolute and 2.8% in PER."
2009.iwslt-evaluation.19,The {U}niversity of {W}ashington machine translation system for {IWSLT} 2009,2009,0,1,4,1,43881,mei yang,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Washington{'}s system for the 2009 International Workshop on Spoken Language Translation (IWSLT) evaluation campaign. Two systems were developed, one each for the BTEC Chinese-to-English and Arabic-to-English tracks. We describe experiments with different preprocessing and alignment combination schemes. Our main focus this year was on exploring a novel semi-supervised approach to N-best list reranking; however, this method yielded inconclusive results."
W08-0314,The {U}niversity of {W}ashington Machine Translation System for {ACL} {WMT} 2008,2008,27,2,4,0.833333,18812,amittai axelrod,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper present the University of Washington's submission to the 2008 ACL SMT shared machine translation task. Two systems, for English-to-Spanish and German-to-Spanish translation are described. Our main focus was on testing a novel boosting framework for N-best list reranking and on handling German morphology in the German-to-Spanish system. While boosted N-best list reranking did not yield any improvements for this task, simplifying German morphology as part of the preprocessing step did result in significant gains."
P08-2010,Beyond Log-Linear Models: Boosted Minimum Error Rate Training for N-best Re-ranking,2008,10,26,2,1,5136,kevin duh,"Proceedings of ACL-08: HLT, Short Papers",0,"Current re-ranking algorithms for machine translation rely on log-linear models, which have the potential problem of underfitting the training data. We present BoostedMERT, a novel boosting algorithm that uses Minimum Error Rate Training (MERT) as a weak learner and builds a re-ranker far more expressive than log-linear models. BoostedMERT is easy to implement, inherits the efficient optimization properties of MERT, and can quickly boost the BLEU score on N-best re-ranking tasks. In this paper, we describe the general algorithm and present preliminary results on the IWSLT 2007 Arabic-English task."
N07-1026,Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in {NLP},2007,20,20,2,1,47333,andrei alexandrescu,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing. All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points. The problem of how to best construct this graph remains largely unsolved. In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classier . We apply this technique in the framework of label propagation and evaluate it on two different classication tasks, a multi-class lexicon acquisition task and a word sense disambiguation task. Signicant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classiers."
2007.mtsummit-papers.39,Semi-automatic error analysis for large-scale statistical machine translation,2007,-1,-1,1,1,3723,katrin kirchhoff,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.iwslt-1.13,The {U}niversity of {W}ashington machine translation system for the {IWSLT} 2007 competition,2007,11,5,1,1,3723,katrin kirchhoff,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"This paper presents the University of Washington{'}s submission to the 2007 IWSLT benchmark evaluation. The UW system participated in two data tracks, Italian-to-English and Arabic-to-English. Our main focus was on incorporating out-of-domain data, which contributed to improvements for both language pairs in both the clean text and ASR output conditions. In addition, we compared supervised and semi-supervised preprocessing schemes for the Arabic-to-English task and found that the semi-supervised scheme performs competitively with the supervised algorithm while using a fraction of the run-time."
W06-1647,Lexicon Acquisition for Dialectal {A}rabic Using Transductive Learning,2006,15,11,2,1,5136,kevin duh,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic. Developing a high-quality lexicon is often the first step towards building a POS tagger, which is in turn the front-end to many NLP systems. We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method. We demonstrate that lexicon learning is an important task in resource-poor domains and leads to significant improvements in tagging accuracy for dialectal Arabic."
N06-2001,Factored Neural Language Models,2006,11,63,2,1,47333,andrei alexandrescu,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models."
E06-1006,Phrase-Based Backoff Models for Machine Translation of Highly Inflected Languages,2006,18,59,2,1,43881,mei yang,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and FinnishEnglish translation and shows improvements over state-of-the-art phrase-based models.
2006.iwslt-evaluation.21,The {U}niversity of {W}ashington machine translation system for {IWSLT} 2006,2006,0,3,1,1,3723,katrin kirchhoff,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2006.amta-papers.22,Ambiguity Reduction for Machine Translation: Human-Computer Collaboration,2006,-1,-1,4,0,49421,marcus sammer,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Statistical Machine Translation (SMT) accuracy degrades when there is only a limited amount of training, or when the training is not from the same domain or genre of text as the target application. However, cross-domain applications are typical of many real world tasks. We demonstrate that SMT accuracy can be improved in a cross-domain application by using a controlled language (CL) interface to help reduce lexical ambiguity in the input text. Our system, CL-MT, presents a monolingual user with a choice of word senses for each content word in the input text. CL-MT temporarily adjusts the underlying SMT system's phrase table, boosting the scores of translations that include the word senses preferred by the user and lowering scores for disfavored translations. We demonstrate that this improves translation adequacy in 33.8{\%} of the sentences in Spanish to English translation of news stories, where the SMT system was trained on proceedings of the European Parliament."
W05-0821,Improved Language Modeling for Statistical Machine Translation,2005,8,36,1,1,3723,katrin kirchhoff,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model."
W05-0708,{POS} Tagging of Dialectal {A}rabic: A Minimally Supervised Approach,2005,19,36,2,1,5136,kevin duh,Proceedings of the {ACL} Workshop on Computational Approaches to {S}emitic Languages,0,"Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic. In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic. We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-of-the-art Modern Standard Arabic tagger applied to Egyptian Arabic."
H05-1125,The Vocal Joystick: A Voice-Based Human-Computer Interface for Individuals with Motor Impairments,2005,31,66,6,1,37500,jeff bilmes,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a novel voice-based human-computer interface designed to enable individuals with motor impairments to use vocal parameters for continuous control tasks. Since discrete spoken commands are ill-suited to such tasks, our interface exploits a large set of continuous acoustic-phonetic parameters like pitch, loudness, vowel quality, etc. Their selection is optimized with respect to automatic recognizability, communication bandwidth, learnability, suitability, and ease of use. Parameters are extracted in real time, transformed via adaptation and acceleration, and converted into continuous control signals. This paper describes the basic engine, prototype applications (in particular, voice-based web browsing and a controlled trajectory-following task), and initial user studies confirming the feasibility of this technology."
W04-1612,Automatic Diacritization of {A}rabic for Acoustic Modeling in Speech Recognition,2004,9,84,2,0,47327,dimitra vergyri,Proceedings of the Workshop on Computational Approaches to {A}rabic Script-based Languages,0,"Automatic recognition of Arabic dialectal speech is a challenging task because Arabic dialects are essentially spoken varieties. Only few dialectal resources are available to date; moreover, most available acoustic data collections are transcribed without diacritics. Such a transcription omits essential pronunciation information about a word, such as short vowels. In this paper we investigate various procedures that enable us to use such training data by automatically inserting the missing diacritics into the transcription. These procedures use acoustic information in combination with different levels of morphological and contextual constraints. We evaluate their performance against manually diacritized transcriptions. In addition, we demonstrate the effect of their accuracy on the recognition performance of acoustic models trained on automatically diacritized training data."
C04-1022,Automatic Learning of Language Model Structure,2004,47,30,2,1,5136,kevin duh,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Statistical language modeling remains a challenging task, in particular for morphologically rich languages. Recently, new approaches based on factored language models have been developed to address this problem. These models provide principled ways of including additional conditioning variables other than the preceding words, such as morphological or syntactic features. However, the number of possible choices for model parameters creates a large space of models that cannot be searched exhaustively. This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish)."
W03-0703,Directions For Multi-Party Human-Computer Interaction Research,2003,10,3,1,1,3723,katrin kirchhoff,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Research Directions in Dialogue Processing,0,"Research on dialog systems has so far concentrated on interactions between a single user and a machine. In this paper we identify novel research directions arising from multi-party human computer interaction, i.e. scenarios where several human participants interact with a dialog system."
N03-2002,Factored Language Models and Generalized Parallel Backoff,2003,9,253,2,0,37500,jeff bilmes,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant."
