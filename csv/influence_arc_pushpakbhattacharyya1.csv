2005.mtsummit-papers.14,C94-2195,0,0.127539,"rse tree. They are a group of entities which demand semantic relations or speech act attributes when the semantic representation of the sentence is ultimately produced. The generation of these SRSs is motivated by the need to generate UNL in a robust and scalable manner. The linguistic insight for the paper is obtained from the following related works: Chomsky (1981), Jackendoff (1990), Levin (1993), Mohanty et. al.(2004, 2005). In NLP, PP-attachment is a classical problem, that has been studied by several researchers, such as, Hindle and Rooth (1993), Dorr (1994), Ratnaparkhi et. al. (1994), Brill and Resnik (1994), Alda and Patrick (2003), Kordoni (2003), Niemann (2003); among others. In spite of years of research on PP-attachment, it is found that none of the parsers resolve PPs accurately. Our work is ultimately an exercise in knowledge representation; the knowledge representation problem has been extensively discussed in the classical treatises by Dorr (1992), Schank (1972), Sowa (2000) and Woods (1985). Inerlingua representations have been studied in the machine translation literature (Hutchins and Somers 1992). One of the early noteworthy interlingua based MT systems is Atlas-II (Uchida, 1989); th"
2005.mtsummit-papers.14,P03-1054,0,0.00436309,"ch demand semantic relations or speech act attributes when the semantic representation of the sentence is ultimately produced. The generation of these SRSs is motivated by the need to generate UNL in a robust and scalable manner. The linguistic insight for the paper is obtained from the following related works: Chomsky (1981), Jackendoff (1990), Levin (1993), Mohanty et. al.(2004, 2005). In NLP, PP-attachment is a classical problem, that has been studied by several researchers, such as, Hindle and Rooth (1993), Dorr (1994), Ratnaparkhi et. al. (1994), Brill and Resnik (1994), Alda and Patrick (2003), Kordoni (2003), Niemann (2003); among others. In spite of years of research on PP-attachment, it is found that none of the parsers resolve PPs accurately. Our work is ultimately an exercise in knowledge representation; the knowledge representation problem has been extensively discussed in the classical treatises by Dorr (1992), Schank (1972), Sowa (2000) and Woods (1985). Inerlingua representations have been studied in the machine translation literature (Hutchins and Somers 1992). One of the early noteworthy interlingua based MT systems is Atlas-II (Uchida, 1989); the comparison of the inter"
2005.mtsummit-papers.14,P98-2177,0,0.0217662,"Missing"
2005.mtsummit-papers.14,C69-0201,0,0.701671,"f (1990), Levin (1993), Mohanty et. al.(2004, 2005). In NLP, PP-attachment is a classical problem, that has been studied by several researchers, such as, Hindle and Rooth (1993), Dorr (1994), Ratnaparkhi et. al. (1994), Brill and Resnik (1994), Alda and Patrick (2003), Kordoni (2003), Niemann (2003); among others. In spite of years of research on PP-attachment, it is found that none of the parsers resolve PPs accurately. Our work is ultimately an exercise in knowledge representation; the knowledge representation problem has been extensively discussed in the classical treatises by Dorr (1992), Schank (1972), Sowa (2000) and Woods (1985). Inerlingua representations have been studied in the machine translation literature (Hutchins and Somers 1992). One of the early noteworthy interlingua based MT systems is Atlas-II (Uchida, 1989); the comparison of the interlingua approach Abstract Motivated by the fact that automatic analysis of language crucially depends on semantic constituent detection and attachment resolution, we present our work on the problem of generating and linking semantically relatable sets (SRS). These sets are of the form &lt;entity1 entity2> or &lt;entity1 function-word entity2> or &lt;fun"
2007.mtsummit-papers.56,1991.mtsummit-papers.3,0,0.107918,"Missing"
2007.mtsummit-papers.56,C92-3168,0,0.113179,"ts of parallel development of various knowledge resources for analyzing source language sentences and generating target language sentences. Being at the top of the Vauquois Triangle (Hutchins and Somers 1992), elaborate knowledge bases and tools are needed for morphological, syntactic, and semantic processing, both for analysis and generation. The UNL representation has the right level of expressive power and granularity. UNL has 45 semantic relations and 87 attributes (which can be augmented with the user defined ones) to express the semantic content of a sentence. In 1992, Interlingua KANT (Nyberg and Mitamura 1992) was designed for large scale MT of technical documentation. However, KANT is a sublanguage system, and handles only constrained technical English. Many phenomena are left out of consideration, which are handled by UNL. UNITRAN- the Interlingua and the eponymous MT- is too detailed a framework for meaningful practical implementation (Dorr 1993). Though traditionally, language analysis has held sway over language generation- as it involves various disambiguation tasks- early 90s saw the reemergence of Natural Language Generation (NLG) problem, mainly because of the fluency and adequacy requirem"
2007.mtsummit-papers.56,P02-1040,0,0.0756686,"Missing"
2007.mtsummit-papers.56,C69-0201,0,0.477958,"d is necessarily restricted in its scope for handling language phenomena. UNL has been influenced by a number of linguisticsheavy Interlingua based Japanese MT systems in the 1980s- notably the ATLAS-II system [Uchida 1989]. However, the presence of researchers from Indo-Iranian, Germanic and Baltic-Slavic language families in the committee for UNL specifications (UNL Specifications 2005, www.undl.org) since 2000, has lent UNL a much more universal character compared to the interlingua used in ATLAS-II. Comparing and contrasting UNL with primitive based interlingua like Conceptual Dependency (Schank 1972) and Conceptual Structures (Sowa 2000), we observe that like UNITRAN, they too are too detailed to admit of practical implementations. 4. Language Generation khatav Figure 1: The UNL Graph for UNL Expression 4 The phrase ‘Manchar region or of Khatav taluka’ is considered as being within a scope. Note that the scope is given a compound UW ID:01 to denote a separate environment of knowledge representation. UNL relations help representing the argument frame of the sentence and also draw a distinction between the argument and the non-argument links of a predicate. The information for number, tense"
2007.mtsummit-papers.56,1999.mtsummit-1.34,0,0.0340731,"Missing"
2010.jeptalnrecital-court.7,W09-3536,1,0.859817,"Missing"
2010.jeptalnrecital-court.7,C08-1068,1,0.872671,"Missing"
2016.gwc-1.22,W14-0130,1,0.822236,"kta as a separate character before search. 3.3.2 Morphological Analysis Before searching in the databases the word is first passed to a morphological analyzer to obtain its root form. We use Hindi Morph Analyzer (Bahuguna et al., 2014) to return the root form of the input word for Hindi language, since by principle, WordNet only contains root forms of the words. Due to non availability of other language Morphological Analyzers, we may not be able to include them in the search process. Though, in the future, we can use a fully automated version of the “Human mediated TRIE base generic stemmer”(Bhattacharyya et al., 2014) for obtaining root forms for other languages later. 3.3.3 Handling Multiple Root forms Figure 2: Devanagari Keyboard tion using Google Transliteration API5 , and a JavaScript based online keyboard (Figure: 2) for input of Hindi Unicode characters. Transliteration for a native user is very convenient. In case, the user does not know the right combination of keys then the keyboard for Devanagari is provided. These two methods ensure that all words can be easily entered for searching. Thereafter, by touching / clicking on “Search”, the synsets with all relevant information are retrieved. 3.3 Sea"
2016.gwc-1.23,W09-3401,0,0.0240783,"Princeton WordNet which form the basis of our query for the OpenClipArt API. We download the images via their URLs, and store them locally, to map them to Hindi WordNet5 (Dipak Narayan and Bhattacharyya, 2002) synset IDs later. The paper is organized as follows. In section 2, we describe our related work. In section 3 and 4, we describe our architecture, and the retrieval procedure along with the scoring algorithm. We describe the results obtained in Section 5. We describe the evaluation tool and qualitative analysis in sections 6 and 7, respectively. We conclude in section 8. 2 Related Work Bond et al. (2009) used OCAL to enhance the Japanese WordNet, and were able to mine 874 links for 541 synsets. On the basis of manual scoring they found 62 illustrations which were best suited for the sense, 642 illustrations to be a good representation, and 170 suitable, but imperfect illustrations. We extend their work for IndoWordNet, and use OCAL to mine the illustrations. Imagenet(Deng et al., 2009) is a similar project for Princeton WordNet which provides images/URLs for a concept. It contains 21841 synsets indexed with 14,197,122 images. We present a much simpler methodology of collecting images from the"
2016.gwc-1.37,van-assem-etal-2006-conversion,0,0.0318841,"Missing"
2016.gwc-1.4,W09-2403,0,0.066777,"Missing"
2016.gwc-1.46,W14-5115,1,0.432007,"put words. This gloss or a concept definition of a synset is given to Paraphrase Validator for further processing. Paraphrase Validator Here, the lexicographer checks if the paraphrased gloss is properly generated. If not, it is created / edited manually by using the three principles of synset creation, viz., principle of minimality, coverage and replaceability (Bhattacharyya, 2010). This is given to the Word Adder module. Word Adder The lexicographer finally fills-in other synset information like examples, gender, etc. and adds to the WordNet using an online synset creation tool - Synskarta (Redkar et al., 2014). The resultant Samāsas will either be the member of an existing synset or it can be a new synset altogether. 3 Salient Features of Samāsa-Kartā Some of the salient features of Samāsa-Kartā are as follows: • Samāsa or compounds are created on the flow. • Samāsa in WordNet helps in identifying meaning or concept of a compound occurring in the literature. • Samāsa-Kartā helps in enriching the standard of the language and to simplify the case-ending words in language under consideration. • It assists in developing vocabulary, which in turn, helps in improving the word count in a language. • It he"
2016.gwc-1.54,agerri-garcia-serrano-2010-q,0,0.0589186,"Missing"
2016.gwc-1.54,baccianella-etal-2010-sentiwordnet,0,0.789808,"y significant positive correlation between P (s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi. 1 Introduction Sentiment analysis is a crucial task for various web and media outlets, such as, e-commerce websites, blogs and newspapers. The general approach of Sentiment Analysis is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Takamura et al., 2005; Baccianella et al., 2010a; Guerini et al., 2013). However, sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of 1 The words, Benevolent and kind are synonyms for the sense well meaning and kindly as per Oxford English dictionary. words to judge the intensity of a sentence. Words that bear the same sense can be used interchangeably to upgrade or downgrade the intensity of the expression. The following example helps illustrate the problem we attempt to address. • the synset (set of synonyms), {sound, levelheaded, intelligen"
2016.gwc-1.54,Q13-1023,0,0.04291,"Missing"
2016.gwc-1.54,D13-1125,0,0.0236997,"elation between P (s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi. 1 Introduction Sentiment analysis is a crucial task for various web and media outlets, such as, e-commerce websites, blogs and newspapers. The general approach of Sentiment Analysis is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Takamura et al., 2005; Baccianella et al., 2010a; Guerini et al., 2013). However, sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of 1 The words, Benevolent and kind are synonyms for the sense well meaning and kindly as per Oxford English dictionary. words to judge the intensity of a sentence. Words that bear the same sense can be used interchangeably to upgrade or downgrade the intensity of the expression. The following example helps illustrate the problem we attempt to address. • the synset (set of synonyms), {sound, levelheaded, intelligent, healthy} (Gloss: exer"
2016.gwc-1.54,P93-1023,0,0.562307,"ectively. 2 Related Work and Discussion Several researchers have made successful attempts for finding opinion words (Wiebe, 2000; Taboada and Grieve, 2004; Takamura et al., 2005; Wilson et al., 2005; Kanayama and Nasukawa, 2006; Liu, 2010; Dragut et al., 2010; Ohana and Tierney, 2009; Agerri and Garc´ıa-Serrano, 2010; Sharma and Bhattacharyya, 2013); however, finding intensity of words still considered as a challenging task. There have been some works on scaling adjectives by their strength, independent of the sense they express. The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. Kim et al. (2013) demonstrated that vector off-set can be used to derive scalar relationship amongst adjectives. De Melo and Bansal (2013) used a pat3 A person who is a linguist as well as a native speaker of the language can annotate words with more accuracy. The availability of the linguists, who are also native speakers of Hindi a"
2016.gwc-1.54,W06-1642,0,0.129903,"Missing"
2016.gwc-1.54,P10-1155,1,0.818747,"mples of synsets (Senses), which are assigned wrong polarity by SentiWordNet. senses were required, because HindiWordNet and MarathiWordNet do not have polarity information for synsets. The total number of observed senses and words in each language are specified in table 5. Language English Hindi Marathi Senses 1024 172 325 Words 3397 2614 1346 Table 5: Observed synset statistics C(wi ,sj ): For English words, the value of the function C is obtained from the English WordNet database file, that is, ‘cntlist’6 . For Hindi and Marathi, we used a sense marked corpus in tourism and health domain7 (Khapra et al., 2010). The total number of sense marked words in each domain are depicted in table 6. If a word shows zero frequency of use for any particular sense, we replace it with 0.1 according to a standard smoothing technique (Han et al., 2006). POS category Noun Verb Adjective Adverb Table 6: statistics 6 Tourism 72932 26086 32499 9820 Health 52230 24291 22699 855 Hindi/Marathi sense marked corpus Gold Standard Data Preparation We asked two linguists in each language to assign words to different intensity levels, viz., high (3), medium (2), and low (1) within a synset. A discrete scale with only three inte"
2016.gwc-1.54,D13-1169,0,0.0557262,"Missing"
2016.gwc-1.54,P04-1035,0,0.0289253,"w) can derive the intensity of a word within the sense. We observe a statistically significant positive correlation between P (s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi. 1 Introduction Sentiment analysis is a crucial task for various web and media outlets, such as, e-commerce websites, blogs and newspapers. The general approach of Sentiment Analysis is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Takamura et al., 2005; Baccianella et al., 2010a; Guerini et al., 2013). However, sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of 1 The words, Benevolent and kind are synonyms for the sense well meaning and kindly as per Oxford English dictionary. words to judge the intensity of a sentence. Words that bear the same sense can be used interchangeably to upgrade or downgrade the intensity of the expression. The following example helps illustrate the problem"
2016.gwc-1.54,W03-1014,0,0.0388424,"sense s given the word w) can derive the intensity of a word within the sense. We observe a statistically significant positive correlation between P (s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi. 1 Introduction Sentiment analysis is a crucial task for various web and media outlets, such as, e-commerce websites, blogs and newspapers. The general approach of Sentiment Analysis is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Takamura et al., 2005; Baccianella et al., 2010a; Guerini et al., 2013). However, sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of 1 The words, Benevolent and kind are synonyms for the sense well meaning and kindly as per Oxford English dictionary. words to judge the intensity of a sentence. Words that bear the same sense can be used interchangeably to upgrade or downgrade the intensity of the expression. The following example helps il"
2016.gwc-1.54,E14-4023,0,0.0176624,"and 73 million speakers respectively. English is chosen, because most of the lexical resources which we had pointed out in our work are in English only. Language English English Hindi Hindi Marathi Marathi Variables Annotator-1∼TFC Annotator-2∼TFC Annotator-1∼TFC Annotator-2∼TFC Annotator-1∼TFC Annotator-2∼TFC Cor-value -0.09 -0.09 -0.04 -0.09 -0.11 -0.10 Table 1: Correlation between Total Frequency Count (TFC) and intensity score assigned by two annotators in each language. tern based approach to identify intensity relation among adjectives, but their approach had a severe coverage problem. Ruppenhofer et al. (2014) provided ordering among polar adjectives that bear the same semantic property. None of the existing works address intensity variation among synonyms. However, choice of a word from a set of synonyms provides a way to intensify the expression. Our approach pin-points the polarity-intensity variation across synonyms. 3 Polarity-intensity Variation And Synonymous Words The classical semantic bleaching theory4 states that a word which has high frequency of use tends to have low intensity in comparison to a word having less frequency of use. For example, the frequent use of the word good makes it"
2016.gwc-1.54,I13-1076,1,0.847443,"e can be used to derive intensity information of a subjective sentence or document, which essentially empowers existing sentiment analysis systems. In addition to this, intensity information of words can be used to reduce or enhance an over-expressed or under- expressed text respectively. 2 Related Work and Discussion Several researchers have made successful attempts for finding opinion words (Wiebe, 2000; Taboada and Grieve, 2004; Takamura et al., 2005; Wilson et al., 2005; Kanayama and Nasukawa, 2006; Liu, 2010; Dragut et al., 2010; Ohana and Tierney, 2009; Agerri and Garc´ıa-Serrano, 2010; Sharma and Bhattacharyya, 2013); however, finding intensity of words still considered as a challenging task. There have been some works on scaling adjectives by their strength, independent of the sense they express. The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. Kim et al. (2013) demonstrated that vector off-set"
2016.gwc-1.54,P05-1017,0,0.369071,"observe a statistically significant positive correlation between P (s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi. 1 Introduction Sentiment analysis is a crucial task for various web and media outlets, such as, e-commerce websites, blogs and newspapers. The general approach of Sentiment Analysis is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Takamura et al., 2005; Baccianella et al., 2010a; Guerini et al., 2013). However, sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of 1 The words, Benevolent and kind are synonyms for the sense well meaning and kindly as per Oxford English dictionary. words to judge the intensity of a sentence. Words that bear the same sense can be used interchangeably to upgrade or downgrade the intensity of the expression. The following example helps illustrate the problem we attempt to address. • the synset (set of synonyms), {soun"
2016.gwc-1.54,H05-2018,0,0.0462445,"sed lexical resource where words which belong to the same sense are assigned three intensity levels, viz., high, medium and low. This resource can be used to derive intensity information of a subjective sentence or document, which essentially empowers existing sentiment analysis systems. In addition to this, intensity information of words can be used to reduce or enhance an over-expressed or under- expressed text respectively. 2 Related Work and Discussion Several researchers have made successful attempts for finding opinion words (Wiebe, 2000; Taboada and Grieve, 2004; Takamura et al., 2005; Wilson et al., 2005; Kanayama and Nasukawa, 2006; Liu, 2010; Dragut et al., 2010; Ohana and Tierney, 2009; Agerri and Garc´ıa-Serrano, 2010; Sharma and Bhattacharyya, 2013); however, finding intensity of words still considered as a challenging task. There have been some works on scaling adjectives by their strength, independent of the sense they express. The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientat"
2016.gwc-1.57,D09-1048,1,0.758561,"Missing"
2016.gwc-1.7,O97-1002,0,0.578678,". 3.2 Information Content Based Measure These measures are based on the information content of the synsets. Information content of a synset measures the specificity or the generality of that synset, i.e. how specific to a topic the synset is. 3.2.1 Resnik’s Measure Resnik (1995) defines the semantic similarity of two synsets as the amount of information they share in common. It is given as, This measure depends completely upon the information content of the lowest common subsumer of the two synsets whose relatedness we wish to measure. 3.2.2 Jiang and Conrath’s Measure A measure introduced by Jiang and Conrath (1997) addresses the limitations of the Resnik measure. It incorporates the information content of the two synsets, along with that of their lowest common subsumer. This measure is given by the formula: Figure 1: IndoWordNet::Similarity Tool Where, IC determines the information content of a synset and LCS determines the lowest common subsuming concept of two given concepts. 3.3 Gloss Overlap Measures Lesk (1986) defines the relatedness in terms of dictionary definition overlap of given synsets. Further, the extended Lesk measure (Banerjee and Pedersen, 2003) computes the relatedness between two syns"
2016.gwc-1.7,N04-3012,0,0.165156,"Missing"
2016.gwc-1.7,P94-1019,0,0.2587,"ordNet taxonomy. The shorter the length of the path between them, the more related they are. The inverse relation between the length of the path and similarity can be characterized as follows: Where, are synsets and D is the maximum depth of the taxonomy. 3.1.2 Leacock and Chodorow’s Measure This measure proposed by Leacock and Chodorow’s (1998) computes the length of the shortest path between two synsets and scales it by the depth D of the IS-A hierarchy. Where, S1 and S2 are the synsets and D represents the maximum depth of the taxonomy. 3.1.3 Wu and Palmer Measures This measure proposed by Wu & Palmer (1994) calculates the similarity by considering the depths of the two synsets, along with the depth of the lowest common subsumer (LCS). The formula is given as, Where, S1 and S2 are the synsets and represents the lowest common subsumer of S1 and S2. 3.2 Information Content Based Measure These measures are based on the information content of the synsets. Information content of a synset measures the specificity or the generality of that synset, i.e. how specific to a topic the synset is. 3.2.1 Resnik’s Measure Resnik (1995) defines the semantic similarity of two synsets as the amount of information t"
2018.gwc-1.31,bhattacharyya-2010-indowordnet,1,0.702734,"Italian, Spanish, German, French, Czech and Estonian. Each of these wordnets is structured in the same way as the Princeton WordNet for English (Miller et al., 1990) - synsets (sets of synonymous words) and semantic relations between them. Each wordnet separately captures a language-specific information. In addition, the wordnets are linked to an Inter-Lingual-Index, which uses Princeton WordNet as a base. This index enables one to go from concepts in one language to similar concepts in any other language. Such features make this resource helpful in crosslingual NLP applications. IndoWordNet (Bhattacharyya, 2010) is a linked wordnet comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach using Hindi WordNet as a pivot, which is partially linked to English WordNet. Previously, Joshi et al. (2012a) come up with a heuristic based measure where they use bilingual dictionaries to link two wordnets. They combine scores using various heuristics and generate a list of potential candidates for"
2018.gwc-1.31,W98-0705,0,0.247819,"ts. 1 Introduction Wordnets (Fellbaum, 1998) have been useful in different Natural Language Processing applications such as Word Sense Disambiguation (TufiS¸ et al., 2004; Sinha et al., 2006), Machine Translation (Knight and Luk, 1994) etc. Linked Wordnets are extensions of wordnets. In addition to language specific information captured in constituent wordnets, linked wordnets have a notion of an interlingual index, which connects similar concepts in different languages. Such linked wordnets have found their application in machine translation (Hovy, 1998), cross-lingual information retrieval (Gonzalo et al., 1998), etc. Given the extensive application of wordnets in different NLP applications, maintenance of wordnets involves expert involvement. Such involvement is costly both in terms of time and resources. This is further amplified in case of linked wordnets, where experts need to have knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP."
2018.gwc-1.31,P12-1092,0,0.0480324,"linked wordnets, where experts need to have knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information"
2018.gwc-1.31,C12-3030,1,0.772856,"Missing"
2018.gwc-1.31,D16-1104,1,0.828077,"ucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of different languages and the relation among them be used to aid linking of corresponding wordnets?” We demonstrate that this is true at least in the case of English and Hindi WordNets. We propose an approach to link them using word embeddings. Given a synset of the source language, the approa"
2018.gwc-1.31,D14-1181,0,0.00254133,"rily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of different languages and the relation among them be used to aid linking of corresponding wordnets?” We demonstrate that this is true at least in the case of English and Hindi WordNets. We propose an approach to lin"
2018.gwc-1.31,P14-2050,0,0.0387504,"e knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of differ"
2018.gwc-1.31,D15-1168,0,0.0741146,"Missing"
2018.gwc-1.31,2016.gwc-1.57,1,0.71929,"net comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach using Hindi WordNet as a pivot, which is partially linked to English WordNet. Previously, Joshi et al. (2012a) come up with a heuristic based measure where they use bilingual dictionaries to link two wordnets. They combine scores using various heuristics and generate a list of potential candidates for linked synsets. Singh et al. (2016) discuss a method to improve the current status of Hindi-English linkage and present a generic methodology i.e., manually creating bilingual mappings for concepts which are unavailable in either of the languages or not present as a synset in the target wordnet. Their method is beneficial for culture-specific synsets, or for non-existing concepts; but, it is cost and time inefficient, and requires a lot of manual effort on the part of a lexicographer. Our approach is mainly geared towards reducing effort on the part of the lexicographers. 3 Problem Statement Given wordnets of two different lang"
2018.gwc-1.31,C04-1192,0,0.177455,"Missing"
2018.gwc-1.34,N15-1132,1,0.867432,"dding models only output one embedding per word, instead of the ideal case of outputting one embedding per sense of a word. Though, some models do exist, which provide one embedding per sense of a word by inferring number of senses either through context clustering approaches (Neelakantan et al., 2015), or by using sense inventory (Chen et al., 2014) . For the rest of this paper, we mean one embedding per word models, when we use the phrase word embeddings. The field of Natural Language Processing is increasingly seeing the use of word embeddings for various problems, and MFS is no exception. Bhingardive et al. (2015) showed that pretrained word embeddings can be used to compute most frequent sense. In this paper, we propose an iterative approach for extracting most frequent sense of words in a raw corpus. The approach uses word embeddings as an input. Thereby, in order to obtain MFS from some raw corpus, one need to apply the following two steps: 1. Train word embeddings on the raw corpus. 2. Apply our approach on the trained word embeddings. The key points of this paper are: • Our work further strengthens the claim by (Bhingardive et al., 2015) that word embeddings indeed capture most frequent sense. • O"
2018.gwc-1.34,D14-1110,0,0.0645225,"Missing"
2018.gwc-1.34,P12-1092,0,0.0609611,"orming sense disambiguation, the MFS baseline gives really strong results. This is because of the inherent skew in the sense distribution of the data. Computing MFS baseline is trivial, if one has access to large amounts of sense-annotated corpora. However, that is not the case as explained earlier. Thus there is a need for uncovering MFS from raw data itself. Word embeddings collectively refers to the set of language modelling and feature learning techniques, which maps words to real valued vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2010; Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014). Do note that most word embedding models only output one embedding per word, instead of the ideal case of outputting one embedding per sense of a word. Though, some models do exist, which provide one embedding per sense of a word by inferring number of senses either through context clustering approaches (Neelakantan et al., 2015), or by using sense inventory (Chen et al., 2014) . For the rest of this paper, we mean one embedding per word models, when we use the phrase word embeddings. The field of Natural Language Process"
2018.gwc-1.34,J04-1003,0,0.0393903,"ngthens the claim by (Bhingardive et al., 2015) that word embeddings indeed capture most frequent sense. • Our approach outperforms others at the task of MFS extraction. The rest of the paper is organized as follows: Section 2 describes the related work. Section 3 explains our approach. Section 4.1 details our experimental setup and results. Section 5 provides some error analysis, followed by conclusion and future work. 2 Related Work Buitelaar and Sacaleanu (2001) present an approach for domain specific sense assignment. They rank GermaNet synsets based on the cooccurrence in domain corpora. Lapata and Brew (2004) acquire predominant sense of verbs. They use Levin’s classes as their sense inventory. McCarthy et al. (2007) use a thesaurus automatically constructed from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. Bhingardive et al. (2015) exploit word embeddings trained on untagged corpora to compute the most frequent sense. Our work is most similar to Bhingardive et al. (2015) owing to our reliance on word embeddings. We therefore evaluate our approach against theirs. 3 the hitherto resolved words for detecting most frequent sense, and so on. Thu"
2018.gwc-1.34,D14-1162,0,0.0831892,"Missing"
2018.gwc-1.37,J90-1003,0,0.219729,"uction A Wordnet is a large digital lexical database of a language in which information is organised around cognitive synonym sets or synsets (Fellbaum, 1998). The underlying basis of such organization are the word association studies in psycholinguistics, which proved that our mental lexicon is structured on associations, i.e. an appear1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/index.php ance of one entity entails the appearance of the other in the mind. Thus, it was found that subjects respond quicker than normal to the word ‘nurse’ if it follows a highly associated word such as ‘doctor’ (Church and Hanks, 1990). This property of the mental lexicon is structurally built in the Wordnets and manifests itself in the lexical and semantic relations which are encoded in it. Thus, a Wordnet is a ready resource of vocabulary of a language, which captures associative learning in its structure. Conventional sources of vocabulary learning, such as the dictionaries and thesauri, do not have these relations due to the very nature of their composition. This is the motivation to present Hindi Wordnet as a tool for vocabulary learning and teaching. The second motivation is the fact that education is undergoing rapid"
2018.gwc-1.37,2016.gwc-1.23,1,0.68317,"ich is perceived only through eyes). Now, रंग (raMga, colour) is such an everyday word that it was quite tough to find an easy-tounderstand definition for it, hence its English translation (कलर / colour) has been provided. The English word is highly in use in the daily language and also occurs frequently in written form too, so it could be readily added to the Hindi wordnet data, thus solving the issue of all such words. 5.3 Picture depiction As rightly mentioned in a famous idiom ‘a picture is worth a thousand words’, a complex concept can be easily explained by a picture or an illustration. Kanojia et al. (2016) tried to automatically collect images for IndoWordNet5 , but due to the lack of tagged images openly available for use, enough images could not be collected. In Hindi Wordnet, there are several concepts which are hard to explain using the gloss. For example, the concept of a word ‘milk’ in Hindi is explained as वह सफेद तरल पदाथर जो सतनपायी जीव क मादा के सतन से िनकलता है (vaha sapheda tarala padaartha jo stanapaayii jiivoM kii maadaa ke stanoM se nikalataa hai, a white nutritious liquid secreted by mammals and used as food by human beings). This gloss seems to be difficult for level 1 and 2"
2018.gwc-1.37,2016.gwc-1.46,1,0.740729,"marking is carried out during word collection process as gloss is not simplified for higher levels. During the process, each word is marked with the grammatical properties corresponding to its POS category. Some of the grammatical features are as follows: Nouns are either countable or uncountable. They can belong to any of these categories: Proper Noun, Abstract Noun, Common Noun, Collective Noun. When a noun is a compound, it may belong to one of these categories: ततपु ष (tatpuruSha), कमर धारय (karmadhaaraya), ि गु (dvigu), अवययीभाव avyayiibhaava), ं (dvaMdva), or बहवरीही (bahuvriihii) (Redkar et al., 2016). The Verbs are either Transitive or Intransitive. The different types of verbs are: Simple verb, Conjunct verb, and Compound Verb. These verbs may also be Causative verb. Kinds of Adverbs that feature in this tool are of Manner, Place, Time and Quantity. Similarly, the Adjectives are categorized as Qualitative,Numeral, Quantitative, Pronominal. 5.5 Audio pronunciation Cognitive theories of multimedia learning (Mayer, 2002) indicate that audio cues are effective aids in a learning scenario, and also help in retaining the material learned (Bajaj et al., 2015). To help in more effective learning"
2018.gwc-1.47,W14-0111,0,0.0219074,"WordNet Library4 has been extensively used for research across various domains in NLP (Chauhan et al., 2013; Zesch et al., 2008; Gurevych et al., 2012). extJWNL5 extend JWNL and provides command-line support, and Maven6 support among many other features in their API. Emerging WordNets like Sinhala WordNet (Welgama et al., 2011) employ JWNL to create an API for their WordNet. Java API for WordNet Searching (JAWS) (Spell, 2009) is another such implementation. The MIT Java WordNet Interface (JWI)7 is also available for the same purposes and is available under the Creative Commons 4.0 License8 . Finlayson (2014) presents an extensive evaluation of the APIs available in Java for accessing Princeton WordNet. All of the work above has been done for Java, and is available for Princeton WordNet. A Python based toolkit, ESTNLTK (Orasmaa et al., 2016) includes Esto3 http://www.nltk.org/ http://jwordnet.sourceforge.net/handbook. html 5 http://extjwnl.sourceforge.net/ 6 https://maven.apache.org/ 7 https://projects.csail.mit.edu/jwi/ 8 https://creativecommons.org/licenses/by/4. 0/ 4 Figure 1: Basic flow of the pyiwn API nian WordNet developed under the EuroWordNet project (Vossen, 1998). Previously, efforts ha"
2018.gwc-1.47,W98-0705,0,0.105705,"d example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development. 1 Introduction WordNets are extensively used in many sub-tasks for Natural language Processing (NLP) (Knight and Luk, 1994; TufiŞ et al., 2004). They are a rich semantic lexicon which are accessible, freeto-use and fairly accurate. They have been used in cross-lingual information retrieval (Gonzalo et al., 1998), word sense disambiguation (Sinha et al., 2006), question answering (Pasca and Harabagiu, 2001) etc. Princeton WordNet (Fellbaum, 1998) or the English WordNet was the first to come into existence. EuroWordNet (Vossen, 1998) followed with a common structure for 12 European languages. Indian language WordNets originated with the advent of Hindi WordNet (Narayan et al., 2002) and based on an expansion approach, the rest of them were created. They form a common lexico-semantic resource called IndoWordNet (IWN) (Bhattacharyya, 2010). India has more than 22 languages and 18 of these have constituen"
2018.gwc-1.47,E12-1059,0,0.0213414,"PI with an aim that IndoWordNet data should also readily available in an easy-to-use framework. Python facilitates prebuilt libraries and datasets for NLP via NLTK. TensorFlow by Google (Abadi et al., 2016) is also built on Python, and other classic Machine Learning algorithms are available for use via the sci-kit learn (sklearn) library (Pedregosa et al., 2011). Hence, we choose Python for implementing the API and build a framework using it. 3 Related Work The Java WordNet Library4 has been extensively used for research across various domains in NLP (Chauhan et al., 2013; Zesch et al., 2008; Gurevych et al., 2012). extJWNL5 extend JWNL and provides command-line support, and Maven6 support among many other features in their API. Emerging WordNets like Sinhala WordNet (Welgama et al., 2011) employ JWNL to create an API for their WordNet. Java API for WordNet Searching (JAWS) (Spell, 2009) is another such implementation. The MIT Java WordNet Interface (JWI)7 is also available for the same purposes and is available under the Creative Commons 4.0 License8 . Finlayson (2014) presents an extensive evaluation of the APIs available in Java for accessing Princeton WordNet. All of the work above has been done for"
2018.gwc-1.47,L16-1390,0,0.0526102,"Missing"
2018.gwc-1.47,W12-5022,0,0.0177519,"g Princeton WordNet. All of the work above has been done for Java, and is available for Princeton WordNet. A Python based toolkit, ESTNLTK (Orasmaa et al., 2016) includes Esto3 http://www.nltk.org/ http://jwordnet.sourceforge.net/handbook. html 5 http://extjwnl.sourceforge.net/ 6 https://maven.apache.org/ 7 https://projects.csail.mit.edu/jwi/ 8 https://creativecommons.org/licenses/by/4. 0/ 4 Figure 1: Basic flow of the pyiwn API nian WordNet developed under the EuroWordNet project (Vossen, 1998). Previously, efforts had been made to create an API for IndoWordNet but they are not Python based. Prabhugaonkar et al. (2012) describe a two-layered architecture of a web-based API created using PHP. It requires one to download data separately and is inconvenient to deploy; we also come across hard-coded paths while trying to deploy their API. A Java-based API9 is available for download on the Hindi WordNet web interface, and also requires one to separately download the database for Hindi WordNet. Redkar et al. (2016) claim to have built an API for WordNets universally but their work is not publicly accessible, and no references to their implementation could be found. Hence, we work on an API which would contain NLT"
2018.gwc-1.47,C04-1192,0,0.0229762,"ke core functionalities in Python. Additionally, we use a pre-built speech synthesis system for Hindi language and augment Hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development. 1 Introduction WordNets are extensively used in many sub-tasks for Natural language Processing (NLP) (Knight and Luk, 1994; TufiŞ et al., 2004). They are a rich semantic lexicon which are accessible, freeto-use and fairly accurate. They have been used in cross-lingual information retrieval (Gonzalo et al., 1998), word sense disambiguation (Sinha et al., 2006), question answering (Pasca and Harabagiu, 2001) etc. Princeton WordNet (Fellbaum, 1998) or the English WordNet was the first to come into existence. EuroWordNet (Vossen, 1998) followed with a common structure for 12 European languages. Indian language WordNets originated with the advent of Hindi WordNet (Narayan et al., 2002) and based on an expansion approach, the rest of them"
2018.gwc-1.49,2016.gwc-1.23,1,0.701514,"Indian Languages have also emerged in the recent past (Patil et al., 2013). Although these systems, which are already available, do not produce the most “natural” sounding output, but they are usable to an extent. Manual evaluations of the speech synthesis systems built for the Hindi Language show that there is still a need for better text processing and additional phonetic coverage (Kishore et al., 2003; Raj et al., 2007). Bengu et al. (2002) create an online context sensitive dictionary using Princeton WordNet and implement a Java based speech interface for the Text-to-Speech (TTS) engine. Kanojia et al. (2016) automatically collect images for IndoWordNet and augment them to the web interface, but due to the lack of tagged images openly available for use, they do not collect enough images. To the best of our knowledge, there has been no other work specifically in the direction of synthesizing audio for WordNet words or Synthesizing audio for Indian Language WordNets. 3 Our Approach Among the three main sub-types of concatenative synthesis, we choose to perform unit selection synthesis and build cluster units of the speech data recorded by a human voice. We use the Festival system to create a synthet"
2019.gwc-1.51,N09-3008,0,0.664538,"es (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also consider the method of Ciobanu and Dinu (2014), which employs dynamic programming based methods for sequence alignment. Among cognate sets common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007), XDice (Brew et al., 1996) or TF-IDF (Wu et al., 2008) could be used to measure similarities and validate the members of the set. 3 Datasets and Methodology We investigate language pairs for major Indian languages namely Marathi (Mr), Gujarati (Gu), Bengali (Bn), Punjabi (Pa), Sanskrit (Sa), Malayalam (Ml), Tamil (Ta), Telugu (Te), Nepali (Ne) and Urdu (Ur) with Hi"
2019.gwc-1.51,P14-2017,0,0.858587,"ide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cog"
2019.gwc-1.51,P15-2071,0,0.548536,"d segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also consider the method of Ciobanu and Dinu (2014), which employs dynamic programming based methods for sequence alignment. Among cognate sets common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007), XDice (Brew et al., 1996) or TF-IDF (Wu et al., 2008) could be used to measure similarities and validate the members of the set. 3 Datasets and Methodology We investigate language pairs for major Indian languages namely Marathi (Mr), Gujarati (Gu), Bengali (Bn), Punjabi (Pa), Sanskrit (Sa), Malayalam (Ml), Tamil (Ta), Telugu (Te"
2019.gwc-1.51,E17-1113,0,0.16851,"Missing"
2019.gwc-1.51,jha-2010-tdil,0,0.092144,"all words, in the concept space, in a comma-separated format. We, then, create word lists by combining all possible permutations of word pairs within each synset. For e.g., If synset ID X on the source side (Hindi) contains words S1 W1 and S1 W2 , and parallelly on the target side (other Indian languages), synset ID X contains T1 W1 and T1 W2 , we create a word list such as: S1 W1 , T1 W1 S1 W2 , T1 W1 S1 W1 , T1 W2 S1 W2 , T1 W2 To avoid redundancy, we remove duplicate word pairs from this list. Dataset 2: Parallel Corpora based dataset We use the ILCI parallel corpora for Indian languages (Jha, 2010) and create word pairs list by comparing all words in the source side sentence with all words on the target side sentence. Our hypothesis, here, is that words with high orthographic similarity which occur in the same context window (a sentence) would be cognates with a high probability. Due to the unavailability of ILCI parallel corpora for Sa and Ne, we download these corpora from Wikipedia and align it with the Hindi articles from Hindi Wikipedia. We calculate exact word matches to align articles to each other thus creating comparable corpora and discard unaligned lines from both sides. We,"
2019.gwc-1.51,mulloni-pekar-2006-automatic,0,0.565835,"ds, such as machine translation and bilingual terminology compilation. For e.g., the German - English cognates, Blume - bloom can be identified as cognates with orthographic similarity methods. Detection of cognates helps various NLP applications like IR (Pranav, 2018). Rama et al. (2018) study various cognate detection techniques and provide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researche"
2019.gwc-1.51,P07-3005,0,0.0465822,"at automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cognates among wor"
2019.gwc-1.51,W97-1102,0,0.859426,"this threshold to 0.5 for both datasets2 . Using 0.5 as threshold, we obtained the best training performance and hence chose to use this as the threshold for similarity calculation. The various similarity measures used are described in the next subsection. 3.4 Similarity Measures Normalized Edit Distance Method (NED) We calculate similarity scores for each word on the source side i.e., Hi by matching it with each word on the target side i.e., Sa, Bn, Gu, Pa, Mr, Ml, Ne, Ta, Te, and Ur. Since we match the words from the same conThe Normalized Edit Distance approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in a synset/concept and then provides the output of probable cognate sets with distance and similarity scores. We assign labels for these sets based on the similarity score obtained from the NED method, where the similarity score is (1 - NED score). It is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). The score is normalized such that 0 equates to no similarity and 1 is an exact match. NED is equal to the minimum number of operations required to transform 1 htt"
2019.gwc-1.51,A00-2038,0,0.692763,"n of cognates for improving IR has already been explored for Indian languages (Makin et al., 2007). String similarity-based methods are often used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usu"
2019.gwc-1.51,P18-3019,0,0.0288967,"m as well. 1 Introduction Cognates are words that have a common etymological origin (Crystal, 2008). They account for a considerable amount of unique words in many lexical domains, notably technical texts. The orthographic similarity of cognates can be exploited in different tasks involving recognition of translational equivalence between words, such as machine translation and bilingual terminology compilation. For e.g., the German - English cognates, Blume - bloom can be identified as cognates with orthographic similarity methods. Detection of cognates helps various NLP applications like IR (Pranav, 2018). Rama et al. (2018) study various cognate detection techniques and provide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical simi"
2019.gwc-1.51,N01-1014,0,0.656344,"et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cognates among word pairs (Kondrak, 2001). The measure of semantic similarity uses the context around both word pairs and helps in the identification of a cognate word pair by looking of similarity among the collected contexts. For our work, we can primarily divide words into four main categories viz. True Cognates, False Cognates, False Friends and NonCognates. In Figure 1, we present this classification with examples from various languages along with their meanings for better understanding. While some false friends are also false cognates, most of them are genuine cognates. Our primary goal is to be able to identify True Cognates."
2019.gwc-1.51,W12-0216,0,0.419883,"ften used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Ko"
2019.gwc-1.51,N18-2063,0,0.583483,"Missing"
2019.gwc-1.51,C16-1097,0,0.522609,"methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also"
2019.gwc-1.51,J99-1003,0,0.669482,"ravidian languages like Malayalam, Tamil, Telugu, and Kannada borrow many words from Sanskrit. Although recently, Kanojia et al. (2019) perform cognate detection for a few Indian languages, but report results with manual verification of their output. Identification of cognates for improving IR has already been explored for Indian languages (Makin et al., 2007). String similarity-based methods are often used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved th"
2019.icon-1.16,W06-0901,0,0.0868757,"iate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model can help us utilise the close interaction between one or more similar tasks. Li et al. (2013) pr"
2019.icon-1.16,Q17-1010,0,0.00608438,"borate our claim, we device two different systems, i). monolingual baseline system, and ii). multi-lingual system. The ‘monolingual baseline’ system only takes input data (sentence wise) from one language and extracts the arguments. For word representation, it uses monolingual word embeddings. The ‘multi-lingual’ argument extraction system uses separate language layers and multi-lingual word embeddings for joint training on all the three languages. 3.0.1 Monolingual Word Embedding The monolingual word-embeddings that are used in our experiments are also known as fastText1 . It was proposed by Bojanowski et al. (2017), and is based on the skipgram model. However instead of using one-hot vector encoding for each word while training, a vector representation of a word that considers character n-grams occurring in the word is formed. To get this representation, the n-grams from all the words for ‘n’ greater than 2 and smaller than 7 are extracted. After this, a dictionary of all the extracted n-grams is created. A given word w, can now be denoted by Γw ⊂ {1, ...., G} i.e the set of n-grams appearing in the word; where G is the size of the n-gram dictionary. With each n-gram in G, a vector representation zg is"
2019.icon-1.16,P11-1098,0,0.0264339,"ated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this"
2019.icon-1.16,P15-1017,0,0.0682416,"onse to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event argument extraction. First is our monolingual system built using CNN (Convolutional Neural Network) and Bi-LSTM (Bi-Directional L"
2019.icon-1.16,N09-2053,0,0.0304979,"side token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model can help us utilise the close interaction between one or more similar tasks. Li et al. (2013) presented a joint mode"
2019.icon-1.16,doddington-etal-2004-automatic,0,0.155659,"Missing"
2019.icon-1.16,P11-1113,0,0.0210488,"i). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upst"
2019.icon-1.16,P11-1114,0,0.0219108,"abel multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model ca"
2019.icon-1.16,D16-1085,0,0.0128527,"ated Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event argument extraction. First is our monolingual system built using CNN (Convolutional Neural Network) and Bi-LSTM (Bi-Directional Long Short 136 Term Memory). To exploit the information from related languages, we develop a second system that can use information from all the languages for training. This multi-lingual syste"
2019.icon-1.16,P08-1030,0,0.105931,"Missing"
2019.icon-1.16,D07-1075,0,0.049315,"Therefore, it has been formulated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the conte"
2019.icon-1.16,D14-1181,0,0.0026806,"n zg is associated. A word represention is obtained by summing up all the n-grams, as described in Equation 1: Vw = ∑ zg W ← (1 + β)W − β(W W T )W (2) Here, β was set to 0.01 for the transformation. For our experiments we trained mapping matrices Whindi and Wbengali that map the Hindi and Bengali word embeddings to the vector space of English embeddings. 3.1 Monolingual Baseline Model The ‘monolingual baseline’ model (c.f Figure 1) is based on Bi-Directional Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) and Convolutional Neural Networks (CNN) (Kim, 2014). The input to the model is a sentence, represented by a sequence of monolingual word embeddings. Since Bi-LSTM and CNN take sequences of equal lengths, the shorter sequences are padded by zero (1) g∈Gw The continuous skip-gram model used these word vectors Vw , to obtain word-embedding representa1 https://github.com/facebookresearch/ fastText 137 Figure 1: monolingual baseline model for argument extraction Figure 2: Multi-lingual baseline model for argument extraction vectors. This sequence is passed through BiLSTM and CNN having filter size 2 and 3. The Bi-LSTM gives contextual representatio"
2019.icon-1.16,D09-1016,0,0.0345474,"w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system ba"
2019.icon-1.16,P18-1074,0,0.0222127,"g. Com135 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 135–142 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) cially helpful in improving the performance when some argument is under-represented in the ‘monolingual’ training data. 1.1 chunk level information from specific contexts. Nguyen and Grishman (2018) explored graph convolutional network over dependency trees and entity mention-guided pooling. For low resource languages, Liu et al. (2018) came up with Gated Multi-Lingual Attention (GMLATT) and Lin et al. (2018) developed a multi-lingual multi-task architecture alleviating data sparsity problem in related tasks and languages. Problem Definition Argument extraction entails classifying each word in the sentence into some argument or not argument. Therefore, it has been formulated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label"
2019.icon-1.16,P12-1088,0,0.0289935,"predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional R"
2019.icon-1.16,N16-1034,0,0.0609126,"e or more similar tasks. Li et al. (2013) presented a joint model for Chinese Corpus which identifies arguments and determines their roles for event extraction using various kinds of discourse-level information. On ACE2005 dataset Sha et al. (2018) proposed a dependency bridge recurrent neural network (dbRNN) built upon LSTM units for event extraction. They use dependency bridges over Bi-LSTM to join syntactically similar words. A tensor layer is applied to get the various argument-argument interactions. Event triggers and arguments are then jointly extracted utilising a max-margin criterion. Nguyen et al. (2016) presented a GRU model to jointly predict events and its arguments. • Input Hindi Sentence: गृह मं ालय मुंबई के बम वफोट के म ेनजर इस बात क वशेष तौर पर जांच कर रहा है क अ रधाम मं दर और १९९३ के मुंबई बम वफोट के फसल क त या के प म तो यह हमले नह ए • Translation: In view of the Mumbai bomb blasts, the Home Ministry is specially investigating the fact that these attacks did not take place as response to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task"
2019.icon-1.16,P15-2060,0,0.0131808,"In view of the Mumbai bomb blasts, the Home Ministry is specially investigating the fact that these attacks did not take place as response to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event ar"
2019.icon-1.19,P16-2060,0,0.0624271,"e on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017)"
2019.icon-1.19,L18-1550,0,0.0175464,"words are created for Hindi and Bengali respectively. We create separate .vec file for the two OOV vocabularies. We similarly transform these vectors of two different languages in a shared space using the existing alignment matrices3 . It is seen that the performance has significantly improved using crosslingual embeddings for OOV words compared to the method of using zero vectors for representing them. 5 Datasets and Experiments 5.1 Dataset Words इंडो नशया म 7.2 क ती ता का भूकंप आया Embedding Each word of the input instance is converted to a numeric representation with the help of fastText (Grave et al., 2018) word embeddings having dimension 300 (de ). The pre-trained word vectors are downloaded from fastText website2 . To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings in order to bridge the language gap between two languages, we use the existing alignment matrices3 which align monolingual vectors from two lanEvent & Argument Trigger Detection B_Arg O B_Arg O O O B_Event O Event & Argument Classification Place None Magnitude None None None Earthquake None Table 2: Sample annotation for the sentence given in Example-2 in Task Description and Contribution Se"
2019.icon-1.19,P11-1113,0,0.0289994,"ing (MTL), which essentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automati"
2019.icon-1.19,P08-1030,0,0.0565471,"cture. Multi-task learning (MTL), which essentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network"
2019.icon-1.19,P13-1008,0,0.024244,"y idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event se"
2019.icon-1.19,P15-1017,0,0.106982,"010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) pr"
2019.icon-1.19,P10-1081,0,0.0324153,"sentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen"
2019.icon-1.19,P16-1201,0,0.0123909,". Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also pr"
2019.icon-1.19,P17-1164,0,0.0230994,"eini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they focus on English and Chinese. Liu et al. (2018b) propose a novel framework to"
2019.icon-1.19,D18-1156,0,0.0408197,"Missing"
2019.icon-1.19,N16-1034,0,0.0221363,"ropose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they f"
2019.icon-1.19,P15-2060,0,0.0181637,"o two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language in"
2019.icon-1.19,D16-1085,0,0.0165577,"thm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue"
2019.icon-1.19,D14-1090,0,0.0189585,"is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more"
2019.icon-1.19,D18-1122,0,0.0385385,"Missing"
2019.icon-1.19,N16-1033,0,0.0124412,"ent detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they focus on English and Chinese. Liu et al. (2018b) propose a novel framework to jointly extract multiple event triggers and arguments. Sha et al. (2018) propose a novel dependency bridge RNN which includes syntactic dependency relationships. Dependency relationship is also used by Nguyen and Grishman (2018). They investigate a CNN based on"
2019.icon-1.19,D11-1001,0,0.030187,"or various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according"
2019.icon-1.19,W11-1807,0,0.0267833,"or various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according"
2019.icon-1.2,P17-2067,0,0.398674,"t musician Texas republican 0 0 2 0 2 an oped column. 4 abortion, federalbudget, health-care plannedparenthood -action-fund Advocacy group Washington, D.C. none 1 0 0 0 0 a radio ad 5 health-care nancy-pelosi House Minority Leader California democrat 3 7 11 2 3 a news conference 6 correctionsandupdates, crime, criminal -justice, sexuality garnetcoleman Texas democrat 1 0 1 0 1 a committee hearing president, ceo of Apartments for America, Inc. fake news detection. Further integrating these two elements improves the performance of the classifier. as entirely false. This problem was addressed by Wang (2017) where they introduced Liar dataset comprising of a substantial volume of short political statements having six different class annotations determining the amount of fake content of each statement. In his work, he showed comparative studies of several statistical and deep learning based models for the classification task and found that the CNN model performed best. Long et al. (2017) in their work used the Liar dataset, and proposed a hybrid attention-based LSTM model for this task, which outperformed W.Yang’s hybrid CNN model, establishing a new state-of-the-art. Problems related to these top"
2019.icon-1.2,D14-1181,0,0.00317989,"viewed concerning binary classification. Likewise, most of the published works also has viewed fake news detection as a binary classification problem (i.e., fake or true). But by observing very closely it can be seen that fake news articles can be classified into multiple classes depending on the fakeness of the news. For instance, there can be certain exaggerated or misleading information attached to a true statement or news. Thus, the entire news or statement can neither be accepted as completely true nor can be discarded In our current work we propose an ensemble architecture based on CNN (Kim, 2014) and Bi11 LSTM (Hochreiter and Schmidhuber, 1997), and this has been evaluated on Liar (Wang, 2017) dataset. Our proposed model tries to capture the pattern of information from the short statements and learn the characteristic behavior of the source speaker from the different attributes provided in the dataset, and finally integrate all the knowledge learned to produce fine-grained multi-class classification. 2 Methodology We propose a deep multi-label classifier for classifying a statement into six fine-grained classes of fake news. Our approach is based on an ensemble model that makes use of"
2019.icon-1.2,I17-2043,0,0.0738317,"a committee hearing president, ceo of Apartments for America, Inc. fake news detection. Further integrating these two elements improves the performance of the classifier. as entirely false. This problem was addressed by Wang (2017) where they introduced Liar dataset comprising of a substantial volume of short political statements having six different class annotations determining the amount of fake content of each statement. In his work, he showed comparative studies of several statistical and deep learning based models for the classification task and found that the CNN model performed best. Long et al. (2017) in their work used the Liar dataset, and proposed a hybrid attention-based LSTM model for this task, which outperformed W.Yang’s hybrid CNN model, establishing a new state-of-the-art. Problems related to these topics have mostly been viewed concerning binary classification. Likewise, most of the published works also has viewed fake news detection as a binary classification problem (i.e., fake or true). But by observing very closely it can be seen that fake news articles can be classified into multiple classes depending on the fakeness of the news. For instance, there can be certain exaggerate"
2019.icon-1.20,D14-1162,0,0.081462,"Missing"
2019.icon-1.20,C18-1179,0,0.0481447,"Missing"
2019.icon-1.20,L18-1030,0,0.0302372,"Missing"
2019.icon-1.20,S12-1033,0,0.138399,"emotion vectors of the dependent word Dp and its modifier word Mp . This will help in strengthening every emotion score related to that word wi of sentence S. (A Agrawal, 2012) EVDp = 3.4 EVDp + EVMp 2 ISEAR dataset (ise) The “International Survey on Emotion Antecedents and Reactions” dataset published by Scherer and Wallbott is built by collecting questionnaires answered by people with different cultural backgrounds (Bostan and Klinger, 2018). A total of 7,665 sentences labeled with single emotions. The labels are joy, fear, anger, sadness, disgust, shame, and guilt. Twitter Emotion Corpus (Mohammad, 2012) is prepared with emotion-word hashtags as emotion labels. These are termed as noisy labels as labelled by users. This corpus contains 21050 sentences labelled with one of the emotions from Ekman’s emotion model. (5) Processing Emojis With growing usage of social media, many times, text messages are accompanied with suitable emoji. Hence, emoji as input can contribute towards detecting emotion. Every emoji is being assigned CLDR short name by Unicode Common Locale Data Repository to describe that emoji. For example, grinning face, beaming face with smiling eyes and so on. Same procedure as men"
2019.icon-1.20,S18-1001,0,0.0492113,"Missing"
2019.icon-1.20,P15-1101,0,0.0673581,"Missing"
2019.icon-1.27,Q17-1010,0,0.0127402,"e. Few notable works in this line could be found in (Yin and Roth, 2018; Nie et al., 2019). 2 3.1 3 Proposed Methods We propose two deep Learning based models to address the problem of fake information detection in the multi-domain platform. In the following subsections, we will discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-trained fastText model3 (Bojanowski et al., 2017). FastText embedding model is an extended version of Word2Vec (Mikolov et al., 2013). Word2Vec (predicts embedding of a word based on given context and vice-versa) and Glove (exploits count and word co-occurrence matrix to predict embedding of a word) (Pennington et al., 2014) both treat each word as an atomic entity. The fastText model produces embedding of each word by combining the embedding of each character n-gram of that word. The model works better on rare words and also produces embedding for A sufficient number of works could be found in the literature in fake news detection. Nowadays"
2019.icon-1.27,N19-1423,0,0.015853,"ta and Tested on Domain wise Data) Figure 4: Word Level Attention on News Topic In these Figures, words with more deeper colour indicate that they are getting more attention. We can observe, the words secretary, education in 4 and President, Donald in 5 are the words having deeper colour, i.e. these words are getting more weight compared to others. These words are Named Entities (NEs). It could be concluded that NEs phrases are important in fake news detection in multi domain setting. 4.1 • It would be interesting for this work to encode the domain information in the Deep Neural Nets. • BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) embedding based model and make a comparison with fastText and ELMo based models in the context of fake news detection. Error Analysis We extract the mis-classified and also the truly classified instances produced by the best performing system. We perform a rigorous analysis of these instances and try to find out the pattern in the mis-classified instances and the linguistics differences between those two categories of instances. It is found that the model fails mostly in the Entertainment followed by the sports and the Business domain etc. To name a few, we are s"
2019.icon-1.27,D14-1162,0,0.0830753,"ll discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-trained fastText model3 (Bojanowski et al., 2017). FastText embedding model is an extended version of Word2Vec (Mikolov et al., 2013). Word2Vec (predicts embedding of a word based on given context and vice-versa) and Glove (exploits count and word co-occurrence matrix to predict embedding of a word) (Pennington et al., 2014) both treat each word as an atomic entity. The fastText model produces embedding of each word by combining the embedding of each character n-gram of that word. The model works better on rare words and also produces embedding for A sufficient number of works could be found in the literature in fake news detection. Nowadays the detection of fake news is a hot area of research and gained much more research interest among the researchers. We could detect fake news at two levels, namely the conceptual level and operational level. Rubin et al. (2015) defined that conceptually there are three types o"
2019.icon-1.27,C18-1287,0,0.0254715,"Missing"
2019.icon-1.27,N16-1138,0,0.0337415,"Missing"
2019.icon-1.27,N18-1202,0,0.0456749,"itics, and Entertainment). The Celebrity dataset is crawled directly from the web of celebrity gossips. It covers celebrity news. The AMT manually generated fake version of a news based on the real news. We extract the data domain wise to get the statistics of the dataset. It is observed that each domain contains equal number of instances (i.e. 80). The class distribution among each domain is also evenly distributed. The statistics of these two datasets is shown in the following Table 1. Model 2 We propose another approach whose embedding layer is based on Embedding for Language Model (ELMo) (Peters et al., 2018) and the MLP Network, which is same as we applied in Model 1. The diagram of this model is shown in the Figure 3. Embedding Layer: Embedding from Language Model (ELMo) has several advantages over the other word vector methods, and found to be a good performer in many challenging NLP problems. It has key features like i. Contextual i.e. representation of each word is based on entire corpus in which it is used ii. Deep i.e. it combines all layers of a deep pre-trained neural network and iii. Character based i.e. it provides representations which are based on character, thus allowing the network"
2019.icon-1.27,C18-1158,0,0.0411402,"Missing"
2019.icon-1.27,P18-1022,0,0.0637221,"Missing"
2019.icon-1.27,W16-0802,0,0.110144,"Missing"
2019.icon-1.27,D18-1010,0,0.0288674,"Missing"
2019.icon-1.27,N18-1074,0,0.0684144,"participating systems of the Fake News Challenge. The work of Saikh et al. (2019) detected fake news through stance detection and also correlated this stance classification problem with Textual Entailment (TE). They tackled this problem using statistical machine learning and deep learning approaches separately and with combination of both of these. This system achieved the state of the art result. Another remarkable work in this line is the verification of a human- generated claim given the whole Wikipedia as evidence. The dataset, namely (Fact Extraction and Verification (FEVER)) proposed by Thorne et al. (2018) served this purpose. Few notable works in this line could be found in (Yin and Roth, 2018; Nie et al., 2019). 2 3.1 3 Proposed Methods We propose two deep Learning based models to address the problem of fake information detection in the multi-domain platform. In the following subsections, we will discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-traine"
2019.icon-1.27,P17-2067,0,0.0568217,"Missing"
2019.icon-1.27,N16-1174,0,0.0234722,"ultiplication to (1-z) with h. Take the summation of i and ii. The bidirectional GRUs consists of the forward GRU, which reads the sentence from the first word (w1 ) to the last word (wL ) and the backward GRU, that reads in reverse direction. We concatenate the representation of each word obtained from both the passes. C. Word Level Attention: We apply the attention model at word level (Bahdanau et al., 2015; Xu et al., 2015). The objective is to let the model decide which words are importance compared to other words while predicting the target class (fake/legit). We apply this as applied in Yang et al. (2016). The diagram is shown in the Figure 2. We take the aggregation of those words’ representation which are multiplied with attention weight to get sentence representation. We do this process for both the news topic and the corresponding document. This particular technique of the word attention mechanism, has not been tried for solving such a problem. In equation 1, z is the update gate at time step t. This z is the summation of the multiplications of xt with it’s own weight U(z) and st−1 (holds the information of previous state) with it’s own W(z). A sigmoid α is applied on the summation to sque"
2020.aacl-main.31,P19-1455,0,0.0300492,"Missing"
2020.aacl-main.31,D19-1566,1,0.830157,"uce a deep-learningbased multi-modal Textual Kernels Model (TKM) and compare it with various existing deep learning architectures on the proposed MMHS150K dataset. Motivation: Swieczkowska et al. (2020) proposes a novel chaining method of neural networks for identifying motivational texts where the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using various syntactic and semantic features, such as frequency of words, the intensity of adverbs and adjectives, the gap between po"
2020.aacl-main.31,2020.acl-main.401,1,0.725064,"ning BERT (Devlin et al., 2018) model for humour detection on three languages (Chinese, Spanish and Russian). Sarcasm: Starting from the traditional approaches, such as rule-based methods (Veale and Hao, 2010), lexical features (Carvalho et al., 2009), and incongruity (Joshi et al., 2015) to all the way up to multi-modal deep learning techniques (Schi282 fanella et al., 2016), sarcasm detection has been showing its presence. Castro et al. (2019) created a multi-modal conversational dataset, MUStARD from the famous TV shows, and provided baseline SVM approaches for sarcasm detection. Recently, Chauhan et al. (2020) proposed a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis to explore how sentiment and emotion helps sarcasm. The author used the MUStARD dataset and extended the MUStARD dataset with sentiment (implicit and explicit) and emotion (implicit and explicit) labels. Offensive: Razavi et al. (2010) used a threelevel classification model taking advantage of various features from statistical models and rulebased patterns and various dictionary-based features. Chen et al. (2012) proposed a feature-based Lexical Syntactic Feature (LSF) architecture to detect the o"
2020.aacl-main.31,D18-1382,1,0.747096,"the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using various syntactic and semantic features, such as frequency of words, the intensity of adverbs and adjectives, the gap between positive and negative terms, the structure of the sentence, synonyms and others (Barbieri and Saggion, 2014). More recently, Badlani et al. (2019) proposed a convolution-based model to extract the embedding by fine-tuning the same for the tasks of sentiment, sarcasm, humour, and hate-speech and"
2020.aacl-main.31,P15-2124,1,0.796519,"nd Pulman, 2007). Some of the recent multi-modal approaches include utilizing information from the various modalities, such as acoustic, visual, and text, using deep learning models (Bertero and Fung, 2016; Yang et al., 2019; Swamy et al., 2020). Yang et al. (2020) employs a paragraph decomposition technique coupled with fine-tuning BERT (Devlin et al., 2018) model for humour detection on three languages (Chinese, Spanish and Russian). Sarcasm: Starting from the traditional approaches, such as rule-based methods (Veale and Hao, 2010), lexical features (Carvalho et al., 2009), and incongruity (Joshi et al., 2015) to all the way up to multi-modal deep learning techniques (Schi282 fanella et al., 2016), sarcasm detection has been showing its presence. Castro et al. (2019) created a multi-modal conversational dataset, MUStARD from the famous TV shows, and provided baseline SVM approaches for sarcasm detection. Recently, Chauhan et al. (2020) proposed a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis to explore how sentiment and emotion helps sarcasm. The author used the MUStARD dataset and extended the MUStARD dataset with sentiment (implicit and explicit) and emotio"
2020.aacl-main.31,P19-1046,0,0.024098,"the offensive contents. Gomez et al. (2020) created a multi-modal hate-speech dataset from Twitter (MMHS150K) to introduce a deep-learningbased multi-modal Textual Kernels Model (TKM) and compare it with various existing deep learning architectures on the proposed MMHS150K dataset. Motivation: Swieczkowska et al. (2020) proposes a novel chaining method of neural networks for identifying motivational texts where the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using vari"
2020.aacl-main.31,2020.semeval-1.156,0,0.061348,"Missing"
2020.aacl-main.33,D19-1366,0,0.0355433,"Missing"
2020.aacl-main.33,N18-1169,0,0.0529168,"Missing"
2020.aacl-main.33,S14-2004,0,0.235349,"level polarity has been well explored as a style transfer task. Zhang et al. (2018) used unsupervised machine translation techniques for polarity transfer in sentences. Yang et al. (2018) equal contribution Service - Negative Salads - Positive Chicken - Positive Figure 1: An example of the proposed aspect-level sentiment style transfer task Introduction § Service - Positive Salads - Positive Chicken - Negative In this paper we explore a more fine-grained style transfer task, where each aspect’s polarities can be changed individually. Recent interest in Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) has shown that sentiment information can vary within a sentence, with differing sentiments expressed towards different aspect terms of target entities (e.g. ‘food’, ‘service’ in a restaurant domain). We introduce the task of aspect-level sentiment transfer - the task of rewriting sentences to transfer them from a given set of aspect-term polarities (such as ‘positive sentiment’ towards the service of a restaurant and a ‘positive sentiment’ towards the taste of the food) to a different set of aspect-term polarities (such as ‘negative sentiment’ towards the service of a restaurant and a ‘positi"
2020.aacl-main.33,P18-1080,0,0.0192363,"ngle, and differing styles can often be present together when discussing different topics and entities. Our work intends to take the first step towards a more controllable form of finegrained style transfer with the task of aspect-level sentiment style transfer. 2 Related Work In this section we present an overview of the related literature. 2.1 Sentiment Transfer To the best of our knowledge, our current work is the first to tackle aspect-level sentiment transfer. Most of the previous works involving sentiment transfer (Li et al., 2018b; Yang et al., 2018; Shen et al., 2017; Xu et al., 2018; Prabhumoye et al., 2018; Wu et al., 2019) consider the style that is present throughout the sentence and seek to transfer only the overall sentiment polarities expressed. Tian et al. (2018) proposed a new training objective for content preservation during style transfer. They used Part-of-Speech (PoS) tagging to collect nouns at inputs, and expect them to be present at the output for content preservation. To achieve this, they proposed a PoS preservation constraint and ‘Content Conditional Language Modelling’. They tested their system on sentiment style transfer task. Wang et al. (2019) proposed a method that can al"
2020.aacl-main.33,W14-4012,0,0.0397672,"Missing"
2020.aacl-main.33,E99-1023,0,0.163673,"sed polarity classification model which were trained on the SemEval ABSA training data, to generate aspect-level sentiment data from the Yelp reviews dataset. Table 1 shows some statistics from the datasets. 4.1.1 Aspect based Sentiment Analysis with BERT A pipeline of BERT-based models was trained for target-extraction and aspect-level polarity classification over the SemEval dataset. These are the models used to extract target-aspects and their polarities from the Yelp dataset. The target extraction task was posed as a sequential token classification problem with BERT using the IOB2 format (SANG, 1999). This BERT model was fed the whole sentence as the input segment and it obtained an F1-score of 0.8012 (evaluation carried out similar to Sang and Buchholz (2000)). The sentiment-polarity prediction task is posed as a sentence-pair classification problem using BERT, with the sentence provided as the first segment and the aspect-term as the second segment. This model obtained an F1-score of 0.9080 for the positive po307 Classifier Score Classifier Score Classifier Score Classifier Score (Overall) (1-Aspect) (2-Aspects) (3-or-more Aspects) BERT-Baseline (BB) 0.5158 0.4983 0.5448 0.5036 BB + MLM"
2020.aacl-main.33,N19-1423,0,0.217718,"be able to correctly process the aspectpolarity query and accordingly delete, replace and generate text sequence to satisfy the query. (iii). The polarities of the aspects not in the query should not be affected. (iv). The non-attribute content and fluency of the text should be preserved. We explore this task in an unsupervised setting (as is common with most style-transfer tasks due to the lack of an aligned parallel corpus) using only monolingual unaligned corpora. In this work, a novel encoder-decoder architecture is proposed to perform unsupervised aspect-level sentiment transfer. A BERT (Devlin et al., 2019) based encoder is used that is trained to understand aspect-specific polarity information. We also propose using a ‘polarity injection’ method, where saliency-weighted aspect-specific polarity information is added to the hidden representations from the encoder to complete the query for the decoder. 1.1 Motivation The Aspect-Based Sentiment Analysis (ABSA) task shows that differing sentiments can be present within the same sentence, localized to different entities or parts of the text. The notion of styles in natural language can be used to refer to the attributes, such as sentiment, formality"
2020.aacl-main.33,W00-0726,0,0.0318882,"dataset. Table 1 shows some statistics from the datasets. 4.1.1 Aspect based Sentiment Analysis with BERT A pipeline of BERT-based models was trained for target-extraction and aspect-level polarity classification over the SemEval dataset. These are the models used to extract target-aspects and their polarities from the Yelp dataset. The target extraction task was posed as a sequential token classification problem with BERT using the IOB2 format (SANG, 1999). This BERT model was fed the whole sentence as the input segment and it obtained an F1-score of 0.8012 (evaluation carried out similar to Sang and Buchholz (2000)). The sentiment-polarity prediction task is posed as a sentence-pair classification problem using BERT, with the sentence provided as the first segment and the aspect-term as the second segment. This model obtained an F1-score of 0.9080 for the positive po307 Classifier Score Classifier Score Classifier Score Classifier Score (Overall) (1-Aspect) (2-Aspects) (3-or-more Aspects) BERT-Baseline (BB) 0.5158 0.4983 0.5448 0.5036 BB + MLM pretraining (BB0.5298 0.5433 0.5310 0.5145 MLM) BB-MLM + one-zero polar0.5415 0.5675 0.5276 0.5290 ity injection BB-MLM + saliency-based 0.5918 0.6125 0.5828 0.57"
2020.aacl-main.33,D13-1176,0,0.0223727,"show that the system is successful in controlling aspect-level sentiments. 1 Query: Service - Negative Salads - Positive Chicken - Positive The service was slow, but the salads were great and the chicken was tasty and fresh. used language models as discriminators to achieve style (polarity) transfer in sentences. Li et al. (2018a) proposed a simpler method where they deleted the attribute markers and devise a method to replace or generate the target attribute-key phrases in the sentence. With a rapid increase in the quality of generated text, due to the rise of neural text generation models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Vaswani et al., 2017), controllable text generation is quickly becoming the next frontier in the field of text generation. Controllable text generation is the task of generating realistic sentences whose attributes can be controlled. The attributes to control can be: (i). Stylistic: Like politeness, sentiment, formality etc, (ii). Content: Like information, entities, keywords etc. or (iii). Ordering: Like ordering of information, events, plots etc. Controlling sentence level polarity has been well explored as a style transfer task. Zhang et al. (2018"
2020.aacl-main.33,N19-1242,0,0.0623264,"Missing"
2020.aacl-main.86,K18-1030,0,0.0211448,"d to a large body of work in psycholinguistic research that shows a relationship between text processing and gaze behaviour. Mishra and Bhattacharyya (2018) also describe some of the ways that eye-tracking can be used for multiple NLP tasks like translation complexity, sentiment analysis, etc. Research has been done on using gaze behaviour at run time to solve downstream NLP tasks like sentence simplification (Klerke et al., 2016), readability (Gonz´alez-Gardu˜no and Søgaard, 2018; Singh 859 et al., 2016), part-of-speech tagging (Barrett et al., 2016), sentiment analysis (Mishra et al., 2018; Barrett et al., 2018; Long et al., 2019), grammatical error detection (Barrett et al., 2018), hate speech detection (Barrett et al., 2018) and named entity recognition (Hollenstein and Zhang, 2019). Different strategies have been adopted to alleviate the need for gaze behaviour at run time. Barrett et al. (2016) use token level averages of gaze features at run time from the Dundee Corpus (Kennedy et al., 2003), to alleviate the need for gaze behaviour at run time. Singh et al. (2016) and Long et al. (2019) predict gaze behaviour at the tokenlevel prior to using it at run time. Mishra et al. (2018), Gonz´alez-Gard"
2020.aacl-main.86,P16-2094,0,0.03146,"e reads, that is what the mind processes. This hypothesis has led to a large body of work in psycholinguistic research that shows a relationship between text processing and gaze behaviour. Mishra and Bhattacharyya (2018) also describe some of the ways that eye-tracking can be used for multiple NLP tasks like translation complexity, sentiment analysis, etc. Research has been done on using gaze behaviour at run time to solve downstream NLP tasks like sentence simplification (Klerke et al., 2016), readability (Gonz´alez-Gardu˜no and Søgaard, 2018; Singh 859 et al., 2016), part-of-speech tagging (Barrett et al., 2016), sentiment analysis (Mishra et al., 2018; Barrett et al., 2018; Long et al., 2019), grammatical error detection (Barrett et al., 2018), hate speech detection (Barrett et al., 2018) and named entity recognition (Hollenstein and Zhang, 2019). Different strategies have been adopted to alleviate the need for gaze behaviour at run time. Barrett et al. (2016) use token level averages of gaze features at run time from the Dundee Corpus (Kennedy et al., 2003), to alleviate the need for gaze behaviour at run time. Singh et al. (2016) and Long et al. (2019) predict gaze behaviour at the tokenlevel prio"
2020.aacl-main.86,D14-1162,0,\N,Missing
2020.aacl-main.86,P13-2062,1,\N,Missing
2020.aacl-main.86,D16-1115,0,\N,Missing
2020.aacl-main.86,D16-1193,0,\N,Missing
2020.aacl-main.86,W17-5050,0,\N,Missing
2020.aacl-main.86,W16-4123,0,\N,Missing
2020.aacl-main.86,P18-1219,1,\N,Missing
2020.aacl-main.90,agerri-etal-2014-ixa,0,0.0254988,"layers to better encode the multilingual and code-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority"
2020.aacl-main.90,W14-3914,0,0.0140445,"This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from the large-scale ima"
2020.aacl-main.90,bojar-etal-2014-hindencorp,0,0.0606349,"Missing"
2020.aacl-main.90,D18-1024,0,0.0918512,"g of dimension 300 using the word embedding algorithm (Bojanowski et al., 2016). In order to obtain the embedding of Roman script, we transliterate6 the Hindi sentence into the Roman script. These sentences are used to train the code-mixed embedding using the same embedding algorithm (Bojanowski et al., 2016), and we generate the embedding of dimension 300. These three word embeddings have the same dimensions but they are different in vector spaces. Finally, we align monolingual vectors of Hindi and Roman words into the vector space of English word embedding using the approach as discussed in Chen and Cardie (2018). While training, the model loss is computed using the categorical cross entropy function. Optimal hyper-parameters are set to: maximum no. of words in a question=15, CNN filter size={2, 3}, # of shared CNN layers=1, # of shared Bi-LSTM layers=2, hidden dimension =1000, # of attention heads=4, image level and object level feature dimension =2048, # of spatial location in image level feature =100, # of objects in object level feature=36, # of rank in bi-linear pooling=3, # of bilinear attention maps=8, # of epochs=100, initial learning rate=0.002. Optimal values of the hyperparameters are chose"
2020.aacl-main.90,D16-1044,0,0.252498,"swering Deepak Gupta‡ , Pabitra Lenka†∗, Asif Ekbal‡ , Pushpak Bhattacharyya‡ ‡ Indian Institute of Technology Patna, India † International Institute of Information Technology Bhubaneswar, India ‡ {deepak.pcs16, asif, pb}@iitp.ac.in † pabitra.lenka18@gmail.com Abstract and predicts the decision by analyzing the complex scene(s). VQA requires language understanding, fine-grained visual processing and multiple steps of reasoning to produce the correct answer. As the existing research on VQA are mainly focused on natural language questions written in English (Antol et al., 2015; Hu et al., 2017; Fukui et al., 2016; Anderson et al., 2018; Li et al., 2018; Xu and Saenko, 2016; Shih et al., 2016), their applications are often limited. In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Visual Question Answering (VQA) focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enable"
2020.aacl-main.90,L18-1278,1,0.921004,"ode-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from"
2020.aacl-main.90,K18-1012,1,0.872702,"ode-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from"
2020.aacl-main.90,P18-1249,0,0.0264607,"esentation of ith language obtained from the j th layer can be denoted as Qi,j = {q1i,j , q2i,j , . . . , qTi,j }. In our work, we use one layer of CNN and two layers of Bi-LSTM to encode multilingual questions. At each layer of encoding, we apply language specific weight to obtain the language specific encoding layer representation. We denote the question representation obtained from the final encoding layer after applying the language specific attention as h = {ht }Tt=1 . 4.1.2 Self-Attention on Question Inspired from the success of self-attention on various NLP tasks (Vaswani et al., 2017; Kitaev and Klein, 2018), we adopt self-attention to our model for better representation of a word by looking at the other words in the input question. The encoding obtained from multilingual encoding layer (c.f. Section 4.1.1) is passed to the self-attention layer. The multi-head self-attention mechanism (Vaswani et al., 2017) used in our model can be precisely described as follows: QK T Attention(Q, K, V ) = softmax( √ )V dh (5) where, Q, K, V and dh are the query, key, value matrices and dimension of the hidden representation obtained from the multilingual encoding layer, respectively. These matrices are obtained"
2020.aacl-main.90,W04-1013,0,0.0127666,"enough to offer multilingual and code-mixing words belonging to the was mostasked frequent languagecrein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qte }Tt=1 . The convo"
2020.aacl-main.90,D15-1166,0,0.0238489,"f different window sizes is apto provide an accurate natural language answer Aˆ plied on the embedded input. The final output qtc from all the possible answers A. Mathematically: at a time step t is computed by the max-pooling operation over different window size filters. Mathˆ I; φ) Aˆ = arg max p(A|Q, (1) ematically, qtc = max(qtl1 ,c , qtl2 ,c , . . . , qtlL ,c ). The ˆ A∈A final representation computed by CNN layer can be where φ is the network parameters. The architec- denoted as {q c }T . Inspired from the success in t t=1 ture of our proposed methodology is depicted in other NLP tasks (Luong et al., 2015; Yue-Hei Ng Fig 3. Our proposed model has the following com- et al., 2015), we employ stacking of multiple Biponents: LSTM (Hochreiter and Schmidhuber, 1997) layers to capture the semantic representation of an entire 4.1 Multilingual Question Encoding question. The input to the first layer of LSTM is the Given a question5 Q = {q1 , q2 , . . . , qT } having convoluted representation of the question {qtc }Tt=1 . T words, we obtain the multilingual embedding 5 r qtr = Bi-LSTM(qt−1 , qtc ) It denotes the question in English, Hindi or Code-mixed 903 (3) 286 287 288 289 290 291 292 293 294 295 296"
2020.aacl-main.90,P02-1040,0,0.106618,"from our MCVQA dataset. A ble enough to offer multilingual and code-mixing words belonging to the was mostasked frequent languagecrein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qt"
2020.aacl-main.90,D16-1121,0,0.0246249,"Missing"
2020.aacl-main.90,C18-1163,0,0.0368273,"Missing"
2020.aacl-main.90,2006.amta-papers.25,0,0.0114059,"crein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qte }Tt=1 . The convo4 Methodology for MVQA luted feature qtl,c for l length filter is computed as follows: Problem Statement: Giv"
2020.acl-main.401,N19-1034,1,0.676575,"earchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in detecting sarcasm, this has not been attempted much, one of the main reasons being the non-availability of multi-modal datasets. Recently, researchers (Castro et al., 2019) have started exploiting multi-modal sources of information for sarcasm detection. It is true that the modalities like acoustic and visual often provide more evidences about the context of the utterance in compar"
2020.acl-main.401,C16-1151,0,0.0266981,"ation across the modalities to effectively classify sarcasm, sentiment, and emotion. (c). We annotate the recently released Sarcasm dataset, MUStARD with sentiment and emotion classes (both implicit and explicit), and (d). We present the state-of-the-art for sarcasm prediction in multi-modal scenario. 2 Related Work A survey of the literature suggests that a multimodal approach towards sarcasm detection is a fairly new approach rather than a text-based classification. Traditionally, rule-based classification (Joshi et al., 2017; Veale and Hao, 2010) approaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair"
2020.acl-main.401,P19-1455,0,0.185013,"Missing"
2020.acl-main.401,D19-1566,1,0.807181,"proaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in det"
2020.acl-main.401,W14-4012,0,0.0231513,"Missing"
2020.acl-main.401,D18-1382,1,0.514853,"a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in detecting sarcasm, this has not been attempted much, one of the main reasons being the non-availability of multi-modal datasets. Recently, researchers (Castro et al., 2019) have started exploiting multi-modal sources of information for sarcasm de"
2020.acl-main.401,P19-1046,0,0.0960793,"proach towards sarcasm detection is a fairly new approach rather than a text-based classification. Traditionally, rule-based classification (Joshi et al., 2017; Veale and Hao, 2010) approaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask"
2020.acl-main.402,P11-1119,0,0.0682516,"Missing"
2020.acl-main.402,P19-1455,0,0.043848,"Missing"
2020.acl-main.402,N18-2008,0,0.0288682,"ed content or its pragmatic content (Barrett et al., 1993). An utterance such as “Okay sure” or “Ya right” (say) can be considered as “agreement” or- in case of sarcasm- “disagreement”. For expressive DAs such as “greeting”, “thanking”, “apologizing” etc., the speaker’s feeling or emotion can assist in recognizing true communicative intent and vice-versa. Thus, it is important to consider the speaker’s emotion when deciding on the DA. There is considerable work on ER (Cowie et al., 2001), (Jain et al., 2018), (Zhang et al., 2018), etc. and adapting the Virtual Agents (VAs) to act accordingly (Huang et al., 2018), (Zhou et al., 2018), (Fung et al., 2018), etc. But very little research has been done, that addresses the impact of emotion while deciding the DA of an utterance (Novielli and Strapparava, 2013), (Bosma and Andr´e, 2004). As DAs primarily dictate the flow of any dialogue conversation (be it human-human or human-computer), such synergy of ER and DAC is required. Research too has shown the benefit of utilizing the combination of text and nonverbal cues (Poria et al., 2017b), (Poria et al., 2017a) etc., for solving various Natural Language Processing (NLP) tasks. The main advantage of integrati"
2020.acl-main.402,W13-3214,0,0.0273457,"nd single task DAC variants. 1 Introduction Dialogue Act Classification (DAC) is concerned with deciding the type i.e., communicative intention (question, statement, command etc.) of the speaker’s utterance. DAC is very important in the context of discourse structure, which in turn supports intelligent dialogue systems, conversational speech transcription and so on. Considerable works have been done on classical Machine Learning (ML) based DAC (Jurafsky et al., 1997), (Stolcke et al., 2000), (Verbree et al., 2006), etc. and Deep ∗ The authors have contributed equally. Learning (DL) based DAC (Kalchbrenner and Blunsom, 2013), (Papalampidi et al., 2017), (Liu et al., 2017), (Ribeiro et al., 2019), (Ortega et al., 2019), (Saha et al., 2019) etc. Humans are emotional entities. A speaker’s emotional state considerably influences or affects its intended content or its pragmatic content (Barrett et al., 1993). An utterance such as “Okay sure” or “Ya right” (say) can be considered as “agreement” or- in case of sarcasm- “disagreement”. For expressive DAs such as “greeting”, “thanking”, “apologizing” etc., the speaker’s feeling or emotion can assist in recognizing true communicative intent and vice-versa. Thus, it is impo"
2020.acl-main.402,C16-1189,0,0.0172081,"was to construct DA-emotion combinations from the pre-annotated corpus. However, such stringent associations or dis-associations amongst DA-emotion pairs may not truly hold for real life conversations. Related Works The tasks of ER and DAC are extensively explored. Dialogue Act Frameworks: DAC has been investigated since late 90s (Reithinger and Klesen, 1997), (Stolcke et al., 1998) and early 2000’s (Stolcke et al., 2000), (Grau et al., 2004). Much of this research, however, uses chat transcripts with only the text mode, due partly due to unavailability of multi-modal open-source dataset. In (Khanpour et al., 2016), authors apply stacked LSTM to classify speech acts. In (Kumar et al., 2018), the author developed a Hierarchical Network based approach using Bi-LSTMs and the CRF. A contextual self-attention system fused with hierarchical recurrent units was proposed by the authors of (Raheja and Tetreault, 2019) to develop a sequence label classifier. The authors of (Yu et al., 2019) proposed a method for the capture of long-range interactions that span a series of words using a Convolutional Network based approach. In (Saha et al., 2019), authors proposed several ML and DL based 3 Dataset To facilitate an"
2020.acl-main.402,D17-1231,0,0.0214761,"ification (DAC) is concerned with deciding the type i.e., communicative intention (question, statement, command etc.) of the speaker’s utterance. DAC is very important in the context of discourse structure, which in turn supports intelligent dialogue systems, conversational speech transcription and so on. Considerable works have been done on classical Machine Learning (ML) based DAC (Jurafsky et al., 1997), (Stolcke et al., 2000), (Verbree et al., 2006), etc. and Deep ∗ The authors have contributed equally. Learning (DL) based DAC (Kalchbrenner and Blunsom, 2013), (Papalampidi et al., 2017), (Liu et al., 2017), (Ribeiro et al., 2019), (Ortega et al., 2019), (Saha et al., 2019) etc. Humans are emotional entities. A speaker’s emotional state considerably influences or affects its intended content or its pragmatic content (Barrett et al., 1993). An utterance such as “Okay sure” or “Ya right” (say) can be considered as “agreement” or- in case of sarcasm- “disagreement”. For expressive DAs such as “greeting”, “thanking”, “apologizing” etc., the speaker’s feeling or emotion can assist in recognizing true communicative intent and vice-versa. Thus, it is important to consider the speaker’s emotion when dec"
2020.acl-main.402,D14-1162,0,0.0837994,"mage classification model. Initially, each of the frames is preprocessed which includes resizing and normalizing. So, the visual representation of each utterance (F ) is obtained by concatenating the obtained df = 4096 dimensional feature vector for every frame, i.e., F ∈ Rf ×df (Castro et al., 2019), (Illendula and Sheth, 2019), (Poria et al., 2017b), (Poria et al., 2017a). 4.2 Here, we discuss, the process of multi-modal feature extraction. Textual Features. The transcriptions available for each video forms the source of the textual modality2 . To extract textual features, pretrained GloVe (Pennington et al., 2014) embeddings of dimension 300 have been used to obtain representation of words as word vectors. The resultant word embeddings of each word are concatenated to obtain a final utterance representation. While it is indeed possible to use more advanced textual encoding techniques (for e.g., convolutional or recurrent neural network), we decided to use the same pre-trained extractive strategy as in the case of other modalities. Audio Features. To elicit features from the audio, openSMILE (Eyben et al., 2010), an open source software has been used. The features obtained by openSMILE include maxima di"
2020.acl-main.402,P13-1096,0,0.0779492,"Missing"
2020.acl-main.402,P17-1081,0,0.133718,"e et al., 2001), (Jain et al., 2018), (Zhang et al., 2018), etc. and adapting the Virtual Agents (VAs) to act accordingly (Huang et al., 2018), (Zhou et al., 2018), (Fung et al., 2018), etc. But very little research has been done, that addresses the impact of emotion while deciding the DA of an utterance (Novielli and Strapparava, 2013), (Bosma and Andr´e, 2004). As DAs primarily dictate the flow of any dialogue conversation (be it human-human or human-computer), such synergy of ER and DAC is required. Research too has shown the benefit of utilizing the combination of text and nonverbal cues (Poria et al., 2017b), (Poria et al., 2017a) etc., for solving various Natural Language Processing (NLP) tasks. The main advantage of integrating other modalities to text is the usage of behavioral signs present in acoustic (vocal modulations) and visual (facial expression) modalities. In addition, the various modalities offer important signals to better identify the speaker’s communicative intention and emotional state. This will in effect help create sturdy and more reliable DAC models. In this paper, we study the influence of emotion on the identification of DAs, by utilizing the com4361 Proceedings of the 58"
2020.acl-main.402,P19-1050,0,0.0501084,"uce a new dataset (EMOTyDA) consisting of short videos of dialogue conversations manually annotated with its DA along with its pre-annotated emotions. 3.1 Data Collection To gather potentially emotion rich conversations to explore its affect on DAC, we scanned the literature for existing multi-modal ER dataset. During our initial search, we obtained several multi-modal ER datasets which include Youtube (Morency et al., 2011), MOUD (P´erez-Rosas et al., 2013), IEMOCAP (Busso et al., 2008), ICT-MMMO (W¨ollmer et al., 2013), CMU-MOSI (Zadeh et al., 2016), CMU-MOSEI (Zadeh et al., 2018) and MELD (Poria et al., 2019) etc. However, we zeroed down on IEMOCAP and MELD datasets for the further 4362 investigations of our problem statement. The reason behind this choice was that remaining all the datasets mentioned above were particularly monologues involving opinions and product reviews. Whereas our research requires task-independent dyadic or multi-party conversations to analyze its full potential. Both these available datasets are not annotated for their corresponding DAs. Also, benchmark DAC datasets such as Switchboard (SWBD) (Godfrey et al., 1992), ICSI Meeting Recorder (Shriberg et al., 2004) consist of"
2020.acl-main.402,N19-1373,0,0.0177999,"has been investigated since late 90s (Reithinger and Klesen, 1997), (Stolcke et al., 1998) and early 2000’s (Stolcke et al., 2000), (Grau et al., 2004). Much of this research, however, uses chat transcripts with only the text mode, due partly due to unavailability of multi-modal open-source dataset. In (Khanpour et al., 2016), authors apply stacked LSTM to classify speech acts. In (Kumar et al., 2018), the author developed a Hierarchical Network based approach using Bi-LSTMs and the CRF. A contextual self-attention system fused with hierarchical recurrent units was proposed by the authors of (Raheja and Tetreault, 2019) to develop a sequence label classifier. The authors of (Yu et al., 2019) proposed a method for the capture of long-range interactions that span a series of words using a Convolutional Network based approach. In (Saha et al., 2019), authors proposed several ML and DL based 3 Dataset To facilitate and enhance the research in multimodal DAC assisted with user emotion, we introduce a new dataset (EMOTyDA) consisting of short videos of dialogue conversations manually annotated with its DA along with its pre-annotated emotions. 3.1 Data Collection To gather potentially emotion rich conversations to"
2020.acl-main.402,W04-2319,0,0.0556343,"018) and MELD (Poria et al., 2019) etc. However, we zeroed down on IEMOCAP and MELD datasets for the further 4362 investigations of our problem statement. The reason behind this choice was that remaining all the datasets mentioned above were particularly monologues involving opinions and product reviews. Whereas our research requires task-independent dyadic or multi-party conversations to analyze its full potential. Both these available datasets are not annotated for their corresponding DAs. Also, benchmark DAC datasets such as Switchboard (SWBD) (Godfrey et al., 1992), ICSI Meeting Recorder (Shriberg et al., 2004) consist of text and audio-based conversations whereas TRAINS (Heeman and Allen, 1995) consist of solely textbased conversations with no emotional tags. HCRC Map Task corpus (Anderson et al., 1991) additionally encompasses audio modality with the transcripts but the corpus itself has task-oriented conversations and is not annotated for its emotion tags. It is to be noted that task-oriented conversations generally restrict the presence of diverse tags which are commonly encountered in task-independent conversations. To the best of our knowledge, at the time of writing, we were unaware of any si"
2020.acl-main.402,J00-3003,0,0.856075,"Missing"
2020.acl-main.402,P18-1208,0,0.0129908,"d with user emotion, we introduce a new dataset (EMOTyDA) consisting of short videos of dialogue conversations manually annotated with its DA along with its pre-annotated emotions. 3.1 Data Collection To gather potentially emotion rich conversations to explore its affect on DAC, we scanned the literature for existing multi-modal ER dataset. During our initial search, we obtained several multi-modal ER datasets which include Youtube (Morency et al., 2011), MOUD (P´erez-Rosas et al., 2013), IEMOCAP (Busso et al., 2008), ICT-MMMO (W¨ollmer et al., 2013), CMU-MOSI (Zadeh et al., 2016), CMU-MOSEI (Zadeh et al., 2018) and MELD (Poria et al., 2019) etc. However, we zeroed down on IEMOCAP and MELD datasets for the further 4362 investigations of our problem statement. The reason behind this choice was that remaining all the datasets mentioned above were particularly monologues involving opinions and product reviews. Whereas our research requires task-independent dyadic or multi-party conversations to analyze its full potential. Both these available datasets are not annotated for their corresponding DAs. Also, benchmark DAC datasets such as Switchboard (SWBD) (Godfrey et al., 1992), ICSI Meeting Recorder (Shri"
2020.bea-1.8,J08-1001,0,0.0952413,"Missing"
2020.bea-1.8,W18-3705,1,0.904217,"rts - the first was essay scoring, and the second was short-answer scoring. The release of the ASAP AEG dataset1 led to a large number of papers on automatic essay grading using a number of different techniques, from machine learning to deep learning. Section 3 lists the different work in automatic essay grading. In addition to the Kaggle dataset, another dataset - the International Corpus of Learner’s English (ICLE) - is also used in some trait-specific essay grading papers (Granger et al., 2009). Our work, though, makes use of only the ASAP dataset, and the trait specific scores provided by Mathias and Bhattacharyya (2018a) for that dataset. The rest of the paper is organized as follows. In Section 2, we give the motivation for our work. In Section 3, we describe related work done for traitspecific automatic essay grading. In Section 4, we describe the Dataset. In Section 5, we describe the experiments, such as the baseline machine learning systems, the string kernel and super word embeddings, the Neural Network system, etc. We report the results and analyze them in Section 6, and conclude our paper and describe future work in Section 7. Essay traits are attributes of an essay that can help explain how well wr"
2020.bea-1.8,P18-1058,0,0.0614649,"ure engineering and ordinal classification / regression to score essay traits. From the late 1990s / early 2000s onwards, there were many commercial systems that used automatic essay grading. Shermis and Burstein (2013) cover a number of systems that are used commercially, such as E-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (Landauer, 2003), Lightside (Mayfield and Ros´e, 2013), etc. 2 Motivation Most of the work dealing with automatic essay grading either deals with providing an overall score to the essay, but often doesn’t provide any more feedback to the essay’s writer (Carlile et al., 2018). One way to resolve this is by using trait-specific scoring, where we either do feature engineering or construct a neural network, for individual traits. 1 The dataset can be downloaded from https://www. kaggle.com/c/asap-aes/data 85 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 85–91 c July 10, 2020. 2020 Association for Computational Linguistics Prompt ID Prompt 1 Prompt 2 Prompt 3 Prompt 4 Prompt 5 Prompt 6 Prompt 7 Prompt 8 Trait Scores Range 1-6 1-6 0-3 0-3 0-4 0-4 0-6 0-12 Word Count 350 350 100 100 125 150 300 600 No. of Traits"
2020.bea-1.8,P18-2080,0,0.0219169,"with different systems for measuring different traits is often going to be a challenge, especially if someone decides to come up with a new trait to score. Our work involves showing how we can take existing general-purpose systems, and use them to score traits in essays. In our paper, we demonstrate that a neural network, built for scoring essays holistically, performs reasonably well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (So"
2020.bea-1.8,D14-1162,0,0.0820919,"Missing"
2020.bea-1.8,D10-1023,0,0.0357512,"essays. In our paper, we demonstrate that a neural network, built for scoring essays holistically, performs reasonably well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay trai"
2020.bea-1.8,D16-1115,0,0.0233547,"say Grading Holistic essay grading is assigning an overall score for an essay. Ever since the release of Kaggle’s Automatic Student Assessment Prize’s (ASAP) Automatic Essay Grading (AEG) dataset in 2012, there has been a lot of work on holistic essay grading. Initial approaches, such as those of Phandi et al. (2015) and Zesch et al. (2015) made use of machine learning techniques in scoring the essays. A number of other works used various deep learning approaches, such as Long Short Term Memory (LSTM) Networks (Taghipour and Ng, 2016; Tay et al., 2018) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016; Dong et al., 2017). The current State-of-the-Art in holistic essay grading makes use of word embedding clusters, called super word embeddings, and string kernels (Cozma et al., 2018). 4 Dataset The dataset we use is the ASAP AEG dataset. The original ASAP AEG dataset only has trait scores for prompts 7 & 8. Mathias and Bhattacharyya (2018a) provide the trait scores for the remaining prompts 2 . Tables 1 and 2 describe the different essay sets and the traits for each essay set respectively. 2 The dataset and scores can be downloaded from http: //www.cfilt.iitb.ac.in/˜egdata/. 86 Essay Set Pro"
2020.bea-1.8,P13-1026,0,0.0260576,"lly, performs reasonably well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by"
2020.bea-1.8,K17-1017,0,0.0873724,"cides to come up with a new trait to score. Our work involves showing how we can take existing general-purpose systems, and use them to score traits in essays. In our paper, we demonstrate that a neural network, built for scoring essays holistically, performs reasonably well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits"
2020.bea-1.8,P14-1144,0,0.019981,". We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (2015) and Mathias and Bhattacharyya (2018a)."
2020.bea-1.8,P15-1053,0,0.0184827,"chine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (2015) and Mathias and Bhattacharyya (2018a). The second uses a string kernel-base app"
2020.bea-1.8,P19-1390,0,0.0120449,"ly well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (201"
2020.bea-1.8,P16-1205,0,0.014313,"ttacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (2015) and Mathias and Bhattacharyya (2018a). The second uses a string kernel-base approach as well as super word embeddings as described by Cozma et al. (20"
2020.bea-1.8,D15-1049,0,0.0296485,"et al. (2017). Our work is also, to the best of our knowledge, the first work that uses the same neural network architecture to automatically score essay traits. Related Work In this section, we describe related work in the area of automatic essay grading. 3.1 Trait-specific Essay Grading Holistic Essay Grading Holistic essay grading is assigning an overall score for an essay. Ever since the release of Kaggle’s Automatic Student Assessment Prize’s (ASAP) Automatic Essay Grading (AEG) dataset in 2012, there has been a lot of work on holistic essay grading. Initial approaches, such as those of Phandi et al. (2015) and Zesch et al. (2015) made use of machine learning techniques in scoring the essays. A number of other works used various deep learning approaches, such as Long Short Term Memory (LSTM) Networks (Taghipour and Ng, 2016; Tay et al., 2018) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016; Dong et al., 2017). The current State-of-the-Art in holistic essay grading makes use of word embedding clusters, called super word embeddings, and string kernels (Cozma et al., 2018). 4 Dataset The dataset we use is the ASAP AEG dataset. The original ASAP AEG dataset only has trait scores for pr"
2020.bea-1.8,C14-1090,0,0.0240046,"network, built for scoring essays holistically, performs reasonably well for scoring essay traits too. We compare it with a task-independent machine learning system using task independent features (Mathias and Bhattacharyya, 2018a), as well as a stateof-the-art string kernel system (Cozma et al., 2018) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of tas"
2020.bea-1.8,Q18-1007,0,0.0227568,"8) and report statistically significant results when we use the attention based neural network (Dong et al., 2017). 3 Over the years, there has been a fair amount of work done in trait-specific essay grading, in essay traits such as organization (Persing et al., 2010; Taghipour, 2017), coherence (Somasundaran et al., 2014), thesis clarity (Persing and Ng, 2013; Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (2015) and Mathias and Bhattacharyya (2018a). The second uses a string kernel-base approach as well as super word embeddings as described by Cozma et al. (2018). The third is a deep learning attention based neural network described by Dong et al. (20"
2020.bea-1.8,D16-1193,0,0.0158264,"n the area of automatic essay grading. 3.1 Trait-specific Essay Grading Holistic Essay Grading Holistic essay grading is assigning an overall score for an essay. Ever since the release of Kaggle’s Automatic Student Assessment Prize’s (ASAP) Automatic Essay Grading (AEG) dataset in 2012, there has been a lot of work on holistic essay grading. Initial approaches, such as those of Phandi et al. (2015) and Zesch et al. (2015) made use of machine learning techniques in scoring the essays. A number of other works used various deep learning approaches, such as Long Short Term Memory (LSTM) Networks (Taghipour and Ng, 2016; Tay et al., 2018) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016; Dong et al., 2017). The current State-of-the-Art in holistic essay grading makes use of word embedding clusters, called super word embeddings, and string kernels (Cozma et al., 2018). 4 Dataset The dataset we use is the ASAP AEG dataset. The original ASAP AEG dataset only has trait scores for prompts 7 & 8. Mathias and Bhattacharyya (2018a) provide the trait scores for the remaining prompts 2 . Tables 1 and 2 describe the different essay sets and the traits for each essay set respectively. 2 The dataset and scor"
2020.bea-1.8,W15-0626,0,0.0233446,"Ke et al., 2019), prompt adherence (Persing and Ng, 2014), argument strength (Persing and Ng, 2015; Taghipour, 2017; Carlile et al., 2018), stance (Persing and Ng, 2016), style (Mathias and Bhattacharyya, 2018b), and narrative quality (Somasundaran et al., 2018). Most of these works use feature engineering with classifiers to score the essay traits. All the above mentioned works describe systems for scoring different traits individually. In our paper, we compare three approaches to score essay traits, which are trait agnostic. The first uses a set of task-independent features as described by Zesch et al. (2015) and Mathias and Bhattacharyya (2018a). The second uses a string kernel-base approach as well as super word embeddings as described by Cozma et al. (2018). The third is a deep learning attention based neural network described by Dong et al. (2017). Our work is also, to the best of our knowledge, the first work that uses the same neural network architecture to automatically score essay traits. Related Work In this section, we describe related work in the area of automatic essay grading. 3.1 Trait-specific Essay Grading Holistic Essay Grading Holistic essay grading is assigning an overall score"
2020.coling-main.111,W13-3520,0,0.0773284,"Missing"
2020.coling-main.111,E12-1004,0,0.0950436,"Missing"
2020.coling-main.111,Q17-1010,0,0.0489395,"Once the network is trained, the learned transformation function T (xi ) is applied to all the words in X to map the pre-trained word embeddings to a new transformed vector space X t ∈ Rn×d . 4 Experimental Setup To evaluate our retrofitting approach, we experimented with three pre-trained word embeddings that are learned using different distributional models: (1) GloVe (Pennington et al., 2014): trained on Common Crawl data; (2) Word2Vec (Mikolov et al., 2013): trained on Wikipedia dump available on polyglot project 1294 Lexical Disjoint Lexical Overlap (Al-Rfou’ et al., 2013); (3) FastText (Bojanowski et al., 2017): trained on Wikipedia 2017. As explained in section 2, we use WordNet to obtain two types of constraints: (1) Similarity relationship constraints: a total of 425,732 constraints from synonymy and antonymy relations; (2) Type hierarchy constraints: a total of 100,100 constraints from hypernymy relation. The margin parameter is set to 0.6 and 0.2 for the similarity relationship constraints and the type hierarchy constraints respectively1 . We compare our model (referred as TripletNet hereafter) with three state of the art retrofitting models: (1) Counterfit (Mrkˇsi´c et al., 2016): It defines t"
2020.coling-main.111,N15-1184,0,0.0790673,"Missing"
2020.coling-main.111,D16-1235,0,0.0295727,"Missing"
2020.coling-main.111,P18-1004,0,0.0345842,"Missing"
2020.coling-main.111,J15-4004,0,0.291944,"y and hypernymy relations in WordNet and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task. 1 Introduction Word embedding models (Pennington et al., 2014; Mikolov et al., 2013) are primarily inspired from the distributional hypothesis (Harris, 1954) viz. words that appear in similar context tend to have similar meaning. However, these models have one major drawback: they mix semantic similarity with other types of semantic relatedness (Hill et al., 2015). Consider for example, cheap and expensive. Though completely opposite in meaning, these words tend to occur in nearly identical contexts and end up having similar distributional vectors. This is problematic for many end applications such as sentiment analysis, text simplification, and so on. To address this issue, researchers have proposed various models to combine information from external knowledge sources such as WordNet, Freebase, etc. into unsupervised learning of word embeddings. These models mainly focus on the constraints extracted from various types of relations such as synonymy, an"
2020.coling-main.111,W19-4310,0,0.0256664,"Missing"
2020.coling-main.111,N15-1098,0,0.0480371,"Missing"
2020.coling-main.111,P15-1145,0,0.0833465,"cal contexts and end up having similar distributional vectors. This is problematic for many end applications such as sentiment analysis, text simplification, and so on. To address this issue, researchers have proposed various models to combine information from external knowledge sources such as WordNet, Freebase, etc. into unsupervised learning of word embeddings. These models mainly focus on the constraints extracted from various types of relations such as synonymy, antonymy, hypernymy, etc. At a high level, these models are categorized into: Joint specialization models (Yu and Dredze, 2014; Liu et al., 2015; Xu et al., 2014); and Retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Kamath et al., 2019). Joint specialization models typically modify the optimization objective of distributional models by integrating the constraints into the objective function. Whereas, retrofitting models update the word vectors of distributional models in a post-processing training step using data generated from the constraints. Current retrofitting models have one limitation. They use constraints that tend to push cosine similarity to extremes (+1 or -1). While this works wel"
2020.coling-main.111,N16-1018,0,0.0358548,"Missing"
2020.coling-main.111,D14-1162,0,0.113763,"s are used as training data to learn a non-linear transformation function that maps original word vectors to a vector space respecting these constraints. The transformation function is learned in a similarity metric learning setting using Triplet network architecture. We applied our model to synonymy, antonymy and hypernymy relations in WordNet and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task. 1 Introduction Word embedding models (Pennington et al., 2014; Mikolov et al., 2013) are primarily inspired from the distributional hypothesis (Harris, 1954) viz. words that appear in similar context tend to have similar meaning. However, these models have one major drawback: they mix semantic similarity with other types of semantic relatedness (Hill et al., 2015). Consider for example, cheap and expensive. Though completely opposite in meaning, these words tend to occur in nearly identical contexts and end up having similar distributional vectors. This is problematic for many end applications such as sentiment analysis, text simplification, and so on."
2020.coling-main.111,D18-1026,0,0.0260425,"Missing"
2020.coling-main.111,N18-1103,0,0.028208,"Missing"
2020.coling-main.111,C14-1212,0,0.0724896,"Missing"
2020.coling-main.111,Q15-1025,0,0.0220202,"for many end applications such as sentiment analysis, text simplification, and so on. To address this issue, researchers have proposed various models to combine information from external knowledge sources such as WordNet, Freebase, etc. into unsupervised learning of word embeddings. These models mainly focus on the constraints extracted from various types of relations such as synonymy, antonymy, hypernymy, etc. At a high level, these models are categorized into: Joint specialization models (Yu and Dredze, 2014; Liu et al., 2015; Xu et al., 2014); and Retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Kamath et al., 2019). Joint specialization models typically modify the optimization objective of distributional models by integrating the constraints into the objective function. Whereas, retrofitting models update the word vectors of distributional models in a post-processing training step using data generated from the constraints. Current retrofitting models have one limitation. They use constraints that tend to push cosine similarity to extremes (+1 or -1). While this works well for relations such as synonymy and anotonymy, it does not work so well for relations"
2020.coling-main.111,P14-2089,0,0.0114688,"ccur in nearly identical contexts and end up having similar distributional vectors. This is problematic for many end applications such as sentiment analysis, text simplification, and so on. To address this issue, researchers have proposed various models to combine information from external knowledge sources such as WordNet, Freebase, etc. into unsupervised learning of word embeddings. These models mainly focus on the constraints extracted from various types of relations such as synonymy, antonymy, hypernymy, etc. At a high level, these models are categorized into: Joint specialization models (Yu and Dredze, 2014; Liu et al., 2015; Xu et al., 2014); and Retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Kamath et al., 2019). Joint specialization models typically modify the optimization objective of distributional models by integrating the constraints into the objective function. Whereas, retrofitting models update the word vectors of distributional models in a post-processing training step using data generated from the constraints. Current retrofitting models have one limitation. They use constraints that tend to push cosine similarity to extremes (+1 or -1). Wh"
2020.coling-main.119,P17-1042,0,0.0201093,"res (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular sim"
2020.coling-main.119,P18-1073,0,0.0179705,"imedia Dumps; as on April 22, 2020 Additional Monolingual Corpus JNU Sanskrit Proses Corpus Indic NLP Library FastText - GitHub 1387 uses the supervised method named MUSE (Conneau et al., 2017)10 which utilizes a manually curated bilingual lexicon11 for alignments. We use Hindi as a pivot language due to the ease of computation and availability of resources (Corpora and WordNet size). We use the monolingual models described above and train 13 cross-lingual word embedding models (thirteen language pairs over 100 dimensions) using this approach. The second cross-lingual methodology uses VecMap (Artetxe et al., 2018), which utilizes the monolingual models created above. VecMap uses an optional normalization feature while it builds the mappings between any two monolingual models. It performs orthogonal transformation and maps semantically related words, similar to MUSE, which was used in our first approach for building cross-lingual models. Additionally, it also reduces the dimensions of the embeddings models, which, is optional. We train it using the same hyperparameters as described above, for consistency while evaluating. We used the supervised approach for training these models as well, and the trainin"
2020.coling-main.119,D16-1162,0,0.0258029,"an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approximately around 1M tokens (Arthur et al., 2016). Our detected cognate list size varies from 930 cognates (Hi-Te) to 15834 (Hi-Mr). Due to the addition of more parallel instances to the corpus, the vocabulary size for NMT increases. Hence, we experiment further by varying the BPE merges, in a close range, to the optimal merge point obtained earlier. We report the results of the best optimal merge setting, for both NMT-BPE Baseline model and the cognate injected NMT-BPE model, in"
2020.coling-main.119,N09-3008,0,0.0125337,"nza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word re"
2020.coling-main.119,Q17-1010,0,0.0429109,"any other script to the Devanagari script. We perform Unicode transliteration using Indic NLP Library8 to convert scripts for Bn, As, Or, Gu, Pa, Ml, Ta, Kn and Te to Devanagari for standardization. Hi, Mr, Ko, Ne, and Sa are already based on the Devanagari script. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size o"
2020.coling-main.119,bojar-etal-2014-hindencorp,0,0.028968,"Missing"
2020.coling-main.119,D18-2029,0,0.0251243,"l dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular similarity distinguishes nearly parallel vectors much better as small changes in vector values yield considerable distances. For each word pair vector and its context vectors, we compute the ‘word-pair similarity’ and ‘contextual similarity’. We use arccos to obtain angular cosine similarity (asim) among vectors ‘u’ and ‘v’, as shown below:     u.v asim(u, v) = 1 − arccos /π (3) kukkvk Each candidate word-pair generates a score i.e., score1, and the average of scores among all words in the context dictionary generates another score i.e., s"
2020.coling-main.119,P14-2017,0,0.106733,"a. Link: Data, code and models This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information"
2020.coling-main.119,P15-2071,0,0.0199277,"(Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation e"
2020.coling-main.119,P19-4007,0,0.0420076,"Missing"
2020.coling-main.119,D18-1027,0,0.0152337,"e two similarity scores (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and thei"
2020.coling-main.119,E17-1113,0,0.0425306,"Missing"
2020.coling-main.119,jha-2010-tdil,0,0.0142001,"Missing"
2020.coling-main.119,N10-1103,0,0.043284,"be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or"
2020.coling-main.119,2019.gwc-1.51,1,0.808977,", Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic simil"
2020.coling-main.119,2020.lrec-1.378,1,0.798533,"ages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894 2560 11295 Non-Cognates 16119 15057 15983 15166 23029 8976 4005 1084 4101 3810 13027 1918 9826 Table 1: Number of cognates and non-cognates for each language pair in the dataset. Hi-Ne* and Hi-Ko* were generated via replicating their approach (Kanojia et al., 2020). Language Hi Bn Gu Mr Pa Sa Ml Ta Te Ne As Kn Ko Or Corpus Size 48142K 1564K 439K 520K 505K 553K 495K 909K 1023K 706K 504K 159K 214K 744K STTR (n=1000) 0.5821 0.5437 0.4587 0.6108 0.4314 0.5350 0.7339 0.6411 0.4950 0.4883 0.5968 0.5338 0.5614 0.4160 Table 2: Corpus Statistics where corpus size is the approximate number of lines, and STTR is the moving average type-token ratio on a windows of 1000 sentences. 3 Dataset and Experimental Setup In this section, we describe our primary dataset for the cognate detection task. We also describe the datasets used for building cross-lingual word embeddi"
2020.coling-main.119,P17-4012,0,0.0362403,"once the learning rate falls below 0.001. We perform our experiments with the feature sets (Orthographic (WLS), Phonetic (PVS), and three different cross-lingual embeddings based feature sets) described above for all the thirteen language pairs. We also perform an ablation test with various feature sets and report the results for the best feature combination in the next section. The results of our classification task can be seen in Table 3 and are discussed in the next section, in detail. 4.5 Cognate-aware Neural Machine Translation (NMT) Task For the NMT task, we use the OpenNMT-Py toolkit (Klein et al., 2017) to perform our experiments. We use a Bidirectional RNN Encoder-Decoder architecture with attention (Bahdanau et al., 2014). We choose three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best bas"
2020.coling-main.119,N03-2016,0,0.112806,"ginated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in French can translate to"
2020.coling-main.119,A00-2038,0,0.178355,"logies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (20"
2020.coling-main.119,N01-1014,0,0.277746,"Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual word embeddings. Cross-lingual w"
2020.coling-main.119,2005.mtsummit-papers.40,0,0.203812,"s that have originated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in Fr"
2020.coling-main.119,W12-0216,0,0.0171469,"tion methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Prev"
2020.coling-main.119,N01-1020,0,0.147349,"ative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language"
2020.coling-main.119,J99-1003,0,0.0606347,"ion 4 presents the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection"
2020.coling-main.119,K19-1011,0,0.0223535,"Missing"
2020.coling-main.119,mulloni-pekar-2006-automatic,0,0.0637471,"the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly i"
2020.coling-main.119,W97-1102,0,0.46133,"entations for each token. 4 Approaches We use various approaches to perform the cognate detection task viz. baseline cognate detection approaches like orthographic similarity-based, phonetic similarity-based, phonetic vectors with SiameseCNN based proposed by Rama (2016), and Recurrent neural network-based approach proposed by Kanojia et al. (2019b). We use the same hyperparameters and architectures, as discussed in these papers. We describe each of these feature sets in this section. 4.1 Weighted Lexical Similarity (WLS) The Normalized Edit Distance (NED) approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in our dataset. Each of the operations has unit cost (except that substitution of a character by itself has zero cost), so NED is equal to the minimum number of operations to transform ‘word a’ to ‘word b’. We use a similarity score provided by NED, which is calculated as (1 - NED Score). We combine NED with q-gram distance (Shannon, 1948) for a better similarity score. The qgrams (‘n-grams’) are simply substrings of length q. This distance measure has been applied previously for various spelling correction approaches (Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et"
2020.coling-main.119,P02-1040,0,0.108173,"e three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best baseline model with an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approx"
2020.coling-main.119,N18-1202,0,0.0510587,"pt. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5"
2020.coling-main.119,P19-1493,0,0.0254159,"ransliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5 Link: Link: 7 Link: 8 Link: 9"
2020.coling-main.119,N18-2063,0,0.0356884,"Missing"
2020.coling-main.119,C16-1097,0,0.0946439,"work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual wor"
2020.coling-main.119,W99-0626,0,0.196426,"put. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for c"
2020.coling-main.119,W19-4720,0,0.0231209,"lingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic similarity arises for non-identical cognates, based on the reaction time from human annotators. Similarly, Merlo and Andueza Rodriguez (2019) show that cross-lingual models exhibit the semantic properties of for bilingual lexicons despite their structural simplicities, which leads us to perform our investigation for low-resource Indian languages. Uban et al. (2019) discuss the semantic change in languages by studying the change in cognate words across Romance languages using cross-lingual similarity. All of the previous approaches discussed above, lack the use of an appropriate cross-lingual similarity-based measure and do not work well for Indian languages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 1702"
2020.coling-main.249,P18-1128,0,0.0143589,"ovement of 4.02 and 3.18 points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric. To analyze the contribution of each component of the proposed model, we perform an ablation study reported in Table 4. Our results suggest that providing multitask learning with shared encoder helps the model to improve the QG performance from 19.55 to 20.64 BLEU-4. Introducing the supporting facts information obtained from the answer-aware supporting fact prediction task further improves the 2 We follow the bootstrap test (Efron and Tibshirani, 1994) using the setup provided by Dror et al. (2018). 2768 QG performance from 20.64 to 21.28 BLEU-4. Joint training of QG with the supporting facts prediction provides stronger supervision for identifying and utilizing the supporting facts information. In other words, by sharing the document encoder between both the tasks, the network encodes better representation (supporting facts aware) of the input document. Such presentation is capable of efficiently filtering out the irrelevant information when processing multiple documents and performing multi-hop reasoning for question generation. Further, the MultiHop-Enhanced Reward (MER) with Rouge r"
2020.coling-main.249,P17-1123,0,0.0188867,"ting questions. The former regime consists of rule-based approaches (Heilman and Smith, 2010; Chali and Hasan, 2015) that rely on human-designed features such as named-entity information, etc. to leverage the semantic information from a context for question generation. In the second category, question generation problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy whe"
2020.coling-main.249,P16-1014,0,0.0845371,"Missing"
2020.coling-main.249,P18-1064,0,0.0240707,"a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer gener"
2020.coling-main.249,L18-1440,1,0.908711,"Missing"
2020.coling-main.249,K18-1012,1,0.85926,"Missing"
2020.coling-main.249,W13-2114,0,0.0390926,"Missing"
2020.coling-main.249,D15-1166,0,0.0436525,"the sentence to be a supporting fact. The architecture of this network is illustrated in Figure 1 (left). This network is then trained with a binary cross entropy loss and the ground-truth supporting facts labels: Lsp = − N nj 1 XX δ j log(pji ) + (1 − δyj 6=1 ) log(1 − pji ) i N j=1 i=1 yi =1 (4) where N is the number of document list, S the number of candidate sentences in a particular training example, δij and pji represent the ground truth supporting facts label and the output Sigmoid probability, respectively. 3.1.3 Question Decoder We use a LSTM network with global attention mechanism (Luong et al., 2015) to generate the question ˆ = {y1 , y2 , . . . , ym } one word at a time. We use copy mechanism (See et al., 2017; Gulcehre et al., Q 2016) to deal with rare or unknown words. At each timestep t, st = LST M (st−1 , yt−1 ) (5) The attention distribution αt and context vector ct are obtained using the following equations: et,i = sTt ∗ hi exp(et,i ) αt,i = PN j=1 exp(et,j ) ct = N X (6) αt,i hi i=1 The probability distribution over the question vocabulary is then computed as, Pvocab = softmax(tanh(Wq ∗ [ct ⊕ st ])) (7) where Wq is a weight matrix. The probability of picking a word (generating) fr"
2020.coling-main.249,P17-1018,0,0.209642,"n problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominant"
2020.coling-main.249,K17-1028,0,0.0198355,"he first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are s"
2020.coling-main.249,Q18-1021,0,0.118683,"QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are spread across multiple documents. 3 Proposed Approach Problem Statement: In multi-hop question generation, we consider a document list L with nL documents, and an m-word answer A. Let the total number of words in all the document"
2020.coling-main.249,D18-1259,0,0.379055,"2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are spread across multiple documents. 3 Proposed Approach Problem Statement: In multi-hop question generation, we consider a document list L with nL documents, and an m-word answer A. Let the total number of words in all the documents Di ∈ L combined be N . Let a document list"
2020.coling-main.249,D19-1253,0,0.0976439,"Missing"
2020.coling-main.249,D18-1424,0,0.210277,"c. to leverage the semantic information from a context for question generation. In the second category, question generation problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang"
2020.coling-main.249,N10-1086,0,\N,Missing
2020.coling-main.249,P02-1040,0,\N,Missing
2020.coling-main.249,W05-0909,0,\N,Missing
2020.coling-main.249,W04-1013,0,\N,Missing
2020.coling-main.249,D14-1162,0,\N,Missing
2020.coling-main.249,W17-2623,0,\N,Missing
2020.coling-main.249,P17-1099,0,\N,Missing
2020.coling-main.249,D18-1427,0,\N,Missing
2020.coling-main.383,P19-1019,0,0.019486,"ences in the training process of NMT, some of these techniques are: providing a weight component in the loss function (Junczys-Dowmunt, 2018), using curriculum learning (Zhang et al., 2019), and dynamically selecting data with iterations (van der Wees et al., 2017). Pretraining in unsupervised NMT is generally focused on language model training of both encoder and decoder to make them understand the language properties and to provide a good initialization to the finetuning phase. Finetuning utilizes the approaches proposed in Lample et al. (2018) which involves denoising and back-translation. Artetxe et al. (2019) proposed a good initialization mechanism using statistical machine translation for training unsupervised NMT. Wu et al. (2019) proposed a new architecture for unsupervised NMT which does not use back-translation but try to find the best possible translations from the target corpus and edit them to make pseudo parallel sentence pairs. Yang et al. (2018) proposed to utilize two independent encoders with sharing some partial weights. Lample and Conneau (2019) proposed a pretraining mechanism for unsupervised NMT to pretrain encoder and decoder separately using monolingual data. Song et al. (2019"
2020.coling-main.383,W19-5206,0,0.016975,"roving the performance of NMT systems for low This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 4334 Proceedings of the 28th International Conference on Computational Linguistics, pages 4334–4339 Barcelona, Spain (Online), December 8-13, 2020 resource languages (Morishita et al., 2018; Imankulova et al., 2019). In Reinforcement learning based approaches, rewards for pseudo parallel sentence pairs based on language model score and round trip reconstruction error helps NMT models (He et al., 2016). Caswell et al. (2019) provides an identification mark for synthetic sentence pairs while training on a mix set of human generated and synthetic sentence pairs . Junczys-Dowmunt (2018) show that filtering the parallel data based on cross-entropy scores which calculates the agreement between both models of both directions from source to target and target to source helps in selection of good pseudo parallel sentence pairs. Dou et al. (2020) show that for iterative back-translation selecting and weighing sentences based on the quality of sentence pairs improves the performance of NMT systems, they use combination of d"
2020.coling-main.383,2020.emnlp-main.475,0,0.0153873,"ent learning based approaches, rewards for pseudo parallel sentence pairs based on language model score and round trip reconstruction error helps NMT models (He et al., 2016). Caswell et al. (2019) provides an identification mark for synthetic sentence pairs while training on a mix set of human generated and synthetic sentence pairs . Junczys-Dowmunt (2018) show that filtering the parallel data based on cross-entropy scores which calculates the agreement between both models of both directions from source to target and target to source helps in selection of good pseudo parallel sentence pairs. Dou et al. (2020) show that for iterative back-translation selecting and weighing sentences based on the quality of sentence pairs improves the performance of NMT systems, they use combination of different scores like round-trip BLEU, tf-idf, language model scores etc. to select top sentences, and then on rest of the sentences they use encoder representation similarities and agreement between forward and backward models to provide weights to back-translated data. Wang et al. (2019a) proposed uncertainty-based confidence measures to select good pseudo parallel sentence pairs. Wang et al. (2019b) proposed to sel"
2020.coling-main.383,D18-1040,0,0.0177816,"ring of back-translated data in unsupervised NMT. We briefly describe some related concepts of back-translation, unsupervised NMT and language model pretraining in this section. Back-translation utilizes target-side monolingual data to create pseudo parallel sentence pairs using a translation system from target to source which is then utilized to train source to target NMT system (Sennrich et al., 2016). Hoang et al. (2018) show that iteratively generating better synthetic data improves the NMT performance. Quality of back-translated data plays an important role in performance of NMT systems (Fadaee and Monz, 2018; Poncelas et al., ). Fadaee and Monz (2018) show that the target side words which have high prediction loss gets most benefit from the addition of synthetic data. Filtered pseudo parallel data selected with a threshold on round-trip BLEU score helps in improving the performance of NMT systems for low This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 4334 Proceedings of the 28th International Conference on Computational Linguistics, pages 4334–4339 Barcelona, Spain (Online), December 8-13, 2020 re"
2020.coling-main.383,W18-2703,0,0.0162664,"of unsupervised NMT, we utilize existing pretraining approaches proposed in Lample and Conneau (2019) and Song et al. (2019). 2 Related Work Our work majorly involves the exploration of filtering of back-translated data in unsupervised NMT. We briefly describe some related concepts of back-translation, unsupervised NMT and language model pretraining in this section. Back-translation utilizes target-side monolingual data to create pseudo parallel sentence pairs using a translation system from target to source which is then utilized to train source to target NMT system (Sennrich et al., 2016). Hoang et al. (2018) show that iteratively generating better synthetic data improves the NMT performance. Quality of back-translated data plays an important role in performance of NMT systems (Fadaee and Monz, 2018; Poncelas et al., ). Fadaee and Monz (2018) show that the target side words which have high prediction loss gets most benefit from the addition of synthetic data. Filtered pseudo parallel data selected with a threshold on round-trip BLEU score helps in improving the performance of NMT systems for low This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativeco"
2020.coling-main.383,W18-6478,0,0.0209119,"censes/by/4.0/. Licence details: 4334 Proceedings of the 28th International Conference on Computational Linguistics, pages 4334–4339 Barcelona, Spain (Online), December 8-13, 2020 resource languages (Morishita et al., 2018; Imankulova et al., 2019). In Reinforcement learning based approaches, rewards for pseudo parallel sentence pairs based on language model score and round trip reconstruction error helps NMT models (He et al., 2016). Caswell et al. (2019) provides an identification mark for synthetic sentence pairs while training on a mix set of human generated and synthetic sentence pairs . Junczys-Dowmunt (2018) show that filtering the parallel data based on cross-entropy scores which calculates the agreement between both models of both directions from source to target and target to source helps in selection of good pseudo parallel sentence pairs. Dou et al. (2020) show that for iterative back-translation selecting and weighing sentences based on the quality of sentence pairs improves the performance of NMT systems, they use combination of different scores like round-trip BLEU, tf-idf, language model scores etc. to select top sentences, and then on rest of the sentences they use encoder representatio"
2020.coling-main.383,P07-2045,0,0.0091895,"from (Lample and Conneau, 2019) and (Song et al., 2019) for language model pretraining. For XLM we utilize masked language model pretraining 2 . We utilize transformer architecture with 6 layers, 8 heads, 1024 hidden units, GELU activation units, attention drop-out of 0.1, learning rate starts from 10−4 and batch size of 32 sentences. We perform decoding using beam search. BPE codes are learnt using FastBPE3 using 60000 BPE codes over the combined data of both languages. The epoch size is set to 200000 sentences. We use adam (Kingma and Ba, 2002) optimizer. We perform tokenization using moses(Koehn et al., 2007). For calculating sentence-wise BLEU scores 1 http://www.statmt.org/wmt16/translation-task.html https://github.com/facebookresearch/XLM 3 https://github.com/glample/fastBPE 2 4336 using tensors we utilize allennlp4 (Gardner et al., 2018) toolkit. We choose best model from different iterations according to BLEU score on validation set. We evaluate our results using tokenized BLEU scores calculated using multi-bleu.pl5 . 4.3 Results We utilize MASS6 (Song et al., 2019) as our base implementation and update the back-translation phase to include weight of the pseudo parallel sentence pairs in the"
2020.coling-main.383,W04-3250,0,0.20924,"rvised NMT (∗ indicates statistically significant improvement) It is clear from Table 2 that our approach to filter back-translated data (providing weight as per pseudo sentence-pair quality) gives better results for Lample and Conneau (2019) approach. We also performed an experiment to examine the impact of denoising in the finetuning phase with filtering of back-translated data for en-fr language pair using masked sequence to sequence pretraining, which gave BLEU score of 27.02 for en-fr and 26.01 for fr-en, which is an improvement over the baseline. We perform paired bootstrap re-sampling (Koehn, 2004) for a p-value less than 0.05 for statistical significance test. Figure 1: BLEU scores for each epoch for en-fr Figure 1 represents the BLEU scores of test data for each epoch while training for Lample and Conneau (2019) with and without filtering. In initial iterations model with filtering is not performing good but as training progresses it starts performing better than model with no filtering. This happens because we start the filtering process from the beginning of the finetuning phase when the quality of generated backtranslated data is poorer than later iterations. We also observe that t"
2020.coling-main.383,W18-6421,0,0.0177521,"). Fadaee and Monz (2018) show that the target side words which have high prediction loss gets most benefit from the addition of synthetic data. Filtered pseudo parallel data selected with a threshold on round-trip BLEU score helps in improving the performance of NMT systems for low This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 4334 Proceedings of the 28th International Conference on Computational Linguistics, pages 4334–4339 Barcelona, Spain (Online), December 8-13, 2020 resource languages (Morishita et al., 2018; Imankulova et al., 2019). In Reinforcement learning based approaches, rewards for pseudo parallel sentence pairs based on language model score and round trip reconstruction error helps NMT models (He et al., 2016). Caswell et al. (2019) provides an identification mark for synthetic sentence pairs while training on a mix set of human generated and synthetic sentence pairs . Junczys-Dowmunt (2018) show that filtering the parallel data based on cross-entropy scores which calculates the agreement between both models of both directions from source to target and target to source helps in selection"
2020.coling-main.383,P02-1040,0,0.115464,"yl,n is 1 when l from vocab is correct word otherwise 0. We utilize the weighted cross-entropy loss function: − BS X b=1 |L ||N | X X wb ∗ ( yl,n log(ˆ yl,n )) (2) l=1 n=1 wb is the batch-wise normalized round-trip BLEU score between source and round-trip translation of source. The normalization function is given by: wun wb = PBS b b=1 wunb (3) where wb is normalized round trip bleu score for bth sentence in the batch. wunb is un-normalized round trip BLEU score for bth sentence. BLEU score is bilingual evaluation understudy which is commonly utilized to evaluate machine translation systems (Papineni et al., 2002). There exist various other methods to evaluate machine translation systems but for start we are considering sentence-wise BLEU score which is the most popular one. We provide results for the same approaches shown in Song et al. (2019), Lample and Conneau (2019) with and without filtering in the back-translation phase. In Song et al. (2019) finetuning is only done using iterative back-translation and in Lample and Conneau (2019) finetuning is done using denoising and back-translation similar to (Lample et al., 2018). 4 Experiment and Results In this section we show the impact of inclusion of f"
2020.coling-main.383,J82-2005,0,0.585613,"Missing"
2020.coling-main.383,P16-1009,0,0.0189274,"is also a key component of unsupervised NMT, we utilize existing pretraining approaches proposed in Lample and Conneau (2019) and Song et al. (2019). 2 Related Work Our work majorly involves the exploration of filtering of back-translated data in unsupervised NMT. We briefly describe some related concepts of back-translation, unsupervised NMT and language model pretraining in this section. Back-translation utilizes target-side monolingual data to create pseudo parallel sentence pairs using a translation system from target to source which is then utilized to train source to target NMT system (Sennrich et al., 2016). Hoang et al. (2018) show that iteratively generating better synthetic data improves the NMT performance. Quality of back-translated data plays an important role in performance of NMT systems (Fadaee and Monz, 2018; Poncelas et al., ). Fadaee and Monz (2018) show that the target side words which have high prediction loss gets most benefit from the addition of synthetic data. Filtered pseudo parallel data selected with a threshold on round-trip BLEU score helps in improving the performance of NMT systems for low This work is licensed under a Creative Commons Attribution 4.0 International Licen"
2020.coling-main.383,D17-1147,0,0.0346507,"Missing"
2020.coling-main.383,D17-1155,0,0.0176386,"nsupervised NMT has three main components: cross-lingual embeddings, denoising, and back-translation, where training involves alternating between denoising and back-translation after a good initialization process (Lample and Conneau, 2019; Song et al., 2019). In this paper our focus is on improving finetuning phase, we are introducing a weight component in unsupervised NMT training based on the quality of pseudo parallel sentence pairs generated for training in back-translation phase. These kinds of techniques have been utilized in domain adaptation to give more weight to in-domain sentences (Wang et al., 2017). Pretraining is also a key component of unsupervised NMT, we utilize existing pretraining approaches proposed in Lample and Conneau (2019) and Song et al. (2019). 2 Related Work Our work majorly involves the exploration of filtering of back-translated data in unsupervised NMT. We briefly describe some related concepts of back-translation, unsupervised NMT and language model pretraining in this section. Back-translation utilizes target-side monolingual data to create pseudo parallel sentence pairs using a translation system from target to source which is then utilized to train source to target"
2020.coling-main.383,D19-1073,0,0.0194067,"oth models of both directions from source to target and target to source helps in selection of good pseudo parallel sentence pairs. Dou et al. (2020) show that for iterative back-translation selecting and weighing sentences based on the quality of sentence pairs improves the performance of NMT systems, they use combination of different scores like round-trip BLEU, tf-idf, language model scores etc. to select top sentences, and then on rest of the sentences they use encoder representation similarities and agreement between forward and backward models to provide weights to back-translated data. Wang et al. (2019a) proposed uncertainty-based confidence measures to select good pseudo parallel sentence pairs. Wang et al. (2019b) proposed to select in-domain and clean data based on co-curricular learning. In Domain adaptation different techniques have been applied to give more weight to in-domain sentences in the training process of NMT, some of these techniques are: providing a weight component in the loss function (Junczys-Dowmunt, 2018), using curriculum learning (Zhang et al., 2019), and dynamically selecting data with iterations (van der Wees et al., 2017). Pretraining in unsupervised NMT is general"
2020.coling-main.383,P19-1123,0,0.0164706,"oth models of both directions from source to target and target to source helps in selection of good pseudo parallel sentence pairs. Dou et al. (2020) show that for iterative back-translation selecting and weighing sentences based on the quality of sentence pairs improves the performance of NMT systems, they use combination of different scores like round-trip BLEU, tf-idf, language model scores etc. to select top sentences, and then on rest of the sentences they use encoder representation similarities and agreement between forward and backward models to provide weights to back-translated data. Wang et al. (2019a) proposed uncertainty-based confidence measures to select good pseudo parallel sentence pairs. Wang et al. (2019b) proposed to select in-domain and clean data based on co-curricular learning. In Domain adaptation different techniques have been applied to give more weight to in-domain sentences in the training process of NMT, some of these techniques are: providing a weight component in the loss function (Junczys-Dowmunt, 2018), using curriculum learning (Zhang et al., 2019), and dynamically selecting data with iterations (van der Wees et al., 2017). Pretraining in unsupervised NMT is general"
2020.coling-main.383,N19-1120,0,0.0219642,"nt, 2018), using curriculum learning (Zhang et al., 2019), and dynamically selecting data with iterations (van der Wees et al., 2017). Pretraining in unsupervised NMT is generally focused on language model training of both encoder and decoder to make them understand the language properties and to provide a good initialization to the finetuning phase. Finetuning utilizes the approaches proposed in Lample et al. (2018) which involves denoising and back-translation. Artetxe et al. (2019) proposed a good initialization mechanism using statistical machine translation for training unsupervised NMT. Wu et al. (2019) proposed a new architecture for unsupervised NMT which does not use back-translation but try to find the best possible translations from the target corpus and edit them to make pseudo parallel sentence pairs. Yang et al. (2018) proposed to utilize two independent encoders with sharing some partial weights. Lample and Conneau (2019) proposed a pretraining mechanism for unsupervised NMT to pretrain encoder and decoder separately using monolingual data. Song et al. (2019) show that training encoder and decoder simultaneously using monolingual data helps in pretraining of unsupervised NMT. 3 Appr"
2020.coling-main.383,P18-1005,0,0.0180637,"oder and decoder to make them understand the language properties and to provide a good initialization to the finetuning phase. Finetuning utilizes the approaches proposed in Lample et al. (2018) which involves denoising and back-translation. Artetxe et al. (2019) proposed a good initialization mechanism using statistical machine translation for training unsupervised NMT. Wu et al. (2019) proposed a new architecture for unsupervised NMT which does not use back-translation but try to find the best possible translations from the target corpus and edit them to make pseudo parallel sentence pairs. Yang et al. (2018) proposed to utilize two independent encoders with sharing some partial weights. Lample and Conneau (2019) proposed a pretraining mechanism for unsupervised NMT to pretrain encoder and decoder separately using monolingual data. Song et al. (2019) show that training encoder and decoder simultaneously using monolingual data helps in pretraining of unsupervised NMT. 3 Approach Current state of the art approaches for unsupervised NMT (Lample and Conneau, 2019; Song et al., 2019) do not consider the quality of each generated pseudo parallel sentence pair in the process of training, all generated ps"
2020.coling-main.383,N19-1189,0,0.0183502,"representation similarities and agreement between forward and backward models to provide weights to back-translated data. Wang et al. (2019a) proposed uncertainty-based confidence measures to select good pseudo parallel sentence pairs. Wang et al. (2019b) proposed to select in-domain and clean data based on co-curricular learning. In Domain adaptation different techniques have been applied to give more weight to in-domain sentences in the training process of NMT, some of these techniques are: providing a weight component in the loss function (Junczys-Dowmunt, 2018), using curriculum learning (Zhang et al., 2019), and dynamically selecting data with iterations (van der Wees et al., 2017). Pretraining in unsupervised NMT is generally focused on language model training of both encoder and decoder to make them understand the language properties and to provide a good initialization to the finetuning phase. Finetuning utilizes the approaches proposed in Lample et al. (2018) which involves denoising and back-translation. Artetxe et al. (2019) proposed a good initialization mechanism using statistical machine translation for training unsupervised NMT. Wu et al. (2019) proposed a new architecture for unsuperv"
2020.coling-main.393,W11-0705,0,0.0410433,"est of the paper is structured as follows. In Section 2, we present a brief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning"
2020.coling-main.393,N19-1034,1,0.87851,"Missing"
2020.coling-main.393,W17-5526,0,0.0606686,"Missing"
2020.coling-main.393,D19-1566,1,0.844274,"lated work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affect analysis and explored a contextual inter-modal attention framework tha"
2020.coling-main.393,L18-1252,0,0.0576593,"on. Our present work differs from these single and multi-label emotion and sentiment classification works as we tend to classifying emotions and sentiments on dialogue conversations that require contextual information of the previous utterances, thereby making the task more challenging and interesting. Every human-machine interactions are grounded in conversations driven by emotions. Hence, identifying the emotion in dialogue is essential for building robust systems capable of such interactions. Recently, investigations on emotion detection in conversations has been in demand. The authors in (Chen et al., 2018) released a dataset taken from Friends TV series for detecting emotions in dialogues. Similarly, in (Yeh et al., 2019) an attention framework was designed for identifying emotions in spoken dialog systems. In (Hazarika et al., 2018b), memory networks were adopted to capture contextual information for emotion detection in conversations. To capture the contextual information in conversations, DialogueRNN (Majumder et al., 2019) employs three gated recurrent units (GRU) for effectively modeling the past utterances of the speaker and the listener in dyadic conversations for emotion detection. As c"
2020.coling-main.393,D18-1382,1,0.752804,"rief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affect analysis and explored a contextual inter-modal"
2020.coling-main.393,D19-1015,0,0.0153091,"classification of sentiments (Poria et al., 2017; Majumder et al., 2018). The authors in (Majumder et al., 2018) proposed a novel hierarchical feature fusion strategy for integrating different modalities, such as audio, video and text for identifying the sentiments. The authors in (Poria et al., 2019) extended the EmotionLines dataset by incorporating audio and visual modalities for correct identification of emotions and sentiments in conversations. The MELD dataset has been further used for building different neural frameworks for jointly identifying emotion and sentiment from conversations (Ghosal et al., 2019; Zhang et al., 2019b; Zhang et al., 2019a). As opposed to these existing works on multimodal emotion and sentiment classification on dialogue data, our present works provides a balanced multimodal multi-label emotion, intensity and sentiment dataset for the classification of multiple emotions and sentiment in the given utterance. 3 Multimodal Multi-label Emotion, Intensity and Sentiment Dialogue (MEISD) Dataset We create the MEISD dataset1 from the 10 famous TV shows belonging to different genres: (i). Comedy: Friends, The Big Bang Theory, How I Met Your Mother, The Office; (ii). Drama: House"
2020.coling-main.393,D18-1280,0,0.0795982,"balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agen"
2020.coling-main.393,N18-1193,0,0.0988445,"balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agen"
2020.coling-main.393,S18-1019,0,0.014699,"ring utterances and their multi-modal information. With the advancements in Artificial Intelligence (AI), emotion classification and sentiment analysis have become a significant task due to its importance in many downstream tasks, such as customer behavior modeling, response generation for conversational agents, multimodal interactions etc. Hence, to maximize user satisfaction and providing a better experience to the customer, it is important to understand the correct emotion and sentiment of the customer. Recently, multi-label emotion classification has been investigated for textual data in (Kim et al., 2018; He and Xia, 2018; Yu et al., 2018; Huang et al., 2019). Using multiple Convolution Neural Network (CNN) networks along with self-attention, the authors in (Kim et al., 2018) performed multi-label emotion classification on twitter data. Similarly, the authors in (Yu et al., 2018) improved the performance of multi-label emotion classification on twitter data by using transfer learning. Lately, sequence-to-sequence framework (Huang et al., 2019) has been employed for multi-label emotion classification. Our present work differs from these single and multi-label emotion and sentiment classificati"
2020.coling-main.393,D14-1181,0,0.00738607,"Torch 3 framework. Based on the validation set, we set the threshold value of 0.2 for the classification of multiple emotions in a given utterance. For all the baselines, in the final output layer we apply softmax activation function for emotion and sentiment classification while we apply sigmoid activation function for intensity prediction. text-CNN: In this approach, we only use the textual information for identifying the emotion and sentiment of every utterance in a dialogue. In this framework, we use the word embeddings of the utterances as input to the convolutional neural network (CNN) (Kim, 2014) for obtaining the sentence representation. In this model, we do not use the contextual information or the additional information from the different modalities for identifying the emotion or sentiment of an utterance. bcLSTM: This baseline employing bi-directional RNN for capturing the contextual information was proposed by (Poria et al., 2017). It employs a two-step hierarchical mechanism that captures the unimodal context first followed by the bi-modal context features. In this methodology, we incorporate the provision of capturing information from all the three modalities. A CNN-LSTM approa"
2020.coling-main.393,W17-5205,0,0.0497398,"Missing"
2020.coling-main.393,S18-1001,0,0.0246803,"conversational context for correctly identifying the emotions, intensity and sentiments in a dialogue. DialogueRNN + BERT: We propose a stronger baseline built upon the DialogueRNN for correct classification of emotion and sentiment, and for intensity prediction. We are able to improve the performance of DialogueRNN by using BERT(Devlin et al., 2018) embedding instead of Glove embedding to represent the textual features. 4.3 Evaluation Metrics For multi-label emotion classification, we use the automatic metrics as mentioned below following the works of (Huang et al., 2019; Yang et al., 2018; Mohammad et al., 2018): Jaccard Index (Rogers and Tanimoto, 1960), Hamming Loss (Schapire and Singer, 1999) and Micro-averaged F1-score (Manning et al., 2008). For sentiment analysis we report Micro-averaged F1-score while for intensity prediction we report Pearson correlation co-efficient (Mohammad and Bravo-Marquez, 2017) in a similar manner as (Akhtar et al., 2019b). 5 Result and Discussion In this section, we provide the results for all the three tasks, i.e. multi-label emotion classification, intensity prediction and sentiment analysis on our proposed MEISD dataset. In Table 5, we provide the results of all th"
2020.coling-main.393,D14-1162,0,0.0833781,"es Train Valid Test 58 22 120 31 1039 114 280 702 93 205 No. of Utterances Train Valid Test 4386 1430 5810 1623 9989 1109 2610 14040 1860 4100 Table 4: Comparison of different multimodal conversational datasets and our proposed MEISD dataset 4 Experiments The extraction of features along with the details of the baseline models to evaluate our proposed MEISD dataset is described in this section. We also discuss the metrics used to evaluate the models on the proposed dataset. 4.1 Feature Extraction Textual Features: For textual features, we take the pre-trained 300-dimensional GloVe embeddings (Pennington et al., 2014) of every word as features. Audio Features: We encode audio tracks with the pre-trained VGGish network (Hershey et al., 2017), which is trained on Audioset (Gemmeke et al., 2017) consisting of 100 million YouTube videos. It has been shown to improve the audio emotion and sentiment classification. We extract audio features of dimension 128 from the last fully connected layer. Visual Features: Due to computational cost, we only consider the middle frame of the video to extract visual feature Vk . We use 2048-dimension pooled features from the last block of Resnet-101 (He et al., 2016) pre-traine"
2020.coling-main.393,P13-1096,0,0.0253225,"Missing"
2020.coling-main.393,P17-1081,0,0.30135,"nsity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agents capable of having real conversations wit"
2020.coling-main.393,P19-1050,0,0.310258,"Missing"
2020.coling-main.393,D13-1170,0,0.00392318,"ructured as follows. In Section 2, we present a brief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affec"
2020.coling-main.393,C18-1330,0,0.0175392,"nits (GRU) to model conversational context for correctly identifying the emotions, intensity and sentiments in a dialogue. DialogueRNN + BERT: We propose a stronger baseline built upon the DialogueRNN for correct classification of emotion and sentiment, and for intensity prediction. We are able to improve the performance of DialogueRNN by using BERT(Devlin et al., 2018) embedding instead of Glove embedding to represent the textual features. 4.3 Evaluation Metrics For multi-label emotion classification, we use the automatic metrics as mentioned below following the works of (Huang et al., 2019; Yang et al., 2018; Mohammad et al., 2018): Jaccard Index (Rogers and Tanimoto, 1960), Hamming Loss (Schapire and Singer, 1999) and Micro-averaged F1-score (Manning et al., 2008). For sentiment analysis we report Micro-averaged F1-score while for intensity prediction we report Pearson correlation co-efficient (Mohammad and Bravo-Marquez, 2017) in a similar manner as (Akhtar et al., 2019b). 5 Result and Discussion In this section, we provide the results for all the three tasks, i.e. multi-label emotion classification, intensity prediction and sentiment analysis on our proposed MEISD dataset. In Table 5, we provi"
2020.coling-main.393,D18-1137,0,0.0195007,"l information. With the advancements in Artificial Intelligence (AI), emotion classification and sentiment analysis have become a significant task due to its importance in many downstream tasks, such as customer behavior modeling, response generation for conversational agents, multimodal interactions etc. Hence, to maximize user satisfaction and providing a better experience to the customer, it is important to understand the correct emotion and sentiment of the customer. Recently, multi-label emotion classification has been investigated for textual data in (Kim et al., 2018; He and Xia, 2018; Yu et al., 2018; Huang et al., 2019). Using multiple Convolution Neural Network (CNN) networks along with self-attention, the authors in (Kim et al., 2018) performed multi-label emotion classification on twitter data. Similarly, the authors in (Yu et al., 2018) improved the performance of multi-label emotion classification on twitter data by using transfer learning. Lately, sequence-to-sequence framework (Huang et al., 2019) has been employed for multi-label emotion classification. Our present work differs from these single and multi-label emotion and sentiment classification works as we tend to classifying"
2020.coling-main.393,P18-1208,0,0.0153658,"iment analysis very interesting as well as challenging. In Figure 3, we show the emotion shift of a speaker as the dialogue grows. Figure 3: A dialogue from the MEISD dataset showcasing the emotion shift as the conversation grows. The text in blue represents the sentiment label while the text in red represents the emotion label of every utterance. 3.2 Comparison with Related Datasets The available datasets for multimodal emotion detection and sentiment classification are nonconversational. The examples of such datasets are MOUD (P´erez-Rosas et al., 2013), MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018) that have been deeply investigated by the researchers for both the tasks. Two dyadic conversational datasets, IEMOCAP (Busso et al., 2008) and SEMAINE (McKeown et al., 2011) have gained popularity for encouraging research on emotion detection for conversations. Recently, MELD (Poria et al., 2019) dataset was released to inspire research on multiparty conversations using information from different modalities. IEMOCAP Dataset: The IEMOCAP (Interactive Emotional Dyadic Motion Capture Database) dataset (Busso et al., 2008) comprises of videos of dyadic interactions between pairs of 10 speakers ac"
2020.coling-main.534,N19-1253,0,0.0231668,"guages, the potential for improvement in performance is substantial. Most of the current approaches for morphological analysis use cross-lingual transfer learning from a higher resource language to some low resource language (McCarthy et al., 2019). But choosing the high resource language for transfer learning is still done in an ad hoc manner, with the most common criteria being the phylogenetic distance in the language family (Cotterell and Heigold, 2017; Johnson et al., 2017). However, it has been shown that all languages from the same family might not share the same linguistic properties (Ahmad et al., 2019). In this paper, we use different cross-lingual training methodologies and analyse the resulting source-target language pair performances based on different linguistic factors. 2 Models We adapt the two-step attention process from the state of the art (Anastasopoulos and Neubig, 2019) on the SIGMORPHON 2019 morphological inflection task (McCarthy et al., 2019), switching the input and output to use it as a lemmatiser. The model has four parts: separate encoders for both the tags and the input character sequence, an attention mechanism, and a decoder. The encoder for the lemma is single layer b"
2020.coling-main.534,D19-1091,0,0.0484014,"resource language for transfer learning is still done in an ad hoc manner, with the most common criteria being the phylogenetic distance in the language family (Cotterell and Heigold, 2017; Johnson et al., 2017). However, it has been shown that all languages from the same family might not share the same linguistic properties (Ahmad et al., 2019). In this paper, we use different cross-lingual training methodologies and analyse the resulting source-target language pair performances based on different linguistic factors. 2 Models We adapt the two-step attention process from the state of the art (Anastasopoulos and Neubig, 2019) on the SIGMORPHON 2019 morphological inflection task (McCarthy et al., 2019), switching the input and output to use it as a lemmatiser. The model has four parts: separate encoders for both the tags and the input character sequence, an attention mechanism, and a decoder. The encoder for the lemma is single layer bidirectional LSTM. Morphological tags are also input to the model for which we use self-attention encoders (Vaswani et al., 2017) without * These authors contributed equally to this work This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence deta"
2020.coling-main.534,2020.acl-main.421,0,0.0176414,"method from Cotterell et al. (2016) to generate additional artificial data to augment the low resource datasets. The method relies on substituting multiple possible stems in a word with random sequences of characters while preserving its length. For each language, the training data is augmented so that the total training set size is equal to 10,000, including the original training data. 3.2 Cross-lingual training For the remainder of this section, let L1 be the source language(high resource) and L2 be the target language(low resource). We use a modified transfer learning method adapted from (Artetxe et al., 2020) that transfers learning from a model learnt on L1 to another language L2 based on results on a validation set (see Appendix B for more details). The entire seq2seq model is broken up into modules, with the encoder, decoder, attention layers (called EDA module for the remainder of the section) the same for both source and transfer language. The embedding layers and the dense output layers are different for each language. The training then proceeds as follows (all the modules have been listed on the right side, with the trainable modules italicised): 6071 bn 45 52 70 64 24 Bengali (bn) Hindi (h"
2020.coling-main.534,N16-1102,0,0.0485693,"Missing"
2020.coling-main.534,D17-1078,0,0.0200316,"an languages, which are a subset of the numerous languages spoken and written in India, have enough resources for training a deep learning model. For the remaining languages, the potential for improvement in performance is substantial. Most of the current approaches for morphological analysis use cross-lingual transfer learning from a higher resource language to some low resource language (McCarthy et al., 2019). But choosing the high resource language for transfer learning is still done in an ad hoc manner, with the most common criteria being the phylogenetic distance in the language family (Cotterell and Heigold, 2017; Johnson et al., 2017). However, it has been shown that all languages from the same family might not share the same linguistic properties (Ahmad et al., 2019). In this paper, we use different cross-lingual training methodologies and analyse the resulting source-target language pair performances based on different linguistic factors. 2 Models We adapt the two-step attention process from the state of the art (Anastasopoulos and Neubig, 2019) on the SIGMORPHON 2019 morphological inflection task (McCarthy et al., 2019), switching the input and output to use it as a lemmatiser. The model has four"
2020.coling-main.534,W16-2002,0,0.0280232,"Script Type LTR Abudgida LTR Abudgida LTR Abudgida LTR Abudgida LTR Abudgida RTL Abjad Total 3,394 10,000 3,506 10,000 61 10,000 High 3,394 10,000 3,506 10,000 10,000 Low 100 100 100 100 61 100 Table 1: Number of inflected-word lemma pairs available for each language. The Total column shows the original number of samples and the High and Low columns show the curated training dataset size in a high and low resource setting respectively. During training, we augment the dataset to 10,000 samples in the low resource setting. LTR: Left-to-right, RTL: Right-to-left We use the alignment method from Cotterell et al. (2016) to generate additional artificial data to augment the low resource datasets. The method relies on substituting multiple possible stems in a word with random sequences of characters while preserving its length. For each language, the training data is augmented so that the total training set size is equal to 10,000, including the original training data. 3.2 Cross-lingual training For the remainder of this section, let L1 be the source language(high resource) and L2 be the target language(low resource). We use a modified transfer learning method adapted from (Artetxe et al., 2020) that transfers"
2020.coling-main.534,D15-1166,0,0.0116622,"al., 2017) without * These authors contributed equally to this work This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/. 6070 Proceedings of the 28th International Conference on Computational Linguistics, pages 6070–6076 Barcelona, Spain (Online), December 8-13, 2020 positional embeddings, since the tag embeddings should be order-invariant. At each timestep, on the decoder side, two context vectors are created via two different attention matrices over the output from the encoding of lemma and tag (Luong et al., 2015). The decoder then computes the output in a two-step process: it first creates a tag-informed state by attending over tags using the output from the decoder at the previous time step. We then compute the state vector by attending over the source characters using the tag-informed state. Using the updated state, the output character for that timestep is produced. This output is passed through a fully connected layer before applying a softmax to get the output character. We also add structural bias to the attention model that encourages Markov assumption over alignments, that is, if the i-th sour"
2020.coling-main.534,W19-4226,0,0.24193,"improving state-of-the-art for high resource languages. In contrast, research on low resource languages has been slow to start. For Indian languages, this is a major issue. Only some of the 22 scheduled Indian languages, which are a subset of the numerous languages spoken and written in India, have enough resources for training a deep learning model. For the remaining languages, the potential for improvement in performance is substantial. Most of the current approaches for morphological analysis use cross-lingual transfer learning from a higher resource language to some low resource language (McCarthy et al., 2019). But choosing the high resource language for transfer learning is still done in an ad hoc manner, with the most common criteria being the phylogenetic distance in the language family (Cotterell and Heigold, 2017; Johnson et al., 2017). However, it has been shown that all languages from the same family might not share the same linguistic properties (Ahmad et al., 2019). In this paper, we use different cross-lingual training methodologies and analyse the resulting source-target language pair performances based on different linguistic factors. 2 Models We adapt the two-step attention process fro"
2020.coling-main.534,P19-1148,0,0.011598,"uickly in this phase compared to Phase 3, although the time to convergence varies with different language pairs. We use the model with the lowest validation loss for training the next phase in each case. A total of 25 cross-lingual models are created. Since sufficient resources for Telugu were not available, models with Telugu as L1 could not be created. All the hyperparameters used are mentioned in Appendix A. We release all our code online for reproducibility and further research. 4 Results and Discussion Table 2 lists the accuracy of our architecture and the hard monotonic attention model (Wu and Cotterell, 2019) for different language pairs in the context of cross-lingual as well as monolingual setting. The hard monotonic attention model in the cross-lingual setting was adapted from the SIGMORPHON 2019 shared task 1 (McCarthy et al., 2019). 6072 4.1 Right to Left scripts We see that both models achieve a very low accuracy for Urdu in the extremely low resource setting. Urdu as a source language in cross-lingual training is not effective as well - the accuracy values for the target languages lie within the corresponding standard deviation range. To identify the possible source of low accuracy, we crea"
2020.eamt-1.21,P19-1122,0,0.0617446,"Missing"
2020.eamt-1.21,J99-2004,0,0.450548,"oehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the current state-of-the-art NMT model. In a reference-simulated setting, we found that our target-language syntax-informed interactive setup can significantly reduce human effort in a Frenchto-English translation task. We also extract syntactic features from constituency-based parse trees of the source French sentences following Akoury et al. (2019), and use them as the conditional context in the interactive-predictive Transformer framework. Experiments show that this context"
2020.eamt-1.21,J09-1002,0,0.0879581,"Missing"
2020.eamt-1.21,1997.mtsummit-papers.1,0,0.723576,"nd model supertags and constituency parse tree-based features collectively as the conditional context for interactive prediction in NMT. Our experimental results indicate that these syntactic feature types are complementary. As a result, this collaborative strategy turns out to be the bestperforming in the French-to-English task while significantly outperforming those setups that include either feature type on WPA and WSR. To the best of our knowledge, this is the very first study that investigates the possibility of integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignme"
2020.eamt-1.21,E12-1025,0,0.0473397,"Missing"
2020.eamt-1.21,P07-1037,1,0.783792,"Missing"
2020.eamt-1.21,W18-1820,0,0.0624757,"Missing"
2020.eamt-1.21,2016.amta-researchers.9,0,0.295217,"oyens, de prendre les choses en main.’ to English. The reference translation is ‘we decide therefore, citizens, to take control of things’ which is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore"
2020.eamt-1.21,W04-3250,0,0.0148959,"Missing"
2020.eamt-1.21,N03-1017,0,0.0815021,"h is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the c"
2020.eamt-1.21,P14-2094,0,0.0219439,"integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic kn"
2020.eamt-1.21,W19-6610,0,0.0153564,"al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully syntactified inter"
2020.eamt-1.21,D14-1107,0,0.0125701,"ts contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch size of 1,800 tokens. EasyCCG4 (Lewis and Steedman, 2014), a CCG supertagger, is used for generating the CCG sequence for the English sentences. Transformer (Baseline) Source Syntactified (SS) Target Syntactified (TS) Fully Syntactified (FS) 26.90 26.96 27.10 27.36 (p-value: 0.059) Table 3: The BLEU scores of baseline and syntactified NMT systems. Table 3 shows the performance of our baseline and syntax-sensitive NMT systems in terms of BLEU. The second and third rows represent the NMT models that incorporate source- and targetlanguage syntactic contexts, respectively, which we call source- (SS) and target-syntactified (TS) NMT systems, respectively"
2020.eamt-1.21,W17-4707,0,0.0327765,"Missing"
2020.eamt-1.21,J16-1004,0,0.0277097,"Missing"
2020.eamt-1.21,P02-1040,0,0.107611,"he sub-word units of a word inherit the CCG category of the word. As an example, we show an English sentence with supertags in Table 1. We see from row E of Table 1 that CCG ‘N’ of a word ‘Oberth¨ur’ is distributed over its sub-words (i.e. Ober@@ th@@ u¨ @@ and r). Our first experimental setup is referred to as PredCCG. Akoury et al. (2019) showed that integrating target-side ground-truth syntactic information into Transformer at decoding time significantly improved translation quality, and their syntaxbased model outperformed the baseline Transformer model by a large margin in terms of BLEU (Papineni et al., 2002). In reality, there is no way of obtaining the target-side ground-truth syntactic information at decoding time. However, in interactive-predictive mode, we found a way to obtain a slightly better CCG sequence for the partial translation (i.e. validated prefix) and inject them into the model at run-time, which we believe can positively impact the model’s subsequent predictions. In other words, in our second setup, we integrate a CCG supertagger into our INMT framework, and apply that on the validated prefix and unchecked suffix on the fly. The tagger is invoked when the user makes a correction."
2020.eamt-1.21,K18-1015,0,0.0135389,"(Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully"
2020.eamt-1.21,2014.eamt-1.5,0,0.116043,"Missing"
2020.eamt-1.21,C16-2004,0,0.0626817,"Missing"
2020.eamt-1.21,P16-1007,0,0.0162171,"tiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT Th"
2020.eamt-1.21,C00-2137,0,0.0138574,"Missing"
2020.eamt-1.21,L16-1561,0,0.0119616,"ble 2, a new CCG tag sequence is generated for the hypothesis, and we see that CCG (N) of the newly added token ‘,’ is correct. Finally, INMT predicts another suggestion (row 9 of Table 2) where we see the remaining predictions are correct in the context. We call this experimental setup OnflyCCG. Note that the model is trained at sub-word level and generates sub-words at output; however, word level tokens are presented to the user. Naturally, On the fly CCG supertagger is applied to a hypothesis of word level. 5.2 MT systems We carry out experiments with French-to-English with the UN corpus2 (Ziemski et al., 2016). The training and development sets contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch"
2020.findings-emnlp.206,N19-1423,0,0.288809,"e, beliefs, and moral values of the respective communities. Linguists have studied the phenomenon of codemixing, put forward many linguistic hypotheses (Belazi et al., 1994; Pfaff, 1979; Poplack, 1978), and formulated various constraints (Sankoff and Poplack, 1981; Di Sciullo et al., 1986; Joshi, 1982) to define a general rule for code-mixing. However, for all the scenarios of code-mixing, particularly for the syntactically divergent languages (Berk-Seligson, 1986), these limitations cannot be postulated as a universal rule. In recent times, the pre-trained language model based architectures (Devlin et al., 2019; Radford et al., 2019) have become the state-of the-art models for language understanding and generation. The underlying data to train such models comes from the huge amount of corpus, available in the form of Wikipedia, book corpus etc. Although, these are readily available in various languages, there is a scarcity of such amount of data in code-mixed form which could be used to train the state-of-the-art transformer (Vaswani et al., 2017) based language model, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLM (Lample and Conneau, 2019) etc. The existing benchmark datasets"
2020.findings-emnlp.206,P18-1128,0,0.0133917,"M feature decreases the Bleu score by 1.36 points. We observe the near similar impact of LM feature on each language pair. Finally, the transfer learning is also proven to be an integral component of the proposed model as it contributes to the maximum of 2.25 Bleu score for en-fr and minimum of 1.02 Bleu score of en-te code-mixed language pair. The difference between the maximum and minimum contribution may be attributed to the fact that, we have sufficient parallel corpus (197, 922) to train the en-fr NMT model as compared to the en-te parallel corpus (10, 105). We follow the bootstrap test (Dror et al., 2018) which confirms that the performance improvement over the baselines are statistically significant as (p &lt; 0.005). 5.3 Human 4.19 2.34 3.26 Qualitative Analysis We assess the quality of the generated code-mixed text, and show these samples in Table 4. We observe that the code-mixed sentences generated using the PG model are able to copy the entities from the given English sentence, but the generated code-mixed sentences are incomplete and not fluent compared to the reference sentences. For example, in en-hi pair the PG based code-mixed sentence missed the ‘main’ word and it copies ‘India’s’ rat"
2020.findings-emnlp.206,N13-1073,0,0.0426302,"English and substitute their aligned counterparts with the identified English words to synthesize the English embedded code-mixed sentences. The input to our synthetic code-mixed generation algorithm (details are in Appendix) is a parallel sentence pair. We use the Indic-nlp-library1 to tokenize the sentences of the Indic languages. Moses based tokenizer2 is used to translate the European and English language texts. Thereafter, we learn the alignment matrix, which guides to select the words or phrases to be mixed in the language. We use the official implementation3 of the fastalign algorithm (Dyer et al., 2013) to obtain the alignment matrix. The alignment matrix is used to construct the aligned phrases between the parallel sentences. We extract the PoS (mainly adjective), named entity (NE) and noun phrase (NP) from the English sentences, and insert them into the appropriate places of the sentences in the other language (i.e. the target language) counterparts. We use the Stanford library4 Stanza (Qi et al., 2020) to 1 https://github.com/anoopkunchukuttan/indic_ nlp_library 2 https://github.com/moses-smt/mosesdecoder 3 https://github.com/clab/fast_align 4 https://github.com/stanfordnlp/stanza Figure"
2020.findings-emnlp.206,D18-1346,0,0.122992,"eness of our proposed approach on eight different language pairs, viz. English-Hindi (en-hi), English-Bengali (en-bn), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering"
2020.findings-emnlp.206,P16-1014,0,0.0132895,"l and generation paradigms. For the given input sentence E : {e1 , e2 , . . . , em }, we extract the language model feature L : {l1 , l2 , . . . , lm }. The extracted language model features are fused to the linguistic features as follows: 4.3 Decoding with Pointer Generator We use the one-layer LSTM network with the attention mechanism (Bahdanau et al., 2015) to generate the code-mixed sentence y1 , y2 , . . . , yn one word at a time. In order to deal with the rare or unknown words, the decoder has the flexibility to copy the words from documents via the pointing mechanism (See et al., 2017; Gulcehre et al., 2016). The LSTM decoder reads the word embedding ut−1 and the hidden state st−1 to generate the hidden state st at time step t. Concretely, h∗t = tanh(Wh ht + bh ) lt∗ = tanh(Wl lt + bl ) g = σ(Wg .[ht ⊕ lt ]) st = LST M (st−1 , ut−1 ) (2) ft = g ⊙ h∗t + (1 − g) ⊙ lt∗ where, ⊕ and ⊙ are the concatenation and elementwise multiplication operator. First, we project both the features ht and lt into the same vector space h∗t and lt∗ via feed-forward network. Thereafter, we learn the gated value g which controls the flow of each feature. The gated value g controls how much of each feature should be the p"
2020.findings-emnlp.206,W16-6331,1,0.891348,"Missing"
2020.findings-emnlp.206,K18-1012,1,0.9198,"exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpus (English to respective language). Thereafter, we utilize this data to develop a neural based model to generate the code-mixed sentences from t"
2020.findings-emnlp.206,C82-1023,0,0.764438,"ing is a common expression of multilingualism in informal text and speech, where there is a switch between the two languages, frequently with one in the character set of the other language. This has been a mean of communication in a multi-cultural and multi-lingual society, and varies according to the culture, beliefs, and moral values of the respective communities. Linguists have studied the phenomenon of codemixing, put forward many linguistic hypotheses (Belazi et al., 1994; Pfaff, 1979; Poplack, 1978), and formulated various constraints (Sankoff and Poplack, 1981; Di Sciullo et al., 1986; Joshi, 1982) to define a general rule for code-mixing. However, for all the scenarios of code-mixing, particularly for the syntactically divergent languages (Berk-Seligson, 1986), these limitations cannot be postulated as a universal rule. In recent times, the pre-trained language model based architectures (Devlin et al., 2019; Radford et al., 2019) have become the state-of the-art models for language understanding and generation. The underlying data to train such models comes from the huge amount of corpus, available in the form of Wikipedia, book corpus etc. Although, these are readily available in vari"
2020.findings-emnlp.206,D18-1330,0,0.0178967,"the sentences into sub-words and use this vocabulary10 to index the sub-words. For the language pairs: enbn, en-ml, en-ta, en-te, we use the learned BPE codes11 on 100 languages from the XLM model to segment the sentences into sub-words and use the correspondent vocabulary to index the sub-words. The same set of vocabulary is used to extract the pre-trained language model feature and the corresponding NMT model for the transfer learning. We use the aligned multilingual word embedding12 of dimension 300 for the language pairs: en-es, ende, en-fr, en-hi and en-bn from Bojanowski et al. (2017); Joulin et al. (2018). For the rest of the language pairs, we obtain the monolingual embedding13 from Bojanowski et al. (2017) and use the MUSE library released by Lample et al. (2018) to align the vector in the same vector space. The embeddings of NE and PoS information are randomly initialized with the dimension of 20. 8 http://www.iitp.ac.in/~ai-nlp-ml/resources. html 9 https://dl.fbaipublicfiles.com/XLM/codes_ xnli_15 10 https://dl.fbaipublicfiles.com/XLM/vocab_ xnli_15 11 https://dl.fbaipublicfiles.com/XLM/codes_ xnli_100 12 https://fasttext.cc/docs/en/ aligned-vectors.html 13 https://fasttext.cc/docs/en/ pre"
2020.findings-emnlp.206,kocmi-bojar-2017-curriculum,0,0.0187515,"mixed sentence reveals that the translated target text (XX7 ) and code-mixed (En-XX) shares many words. For example: • Source (En): The situation in Mumbai has not yet come to normal. • Target (Hi): मुब ं ई म िःथित अभी तक सामा य नह ं हुई है | • Code-Mixed (En-Hi): Mumbai म situation अभी normal नह ं हुई है । In the above sentences, Target (Hi) and Codemixed (En-Hi) share many words (underlined words). Because of this underlying similarity between the machine translation and code-mixed sentence generation, we adapted the transfer learning approach used in machine translation (Zoph et al., 2016; Kocmi and Bojar, 2017) for code-mixed text generation. We first train an NMT model on a large corpus of parallel sentences as discussed in Section 3. Next, we initialize the code-mixed text generation model with the already-trained NMT model. This is then trained on the synthetic code-mixed dataset. Rather than initializing the code-mixed model from the random parameters, we initialize it with the weights from the NMT model. By doing this, we achieve strong prior distribution from the NMT model to code-mixed text generation. When 7 ‘te’ XX may belong to ‘es’, ‘de’, ‘fr’, ‘hi’, ‘bn’, ‘ml’, ‘ta’, we train the code-mi"
2020.findings-emnlp.206,2005.mtsummit-papers.11,0,0.218404,"Missing"
2020.findings-emnlp.206,W04-1013,0,0.0128261,"eature. We use beam search of beam size 4 to generate the code-mixed sentence. Adam (Kingma and Ba, 2015) optimizer is used to train the model with (i) β1 = 0.9, (ii) β2 = 0.999, and (iii) ϵ = 10−8 and initial learning rate of 0.0001. The maximum length of English and code-mixed tokens are set to 60 and 30, respectively. We set 5 as minimum decoding steps in each code-mixed language pair. We use the en-hi development dataset to tune the network hyper-parameters. All the model updates use a batch size of 16. We evaluate the generated text using the metrics, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). 14 https://dl.fbaipublicfiles.com/XLM/mlm_tlm_ xnli15_1024.pth 15 https://dl.fbaipublicfiles.com/XLM/mlm_100_ 1280.pth 5.2 Quantitative Analysis We report the results of our proposed model in Table 2 and Table 3. Performance comparisons to the three baselines are reported in Table 2 and Table 3. The Pointer Generator based baseline is the superior amongst all the baselines and achieve the maximum Bleu score of 21.45 for the en-de code-mixed language pair. Our proposed model achieves the maximum Bleu score of 24.89 for the en-fr code-mixed language pair."
2020.findings-emnlp.206,2021.ccl-1.108,0,0.0991104,"Missing"
2020.findings-emnlp.206,P02-1040,0,0.107357,"to extract the language model feature. We use beam search of beam size 4 to generate the code-mixed sentence. Adam (Kingma and Ba, 2015) optimizer is used to train the model with (i) β1 = 0.9, (ii) β2 = 0.999, and (iii) ϵ = 10−8 and initial learning rate of 0.0001. The maximum length of English and code-mixed tokens are set to 60 and 30, respectively. We set 5 as minimum decoding steps in each code-mixed language pair. We use the en-hi development dataset to tune the network hyper-parameters. All the model updates use a batch size of 16. We evaluate the generated text using the metrics, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). 14 https://dl.fbaipublicfiles.com/XLM/mlm_tlm_ xnli15_1024.pth 15 https://dl.fbaipublicfiles.com/XLM/mlm_100_ 1280.pth 5.2 Quantitative Analysis We report the results of our proposed model in Table 2 and Table 3. Performance comparisons to the three baselines are reported in Table 2 and Table 3. The Pointer Generator based baseline is the superior amongst all the baselines and achieve the maximum Bleu score of 21.45 for the en-de code-mixed language pair. Our proposed model achieves the maximum Bleu score of 24.89 for the en-fr code-mi"
2020.findings-emnlp.206,P18-1143,0,0.185921,"d data for the various NLP tasks not only limited to the language modelling and speech recognition as it is generally been focused in the literature. In contrast to the previous studies, where only a few of the language pairs were considered for code-mixing, we propose an effective approach which shows its effectiveness in generating codemixed sentences for eight different language pairs of diverse origins and linguistic properties. 2 3 Synthetic Code-Mixed Generation Related Work In the literature, there have been efforts for creating code-mixed texts by leveraging the linguistic properties. Pratapa et al. (2018) explored the equivalence constraint theory to construct artificial code-mixed data to reduce the perplexity of the We follow the matrix language frame (MLF) (Myers-Scotton, 1997; Joshi, 1982) theory to generate the code-mixed text. It is less restrictive and can easily be applied on many language pairs. According to MLF, a code-mixed text will have a 2268 Language (L1) India’s agriculture is their main strength. Especially valuable people like Connor Rooney. en en Glasses and cups, whatever they are, can be turned upside down. en Democracy and development go hand in hand. We abolish national"
2020.findings-emnlp.206,2020.acl-demos.14,0,0.0604211,"Missing"
2020.findings-emnlp.206,D16-1121,0,0.0281257,"Missing"
2020.findings-emnlp.206,P17-1099,0,0.278036,"f the cross-lingual and generation paradigms. For the given input sentence E : {e1 , e2 , . . . , em }, we extract the language model feature L : {l1 , l2 , . . . , lm }. The extracted language model features are fused to the linguistic features as follows: 4.3 Decoding with Pointer Generator We use the one-layer LSTM network with the attention mechanism (Bahdanau et al., 2015) to generate the code-mixed sentence y1 , y2 , . . . , yn one word at a time. In order to deal with the rare or unknown words, the decoder has the flexibility to copy the words from documents via the pointing mechanism (See et al., 2017; Gulcehre et al., 2016). The LSTM decoder reads the word embedding ut−1 and the hidden state st−1 to generate the hidden state st at time step t. Concretely, h∗t = tanh(Wh ht + bh ) lt∗ = tanh(Wl lt + bl ) g = σ(Wg .[ht ⊕ lt ]) st = LST M (st−1 , ut−1 ) (2) ft = g ⊙ h∗t + (1 − g) ⊙ lt∗ where, ⊕ and ⊙ are the concatenation and elementwise multiplication operator. First, we project both the features ht and lt into the same vector space h∗t and lt∗ via feed-forward network. Thereafter, we learn the gated value g which controls the flow of each feature. The gated value g controls how much of each"
2020.findings-emnlp.206,P16-1162,0,0.0248896,"n-Hi code-mixed sentence. We show some samples in Table 1, and more details in the Appendix. 4 Methodology ht = LST M (ht−1 , [ut , nt , pt ]) We depict the architecture of our proposed model in Figure 2. Problem Statement: Given an English sentence E having m words e1 , e2 , . . . , em , the task is to generate the code-mixed sentence Cˆ having a sequence of n words Cˆ = {y1 , y2 , . . . , yn }. 4.1 Sub-word Vocabulary The task of generation using neural networks requires a fixed-sized vocabulary. To deal with the problem of Out-of-Vocabulary (OOV) words, we use the Byte-pair encoding (BPE) (Sennrich et al., 2016), and segment the words into sub-words. The sub-word based tokenization schemes inspired by BPE have become the norm in most of the advanced models including the very popular family of contextual language models like XLM (Lample and Conneau, 2019), GPT-2 (Radford et al., 2019), etc. In this work, we process the language pairs with the vocabulary created using the BPE. 4.2 addition, we also incorporate the linguistic features in the form of NE and PoS. The motivation to use these linguistic features comes from the synthetic code-mixed text generation (c.f. section 3) itself, where these feature"
2020.findings-emnlp.206,D08-1102,0,0.0330777,"sing a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpu"
2020.findings-emnlp.206,D08-1110,0,0.0573687,"sing a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpu"
2020.findings-emnlp.206,W19-4320,0,0.0926813,"), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidaya"
2020.findings-emnlp.206,W18-3207,0,0.0472563,"e attempt to propose a generic method that produces the correct and fluent code-mixed sentences on multiple language pairs. The generated synthetic dataset will be a useful resource for machine translation and multilingual applications. (iii). We demonstrate with detailed empirical evaluations the effectiveness of our proposed approach on eight different language pairs, viz. English-Hindi (en-hi), English-Bengali (en-bn), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 201"
2020.findings-emnlp.206,K19-1026,0,0.393076,"), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidaya"
2020.findings-emnlp.206,D16-1163,0,0.0236565,"closer to the code-mixed sentence reveals that the translated target text (XX7 ) and code-mixed (En-XX) shares many words. For example: • Source (En): The situation in Mumbai has not yet come to normal. • Target (Hi): मुब ं ई म िःथित अभी तक सामा य नह ं हुई है | • Code-Mixed (En-Hi): Mumbai म situation अभी normal नह ं हुई है । In the above sentences, Target (Hi) and Codemixed (En-Hi) share many words (underlined words). Because of this underlying similarity between the machine translation and code-mixed sentence generation, we adapted the transfer learning approach used in machine translation (Zoph et al., 2016; Kocmi and Bojar, 2017) for code-mixed text generation. We first train an NMT model on a large corpus of parallel sentences as discussed in Section 3. Next, we initialize the code-mixed text generation model with the already-trained NMT model. This is then trained on the synthetic code-mixed dataset. Rather than initializing the code-mixed model from the random parameters, we initialize it with the weights from the NMT model. By doing this, we achieve strong prior distribution from the NMT model to code-mixed text generation. When 7 ‘te’ XX may belong to ‘es’, ‘de’, ‘fr’, ‘hi’, ‘bn’, ‘ml’, ‘t"
2020.findings-emnlp.386,W19-1909,0,0.0456606,"Missing"
2020.findings-emnlp.386,P98-1015,0,0.656321,"rrect translation of the compound is ‘g¯aya ka d¯udha’ (lit. ‘cow -of milk’; ‘milk of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020."
2020.findings-emnlp.386,D19-1371,0,0.0126854,"t segment follows the first text segment. This is hypothesized to improve BERT’s understanding of the relationship between two text sentences. For MLM, given the input token sequence, a portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. This allows representations to be conditioned on the left and right context. Note that BERT predicts plausible words for each [MASK] token independently. The success of BERT inspired many variants such as training on domain/application specific corpus (Lee et al., 2020; Beltagy et al., 2019; Huang et al., 2019; Alsentzer et al., 2019; Adhikari et al., 2019; Lee and Hsiang, 2019), training on monolingual corpora (Pires et al., 2019), incorporating knowledge graph in the input (Zhang et al., 2019), etc. BERT requires a task-specific output layer. So, one needs to modify BERT’s architecture to adapt it for a new task. Recent text-to-text models, such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2019), use encoder-decoder architectures which share output layer for all tasks effectively eliminating the requirement to modify architecture for a new task. These models convert all"
2020.findings-emnlp.386,S13-2026,0,0.0344344,"Missing"
2020.findings-emnlp.386,N19-1423,0,0.0103564,"its deriving noun compound with test noun compound. The final generated paraphrases were ranked using a language model and MaxEnt model. Recently, Shwartz and Dagan (2018) proposed a semi-supervised method by formulating paraphrasing as a multi-task learning objective. The authors first generated 250 most likely paraphrases using a neural model, and then re-ranked the paraphrases using an SVM. 3 Background With the introduction of the Transformer networks (Vaswani et al., 2017), pre-trained language models have become a key component in advancing the state-of-the-art for many NLP tasks. BERT (Devlin et al., 2019), a transformer-based encoder, has advanced the state-of-the-art for various NLP tasks. For pre-training, BERT uses two self-supervised objectives: next sentence prediction (NSP), and masked language model (MLM). For NSP, BERT is trained to predict whether the second text segment follows the first text segment. This is hypothesized to improve BERT’s understanding of the relationship between two text sentences. For MLM, given the input token sequence, a portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version."
2020.findings-emnlp.386,P16-3011,0,0.395432,"milk’; ‘milk of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing, u"
2020.findings-emnlp.386,P07-1072,0,0.0419167,"a baby chair is a chair for a baby, and reactor waste is waste from a reactor. Lauer’s approach is attractive and simple. It yields prepositions representing paraphrases directly usable in NLP applications. However, it is also problematic, since mapping prepositions with constituent nouns as inputs to abstract relations is hard, e.g., in, on, and at, all can refer to both L O C A T I O N and T I M E . Lauer (1995) and Lapata and Keller (2004) gave unsupervised approaches to prepositional paraphrasing of noun compounds. Both approaches used frequencies of patterns in a large corpus of the Web. Girju (2007) trained various classifiers for the task and observed that SVM performs the best. Recently, Ponkiya et al. (2018b) have proposed an LSTM-based system which encodes nouns compounds and their candidate prepositional paraphrases such that encoding of a noun compound is the most similar to the encoding of its correct prepositional paraphrase. The system was trained in two steps: (1) distant supervision: prepared a large dataset by annotating noun compounds automatically, and trained the system on the dataset; (2) the distant supervision system was further trained on manually annotated data. The a"
2020.findings-emnlp.386,S13-2025,0,0.472922,"Missing"
2020.findings-emnlp.386,I05-1082,0,0.103033,"large models) and different patterns. Overall results were similar. We have not observed improvement compared to the three patterns. So, we did not include them in this paper. We expected Pattern-2 and Pattern-3 to perform better than Pattern-1 Pattern-2, respectively, as they provide more context (§4.1). The performance of NC-RoBERTa is as expected on all three datasets. However, we see a reverse trend for NC-BERT. We analyzed the performance of the patterns on Ponkiya et al. (2018b)’s dataset using BERTbase and RoBERTa-large models. The dataset was prepared by annotating noun compounds from Kim and Baldwin (2005)’s dataset with prepositions. For every example, we have a semantic relation from Kim and Baldwin (2005) and a preposition from Ponkiya et al. (2018b). We observe that the major reason behind pattern3 underperforming compared to pattern-2 is: the correct preposition of predicted by pattern-2, but pattern-3 predicted for. Some examples are (using BASE-base model): 40 20 0 0 25 50 75 No. of paraphrases to generate (k) 100 Figure 3: Performance of T5-based system for different value of k (number of paraphrases to generate) on train and test sets of SemEval-2013 Task-9 dataset. T5 models. However,"
2020.findings-emnlp.386,N04-1016,0,0.466586,"to Hindi. The problem tackled was to classify a given noun compound into one of these prepositions such that the assigned preposition can paraphrase that compound. For example, a baby chair is a chair for a baby, and reactor waste is waste from a reactor. Lauer’s approach is attractive and simple. It yields prepositions representing paraphrases directly usable in NLP applications. However, it is also problematic, since mapping prepositions with constituent nouns as inputs to abstract relations is hard, e.g., in, on, and at, all can refer to both L O C A T I O N and T I M E . Lauer (1995) and Lapata and Keller (2004) gave unsupervised approaches to prepositional paraphrasing of noun compounds. Both approaches used frequencies of patterns in a large corpus of the Web. Girju (2007) trained various classifiers for the task and observed that SVM performs the best. Recently, Ponkiya et al. (2018b) have proposed an LSTM-based system which encodes nouns compounds and their candidate prepositional paraphrases such that encoding of a noun compound is the most similar to the encoding of its correct prepositional paraphrase. The system was trained in two steps: (1) distant supervision: prepared a large dataset by an"
2020.findings-emnlp.386,W10-0805,0,0.168777,"onal if the meaning of the compounds can be derived from the meaning of its components. The component nouns are related through a semantic relation that is constituents dependent. For instance, ‘student protest’ and ‘university protest’ are protests. However, the student(s) are A G E N T (doer of an event), whereas university is L O C A T I O N of the protest. The task of identifying such relations between the components of a noun compound is called noun compound interpretation (NCI). Such interpretation can help a wide variety of NLP tasks, like machine translation (Baldwin and Tanaka, 2004; Paul et al., 2010; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), and semantic parsing (Tratz, 2011). For instance, to translate the English noun compound ‘cow milk’ to Hindi, a machine translation system needs to generate the postposition kA (of ) in addition to translating the individual nouns. The correct translation of the compound is ‘g¯aya ka d¯udha’ (lit. ‘cow -of milk’; ‘milk of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular"
2020.findings-emnlp.386,D19-1250,0,0.0404702,"Missing"
2020.findings-emnlp.386,P19-1493,0,0.0126149,"r MLM, given the input token sequence, a portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. This allows representations to be conditioned on the left and right context. Note that BERT predicts plausible words for each [MASK] token independently. The success of BERT inspired many variants such as training on domain/application specific corpus (Lee et al., 2020; Beltagy et al., 2019; Huang et al., 2019; Alsentzer et al., 2019; Adhikari et al., 2019; Lee and Hsiang, 2019), training on monolingual corpora (Pires et al., 2019), incorporating knowledge graph in the input (Zhang et al., 2019), etc. BERT requires a task-specific output layer. So, one needs to modify BERT’s architecture to adapt it for a new task. Recent text-to-text models, such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2019), use encoder-decoder architectures which share output layer for all tasks effectively eliminating the requirement to modify architecture for a new task. These models convert all NLP problems into a text-to-text format, i.e., input and output for any NLP task (including classification task) are sequences. A text-to-text"
2020.findings-emnlp.386,L18-1489,1,0.169755,"of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing, uses only a preposition"
2020.findings-emnlp.386,C18-1155,1,0.444659,"of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing, uses only a preposition"
2020.findings-emnlp.386,P18-1111,0,0.0702197,"ng paraphrases. The prepositional paraphrases are quite frequent and well covered. To handle sparsity, they used prepositional paraphrase to predict a semantic relation, and then, selected verbs that mostly co-occur with that relation. Versley (2013) retrieved mutually more similar compounds from training data, extracted templates 4315 and fillers from paraphrases of the similar compounds. The templates were weighted by its frequency and similarity its deriving noun compound with test noun compound. The final generated paraphrases were ranked using a language model and MaxEnt model. Recently, Shwartz and Dagan (2018) proposed a semi-supervised method by formulating paraphrasing as a multi-task learning objective. The authors first generated 250 most likely paraphrases using a neural model, and then re-ranked the paraphrases using an SVM. 3 Background With the introduction of the Transformer networks (Vaswani et al., 2017), pre-trained language models have become a key component in advancing the state-of-the-art for many NLP tasks. BERT (Devlin et al., 2019), a transformer-based encoder, has advanced the state-of-the-art for various NLP tasks. For pre-training, BERT uses two self-supervised objectives: nex"
2020.findings-emnlp.386,S13-2028,0,0.174962,"rases on average). For evaluation, the predicted paraphrases for a test example were ranked, and then the overall scores were computed by matching predicated paraphrase with the reference paraphrases. The matching was done in two ways: based on whether multiple generated paraphrases can be matched with a reference paraphrase or not. A simple baseline for the task used a fixed set of prepositional paraphrases in a fixed order. None of the four proposed systems (submitted by three teams) beat the baseline in both evaluation techniques. All three participating systems (Van de Cruys et al., 2013; Surtani et al., 2013; Versley, 2013) were supervised. Van de Cruys et al. (2013) used a distributional model to extract word features, which were then used to train a maximum-entropy classifier. The classifier predicted a probability distribution over a set of paraphrases. A threshold was used to decide whether the paraphrases should be included in the final output or not. A higher threshold value resulted in fewer paraphrases, where a lower threshold value generated more paraphrases. It was observed that using only features of the head noun (the second word in a compound) performs better than when using feature"
2020.findings-emnlp.386,P10-1070,0,0.603626,"¯udha’ (lit. ‘cow -of milk’; ‘milk of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020. 2020 Association for Computational Lingu"
2020.findings-emnlp.386,C94-2125,0,0.877755,"ound is ‘g¯aya ka d¯udha’ (lit. ‘cow -of milk’; ‘milk of cow’). Without understanding the underlying relation, a machine translation system might fail. Interpretation via abstract labels (representing semantic relations) is popular in the literature. Given a noun compound, the task is to assign an abstract label from a predefined set, e.g., ‘student protest’: P R O T E S T E R . Past work has proposed a wide variety of inventories for semantic relations (Levi, 1978; Warren, 1978; Lauer, 1995; Nastase and Sz´ S´eaghdha, 2007; Rosario et al., pakowicz, 2003; O 2001; Barker and Szpakowicz, 1998; Vanderwende, 1994; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a); however, there is no community agreed standard inventory. Interpretation can be done via paraphrasing as well. Here, one can use extract words (along with component nouns) to paraphrase a noun compound, e.g., ‘student protest’: ‘protest by student’, ‘protest held by students’, etc. The paraphrase reveals the underlying relation. A simpler version of paraphrasing, also known as prepositional paraphras4313 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4313–4323 c November 16 - 20, 2020. 2020 Association f"
2020.findings-emnlp.386,S13-2027,0,0.172344,"evaluation, the predicted paraphrases for a test example were ranked, and then the overall scores were computed by matching predicated paraphrase with the reference paraphrases. The matching was done in two ways: based on whether multiple generated paraphrases can be matched with a reference paraphrase or not. A simple baseline for the task used a fixed set of prepositional paraphrases in a fixed order. None of the four proposed systems (submitted by three teams) beat the baseline in both evaluation techniques. All three participating systems (Van de Cruys et al., 2013; Surtani et al., 2013; Versley, 2013) were supervised. Van de Cruys et al. (2013) used a distributional model to extract word features, which were then used to train a maximum-entropy classifier. The classifier predicted a probability distribution over a set of paraphrases. A threshold was used to decide whether the paraphrases should be included in the final output or not. A higher threshold value resulted in fewer paraphrases, where a lower threshold value generated more paraphrases. It was observed that using only features of the head noun (the second word in a compound) performs better than when using feature vectors of both"
2020.findings-emnlp.386,P19-1139,0,0.0207812,"placed by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. This allows representations to be conditioned on the left and right context. Note that BERT predicts plausible words for each [MASK] token independently. The success of BERT inspired many variants such as training on domain/application specific corpus (Lee et al., 2020; Beltagy et al., 2019; Huang et al., 2019; Alsentzer et al., 2019; Adhikari et al., 2019; Lee and Hsiang, 2019), training on monolingual corpora (Pires et al., 2019), incorporating knowledge graph in the input (Zhang et al., 2019), etc. BERT requires a task-specific output layer. So, one needs to modify BERT’s architecture to adapt it for a new task. Recent text-to-text models, such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2019), use encoder-decoder architectures which share output layer for all tasks effectively eliminating the requirement to modify architecture for a new task. These models convert all NLP problems into a text-to-text format, i.e., input and output for any NLP task (including classification task) are sequences. A text-to-text model can generate a variable length span for a single masked tok"
2020.fnp-1.22,2020.fnp-1.1,0,0.268936,"Missing"
2020.fnp-1.22,W04-1013,0,0.0722343,"h(KG) by considering words in the triples. Our novel methods choose the informative sentences based on the count of frequencies calculated using generated KG. We also have implemented machine Learning(ML) and Deep Neural Network(DNN) based models. These models make use of the KG based features which try to capture information available. We are making use of dataset made available for FNS-2020 shared task by El-Haj et al. (2020). 2 We have used SpaCy library for extracting triples. We have used python implementation of Rouge package made available by 3 PyPI, which implements ROUGE described by Lin (2004). 2 Implemented Approaches In general, we pose extractive summarization as a sentence classification and a triple classification task. We perform this classification using algorithms like SVM, SVR, Neural Network(NN) and Long ShortTerm Memory(LSTM/DNN). This section describes our implemented approaches in details. 2.1 Labelling and Feature Extraction Train Set Validation Set Summary Sentences 0.3M 0.051M Non Summary Sentences 2.6M 0.663M Total Sentences 2.9M 0.714M Table 1: Distribution of Summary, Non-Summary Sentences Extracted from Training and Validation Set FNS-2020 Shared task training a"
2020.fnp-1.22,W04-3252,0,0.431206,"dding tries to get an exact representation of the content of the sentence represented by the triple. Architecture details of this model remain the same as S-LSTM. Table 3 represents parameters used for S-LSTM and T-LSTM and table 4 represents extraction statistics related to T-LSTM model. Training Set Validation Set Total Triples 1389758 352435 Non Summary Triples 1158854 315418 Summary Triples 230904 37017 Table 4: Extraction Statistics of Triples from Sentences from FNS-2020 Shared Task Dataset 3 Results and Analysis We have implemented eight different approaches while considering TextRank (Mihalcea and Tarau, 2004) as the baseline approach. In this section, we discuss the performance of the implemented models on the validation set followed by a performance on the test set. 3.1 Validation Set The validation set of FNS-2020 Shared Task consists of 363 documents each having up to seven gold summaries. We are comparing results obtained over single gold summary of the specific document. System generated summaries were constrained to have 1000 words and our approaches select first 1000 words because, After segmenting given text in three parts each containing equal portions of the text we have found that in th"
2020.fnp-1.22,D14-1162,0,0.0835544,"4 frequent words present in the sentence, Indicator WordsCount of words present in the available 5 synset(i.e. the group of synonymous words) of words ’conclusion’ and ’summary’, Uppercase- number of uppercase words present in the 6 sentence, Important Word Feature- It represents quotient of the available triple in the sentence to the total triples in the text file, and KG File Feature- It represents quotient of the available triples in the sentence represented in terms of 7 lookup frequencies to the number of total triples in the text file. We also have used pre-trained 100-dimensional GloVe(Pennington et al., 2014) embedding for DNN based approaches. 2.2 Triple Frequency-based Models(TFM) Subject, verb and object(&lt;S, V, O> i.e. triple) are main content words available in any sentence. Individual count of S, V and O present in the document fails to represent content available the sentence. To generate a content-aware extractive summary, TFM chooses sentences containing the highest scoring triples. Based on score computation, we have defined three different models. 2.2.1 Plain Frequency Model(PKG) This is the simplistic approach to generate extractive summaries by making use of Triple Frequencies. This me"
2020.icon-main.23,P18-2080,0,0.0112667,"n training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all of their works, they defined a source promp"
2020.icon-main.23,D16-1115,0,0.162933,"written in response to a completely different prompt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (Attali and Burstein, 2006) from the Educational Testing Service (ETS), most modern-day systems use deep learning and neural networks, like convolutional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader"
2020.icon-main.23,K17-1017,0,0.0143879,"se the properties of the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all"
2020.icon-main.23,P18-1219,1,0.88028,"Missing"
2020.icon-main.23,2020.aacl-main.86,1,0.783741,"utional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader’s gaze behaviour for helping our system grade new essays. In this paper, we look at a similar approach proposed by Mathias et al. (2020) to grade essays using cognitive information, which is learnt as an auxiliary task in a multi-task learning approach. Multi-task learning is a machine-learning approach, where the model tries to solve one or more auxiliary tasks to solve a primary task (Caruana, 1998). Similar to Mathias et al. (2020), the scoring of the essay is the primary task, while learning the gaze behaviour is the auxiliary task. Contribution. In this paper, we"
2020.icon-main.23,D14-1162,0,0.0849202,"viour attributes as described in Mathias et al. (2020). Binning is done to take into account the idiosyncracies of the gaze behaviour of individual readers (i.e. some people may read faster, others slower, etc.). Whenever we use gaze behaviour, we scale the value of the gaze behaviour bins to the range of [0, 1] as well. 5.5 Figure 1: Architecture of our gaze behaviour system, showing an input essay of n sentences, with the outputs being the gaze behaviour (whenever applicable), and the overall essay score. 5.3 Network Hyperparameters We use the 50 dimension GloVe pre-trained word embeddings (Pennington et al., 2014). We run our experiments over a batch size of 200, for 50 epochs. We set the learning rate as 0.001, and the dropout rate as 0.5. The word-level CNN layer has a kernel size of 5, with 100 filters. The sentence-level LSTM layer has 100 hidden units. We use the RMSProp Optimizer (Dauphin et al., 2015) with an initial learning rate of 0.001 and momentum of 0.9. Along with the network hyperparameters, we also weigh the loss functions of the different gaze behaviour attributes differently, using the same weights as Mathias et al. (2020), namely 0.05 for DT and FFD, 0.01 for IR and Experiment Config"
2020.icon-main.23,D15-1049,0,0.0284022,"pt. One drawback of this approach is that it would not be able to use the properties of the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. ("
2020.icon-main.23,D16-1193,0,0.0605172,"mpt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (Attali and Burstein, 2006) from the Educational Testing Service (ETS), most modern-day systems use deep learning and neural networks, like convolutional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader’s gaze behaviour for helping our system grade new e"
2020.icon-main.23,W15-0626,0,0.151979,"rmation, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK. 1 Introduction One of the major challenges in machine learning is the requirement of a large amount of training data. AEG systems perform at their best when they are trained in a prompt-specific manner - i.e. the essays that they are tested on are written in response to the same prompt as the essays they are trained on (Zesch et al., 2015). These systems perform badly when they are tested against essays written in response to a different prompt. Zero-shot AEG is when our AEG system is used to grade essays written in response to a completely different prompt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (A"
2020.icon-main.23,W18-0549,0,0.0146289,"f the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all of their works, they de"
2020.icon-main.25,P18-1063,0,0.119205,"its less storage requirement (opposed to other modes of communication like audio and video). Text summarization is a problem at the very core of natural ∗ * means equal contribution. https://news.gallup.com/poll/179288/new-eracommunication-americans.aspx 1 language processing, and has various applications in the spoken languages, including summarization of conversations, and public speeches. Some works have been done in the field of reinforcement learning based text summarization (Dong et al., 2018; Liu et al., 2018), the most prominent architecture being extractor-abstractor (EXT-ABS) model (Chen and Bansal, 2018). Inspired from this architecture, in this manuscript, we have proposed an extractor-paraphraser system that uses semantic information overlap as the underlying guidance strategy. The model is further enhanced to surpass its limits using reinforcement learning, for which we have proposed a novel semantic overlap based reward function. Word Mover Similarity (WMS) (Clark et al., 2019) is utilized to evaluate semantic similarity across generated sentences and the true ground truth summary sentences. We assume that paraphrasing is a relatively simpler task than abstractive summarization, with the"
2020.icon-main.25,D14-1181,0,0.00422376,"Missing"
2020.icon-main.25,N16-1012,0,0.0306774,"d to tackle both extractive and abstractive summarization. Initial research (Paice, 1990; Kupiec et al., 1995) focused on extractive summarization due to its easier setup. Various techniques ranging from integer linear programming (Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specific summarization techniques have also been explored, like radiology findings summarization (Zhang et al., 2018), across-time summarization (Duan and Jatowt, 2019), movie review summarization (Zhuang et al., 2006), book summarization (Mihalcea and Ceylan, 2007), and customer review based opinion summ"
2020.icon-main.25,P19-1264,0,0.142154,"eches. Some works have been done in the field of reinforcement learning based text summarization (Dong et al., 2018; Liu et al., 2018), the most prominent architecture being extractor-abstractor (EXT-ABS) model (Chen and Bansal, 2018). Inspired from this architecture, in this manuscript, we have proposed an extractor-paraphraser system that uses semantic information overlap as the underlying guidance strategy. The model is further enhanced to surpass its limits using reinforcement learning, for which we have proposed a novel semantic overlap based reward function. Word Mover Similarity (WMS) (Clark et al., 2019) is utilized to evaluate semantic similarity across generated sentences and the true ground truth summary sentences. We assume that paraphrasing is a relatively simpler task than abstractive summarization, with the underlying intuition that paraphrasing is a subproblem within abstractive summarization. To bolster our hypothesis, experiments are conducted on the extractor-abstractor (EXT-ABS) model (Chen and Bansal, 2018) and the Pointer Generator Network (PGN) (See et al., 2017), which is used as the basic abstraction unit in the former architecture. The results are rather staggering and revea"
2020.icon-main.25,D18-1409,0,0.0229837,"Missing"
2020.icon-main.25,C12-1056,0,0.0303599,"setup and the datasets used. A thorough discussion and state the results are provided in Section 5, followed by the conclusion and future work in Section 6. 2 Related Work Automatic text summarization has been extensively researched over more than three decades, and has shown a lot of progress and promise over the course of time. Various approaches have been explored to tackle both extractive and abstractive summarization. Initial research (Paice, 1990; Kupiec et al., 1995) focused on extractive summarization due to its easier setup. Various techniques ranging from integer linear programming (Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010)"
2020.icon-main.25,C10-1039,0,0.051052,"(Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specific summarization techniques have also been explored, like radiology findings summarization (Zhang et al., 2018), across-time summarization (Duan and Jatowt, 2019), movie review summarization (Zhuang et al., 2006), book summarization (Mihalcea and Ceylan, 2007), and customer review based opinion summarization (Pecar, 2018). Lately, multi-modal summarization (Jangra et al., 2020a,b; Zhu et al., 2020; Saini et al., 2020) has also gained popularity . Recently, people have also explored reinforcement learning to tackle the problem o"
2020.icon-main.25,W04-1013,0,0.0626996,"r model [Id − ext M2On=2 ]: The ’n-to-1’ paraphraser with n = 2 METEOR WMS 22.81 21.86 21.62 14.28 14.24 18.72 19.39 19.24 16.13 13.36 13.81 12.92 20.38 20.52 21.47 21.34 18.25 13.7 14.56 14.42 14.6 13.52 24.36 24.61 22.22 19.34 20.14 17.99 and the exemplary-extracted sentences assumed as the extractor (generating exemplary-extracted sentences as proposed in Eq. 4). 5 Results Results of different baselines and the proposed approach are discussed in this section. Table 1 illustrates that our proposed techniques perform better than the rest of the systems. We have used ROUGE1, ROUGE-2, ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and word mover similarity (WMS) (Clark et al., 2019) as evaluation metrics. We believe that ROUGE as an evaluation metric is incapable of judging the quality of an abstractive summary due to its emphasis on syntactic overlap over semantic overlap (Liu et al., 2016; Clark et al., 2019; Novikova et al., 2017). To overcome this, we have also used WMS as an evaluation metric. 195 5.1 Semantic information overlap It is noticed that the proposed paraphraser in model O2O (w/o RL) outperforms the abstractor from EXT − ABS (w/o RL) in terms of ROUGE scores, while sco"
2020.icon-main.25,D16-1230,0,0.0128403,"extractor (generating exemplary-extracted sentences as proposed in Eq. 4). 5 Results Results of different baselines and the proposed approach are discussed in this section. Table 1 illustrates that our proposed techniques perform better than the rest of the systems. We have used ROUGE1, ROUGE-2, ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and word mover similarity (WMS) (Clark et al., 2019) as evaluation metrics. We believe that ROUGE as an evaluation metric is incapable of judging the quality of an abstractive summary due to its emphasis on syntactic overlap over semantic overlap (Liu et al., 2016; Clark et al., 2019; Novikova et al., 2017). To overcome this, we have also used WMS as an evaluation metric. 195 5.1 Semantic information overlap It is noticed that the proposed paraphraser in model O2O (w/o RL) outperforms the abstractor from EXT − ABS (w/o RL) in terms of ROUGE scores, while scoring marginally less in terms of METEOR. The extractor counterparts EXT − ABS (Ext only) and O2O (Ext only) 4.0 PGN Gold Standard EXT-ABS 3.5 Word Mover Distance (WMD) It is established by the fact that the models using WMS as the reward function attain comparable ROUGE scores as well (while the rev"
2020.icon-main.25,P04-3020,0,0.136144,"are provided in Section 5, followed by the conclusion and future work in Section 6. 2 Related Work Automatic text summarization has been extensively researched over more than three decades, and has shown a lot of progress and promise over the course of time. Various approaches have been explored to tackle both extractive and abstractive summarization. Initial research (Paice, 1990; Kupiec et al., 1995) focused on extractive summarization due to its easier setup. Various techniques ranging from integer linear programming (Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specifi"
2020.icon-main.25,D07-1040,0,0.0653682,"volves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specific summarization techniques have also been explored, like radiology findings summarization (Zhang et al., 2018), across-time summarization (Duan and Jatowt, 2019), movie review summarization (Zhuang et al., 2006), book summarization (Mihalcea and Ceylan, 2007), and customer review based opinion summarization (Pecar, 2018). Lately, multi-modal summarization (Jangra et al., 2020a,b; Zhu et al., 2020; Saini et al., 2020) has also gained popularity . Recently, people have also explored reinforcement learning to tackle the problem of automatic text summarization in both extractive (Dong et al., 2018; Gao et al., 2019) and abstractive domains (Xiao et al., 2020; Chen and Bansal, 2018). Chen and Bansal (2018) have proposed an extractorabstractor architecture, separating the relevant data searching part and the paraphrasing part to individual modules. In t"
2020.icon-main.25,W04-3252,0,0.027919,"ion and state the results are provided in Section 5, followed by the conclusion and future work in Section 6. 2 Related Work Automatic text summarization has been extensively researched over more than three decades, and has shown a lot of progress and promise over the course of time. Various approaches have been explored to tackle both extractive and abstractive summarization. Initial research (Paice, 1990; Kupiec et al., 1995) focused on extractive summarization due to its easier setup. Various techniques ranging from integer linear programming (Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot"
2020.icon-main.25,K16-1028,0,0.115293,"active and abstractive summarization. Initial research (Paice, 1990; Kupiec et al., 1995) focused on extractive summarization due to its easier setup. Various techniques ranging from integer linear programming (Galanis et al., 2012), graph based approaches (Mihalcea and Tarau, 2004; Mihalcea, 2004), genetic algorithms (Saini et al., 2019a,b), and neural networks (Nallapati et al., 2017; Zhang et al., 2016) have been adopted to solve the extractive summarization task. The majority of the research in abstractive summarization revolves around deep learning (See et al., 2017; Chopra et al., 2016; Nallapati et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specific summarization techniques have also been explored, like radiology findings summarization (Zhang et al., 2018), across-time summarization (Duan and Jatowt, 2019), movie review summarization (Zhuang et al., 2006), book summarization (Mihalcea and Ceylan, 2007), and customer review based opinion summarization (Pecar, 2018)."
2020.icon-main.25,D17-1238,0,0.0366409,"Missing"
2020.icon-main.25,P18-3001,0,0.0144486,"et al., 2016). Liu et al. (2018) proposed a generative adversarial network based model to generate document abstracts. A handful of works however also use ILP (Banerjee et al., 2015) and graph-based (Ganesan et al., 2010) techniques to attempt to solve the problem. A lot of domain specific summarization techniques have also been explored, like radiology findings summarization (Zhang et al., 2018), across-time summarization (Duan and Jatowt, 2019), movie review summarization (Zhuang et al., 2006), book summarization (Mihalcea and Ceylan, 2007), and customer review based opinion summarization (Pecar, 2018). Lately, multi-modal summarization (Jangra et al., 2020a,b; Zhu et al., 2020; Saini et al., 2020) has also gained popularity . Recently, people have also explored reinforcement learning to tackle the problem of automatic text summarization in both extractive (Dong et al., 2018; Gao et al., 2019) and abstractive domains (Xiao et al., 2020; Chen and Bansal, 2018). Chen and Bansal (2018) have proposed an extractorabstractor architecture, separating the relevant data searching part and the paraphrasing part to individual modules. In this work, we have proposed a system inspired from Chen and Bans"
2020.icon-main.25,P17-1099,0,0.479423,"learning, for which we have proposed a novel semantic overlap based reward function. Word Mover Similarity (WMS) (Clark et al., 2019) is utilized to evaluate semantic similarity across generated sentences and the true ground truth summary sentences. We assume that paraphrasing is a relatively simpler task than abstractive summarization, with the underlying intuition that paraphrasing is a subproblem within abstractive summarization. To bolster our hypothesis, experiments are conducted on the extractor-abstractor (EXT-ABS) model (Chen and Bansal, 2018) and the Pointer Generator Network (PGN) (See et al., 2017), which is used as the basic abstraction unit in the former architecture. The results are rather staggering and reveal that the PGN model also paraphrases input document sentences, albeit implicitly. The major contributions of the paper are as follows: • A novel semantic overlap based reward function is proposed for reinforcement of extractorparaphraser model. • To the best of our knowledge, we are the first ever to discover the fact that PGN networks are indeed doing an implicit extraction-paraphrasing operation, revealing the true nature of existing abstractive summarization models. 191 Proc"
2020.icon-main.25,W18-5623,0,0.0245456,"Missing"
2020.icon-main.42,P18-2096,0,0.035607,"Missing"
2020.icon-main.43,P15-1136,0,0.0666516,"Missing"
2020.icon-main.43,D16-1245,0,0.0284791,"Missing"
2020.icon-main.43,P19-1064,0,0.0220793,"Missing"
2020.icon-main.43,D19-1588,0,0.180213,"Missing"
2020.icon-main.43,P19-1066,0,0.0409852,"Missing"
2020.icon-main.43,D17-1018,0,0.0315973,"Missing"
2020.icon-main.43,N18-2108,0,0.0739962,"Missing"
2020.icon-main.43,Q15-1029,0,0.0419669,"Missing"
2020.icon-main.43,D14-1162,0,0.0849669,"2) output of right-LSTM and 3) final embedding have been considered in the c2f-model. Similarly, to strengthen the learning from word context representation we generate word context representation from triplet of embedding outputs. We consider the raw form of embedding 324 outputs from sa layer norm of Layer-6 and output layer norm of Layer-5 and final embedding output from the output layer norm of Layer-6 of DistilBERT. Word context representation means representation of word in the input sentence. Word representation as defined in c2f-model, are generated by character embedding using GloVe(Pennington et al., 2014) vector. Mapping Embedding from WordPiece tokens to Term-tokens : The dimension of embedding matrix generated from DistilBERT is (N one, N 0 , 786). Here, N 0 is the maximum of the number of WordPice tokens for a sample point in the batch. Learning the coreference in context of WordPiece token is complex to understand, and its analysis and explanation seem unusual. So we have mapped the output of WordPiece token to TermToken by averaging the corresponding WordPiece embeddings. Let, in a batch of size B, N be the maximum of number of Term-Tokens in a sample, 0 N be the maximum number of WordPie"
2020.icon-main.43,N18-1202,0,0.0211555,"ion of three vectors: the LSTM states of both the span endpoints and an attention vector computed over those span tokens. The score s(u, v) is computed by the mention score of u (sm (u)), mention score of v (sm (v)), the joint compatibility score (sc (u, v)) of u and v. The mention score of a span signifies the probability of a span to be a mention. The joint compatibility score signifies the probability of the two spans as corefering. The components are computed as follows: Extraction of Embedding from DistilBERT for word context representation Extraction of embeddings from ELMO is shown in (Peters et al., 2018). We have shown the embedding extraction from DistilBERT for word representation in Fig. 1. This extraction of word representation is performed in 4 steps, which are explained below. Conversion of Term-Tokens into WordPiece tokens: DistilBERT takes token embedding and position embeddings as input (Sanh et al., 2019). ....... WPN' WordPiece tokens DistilBERT Tokenizer Sentence formation E1 E2 E3 ....... EN Term-Tokens Figure 1: Word context representation from DistilBERT Collecting outputs from DistilBERT: After, the generation of WordPiece tokens, these are given as input to the DistilBERT for"
2020.icon-main.43,W12-4501,0,0.0320448,".e., 3 EmbOut[i] = [ej,k ]1≤j≤N ; 1≤k≤768 ; ∀1 ≤ i ≤ B where 0 ej,k = ej,k ; (5) l 1X 0 ej+p,k ; l p=1 (6) &if Ψ(j, l) = T rue (7) if W Pj = Ej × ∀ 1 ≤ k ≤ 768 and the function Ψ(j, l) returns True if the WordPiece tokens hW Pj+1 , . . . , W Pj+l i lead to term-token, Ej . Similar procedure is followed to get the embedding output from the rest of the two layers. Formation of the final word context representation: The output from Layer-6, sa layer norm Overview of the proposed system Dataset used and experimental set-up CoNLL-2012 shared task corpus is a standard coreference resolution corpus (Pradhan et al., 2012). We have used the English-based corpus for evaluating the performance of our proposed approach. Our experimental setup is almost similar to that of c2f-model and we have modified some parts of their code to generate word context representation from DistilBERT embeddings, which are: 1) The ELMo embeddings are replaced by the DistilBERT embeddings which are lighter and faster. 2) We have experimented with word context representation, generated from DistilBERT. The two different experimental setups are discussed below: i) D-Coref-Small: In our proposed D-coref model, we have extracted the embedd"
2020.icon-main.43,N16-1114,0,0.0416383,"Missing"
2020.icon-main.43,P15-1137,0,0.0607816,"Missing"
2020.icon-main.62,W14-4012,0,0.084992,"Missing"
2020.icon-main.62,D14-1181,0,0.00577616,"Missing"
2020.icon-main.62,I17-1099,0,0.0228453,"Emotion Corpus, also known as Twitter Emotion Corpus (TEC), was published by (Mohammad and Kiritchenko, 2015), and consists of 21,051 tweets. This resource was created to understand if emotion-word hashtags can successfully be used as emotion labels. Ekman’s basic emotions 461 3 https://data.world/crowdflower/ sentiment-analysis-in-text (Ekman, 1992) have been considered for the annotation process. Tweets were scraped that contained hashtags in the form #emotion corresponding to Ekman’s (Ekman, 1992) 6 basic emotions (like #anger, #disgust). DailyDialogs is a dataset of dialogs published by (Li et al., 2017) spanning over a variety of topics and better structured than any social media data. The SSEC corpus (Schuff et al., 2017) is an annotation of the SemEval 2016 Twitter stance and sentiment corpus (Mohammad et al., 2017) with Plutchik’s emotion labels (Plutchik, 2001). The authors studied the relation between emotion annotation and the other annotation layers like stance and sentiment. The EmoInt dataset published by (Mohammad and Bravo-Marquez, 2017) for evaluation of the WASAA-2017 Shared Task of Emotion Intensity (EmoInt) contains 7,097 tweets annotated with a pair of emotion tag and intensi"
2020.icon-main.62,W17-5205,0,0.0418945,"Missing"
2020.icon-main.62,S18-1001,0,0.0534959,"Missing"
2020.icon-main.62,D14-1162,0,0.0849914,"our severely under-represented classes, namely (disgust, fear, sadness and surprise) and one over-represented class (others). 4 Methodologies We develop various deep learning-based multi-task models for automatic detection of emotion and its intensity. As base learning techniques, we use Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014). We build three separate multi-task models (CNN based, Bi-GRU based and Bi-LSTM based) on top of pre-trained word embedding (GloVe 7 (Pennington et al., 2014)). The embedding layer is initialized with the pre-trained weights and is learned during the training in accordance with our dataset. We employ word attention (Bahdanau et al., 2014) mechanism to focus on the informative words in a document (tweet) and obtain an aggregated representation(document vector) which is passed through two fully-connected layers (100 neurons in each 7 http://nlp.stanford.edu/data/ wordvecs/glove.840B.300d.zip layer) and an output layer (with 7 neurons, one for each class) with Softmax activation. We use the categorical cross-entropy as the loss function. 4.1 Convoluti"
2020.icon-main.62,W17-5203,0,0.0382475,"Missing"
2020.icon-main.62,C16-1287,1,0.892658,"Missing"
2020.icon-main.62,S07-1013,0,0.0863487,"s annotation schemes were introduced to serve the specific purpose for which the corpus is created. (Scherer and Wallbott, 1994) collected questionnaires answered by people with different cultural backgrounds to form The International Survey on Emotion Antecedents and Reactions (ISEAR) dataset. People reported on their emotional events. The dataset contains a total of 7,665 sentences from reports by approximately 3,000 respondents. Sentences are annotated with single labels, chosen from the set of following labels: joy, fear, anger, sadness, disgust, shame, and guilt. The Affective Text task (Strapparava and Mihalcea, 2007) in SemEval 2007 was proposed to focus on the emotion classification of news headlines extracted from news web sites. Given a set of predefined six emotion labels (Paul Ekman’s basic emotions (Ekman, 1992)), classify the titles with the appropriate emotion label and/or with a valence indication (positive/negative). (Aman and Szpakowicz, 2007) published a dataset of blog content consisting of 5,205 sentences from 173 blogs. Each instance is annotated with an emotion label from Ekman’s basic emotions (Ekman, 1992) and also with an intensity score for that emotion. (Alm, 2008) researched the text"
2020.icon-main.62,N16-1174,0,0.0372911,"et al., 2011) and an output layer (with 7 neurons, one for each class) with Softmax activation. 4.4 level). HAtED focuses on each sentence in a tweet individually resulting in sentence vectors which are further attended upon to produce a document vector. The intuition is to focus upon important words in a sentence as well as important sentences in a document (tweet) for a particular emotion. For encoding of the sentences, we leverage Bi-GRU (256 neurons) based word encoder. Without making major changes to the basic architecture of the hierarchical attention framework as in the original work (Yang et al., 2016), we tweaked the last few layers to solve our objective. We pass the document vector through a dense layer (100 neurons with ReLU activation) followed by an output layer (7 neurons with Softmax activation). We use categorical crossentropy loss function for the classification task. Besides HAtED, we also develop two separate Hierarchical Attention-based models considering various sets of emotion classes. They are as follows: Hierarchical Attention Based Deep Neural Framework for Emotion Detection (HAtED) In recent works, Hierarchical attention (Bahdanau et al., 2014) based deep learning systems"
2020.icon-main.62,N16-1000,0,0.21503,"Missing"
2020.iwslt-1.22,1983.tc-1.13,0,0.201188,"Missing"
2020.iwslt-1.22,cho-etal-2014-corpus,0,0.0472707,"Missing"
2020.iwslt-1.22,W14-3348,0,0.0696908,"Missing"
2020.iwslt-1.22,P07-2045,0,0.00899044,"Missing"
2020.iwslt-1.22,2013.iwslt-papers.14,0,0.0610201,"s for the aha token along with its successive repetitions. We repeat this for all filler words and store them in a comma-separated value file. Filler phrase ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah ah Data 2.1 Fisher Corpus For our experiments, we use the Fisher Spanish dataset (David Graff and Cieri.), comprising telephone conversations between mostly native Spanish speakers. The dataset contains speech utterances (disfluent Spanish), their corresponding ASR outputs (disfluent Spanish), and two sets of English translations (both fluent and disfluent) (Salesky et al., 2018; Post et al., 2013). The Fisher dataset has disfluent Spanish ASR output (text) which we use as input to our model. Additionally, two sets of English translations (disfluent & fluent) are also available in (text) form. For training, we only make use of disfluent English sentences. i.e. we train a text-to-text model. We explicitly note here that no fluent English reference text was used during training. The corpus consists of 819 transcribed conversations on predetermined topics between strangers, yielding ≈ 160 hours of speech and 150k utterances. We used one reference during training and evaluation with the val"
2020.iwslt-1.22,N19-1285,0,0.736896,"disfluent and fluent references, respectively. To produce fluent translations from disfluent text, one would typically require access to disfluent speech (or text) and its corresponding fluent translations during training. While some corpora with labeled disfluencies exist (Cho et al., 2014; Burger et al., 2002), only subsets have been translated and/or released. (Salesky et al., 2018) introduced a set of fluent references for the Fisher SpanishEnglish conversational speech corpus (David Graff and Cieri.). This has enabled a new task of endto-end training and evaluation on fluent references. (Salesky et al., 2019) reports results using a speechto-text model trained on this corpus using both fluent and disfluent translations. However, fluent translations of disfluent speech or text are a scarce resource. It would be highly desirable to build a system for disfluency removal that does not rely on fluent references. Index Terms- disfluency removal, machine translation, noise induction, leveraging monolingual data, denoising for disfluency removal. 1 Introduction and Related Work Spoken language translation often suffers due to the presence of disfluencies. In conversational speech, speakers often use disfl"
2020.lrec-1.201,W14-4012,0,0.0215111,"Missing"
2020.lrec-1.201,D14-1181,0,0.00246024,"es are highly vulnerable to commit suicide compared to females. These handful number of notes may not be sufficient to derive such conclusions but they can surely give a rough idea of the possible trend. Along with gender, length of notes also varies across the different age groups. Figure 2 depicts the relation among the size of notes, the number of notes and age interval of the deceased. Most lengthy notes belong to the young deceased whose age falls between 11 and 30. Also, among Methodology We develop and train three basic deep learning-based models, viz. Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014) on top of pre-trained word embeddings. Since GloVe 4 (Pennington et al., 2014) embedding model captures syntactic and semantic relations among words, we utilize it to learn our emotional word embeddings. The representation from the CNN/GRU/LSTM network is passed through an attention layer (Yang et al., 2016) that maps the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction."
2020.lrec-1.201,D14-1162,0,0.0866098,"Missing"
2020.lrec-1.201,W08-0616,0,0.323506,"well-being concern and a major cause of death worldwide, can be marked as the death caused by self-directed harmful behaviour with any intent to end one’s life. Moments before committing this horrendous act, usually an individual’s mind is flooded with a range of unpleasant emotions. (Pestian et al., 2008; Pestian et al., 2010; Duch et al., 2008). A suicide note is a significant asset when attempting to evaluate a patient’s risk of attempting suicide repeatedly. The suicide note furnishes us with the firsthand information about that individual’s specific personality status and mind rationale (Pestian and Matykiewicz, 2008). Suicide notes may serve some informative purpose and may have a remedial role in helping the surviving relatives to understand the reason behind suicide. An understanding of the messages contained inside suicide notes could be helpful for suicide aversion programs. Analyzing the inward feelings uncovered in the suicide note may assist us with identifying individuals who conceivably have suicide ideation (Xu et al., 2012), and thus prevent the misery from happening. Modelling the emotions present in such notes may help health experts in surveying suicide hazard, by contrasting the model with"
2020.lrec-1.201,C16-1287,1,0.88981,"Missing"
2020.lrec-1.201,N16-1174,0,0.371987,"ceased whose age falls between 11 and 30. Also, among Methodology We develop and train three basic deep learning-based models, viz. Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014) on top of pre-trained word embeddings. Since GloVe 4 (Pennington et al., 2014) embedding model captures syntactic and semantic relations among words, we utilize it to learn our emotional word embeddings. The representation from the CNN/GRU/LSTM network is passed through an attention layer (Yang et al., 2016) that maps the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction. Finally, 4 http://nlp.stanford.edu/data/wordvecs/ glove.840B.300d.zip 1621 we combine predictions of these models using the following two methods: using majority voting (MV), and using a MultiLayer Perceptron (MLP) network. For comparison purpose, we train four popularly used classical supervised models, namely Multinomial Naive Bayes, Support Vector Machine, Random Forest and Logistic Regression on our curated dataset, and compare with"
2020.lrec-1.273,P16-1072,0,0.0144283,"s model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers used both RNN and CNN to capture local features as well as long term relationships (Cai et al., 2016; Zhang et al., 2018b). Apart from CNN and RNN, some other approaches are also reported in the literature. He et al. (2018) proposed a syntax-aware entity embeddings based on tree-GRU for neural relation classification. Reinforcement learning is used by Feng et al. (2018a) for relation classification from noisy data. Inspired by Generative Adversarial Networks (GANs), Zeng et al. (2018a) proposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised data"
2020.lrec-1.273,P15-1017,0,0.130806,"um, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to u"
2020.lrec-1.273,P17-1038,0,0.012687,"includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conf"
2020.lrec-1.273,N15-1151,0,0.028377,"rial training (Goodfellow et al., 2014) to enhance the robustness of the classifier. CNN also fails to capture long distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities an"
2020.lrec-1.273,P16-2060,0,0.0865811,"xtraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make"
2020.lrec-1.273,D19-1041,0,0.0210277,"explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-resource scenario, especially for the Indic languages. Moreover, event extraction in the disaster domain can be very much helpful as we can mine the vast amount of online news 2242 and extract crucial information from these, and inform to the various stakeholders ranging from the government agencies t"
2020.lrec-1.273,P11-1113,0,0.0353007,"ng one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It incl"
2020.lrec-1.273,P08-1030,0,0.0555903,"in situations of crisis. 2. Related Studies Being one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction"
2020.lrec-1.273,N16-1030,0,0.0293008,"मे भूकंप की खबर है Feature # documents # sentences # Event Trigger words # Argument Trigger words Evaluation Event and Argument Trigger Classification : We formulate both event trigger detection and argument detection as sequence labelling problems. We also argue that they can be solved jointly as both of these tasks are co-related. Thus, we can define the task as : Given a sentence of the form w1 ,w2 ,w3 , . . . , wn , the task is to predict the event and argument labels (li ) for each word (wi ), where li ∈ {I,O,B}4 . We develop two baseline models based on Bi-LSTM+CRF (Huang et al., 2015; Lample et al., 2016) and Bi-LSTM+Softmax. Bi-LSTM (Schuster and Paliwal, 1997) is a very good sentence encoder and CRF (Conditional Random Field) (Lafferty et al., 2001) is very efficient in sequence labeling as it uses state transition matrix to take care of the past and future tags to predict the current tag (Sutton et al., 2012). Sequence labeling problem can 4 The encoding scheme is according to IOB2, where I indicates the word tokens that appear inside the trigger, B denotes the beginning of a trigger and O denotes the outside of an event trigger or argument trigger. The B is used only when two event or argu"
2020.lrec-1.273,P13-1008,0,0.0282259,"long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). So"
2020.lrec-1.273,N19-4019,0,0.0177926,"oposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised dataset. Relation extraction using deep learning techniques in cross-lingual setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-re"
2020.lrec-1.273,P10-1081,0,0.225496,"s. 2. Related Studies Being one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other f"
2020.lrec-1.273,P17-1004,0,0.0124793,"distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers"
2020.lrec-1.273,P16-1201,0,0.028074,"Missing"
2020.lrec-1.273,P17-1164,0,0.0125344,"shman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conference (TAC). Some of the existing works in Event extraction in disaster domain are reported in (Ta"
2020.lrec-1.273,D18-1156,0,0.0118356,"neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perfo"
2020.lrec-1.273,P16-1105,0,0.0131565,"o capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers used both RNN and CNN to capture local features as well as long term relationships (Cai et al., 2016; Zhang et al., 2018b). Apart from CNN and RNN, some other approaches are also reported in the literature. He et al. (2018) proposed a syntax-aware entity embeddings based on tree-GRU for neural relation classification. Reinforcement lea"
2020.lrec-1.273,P15-2060,0,0.100025,"searchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al."
2020.lrec-1.273,W15-1506,0,0.155737,"searchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al."
2020.lrec-1.273,D16-1085,0,0.0331386,"Missing"
2020.lrec-1.273,N16-1034,0,0.0157544,"se features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) propos"
2020.lrec-1.273,W03-2120,0,0.280326,"Missing"
2020.lrec-1.273,D18-1122,0,0.0180829,"or Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conference (TAC). Some of the existing works in Event extraction in disaster domain are reported in (Tanev et al., 2008; Yun, 2011; Klein et al., 2013; Dittrich and Lucas, 2014; Nugent et al., 2017; Burel et al., 2017). However, all the above are mostly in English language. Any significant attempt t"
2020.lrec-1.273,C18-1100,0,0.0124457,"(SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchical layers and dependency embedding to CNN architecture. Recently Ren et al. (2018) proposed a CNN based method with Adversarial training (Goodfellow et al., 2014) to enhance the robustness of the classifier. CNN also fails to capture long distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entiti"
2020.lrec-1.273,D11-1001,0,0.0285231,"xtensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et"
2020.lrec-1.273,W11-1807,0,0.0236237,"xtensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et"
2020.lrec-1.273,P15-1061,0,0.0172817,"ated to relation extraction where the relations between a pair of entities are extracted in contrast to our case where relation between event trigger and argument trigger is extracted. Deep Neural Network is being used for relation extraction in recent time. Zeng et al. (2014) proposed to use CNN in relation extraction for the first time where CNN was used for lexical and sentence level feature extraction. In this paper the authors proposed a novel Position Embedding (PE) feature which was very helpful to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irreleva"
2020.lrec-1.273,D19-1030,0,0.0142445,"earning is used by Feng et al. (2018a) for relation classification from noisy data. Inspired by Generative Adversarial Networks (GANs), Zeng et al. (2018a) proposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised dataset. Relation extraction using deep learning techniques in cross-lingual setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research comm"
2020.lrec-1.273,D14-1090,0,0.0519707,"Missing"
2020.lrec-1.273,D19-1585,0,0.0181788,"l setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-resource scenario, especially for the Indic languages. Moreover, event extraction in the disaster domain can be very much helpful as we can mine the vast amount of online news 2242 and extract crucial information from these, and inform to the various stakeholders ranging from the go"
2020.lrec-1.273,P16-1123,0,0.0357012,"Missing"
2020.lrec-1.273,C18-1099,0,0.0241651,"Missing"
2020.lrec-1.273,D15-1062,0,0.0179022,"to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchic"
2020.lrec-1.273,D15-1206,0,0.0127363,"to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchic"
2020.lrec-1.273,N16-1033,0,0.0167622,"ally. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency rela"
2020.lrec-1.273,C14-1220,0,0.0608331,"Missing"
2020.lrec-1.378,D16-1250,0,0.0220935,"lingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different given different contexts and can confuse an NLP task. Hence, we believe it is also impor"
2020.lrec-1.378,N09-3008,0,0.691237,"ng based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Beng"
2020.lrec-1.378,W18-3903,0,0.0630431,"Missing"
2020.lrec-1.378,P14-2017,0,0.425837,", 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Ass"
2020.lrec-1.378,P15-2071,0,0.353185,"(Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gu"
2020.lrec-1.378,I11-1097,0,0.0276839,"ngst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) emplo"
2020.lrec-1.378,E17-1113,0,0.143587,"Missing"
2020.lrec-1.378,2019.gwc-1.51,1,0.582606,"bda Kosha” and its annotation with linked Wordnet IDs. With the help of a lexicographer, we perform the digitization of this dictionary. Further, we annotate the cognate sets from the dicCognate False Friend Hindi (Hi) Marathi (Mr) Hindi Meaning Marathi Meaning ank shikshA ank shikshA Number Education Number Punishment Table 1: An example each of a cognate pair and a false friend pair from the closely related Indian languages Hindi (Hi) and Marathi (Mr) tionary with Wordnet synset IDs based on manual validation, where the lexicographer checks each Wordnet in the existing linked sense.Based on Kanojia et al. (2019b)’s approach, we use linked Indian Wordnets to generate true cognate data and create another cognate dataset. Additionally, we use the same Wordnet data to produce a list of False Friends and release2 all the three datasets publicly. Our cognate sets can be utilized for lookup in phrase tables produced during Machine Translation to assess the quality of the translation system in question. They can be utilized as candidate translations for words, and our false friends’ list can be utilized by language learners to avoid pitfalls during the acquisition of a second language. False Friend and Cogn"
2020.lrec-1.378,C04-1137,0,0.122835,"True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a co"
2020.lrec-1.378,A00-2038,0,0.820934,"s are often used as baseline methods for cognate detection, and the most commonly used method amongst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu"
2020.lrec-1.378,W12-0216,0,0.467071,"Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment."
2020.lrec-1.378,N01-1020,0,0.505957,"l 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Roman"
2020.lrec-1.378,J99-1003,0,0.897363,"ics (Rama et al., 2018) as well as cross-lingual 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Wo"
2020.lrec-1.378,K19-1011,0,0.115603,"ngual embeddings may not be an appropriate feature. Cross-lingual word embeddings project monolingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different"
2020.lrec-1.378,W97-1102,0,0.888502,"from gold-standard translation candidates. Keeping the application of our dataset in mind, we ignore the inclusion of partial cognates from this dataset. 3.2. D2 - True Cognate Pairs via IndoWornet In their paper, Kanojia et al. (2019b) identify IndoWordnet (Bhattacharyya, 2017) as a potential resource for the task of cognate detection. They utilize deep neural network based approaches to validate their approach for cognate detection. We build this dataset using a simple orthographic similarity based approach from the IndoWordnet dataset. Our approach combines Normalized Edit Distance (NED) (Nerbonne and Heeringa, 1997) and Cosine Similarity (CoS) (Salton and Buckley, 1988) between words. We compare synset words from every language pair using NED and populate a list of cognate sets where NED score is 0.7 and above. Similarly, we populate another list of cognate sets from every language pair using a shingle (n-gram) based Cosine Similarity with the same threshold. Due to the different methods using which NED and CoS similarity techniques compute scores, both NED and CoS output a different number of word pairs. We choose a common intersection of cognate pairs from among both the lists, and populate a final ‘po"
2020.lrec-1.378,N18-2063,0,0.455793,"Missing"
2020.lrec-1.378,C16-1097,0,0.756328,"automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognat"
2020.lrec-1.378,C02-1002,0,0.191073,"st in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language fa"
2020.lrec-1.378,L18-1538,0,0.0193274,"nducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a cognate set data has not been released previously for Indian languages, to the best of our knowledge. Additionally, we release lists of false friends’ for language pairs. These cognates can be used to challenge the previously established cognate detection approaches further. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verifica"
2020.lrec-1.514,P18-1073,0,0.0151351,"istory and generic response; Gradient reversal layer is used to learn language invariant features; Polite responses for both Hindi and English are the generated outputs of the proposed model. Embedding Layer: Word embeddings are usually trained through an unsupervised manner on a huge dataset, and then the embeddings are fine-tuned by the supervised training process. For word embedding, we use the pre-trained embedding model, FastText2 for both English and Hindi. The monolingual embeddings for Hindi and English are mapped in the same vector space using linear transformation as illustrated in (Artetxe et al., 2018). With this technique, embeddings for every language exist in the same vector space and maintain the property that words with similar meanings (regardless of language) are close together in the vector space. Hence, the words in English appears close to the words in Hindi in the embedding space. Thus, we train on one or more languages and learn a model that operates on words of a particular language that was not present during training. catenated with the emotional representation of the given utterance to give the final utterance representation fd . The final utterance representation f1 , f2 ,"
2020.lrec-1.514,Q17-1010,0,0.00732333,"nts are first obtained. Each tweet is then divided into the three types of sentences, (i). informative if they have purely information, and no courteous expression in it; (ii). purely courteous utterances; and (iii). hybrid sentence denoting both informative and courteous. • Clustering: The customer care agents of a particular company mainly use expressions and sentences belonging to similar patterns. Hence grouping these similar expressions and sentences before annotation helps in making the annotation process faster. The vector representation of the utterances using the FastText embeddings (Bojanowski et al., 2017) for the Hindi language is used to represent the utterances. We then use the K-means clustering algorithm (Aggarwal and Zhai, 2012) with k = 300 to cluster these sentences. Basically, by clustering, we intend to divide the sentences into groups, where the sentences in a particular group are highly similar to each other in comparison to sentences in other groups. • Annotation: The segmented and clustered sentences are annotated by three annotators proficient in the Hindi language. The annotators were asked to label each sentence into three categories that are courteous, informative and hybrid."
2020.lrec-1.514,D17-1169,0,0.0314411,"states h21 , h22 , ...., h2n˜ . The conversational history is represented by the last hidden state h2n˜ , and is thereby referred to as the conversational context vector c. Contextual Encoder: The context encoder captures the conversational history C, which is a sequence of user utterances u1 , u2 , ....., un , where n is the total number of utterances in a given conversation. Each user utterance un comprises of a sequence of words w1 , w2 , ..., wn′ where n′ is the total number of words in a given utterance, and every word is represented by their pre-trained embeddings. We use the DeepMoji (Felbo et al., 2017) output distribution that is pre-trained on the emoji prediction task to encode the utterances with their corresponding emotional states. A Bi-directional Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) layer is used for encoding the utterances, and their representations are denoted by h1i , h12 , ....h1n , where n denotes the nth word in the utterance. The last hidden state h1n of the BiLSTM denoting the utterance representation is coneti = v T tanh(Wh hi + Ws st + battn ) (1) αt = sof tmax(et ) (2) 2 Generic Response Encoder: The word embedding sequence of the generic res"
2020.lrec-1.514,N19-1091,1,0.176764,"their preferred language by making the responses polite and courteous, eventually leading to user satisfaction and high customer retention for any given brand or company. The ability of such systems to understand the emotions of the users in different languages and responding in accordance with the emotion is a challenging task. Also, politeness is a virtue of humans, and to make a machine understand and behave amicably and courteously is an additional task for such systems. Hence, in this work, we propose a large-scale Hindi dataset for this task and evaluate using the baseline approach of (Golchha et al., 2019) to incorporate politeness in customer care responses belonging to different languages and providing new research directions for showcasing the differences in politeness and courteous behavior across the languages. We summarize the key contributions as follows: (i) We create a large-scale Hindi conversational data, prepared from the actual conversations on Twitter. (ii) We propose a robust response generation model for both Hindi and English languages by modeling the conversational history and the emotional state of the user by learning language invariant representation using adversarial train"
2020.lrec-1.514,W16-3609,0,0.0273755,"t dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversa"
2020.lrec-1.514,N18-2008,0,0.0119683,"g NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018"
2020.lrec-1.514,D16-1127,0,0.0240706,"results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms"
2020.lrec-1.514,N18-1169,0,0.0178495,"otional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018) in an unsupervised fashion. The authors in (Golchha et al., 2019) proposed a reinforced pointer generator network for inducing courteous behavior in customer care responses. Also, there is a recent shift in building systems that are capable of understanding different languages (Li et al., 2018b; Do and Gaspers, 2019; Masumura et al., 2018a), hence making conversational agents robust in their applications. In this work, we propose a novel system that is ca4173 pable of generating polite responses in different languages (in our case, H"
2020.lrec-1.514,W04-1013,0,0.0107642,"set for early stopping and finding the best models for decoding. Language English Hindi Model Seq2Seq Our Model Seq2Seq Our Model 0 16.88 9.87 15.42 10.56 F 1 41.32 42.05 40.54 41.28 2 41.80 48.08 44.04 48.16 0 16.74 13.52 17.23 14.11 CA 1 40.33 39.27 41.63 38.77 2 42.93 47.21 41.14 47.12 -1 24.56 13.24 25.84 14.62 PC 0 48.71 37.19 50.66 38.39 1 26.73 49.57 23.50 46.99 Table 4: Human evaluation results for Fluency, Content Adequacy and Politeness Consistency (All values are in percentages.) Automatic evaluation: In addition to conventional metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and perplexity, we also use two taskspecific metrics such as Emotional Accuracy (EA) and Content preservation (CP) for automatic evaluation as described in (Golchha et al., 2019). Human evaluation: We adopt a human assessment to compare the efficiency of various models to comprehend the quality of the generated polite responses same as (Golchha et al., 2019). For human evaluation, we randomly chose 700 samples from the test set for both the languages. Six human annotators with postgraduate exposure on both Hindi and English languages (three each for a language) were allocated to assess the po"
2020.lrec-1.514,D15-1166,0,0.0317492,"Missing"
2020.lrec-1.514,D18-1064,0,0.257098,"ssist humans and help them in the smallest possible ways as humanly as possible. Natural language generation (NLG) module of every dialogue system is an essential component as it presents the information to the user. To enhance the interactions between human and computers, recently researchers have focused on adapting different styles, emotions and personalities in text generation. Recent research has been inclined to make the system understand different languages giving rise to multi-lingual applications. With the significant focus on making the computer understand different languages as in (Masumura et al., 2018b; Masumura et al., 2018a; Upadhyay et al., 2018), researchers are aiming to make the dialogue systems language invariant as in real-world scenario. Also, for more extensive applications, it is crucial for these systems to be able to converse with humans in their preferred language, thereby increasing the usage and advancement of technology. Providing assistance to the customer through social media channels is attaining high popularity. The centre of our current work focuses on incorporating politeness in customer care responses belonging to different languages. Due to the unavailability of la"
2020.lrec-1.514,C18-1304,0,0.253702,"ssist humans and help them in the smallest possible ways as humanly as possible. Natural language generation (NLG) module of every dialogue system is an essential component as it presents the information to the user. To enhance the interactions between human and computers, recently researchers have focused on adapting different styles, emotions and personalities in text generation. Recent research has been inclined to make the system understand different languages giving rise to multi-lingual applications. With the significant focus on making the computer understand different languages as in (Masumura et al., 2018b; Masumura et al., 2018a; Upadhyay et al., 2018), researchers are aiming to make the dialogue systems language invariant as in real-world scenario. Also, for more extensive applications, it is crucial for these systems to be able to converse with humans in their preferred language, thereby increasing the usage and advancement of technology. Providing assistance to the customer through social media channels is attaining high popularity. The centre of our current work focuses on incorporating politeness in customer care responses belonging to different languages. Due to the unavailability of la"
2020.lrec-1.514,Q18-1027,0,0.089795,"8; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018) in an unsupervised fashion. The authors in (Golchha et al., 2019) proposed a reinforced pointer generator network for inducing courteous behavior in customer care responses. Also, there is a recent shift in building systems that are capable of understanding different languages (Li et al., 2018b; Do and Gaspers, 2019; Masumura et al., 2018a), hence making conversational agents robust in their applications. In this work, we propose a novel system that is ca4173 pable of generating polite responses in different languages (in our case, Hindi and English) by learning language inv"
2020.lrec-1.514,P02-1040,0,0.109721,"running loss on the validation set for early stopping and finding the best models for decoding. Language English Hindi Model Seq2Seq Our Model Seq2Seq Our Model 0 16.88 9.87 15.42 10.56 F 1 41.32 42.05 40.54 41.28 2 41.80 48.08 44.04 48.16 0 16.74 13.52 17.23 14.11 CA 1 40.33 39.27 41.63 38.77 2 42.93 47.21 41.14 47.12 -1 24.56 13.24 25.84 14.62 PC 0 48.71 37.19 50.66 38.39 1 26.73 49.57 23.50 46.99 Table 4: Human evaluation results for Fluency, Content Adequacy and Politeness Consistency (All values are in percentages.) Automatic evaluation: In addition to conventional metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and perplexity, we also use two taskspecific metrics such as Emotional Accuracy (EA) and Content preservation (CP) for automatic evaluation as described in (Golchha et al., 2019). Human evaluation: We adopt a human assessment to compare the efficiency of various models to comprehend the quality of the generated polite responses same as (Golchha et al., 2019). For human evaluation, we randomly chose 700 samples from the test set for both the languages. Six human annotators with postgraduate exposure on both Hindi and English languages (three each for a language) were allocat"
2020.lrec-1.514,P17-1099,0,0.0184945,"Missing"
2020.lrec-1.514,N18-1186,0,0.0252995,"e dataset description followed by the proposed methodology in Section 4. Experimental details, evaluation metrics and results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG system"
2020.lrec-1.514,P17-1061,0,0.019961,"Section 4. Experimental details, evaluation metrics and results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it i"
2020.lrec-1.514,P18-1104,0,0.0150401,"ystem can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; N"
2020.lrec-1.613,baccianella-etal-2010-sentiwordnet,0,0.0139889,"2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity dis"
2020.lrec-1.613,W06-1615,0,0.226339,"ch is based on the hypothesis that if source and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domai"
2020.lrec-1.613,Q17-1010,0,0.0320651,"ing D1 in D2 and ∆E indicates percentage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is"
2020.lrec-1.613,D18-2029,0,0.0254947,"three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cos"
2020.lrec-1.613,N19-1149,0,0.0182451,"int sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a method for source domain selection as a weighted sum of similarity metrics. They use statistical classifiers such as logistic regression and support vector machines. However, the similarity measures used are computationally intensive. To the best of our knowledge, this is the first work at this scale that compares different costeffective similarity metrics with the performance of CDSA. 3. 1. Labelled Data: Here, each review"
2020.lrec-1.613,P07-1033,0,0.0418056,"Missing"
2020.lrec-1.613,L16-1041,0,0.0167249,"all our metrics in detail later in this section. These 11 metrics can also be classified into two categories: • Symmetric Metrics - The metrics which consider domain-pairs (D1 , D2 ) and (D2 , D1 ) as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings. Sentiment Classifier The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset (Dragoni et al., 2016), which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics. We normalize the dataset by removing numerical values, punctuations, stop words, and c"
2020.lrec-1.613,P11-1013,0,0.0355759,"Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Re"
2020.lrec-1.613,P12-1043,0,0.0274583,"aume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a metho"
2020.lrec-1.613,J81-4005,0,0.668614,"Missing"
2020.lrec-1.613,P18-2064,1,0.804252,"just need to search for them in the target domain to find out common significant words. LM2: Symmetric KL-Divergence (SKLD) KL Divergence can be used to compare the probabilistic distribution of polar words in two domains (Kullback and Leibler, 1951). A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains 4984 is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by Murthy et al. (2018) and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as ‘polar’ for a domain if, |P − N |>= 0.5 (2) where P is the probability of a word appearing in a review which is labelled positive and N is the probability of a word appearing in a review which is labelled negative. SKLD of a polar word for domain-pair (D1 , D2 ) is calculated as: ! ! P1 N1 + P1 ∗ log (3) A = N1 ∗ log N2 P2 N2 B = N2 ∗ log N1 ! P2 + P2 ∗ log P1 (4) A+B (5) 2 where Pi and Ni are probabilities of a word appearing under positively labelled and negatively labelle"
2020.lrec-1.613,D14-1162,0,0.0821559,"Missing"
2020.lrec-1.613,N18-1202,0,0.00906568,"tage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain"
2020.lrec-1.613,I13-1076,1,0.806812,"main-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair (D1 , D2 ) using the Jaccard Similarity Coefficient which is calculated as follows: C W1 + W2 − C |P1 − P2 |+ |N1 − N2 | (8) The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs. (L1 Distance)avg + 1 J (9) Domain pairs are ranked in increasing order of final value. ! SKLD = J= which change their polarity across domains (Sharma and Bhattacharyya, 2013). The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc. For every common polar word between two domains, L1 Distance between two vectors [P1 , N1 ] and [P2 , N2 ] is calculated as; LM4: Entropy Change Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution"
2020.lrec-1.613,P18-1089,1,0.855125,"for each source domain can be highly intensive both in terms of time and resources. This makes it important to devise easy-to-compute metrics that use labelled data in the source and target domains. When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain: LM1: Significant Words Overlap All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the ‘Clothing’ domain but not as significant in the ‘Movie’ domain. In this metric, we build upon existing work by Sharma et al. (2018) and extract significant words from 4983 D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D1 84.84 76.34 74.47 75.66 81.14 74.19 75.57 73.83 75.39 82.75 77.11 78.31 76.00 74.28 71.91 72.15 77.14 77.15 78.83 79.08 D2 70.15 83.24 75.00 74.32 69.81 73.87 77.93 73.29 72.90 72.69 65.46 79.11 79.46 77.31 72.34 75.18 77.22 80.04 71.26 70.15 D3 72.58 77.71 85.78 80.31 70.86 71.99 75.67 78.39 78.70 73.83 66.81 78.49 77.00 80.29 71.26 76.59 77.27 76.21 75.33 71.98 D4 73.94 77.83 80.16 84.49 73.09 76.37 75.08 79.82 76.93 73.59 72.53 78.69 78.42 78.66 75.29 75.44 77.06 79.09 76.18 73"
2020.lrec-1.613,P09-1043,0,0.0453219,"e and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been ex"
2020.lrec-1.621,C16-1047,1,0.851627,"U layer. Figure 2: The proposed Ensemble architecture embeddings from fastText ((Joulin et al., 2016)). The fastText uses subword information to generate embedding for a word, and hence it is able to handle the out-of-vocabulary (OOV) problem. The embedding of each word is then used as an input to the individual deep learning model to learn the representation of tweet. 4.1.2. Convolutional Neural Network (CNN) CNN architecture has been widely used in variety of NLP task (Kumar and Singh, 2019; Kim, 2014) and it has been successfully applied to solve sentiment classification at various levels (Akhtar et al., 2016). Convolutional neural network (CNN) consists of convolutional layers. Convolutional process on sentence is used to conserve n-gram information of sentence. Convolutional layers are followed by non-linear layer, Relu, followed by the pooling layers. We use 2 convolutional layers. The first convolutioanl layer contains 128 filters of sizes 2, 3, and 4 each, and second convolutional layer contains 128 filters of size 3. It is then followed by max pooling layer, dense layer and output layer. Filters of sizes 2, 3, and 4 correspond that the filter can slide over 2, 3, or 4 words at a time. 4.1.3."
2020.lrec-1.621,baccianella-etal-2010-sentiwordnet,0,0.524625,"Missing"
2020.lrec-1.621,P07-1056,0,0.971504,"blic and to maintain the law and order. The very first step in building such type of intelligent system is the mining of user sentiments. Deep learning has evolved as a popular technique over the years to solve many Natural Language Processing (NLP) problems including sentiment analysis. Annotated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus for sentiment analysis, and then develop a deep neural network based baseline model to tag each twe"
2020.lrec-1.621,D14-1181,0,0.00786971,"the pre-trained word 5049 layers of bidirectional GRU on top of each other with 128 units in each GRU layer. Figure 2: The proposed Ensemble architecture embeddings from fastText ((Joulin et al., 2016)). The fastText uses subword information to generate embedding for a word, and hence it is able to handle the out-of-vocabulary (OOV) problem. The embedding of each word is then used as an input to the individual deep learning model to learn the representation of tweet. 4.1.2. Convolutional Neural Network (CNN) CNN architecture has been widely used in variety of NLP task (Kumar and Singh, 2019; Kim, 2014) and it has been successfully applied to solve sentiment classification at various levels (Akhtar et al., 2016). Convolutional neural network (CNN) consists of convolutional layers. Convolutional process on sentence is used to conserve n-gram information of sentence. Convolutional layers are followed by non-linear layer, Relu, followed by the pooling layers. We use 2 convolutional layers. The first convolutioanl layer contains 128 filters of sizes 2, 3, and 4 each, and second convolutional layer contains 128 filters of size 3. It is then followed by max pooling layer, dense layer and output la"
2020.lrec-1.621,P11-1015,0,0.509579,"sitive, negative, mixed, or other classes. The authors have shown an inter-annotator agreement of 0.655. (Thelwall et al., 2012) created a dataset consisting of 4,424 tweets. Tweets were manually annotated with positive (1 to 5) and negative strength (-1 to -5). (Socher et al., 2013) introduced a Sentiment Treebank (STB) dataset constructed from the movie reviews domain. This dataset contains 215,154 phrases in the parse trees of 11,855 sentences annotated at the fine-grained level. This dataset was annotated with 5 classes, viz., positive, negative, neutral, very positive and very negative. (Maas et al., 2011) introduced a larger IMDB dataset containing 50000 movie reviews for binary classification. Only highly polarized reviews were considered by them. For example, a negative review had score ≤ 4 out of 10 and a positive review had a score ≥ 7 out of 10. (Go et al., 2009) used distant supervision to create Stanford Twitter Sentiment (STS) corpora containing 160,000 tweets. The data was crawled by using positive and negative emoticons from Twitter using Twitter Search API. Tweets with positive emoticons were considered as positive and tweets with negative emoticons were considered as negative. It a"
2020.lrec-1.621,S13-2053,0,0.0832409,"Missing"
2020.lrec-1.621,W16-0429,0,0.168387,"r annotations to make the data suitable for performing experiments. Some examples of the tweets along with their irrelevant categories are shown in Table 2. 3.3. Data Annotation After data extraction and pre-processing, we conduct manual annotation of the dataset. Three annotators with postgraduate level knowledge in English are employed for annotation. Annotators are asked to write the overall polarity of the tweet for 3 classes, viz., neutral, negative and positive, if any opinion expression is found. For annotation, we follow the guidelines used in the SemEval task (Rosenthal et al., 2015; Mohammad, 2016). We provided some tweets to the annotators with gold labels to create understanding of the class labels. Annotators were also instructed to annotate the tweet without being biased towards any specific demographic area, religion, etc. 3.4. Challenges During annotation, we faced the following challenges: 5 • If a writer makes a request to do something positive in the context of a negative situation, then we assumed the sentiment to be positive. For example, we should unite together to remove crime from our country. In this example tweet, the writer is requesting everyone with positive attitude,"
2020.lrec-1.621,P04-1035,0,0.165,"the years have been created for sentiment analysis. But, most of these efforts have been put in the domain of movie reviews, product reviews, hotel reviews, etc. Below we present a survey on the various resources created for sentiment analysis. (Pang et al., 2002) created a sentiment corpora from internet movie database which contains only those reviews where the author rating was expressed either with stars or some numerical value. Based on the rating, sentiment polarity was automatically decided from 2 categories, positive and negative. This contains 752 negative and 1301 positive reviews. (Pang and Lee, 2004) published another movie review polarity dataset for 2 classes, positive and negative. Dataset contains 1000 positive and 1000 negative movie reviews. (Blitzer et al., 2007) created a MultiDomain Sentiment (MDS) Dataset consisting of four different types of product reviews taken from Amazon.com including Books, DVDs, Electronics, and Kitchen appliances. Dataset comprises of 1000 positive and 1000 negative reviews for each domain. (Shamma et al., 2009) constructed Obama-McCain Debate dataset by crawling the first U.S. presidential TV debate tweets in September 2008. They annotated the tweets fo"
2020.lrec-1.621,W02-1011,0,0.0473072,"p to monitor terrorist groups, domestic threats, and crime activities to provide security to public and to maintain the law and order. The very first step in building such type of intelligent system is the mining of user sentiments. Deep learning has evolved as a popular technique over the years to solve many Natural Language Processing (NLP) problems including sentiment analysis. Annotated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus"
2020.lrec-1.621,D14-1162,0,0.0846734,"Missing"
2020.lrec-1.621,S15-2078,0,0.0243974,"bove were performed after annotations to make the data suitable for performing experiments. Some examples of the tweets along with their irrelevant categories are shown in Table 2. 3.3. Data Annotation After data extraction and pre-processing, we conduct manual annotation of the dataset. Three annotators with postgraduate level knowledge in English are employed for annotation. Annotators are asked to write the overall polarity of the tweet for 3 classes, viz., neutral, negative and positive, if any opinion expression is found. For annotation, we follow the guidelines used in the SemEval task (Rosenthal et al., 2015; Mohammad, 2016). We provided some tweets to the annotators with gold labels to create understanding of the class labels. Annotators were also instructed to annotate the tweet without being biased towards any specific demographic area, religion, etc. 3.4. Challenges During annotation, we faced the following challenges: 5 • If a writer makes a request to do something positive in the context of a negative situation, then we assumed the sentiment to be positive. For example, we should unite together to remove crime from our country. In this example tweet, the writer is requesting everyone with p"
2020.lrec-1.621,S17-2088,0,0.189646,"(Go et al., 2009) used distant supervision to create Stanford Twitter Sentiment (STS) corpora containing 160,000 tweets. The data was crawled by using positive and negative emoticons from Twitter using Twitter Search API. Tweets with positive emoticons were considered as positive and tweets with negative emoticons were considered as negative. It also contains a test set which contains 498 tweets, manually annotated for 3 classes, viz., positive, negative, and neutral. Test set was crawled using names of products, companies, and people. SemEval 2017 dataset was constructed as a part of Task 4 (Rosenthal et al., 2017). Dataset was annotated on points of 2 (positive and negative), 3 (positive, negative, and neutral), and 5 (strongly positive, weakly positive, neutral, weakly negative, and strongly negative) scales. All the annotations were performed using CrowdFlower. SemEval 2017 dataset was built by merging all previous year’s SemEval datasets, consisting of 50,333 tweets related to twitter trends Donald Trump, iPhone, etc. Researchers have also put their efforts towards building sentiment corpora for the financial domain. As an example, (O’Hare et al., 2009) created financial blog corpus. They collected"
2020.lrec-1.621,D13-1170,0,0.0191802,"views taken from Amazon.com including Books, DVDs, Electronics, and Kitchen appliances. Dataset comprises of 1000 positive and 1000 negative reviews for each domain. (Shamma et al., 2009) constructed Obama-McCain Debate dataset by crawling the first U.S. presidential TV debate tweets in September 2008. They annotated the tweets for positive, negative, mixed, or other classes. The authors have shown an inter-annotator agreement of 0.655. (Thelwall et al., 2012) created a dataset consisting of 4,424 tweets. Tweets were manually annotated with positive (1 to 5) and negative strength (-1 to -5). (Socher et al., 2013) introduced a Sentiment Treebank (STB) dataset constructed from the movie reviews domain. This dataset contains 215,154 phrases in the parse trees of 11,855 sentences annotated at the fine-grained level. This dataset was annotated with 5 classes, viz., positive, negative, neutral, very positive and very negative. (Maas et al., 2011) introduced a larger IMDB dataset containing 50000 movie reviews for binary classification. Only highly polarized reviews were considered by them. For example, a negative review had score ≤ 4 out of 10 and a positive review had a score ≥ 7 out of 10. (Go et al., 200"
2020.lrec-1.621,takala-etal-2014-gold,0,0.47495,"Missing"
2020.lrec-1.621,L18-1442,1,0.920473,"otated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus for sentiment analysis, and then develop a deep neural network based baseline model to tag each tweet into three affect classes, namely positive, negative, and neutral. This is the very first attempt towards creating a benchmark setup for sentiment analysis in the above mentioned socially relevant domains. The corpus is manually annotated by three expert annotators. The inter-annotator"
2020.lrec-1.621,S14-2077,0,0.0391117,"Missing"
2020.lrec-1.675,P16-1223,0,0.0488709,"Missing"
2020.lrec-1.675,P17-1171,0,0.0972473,"ls. But both of these works are based on Cloze Style MRC task. In contrast, our task focuses on preparing a dataset for span-of-words prediction (Span Prediction) based MRC model, where the system has to extract span-of-words as answer to a particular question based on the context. We employ articles from Elsevier Computer Science Journals (like ARTINT, COMNET etc.). 1.1. Related Work The problem of document understanding falls in the domain of Natural Language Understanding (NLU), and has a long history. Machine Reading Comprehension (Hermann et al., 2015) and Open Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unans"
2020.lrec-1.675,P17-1152,0,0.19737,"ls. But both of these works are based on Cloze Style MRC task. In contrast, our task focuses on preparing a dataset for span-of-words prediction (Span Prediction) based MRC model, where the system has to extract span-of-words as answer to a particular question based on the context. We employ articles from Elsevier Computer Science Journals (like ARTINT, COMNET etc.). 1.1. Related Work The problem of document understanding falls in the domain of Natural Language Understanding (NLU), and has a long history. Machine Reading Comprehension (Hermann et al., 2015) and Open Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unans"
2020.lrec-1.675,D14-1179,0,0.00935404,"Missing"
2020.lrec-1.675,P18-1078,0,0.0137731,"se MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ questions generated by annotators. RACE: This dataset was created by Lai et al. (2017), and contains nearly 100,000 multiple choice questions and 27,000 passages from standardized tests for Chinese students, who are learning English as a foreign language. The aim of creating this dataset is to test the students0 ability in understanding and reasoning, covering variety of topics. AI2 Reasoning Challenge (ARC): A team (Clark and Gardner, 2018) of Allen Institute for Artificial Intelligence prepared this dataset. It consists of 7,787 grade-school multiple choice (with 4 possible options) science question. ReCoRD: Zhang et al. (2018) represents this MRC dataset that requires commonsense reasoning. It contains 12,000 cloze-style question passage pairs extracted from CNN/Daily Mail news articles. It requires common sense reasoning to answer the queries of this dataset. 1.2. Motivation and Contribution Our understanding and survey reveal that- although there are many benchmark datasets available for questionanswering- but there has not"
2020.lrec-1.675,D19-1600,0,0.019686,"tion data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous datasets and models are controled by questions that can be answered by selecting answers using local contextual similarity or global term frequency. This dataset encourages the deeper understanding of languages. Span Extract Chinese MRC Dataset: Cui et al. (2019) recently proposed a novel dataset to add language diversities in this area as the existing datasets focus on only English language. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. Delta Reading Comprehension Dataset (DRCD): Shao et al. (2018) proposed an open domain traditional Chinese MRC dataset. The main aim of this dataset is to be a standard Chinese MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ question"
2020.lrec-1.675,W18-2605,0,0.019662,"questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chinese MRC dataset proposed by He et al. (2018). The dataset was created with the real application data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous datasets and models are controled by questions that can be answered by selecting answers using local contextual similarity or global term frequency. This dataset encourages the deeper understanding o"
2020.lrec-1.675,P14-1026,0,0.0812576,"Missing"
2020.lrec-1.675,D17-1082,0,0.0393713,"Missing"
2020.lrec-1.675,L18-1439,0,0.0322515,"Missing"
2020.lrec-1.675,D14-1162,0,0.0844653,"Missing"
2020.lrec-1.675,D16-1264,0,0.0730864,"Missing"
2020.lrec-1.675,P18-2124,0,0.0166837,"Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unanswerable questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chi"
2020.lrec-1.675,D13-1020,0,0.0815989,"Missing"
2020.lrec-1.675,W17-2623,0,0.022775,"the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unanswerable questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chinese MRC dataset proposed by He et al. (2018). The dataset was created with the real application data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous"
2020.lrec-1.675,D15-1237,0,0.0381738,"Missing"
2020.lrec-1.675,W17-2603,0,0.0185071,"rea as the existing datasets focus on only English language. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. Delta Reading Comprehension Dataset (DRCD): Shao et al. (2018) proposed an open domain traditional Chinese MRC dataset. The main aim of this dataset is to be a standard Chinese MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ questions generated by annotators. RACE: This dataset was created by Lai et al. (2017), and contains nearly 100,000 multiple choice questions and 27,000 passages from standardized tests for Chinese students, who are learning English as a foreign language. The aim of creating this dataset is to test the students0 ability in understanding and reasoning, covering variety of topics. AI2 Reasoning Challenge (ARC): A team (Clark and Gardner, 2018) of Allen Institute for Artificial Intelligence prepared this dataset. It consists of 7,787 grade-school multiple choice (with 4 possible options) science question. ReCoRD: Zhang et al. (2018) represents this MRC dataset that requires common"
2020.lrec-1.675,P13-1043,0,0.0822359,"Missing"
2020.nuse-1.11,W13-3520,0,0.011428,"rimental Setup We used the Google SyntaxNet2 for dependency parsing the Hindi sentences. SyntaxNet is a TensorFlow toolkit for deep learning powered natural language understanding developed at Google. The Parsey Universal component of SyntaxNet supports NLP preprocessing tasks such as POS tagging, morphological analysis and dependency parsing for 40 different languages including Hindi. We employed two different NER approaches proposed for Hindi and consider a word as part of a named entity if either or both of them identify it as a named entity. One of the approaches is Polyglot, proposed in (Al-Rfou et al., 2013). It is based on using language agnostic techniques involving Wikipedia and Freebase and no human annotated NER training data. The second approach is proposed by (Murthy et al., 2019) and is based on a supervised deep learning architecture for NER in Hindi. To access the Hindi WordNet (Narayan et al., 2002), we use the pyiwn toolkit (Panjwani et al., 2018) which is a python API to access Indian language WordNets. We use the Sardar dataset as a training set to iteratively revise and improve the extraction algorithms while keeping the other datasets unseen. In order to filter out an irrealis or"
2020.nuse-1.11,W17-5912,1,0.737164,"in the sentence rAm n  EktAb dF (’Ram ne Kitab di’; Ram gave the book), dF (di;give) is the event, however in the sentence rAm n  EktAb Ko dF (’Ram ne Kitab kho di’; Ram lost the book.), Ko dF(’kho di’; lost) is the event. Similarly, in the sentence rAm s  EktAb gm ho gI (’Ram se kitab gum ho gayee’; Ram lost the book), gm ho gI (’gum ho gayee’; lost) is the event. As compared to English, LVCs are more common in Hindi, have different We also address the problem of automatically extracting the MSC representation from a given narrative text. MSC extends the notion of a single event-timeline (Bedi et al., 2017) for a narrative by providing a timeline per actor (entity of interest). MSC representation captures all the actors and interactions in an easy to visualize manner and hence make the text more comprehensible. Further, the representation’s support for inference mechanisms opens up possibilities of tackling natural language understanding problems like text comprehension and question answering. Our work is similar to (Palshikar et al., 2019) to represent a English narrative using MSC. However, due to intricacies of events in Hindi language, their approach cannot be used for construction of MSC of"
2020.nuse-1.11,2018.gwc-1.47,1,0.720307,"rent languages including Hindi. We employed two different NER approaches proposed for Hindi and consider a word as part of a named entity if either or both of them identify it as a named entity. One of the approaches is Polyglot, proposed in (Al-Rfou et al., 2013). It is based on using language agnostic techniques involving Wikipedia and Freebase and no human annotated NER training data. The second approach is proposed by (Murthy et al., 2019) and is based on a supervised deep learning architecture for NER in Hindi. To access the Hindi WordNet (Narayan et al., 2002), we use the pyiwn toolkit (Panjwani et al., 2018) which is a python API to access Indian language WordNets. We use the Sardar dataset as a training set to iteratively revise and improve the extraction algorithms while keeping the other datasets unseen. In order to filter out an irrealis or non-punctual event, we check its respective P-COMP against a manually curated list of verbs such as яAtA ,яAyA, rhF, rh , cAh , cAhtF, honA, hotF indicating either continuity or uncertainty of events. We also ignore events appearing in quotes as they are likely to be authors’ opinions. 4 emergency 56 1373 78 74 Table 2: Dataset Statistics Figure 2: Examp"
2020.nuse-1.11,2020.isa-1.2,0,0.02677,"he Indian Independence Movement in Table 1 and the corresponding MSC in Figure 1. 2 Key Annotation and Extraction Challenges for Events in Hindi Annotation and extraction of events and their arguments from English texts is a challenging task (Mitamura et al., 2015). In case of Hindi there are more challenges as compared to English. Following are the key challenges we observed while processing Hindi narratives. I. Absence of annotation guidelines and labelled data similar to ACE: Only few attempts have been made to define comprehensive event annotation guidelines for Hindi. (Goud et al., 2019; Goel et al., 2020) propose a set of guidelines for annotation of event mentions in Hindi. However, they do not consider arguments of events, which are vital for narrative processing. II. Annotation of events with Light Verb Constructs (LVCs): LVCs are formed from a commonly used verb and usually a noun phrase (NP) in its direct object position, such as have a look or take an action. For example, in the sentence rAm n  EktAb dF (’Ram ne Kitab di’; Ram gave the book), dF (di;give) is the event, however in the sentence rAm n  EktAb Ko dF (’Ram ne Kitab kho di’; Ram lost the book.), Ko dF(’kho di’; lost) is the e"
2020.nuse-1.11,P96-1054,0,0.378358,"happened or not. Following are examples of sentences with realis and irrealis mood events: • Realis: ldn яAkr uho
  b {Er-VrF kF pYAI kF (landan jakar unhone baristari ki padhai ki; He went to London and studied law) • Irrealis: yEd srdAr kC vq aOr яFEvt rht  to BArt kA kAyAkSp ho яAtA (yadi sardar kuch varsh aur jeewit rehte to pure bharat ka kayakalp ho jata; If Sardar remained alive for a few more years, India would have been transformed.) IV. Only punctual events are annotated as events. An event is punctual if it “does not have a transitional phase between its start and end point” (Kay and Aylett, 1996). This implies that a process in continuation is not punctual and hence not marked as a valid event. • Punctual event: EksAno n  ag  }я srkAr s  BArF kr m  C  V kF mAg kF (kisano ne angrejh sarkar se bhari kar me chut ki mang ki; The farmers demanded the British government, a waiver in the heavy taxes) • Non-punctual event: gяrAt kA K XA KX un Edno sK  kF cp V m  TA (gujrat ka kheda khand un dino sukhe ki II. The head verb of an event predicate is tagged as PIVOT. If an event predicate is a conjunct predicate, its noun element is tagged as P-CONJ. In case of a compound predica"
2020.nuse-1.11,H05-1004,0,0.064958,"Missing"
2020.nuse-1.11,P14-5010,0,0.00244224,"t is described (Chatman, 1975). In this paper we focus on visualization of the plot aspect of a Hindi narrative. Hindi is an Indo-Aryan language spoken by around 300 million people in India. Additionally, Hindi is the fourth most-spoken first language in the world1 . In comparison to English, Hindi has different linguistic characteristics leading to a different set of NLP challenges. First of all, Hindi is a Subject-Object-Verb (SOV) language with relatively free word order, as against the SVO order in English. Secondly, Hindi does not have high accuracy NLP toolkits such as Stanford CoreNLP (Manning et al., 2014). In this paper, we make one of the first attempts to facilitate event processing in Hindi by proposing annotation guidelines for events as well as their arguments. We propose to represent a Hindi narrative using Message Sequence Charts (MSC) (Rudolph Introduction Narratives are used to communicate complex ideas, detailed accounts of complex events or arguments about one’s beliefs (Valls Vargas, 2017). Moreover, a narrative is a powerful tool not just from entertainment perspective but is one of the core component of human memory, knowledge and intelligence (Schank and Morson, 1995). Narrative"
2020.nuse-1.11,C16-1125,0,0.028027,"and guidelines for annotation of event arguments, (ii) we propose an approach to identify event predicates and their arguments, (iii) MSC based visu88 found. As pronouns are a closed set of words, we use a manually prepared list of pronouns and corresponding types to identify all pronoun mentions of the actors. It is ensured that the list does not include any demonstrative pronouns (such as yh, vh, un, us). Each pronoun in the text is checked against the list and marked as an actor, if found. characteristics and are used as a preferred method for introducing new predicates into the language (Vaidya et al., 2016). A state-of-the-art approach (Chen et al., 2015) propose a supervised approach to identify LVCs in English using resources like PropBank, the OntoNotes sense groupings, WordNet and the British National Corpus. However, it is difficult to extend this approach for Hindi as it would require extensive efforts to create labelled data based on such resources for Hindi. III. Annotation of nominal events: An event is a nominal event if it is described by a noun. Annotation of nominal events is a challenging task, as eventiveness of a noun depends on the context in which it is used. In this paper, we"
2020.nuse-1.11,M95-1005,0,0.0648782,"e facets like actor identification, actor coreference resolution, event extraction and event argument finding. To assess each of these facets, we carry out evaluation of the proposed approach at multiple levels. As the first level, we check the performance of actor identification and coreference resolution. If an actor predicted by the approach is present in the gold standard, it is marked as a true positive. False positives and false negatives are computed accordingly. We report the F1 scores for actor mention identification for each dataset in Table 3. At this level, we also report the MUC (Vilain et al., 1995), the B 3 (Bagga and Baldwin, 1998) and the Experimentation Details Datasets We carry out our experiments on the four text narratives from Indian History, contributed by (Ramrakhiyani et al., 2018). We obtain the dataset text and gold actor annotations and we carry out the event annotations for these datasets with the help of three annotators. The statistics about the datasets are described in Table 2. 2 https://github.com/tensorflow/models/ tree/master/research/syntaxnet 92 nsubj dobj mhA(mA gADF n  srdAr mahatma gandhi ne sardar |{z } sender pV l patel |{z } receiver ko in EryAsto k  bA"
2020.sdp-1.27,D18-2029,0,0.0151285,"ext-spans using citation context, we have used an unsupervised approach where we have extracted the top 5 sentences by calculating cosine similarity between each citance and sentences of the RP. These 5 sentences are considered as cited/reference text spans. Note that before calculating the similarity, we have converted the text-space into a (numeric) vector-space for which we have utilized different types of sentence embeddings namely, Albert (Beltagy et al., 2019a), ELMO (Peters et al., 2018), fastText (Athiwaratkun et al., 2018), SciBERT (Beltagy et al., 2019a), Universal Sentence Encoder (Cer et al., 2018), XLNET (Yang et al., 2019), which are capable of capturing the semantics of the sentences. Thus, in total, six systems are developed for Task 1(A). 2.2.2 Section Aim Method Hypothesis Implication Results Total Dataset Description The dataset associated with CL-SciSumm 2020 shared task, consists of 40 annotated scientific articles and their citations for training. In addition to this, a corpus of 1000 documents released as a part of ScicummNet (Yasunaga et al., 2019) dataset for scientific document summarization is readily available for training. For testing, a blind test set of 20 articles us"
2020.sdp-1.27,K16-1028,0,0.0710665,"Missing"
2020.sdp-1.27,N18-1202,0,0.00791843,"ng flowchart is shown in Figure 1. 2.2.1 Task 1(A) For a given reference paper (RP), in order to identify the reference text-spans using citation context, we have used an unsupervised approach where we have extracted the top 5 sentences by calculating cosine similarity between each citance and sentences of the RP. These 5 sentences are considered as cited/reference text spans. Note that before calculating the similarity, we have converted the text-space into a (numeric) vector-space for which we have utilized different types of sentence embeddings namely, Albert (Beltagy et al., 2019a), ELMO (Peters et al., 2018), fastText (Athiwaratkun et al., 2018), SciBERT (Beltagy et al., 2019a), Universal Sentence Encoder (Cer et al., 2018), XLNET (Yang et al., 2019), which are capable of capturing the semantics of the sentences. Thus, in total, six systems are developed for Task 1(A). 2.2.2 Section Aim Method Hypothesis Implication Results Total Dataset Description The dataset associated with CL-SciSumm 2020 shared task, consists of 40 annotated scientific articles and their citations for training. In addition to this, a corpus of 1000 documents released as a part of ScicummNet (Yasunaga et al., 2019) dataset fo"
2020.sdp-1.27,D19-1628,0,0.0300865,"Missing"
2020.sdp-1.27,D14-1181,0,0.0256688,"using MMR and it’s variant for CL-LaySumm 2020. Here, R in second row stands for ‘ROUGE’. λ1 0.25 0.50 0.75 1.00 0.75 0.75 λ2 0.0 0.0 0.0 0.0 0.1 0.2 ROUGE 1-F 0.3876 0.3940 0.4009 0.3971 0.4031 0.4048 4.2 Table 6: Study of parameters used in M M R1 and M M R2 for Lay Summary generation. Here, we have used only ABSTRACT for generating summary. Variant Data CW R + M M R1 CW R + M M R2 ABS ABS R-1 0.3986 0.4033 F1 Scores R-2 R-L 0.1586 0.2187 0.1614 0.2209 Methodology To solve the LongSumm in an extactive way, we have utilized the neural network based approach, i.e., convolution neural network (Kim, 2014). The sentences which are part of the summary are assigned 1 and remaining sentences are assigned 0. In other words, we have posed this task as a binary classification problem where task is to identify whether the given sentence can be a part of the summary or not. Positional embedding is also used along with sentence embedding. The detailed methodology used in our CNN is described below: Table 7: Results attained by applying CWR on the generated summary using abstract (ABS). Here, R in second row stands for ‘ROUGE’. etc., are trivial for paper (S016882782030009X) but are not present in wordne"
2020.sdp-1.27,P19-1204,0,0.0966187,"tabases like wordnet (Miller, 1995) can be important and trivial in the context of the paper. For example words like ”hepatocellular”, ”carcinoma” 246 Variant Data M M R1 M M R2 M M R1 M M R2 M M R1 M M R2 ABS ABS ABS+CON ABS+CON FULL FULL R-1 0.4009 0.4048 0.3837 0.3855 0.2835 0.2875 F1 Scores R-2 0.1679 0.1690 0.1411 0.1394 0.0604 0.0628 4.1 R-L 0.2239 0.2244 0.2050 0.2055 0.1609 0.1592 The training corpus for this task includes 1705 extractive summaries, and 531 abstractive summaries of NLP/ML scientific papers. The extractive summaries are based on video talks from associated conferences (Lev et al., 2019), while the abstractive summaries are from blog posts created by NLP and ML researchers. The test set consists of 22 research papers for both extactive and abstractive summarization and task is to generate a summary of 600 words. In the current paper, we have focused only on the extractive summarization of LongSumm. Table 5: Results attained using MMR and it’s variant for CL-LaySumm 2020. Here, R in second row stands for ‘ROUGE’. λ1 0.25 0.50 0.75 1.00 0.75 0.75 λ2 0.0 0.0 0.0 0.0 0.1 0.2 ROUGE 1-F 0.3876 0.3940 0.4009 0.3971 0.4031 0.4048 4.2 Table 6: Study of parameters used in M M R1 and M"
2020.sdp-1.30,N15-1110,0,0.0277913,"long summarization problem, we have an unsupervised technique similar to CL-SciSumm. To solve the abstractive LongSumm problem, we have used the encoder-decoder based generative model. • Objective of CL-LaySumm is to generate a lay summary that can be understood by a non-technical reader. The generated summary should not contain any technical words or jargon. We have solved this task using the abstractive summarization technique. Here, Fine-tuned BERT based encoder-decoder architecture is used to solve the problem. Introduction Massive amounts of scientific articles are published day by day (Cohan et al., 2015; Cohan and Goharian, 2017, 2018), which impose a big challenge for researchers in various fields to keep themselves upto-date with the new developments. A bibliometric analyst’s study shows that after nine years, the number of published articles will be doubled (Bornmann and Mutz, 2015). The scientific document summarization objective is to provide a summary of the reference paper. This summary should contain all the important facts. Therefore, it reduces the human effort to understand the document. Challenges of each style of the summary are as follows: • Objective of CL-SciSumm is to genera"
2020.sdp-1.30,D18-1443,0,0.0198746,"es. 4 −1.5 ) (2) lrD = ˜lrD .min(step0.5 , step.warmupD where lr = 2e− 3 and warm-up = 20000 for the encoder whereas lrD = 0.1 and warm-up =10000 for decoder. Here the assumption is that the pretrained encoder must be trained with a lower learning rate and a lower learning rate smoothens the decay. This process helps the encoder in training with a better gradient when the decoder is in stable condition. We have used a two-stage fine-tuning approach, first is fine-tuning for extractive summarization and then for abstractive summarization. It has been shown in the literature (Li et al., 2018) (Gehrmann et al., 2018) extractive object helps in obtaining a better abstractive summary. 3.3 Result Our system has the following score (shown in Table 5). LongSumm 2020 In all the previous works of scientific summarization (Cohan and Goharian, 2017, 2018), there is a summary length constraint of a maximum of 250 words. But in the current LongSumm shared task, the generated summary can be the length of between 100-1500 words. 4.1 Dataset This dataset consists of a training set of 1705 papers associated with extractive summaries and 531 papers associated with abstractive summaries. It has a blind test set of 22 file"
2020.sdp-1.30,D18-1205,0,0.0197849,"values are f1 scores. 4 −1.5 ) (2) lrD = ˜lrD .min(step0.5 , step.warmupD where lr = 2e− 3 and warm-up = 20000 for the encoder whereas lrD = 0.1 and warm-up =10000 for decoder. Here the assumption is that the pretrained encoder must be trained with a lower learning rate and a lower learning rate smoothens the decay. This process helps the encoder in training with a better gradient when the decoder is in stable condition. We have used a two-stage fine-tuning approach, first is fine-tuning for extractive summarization and then for abstractive summarization. It has been shown in the literature (Li et al., 2018) (Gehrmann et al., 2018) extractive object helps in obtaining a better abstractive summary. 3.3 Result Our system has the following score (shown in Table 5). LongSumm 2020 In all the previous works of scientific summarization (Cohan and Goharian, 2017, 2018), there is a summary length constraint of a maximum of 250 words. But in the current LongSumm shared task, the generated summary can be the length of between 100-1500 words. 4.1 Dataset This dataset consists of a training set of 1705 papers associated with extractive summaries and 531 papers associated with abstractive summaries. It has a b"
2020.semeval-1.214,N19-1423,0,0.0183621,"n visual media. Each word of the input sequence is represented with word embedding. Part-of-speech (POS) and sentence embedding are added as additional information. Two bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layers are used to capture the sequence information. The last layer of the model is fully-connected and assigns probability using the hidden state of LSTM. Figure 2 shows the overall architecture of the DL-BiLSTM model. We have used different sets of embedding to train the models. WordBERT and SentBERT represent word, and sentence embedding generated using pre-trained BERT (Devlin et al., 2019), respectively. POSEmbd represents one-hot-encoded POS tag. In Model-4, Model-5 and Model-6, we use ELMo (Peters et al., 2018) as word embedding. Below is the list of models and used embeddings: Model-1: DL-BiLSTM + WordBERT Model-2: DL-BiLSTM + WordBERT + POSEmbd Model-3: DL-BiLSTM + WordBERT + POSEmbd + SentBERT Model-4: DL-BiLSTM + ELMo Model-5: DL-BiLSTM + ELMo + POSEmbd Model-6: DL-BiLSTM + ELMo + POSEmbd + SentBERT 4.1 Baseline Models Here, we discuss the baseline models and their implementation. SL-BiLSTM: This model has same architecture as DL-BiLSTM, but the distribution is mapped to"
2020.semeval-1.214,C10-2042,0,0.0259011,"n in visual media. • We show a comparison of our results with baselines and state-of-the-art models and also give a qualitative comparison of different methods. 2 Related Work In text data, the majority of works focus on important keyword identification from long texts. There are mainly two methods for keyword extraction: supervised and unsupervised. Supervised methods generally treat the keyword identification as a classification problem and classify words as either keyword or not (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Unsupervised methods usually utilize TF-IDF (Hasan and Ng, 2010) scores or clustering methods (Liu et al., 2009) for keyword identification. Recently, (Zhang et al., 2016) also proposed a model using RNNs for keyword identification. Different methods are proposed for the emphasis selection in audio. Most works use acoustic features like loudness, pitch to detect emphasized words in audio data (Kochanski et al., 2005; Wang and Narayanan, 2007). Recently, some works are proposed to predict word emphasis in text to improve text-to-speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). (Sun, 2002) proposed ensemble-based models for emphasis selection"
2020.semeval-1.214,D09-1027,0,0.0217973,"esults with baselines and state-of-the-art models and also give a qualitative comparison of different methods. 2 Related Work In text data, the majority of works focus on important keyword identification from long texts. There are mainly two methods for keyword extraction: supervised and unsupervised. Supervised methods generally treat the keyword identification as a classification problem and classify words as either keyword or not (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Unsupervised methods usually utilize TF-IDF (Hasan and Ng, 2010) scores or clustering methods (Liu et al., 2009) for keyword identification. Recently, (Zhang et al., 2016) also proposed a model using RNNs for keyword identification. Different methods are proposed for the emphasis selection in audio. Most works use acoustic features like loudness, pitch to detect emphasized words in audio data (Kochanski et al., 2005; Wang and Narayanan, 2007). Recently, some works are proposed to predict word emphasis in text to improve text-to-speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). (Sun, 2002) proposed ensemble-based models for emphasis selection in audio. (Shirani et al., 2019) proposed the u"
2020.semeval-1.214,Y14-1022,0,0.234417,"g et al., 2004; Medelyan and Witten, 2006). Unsupervised methods usually utilize TF-IDF (Hasan and Ng, 2010) scores or clustering methods (Liu et al., 2009) for keyword identification. Recently, (Zhang et al., 2016) also proposed a model using RNNs for keyword identification. Different methods are proposed for the emphasis selection in audio. Most works use acoustic features like loudness, pitch to detect emphasized words in audio data (Kochanski et al., 2005; Wang and Narayanan, 2007). Recently, some works are proposed to predict word emphasis in text to improve text-to-speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). (Sun, 2002) proposed ensemble-based models for emphasis selection in audio. (Shirani et al., 2019) proposed the use of label distribution learning (LDL) for emphasis selection in short texts in visual media. Here, we show that the use of ensemble models trained with different embeddings performs better than base models. 3 3.1 Approach Problem Definition Given a sentence S with tokens C = {w1 , w2 , ..., wn }, where 1 &lt; |S |&lt; n, emphasis selection is the task to find candidate words in C to emphasize on for conveying the meaning of message in an effective manner. 3.2 Label"
2020.semeval-1.214,N18-1202,0,0.055472,"g are added as additional information. Two bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layers are used to capture the sequence information. The last layer of the model is fully-connected and assigns probability using the hidden state of LSTM. Figure 2 shows the overall architecture of the DL-BiLSTM model. We have used different sets of embedding to train the models. WordBERT and SentBERT represent word, and sentence embedding generated using pre-trained BERT (Devlin et al., 2019), respectively. POSEmbd represents one-hot-encoded POS tag. In Model-4, Model-5 and Model-6, we use ELMo (Peters et al., 2018) as word embedding. Below is the list of models and used embeddings: Model-1: DL-BiLSTM + WordBERT Model-2: DL-BiLSTM + WordBERT + POSEmbd Model-3: DL-BiLSTM + WordBERT + POSEmbd + SentBERT Model-4: DL-BiLSTM + ELMo Model-5: DL-BiLSTM + ELMo + POSEmbd Model-6: DL-BiLSTM + ELMo + POSEmbd + SentBERT 4.1 Baseline Models Here, we discuss the baseline models and their implementation. SL-BiLSTM: This model has same architecture as DL-BiLSTM, but the distribution is mapped to binary labels. Also, for training the SL-BiLSTM model (Shirani et al., 2019), we use negative log-likelihood loss in place of"
2020.semeval-1.214,P19-1112,0,0.681068,"lustering methods (Liu et al., 2009) for keyword identification. Recently, (Zhang et al., 2016) also proposed a model using RNNs for keyword identification. Different methods are proposed for the emphasis selection in audio. Most works use acoustic features like loudness, pitch to detect emphasized words in audio data (Kochanski et al., 2005; Wang and Narayanan, 2007). Recently, some works are proposed to predict word emphasis in text to improve text-to-speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). (Sun, 2002) proposed ensemble-based models for emphasis selection in audio. (Shirani et al., 2019) proposed the use of label distribution learning (LDL) for emphasis selection in short texts in visual media. Here, we show that the use of ensemble models trained with different embeddings performs better than base models. 3 3.1 Approach Problem Definition Given a sentence S with tokens C = {w1 , w2 , ..., wn }, where 1 &lt; |S |&lt; n, emphasis selection is the task to find candidate words in C to emphasize on for conveying the meaning of message in an effective manner. 3.2 Label distribution learning (LDL) We use ”IO” scheme for labels, where ”I” represents emphasis and ”O” represents non-emphasi"
2020.semeval-1.214,2020.semeval-1.184,0,0.0562082,"Missing"
2020.semeval-1.214,D16-1080,0,0.298282,"o give a qualitative comparison of different methods. 2 Related Work In text data, the majority of works focus on important keyword identification from long texts. There are mainly two methods for keyword extraction: supervised and unsupervised. Supervised methods generally treat the keyword identification as a classification problem and classify words as either keyword or not (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Unsupervised methods usually utilize TF-IDF (Hasan and Ng, 2010) scores or clustering methods (Liu et al., 2009) for keyword identification. Recently, (Zhang et al., 2016) also proposed a model using RNNs for keyword identification. Different methods are proposed for the emphasis selection in audio. Most works use acoustic features like loudness, pitch to detect emphasized words in audio data (Kochanski et al., 2005; Wang and Narayanan, 2007). Recently, some works are proposed to predict word emphasis in text to improve text-to-speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). (Sun, 2002) proposed ensemble-based models for emphasis selection in audio. (Shirani et al., 2019) proposed the use of label distribution learning (LDL) for emphasis select"
2020.semeval-1.261,S17-2126,0,0.0409537,"Missing"
2020.semeval-1.261,W17-3007,0,0.0206339,"Missing"
2020.semeval-1.261,W17-3013,0,0.0543949,"Missing"
2020.semeval-1.261,S19-2081,0,0.0284817,"mination of the same to curtail its effect on social media. One promising solution is to develop automated tools for identification of offensive language using various Natural Language Processing (NLP) and Machine Learning (ML) techniques. In the past few years, many shared tasks and seminars were introduced to address this problem and provide relevant annotated data collected from various social media. Some of the related tasks are: ALW1 (related to Abusive Language Identification), TRAC 1 2018 2 related to Aggression Identification (Kumar et al., 2018), GermEval Task 2 3 , HatEval 2019 4 (i Orts, 2019), HASOC 2019 5 and OffensEval 2019 Task 6 6 related to Offensive Language Identification (Zampieri et al., 2019b). Recent works have considered categorizing hate speech problem into sub-classes like abusive, aggressive, or offensive speech. Such categorization of social media posts help law-enforcement agencies with the surveillance of social media. 1.1 Problem definition Similar to OffensEval-2019 shared task, the OffensEval-2020 shared task-organized at SemEval-2020 involves detection of Offensive Language and Target Identification from Tweets in English. OffensEval2020 (Zampieri et al., 202"
2020.semeval-1.261,W18-4401,0,0.0166528,"ene posts, comments, images, etc. and prevent further dissemination of the same to curtail its effect on social media. One promising solution is to develop automated tools for identification of offensive language using various Natural Language Processing (NLP) and Machine Learning (ML) techniques. In the past few years, many shared tasks and seminars were introduced to address this problem and provide relevant annotated data collected from various social media. Some of the related tasks are: ALW1 (related to Abusive Language Identification), TRAC 1 2018 2 related to Aggression Identification (Kumar et al., 2018), GermEval Task 2 3 , HatEval 2019 4 (i Orts, 2019), HASOC 2019 5 and OffensEval 2019 Task 6 6 related to Offensive Language Identification (Zampieri et al., 2019b). Recent works have considered categorizing hate speech problem into sub-classes like abusive, aggressive, or offensive speech. Such categorization of social media posts help law-enforcement agencies with the surveillance of social media. 1.1 Problem definition Similar to OffensEval-2019 shared task, the OffensEval-2020 shared task-organized at SemEval-2020 involves detection of Offensive Language and Target Identification from Twee"
2020.semeval-1.261,malmasi-zampieri-2017-detecting,0,0.0271939,"Missing"
2020.semeval-1.261,W17-3008,0,0.0118492,"f hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentences in Chinese. The GermEval shared task (Schmidt and Wiegand, 2017) was introduced to facilitate research on the offensive language identification in microposts in the German language. 3 Data 3.1 Data Des"
2020.semeval-1.261,D17-1117,0,0.0156697,"et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier for detection of hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentenc"
2020.semeval-1.261,D14-1162,0,0.0837174,"shown under column Undersampled. 3.3 Preprocessing We perform a series of preprocessing of the tweets in the dataset before using them to build our models. We replace smileys (like : −) or :) is replaced by the word Happy) and emojis7 by their meaning. We use 7 https://pypi.org/project/emoji/ 1985 a dictionary of popularly used contractions8 to replace them with their elongated forms. We also perform basic pre-processing steps like URLs and hashtags removal, extra blank spaces removal, conversion to lower case, the omission of non-ASCII characters etc. 4 Methodology We use pre-trained GloVe9 (Pennington et al., 2014) word embeddings to initialize our embedding layer and further fine-tune it on our training data while learning. The output from the embedding layer is passed through a BiGRU (256 units) layer which encodes the input representation to hidden representation. We leverage the effectiveness of Hierarchical attention (HATT) based Document Classification technique (Yang et al., 2016) to attend upon the instances more precisely. The attended vector is passed through two separate task-specific fully connected layers followed by their respective output layers (2 neurons with softmax activation for the"
2020.semeval-1.261,W17-1101,0,0.0118754,"non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier for detection of hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments"
2020.semeval-1.261,W17-3003,0,0.0138549,"still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentences in Chinese. The GermEval shared task (Schmidt and Wiegand, 2017) was introduced to facilitate research on the offensive language identification in microposts in the German language. 3 Data 3.1 Data Description The OffensEval-2020 dataset (Rosenthal et al., 2020) (Zampieri et al., 2020) follows the similar hierarchical annotation scheme as used in creating Offensive Language Identification Dataset (OLID v1.0) (Zampieri et al., 2019a). The first level of classification is to distinguish between offensive (OFF) and non-offensive"
2020.semeval-1.261,W12-2103,0,0.114814,"Missing"
2020.semeval-1.261,W17-3012,0,0.0121186,"observations and results related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to bui"
2020.semeval-1.261,W16-5618,0,0.0312473,"ults related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier"
2020.semeval-1.261,N12-1084,0,0.0371778,"In Section 2 we briefly discuss the related work in this area. In Section 3, we describe the dataset and various preprocessing steps on the dataset. We discuss the methodology in Section 4. In Section 5, we present results and give a brief analysis. In Section 6, we give our conclusion along with future works. 2 Related Work For quite some time now, researchers have studied and reported their observations and results related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general"
2020.semeval-1.261,N16-1174,0,0.308473,". We also perform basic pre-processing steps like URLs and hashtags removal, extra blank spaces removal, conversion to lower case, the omission of non-ASCII characters etc. 4 Methodology We use pre-trained GloVe9 (Pennington et al., 2014) word embeddings to initialize our embedding layer and further fine-tune it on our training data while learning. The output from the embedding layer is passed through a BiGRU (256 units) layer which encodes the input representation to hidden representation. We leverage the effectiveness of Hierarchical attention (HATT) based Document Classification technique (Yang et al., 2016) to attend upon the instances more precisely. The attended vector is passed through two separate task-specific fully connected layers followed by their respective output layers (2 neurons with softmax activation for the classification task and 1 neuron with sigmoid activation for the regression task). For sub-task C, we train our model on the classification task only so there is a single output layer with 3 neurons (signifying 3 classes) with softmax activation. Rest of the architecture is similar to the models for sub-task A and B with an exception that the output from the attention layer is"
2020.semeval-1.261,N19-1144,0,0.0232996,"Missing"
2020.semeval-1.261,S19-2010,0,0.0360722,"Missing"
2020.sltu-1.49,C18-1139,0,0.0255478,"st the quality of non-contextual word embeddings. The Named Entity Recognition task, collected from (Murthy et al., 2018), and FIRE 2014 workshop for NER, contains NER tagged data for 5 Indian languages, namely Hindi, Tamil, Bengali, Malayalam, and Marathi. We also use a Universal POS (UPOS), as well as an XPOS (language-specific PoS tags) tagged dataset, available from the Universal Dependency (UD) treebank (Nivre et al., 2016), which contains POS tagged data for 4 Indian languages, Hindi, Tamil, Telugu, and Marathi. For the tasks of NER, UPOS tagging, XPOS tagging, we use the Flair library (Akbik et al., 2018), which embeds our pre-trained embeddings as inputs for training the corresponding tagging models. The tagging models provided by Flair are vanilla BiLSTM-CRF sequence labellers. For the task of word analogy dataset, we simply use the vector addition and subtraction operators to check accuracy (i.e., v(France) − v(Paris) + v(Berlin) should be close to v(Germany)). For contextual word embeddings, we collect the statistics provided at the end of the pre-training phase to gauge the quality of the embeddings - perplexity scores for ELMo, masked language model accuracy for BERT, and so on. We repor"
2020.sltu-1.49,P18-1073,0,0.154385,"oblem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, includi"
2020.sltu-1.49,D18-1399,0,0.0872982,"oblem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, includi"
2020.sltu-1.49,P19-1019,0,0.0116312,"s to be extracted in a supervised manner, embeddings can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Al"
2020.sltu-1.49,Q17-1010,0,0.311779,"ntroduced in (Y. Bengio, 2003) when it was realised that learning the joint probability of sequences was not feasible due to the ‘curse of dimensionality’, i.e., at that time, the value added by an additional dimension seemed much smaller than the overhead it added in terms of computational time, and space. Since then, several developments have occurred in this field. Word2Vec (Mikolov et al., 2013a) showed the way to train word vectors. The models introduced by them established new stateof-the-art on tasks such as Word Sense Disambiguation (WSD). GloVE (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) further improved on results shown by Mikolov et al. (2013a), where GloVE used a co-occurrence matrix and FastText utilized the sub-word information to generate word vectors. Sent2Vec (Pagliardini et al., 2017) generates sentence vectors inspired by the same idea. Universal Sentence Embeddings (Cer et al., 2018), on the other hand, creates sentence vectors using two variants: transformers and DANs. Doc2Vec (Le and Mikolov, 2014) computes a feature vector for every document in the corpus. Similarly, Context2vec (Melamud et al., 2016) learns embedding for variable length sentential context for t"
2020.sltu-1.49,bojar-etal-2014-hindencorp,0,0.0478607,"Missing"
2020.sltu-1.49,D14-1067,0,0.0205245,"hine Learning wherein features have at times to be extracted in a supervised manner, embeddings can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is"
2020.sltu-1.49,D18-2029,0,0.030248,"several developments have occurred in this field. Word2Vec (Mikolov et al., 2013a) showed the way to train word vectors. The models introduced by them established new stateof-the-art on tasks such as Word Sense Disambiguation (WSD). GloVE (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) further improved on results shown by Mikolov et al. (2013a), where GloVE used a co-occurrence matrix and FastText utilized the sub-word information to generate word vectors. Sent2Vec (Pagliardini et al., 2017) generates sentence vectors inspired by the same idea. Universal Sentence Embeddings (Cer et al., 2018), on the other hand, creates sentence vectors using two variants: transformers and DANs. Doc2Vec (Le and Mikolov, 2014) computes a feature vector for every document in the corpus. Similarly, Context2vec (Melamud et al., 2016) learns embedding for variable length sentential context for target words. The drawback of earlier models was that the representation for each word was fixed regardless of the context in which it appeared. To alleviate this problem, contextual word embedding models were created. ELMo (Peters et al., 2018) used bidirectional LSTMs to improve on the previous works. Later, BE"
2020.sltu-1.49,P16-1035,0,0.0233854,"gs can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus t"
2020.sltu-1.49,L18-1155,0,0.0172201,"paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, including Wikimedia dumps. (2) We train various embedding models and evaluate them. (3) We release these embedding models and evaluation data in a single repository2 . 2 Source Link 352 Repository Link The roadmap of the paper is as follows: in section 2, we discuss previous work; section 3 discusses the corpora and our evaluation datasets; section 4 briefs on the approaches used for training our models, section 5 discusses the resultant models and their evaluation; section 6 concludes the paper. Haider (2018) release word embeddings for the Urdu language, which is one of the Indian languages we do not cover with this work. To evaluate the quality of embeddings, they were tested on Urdu translations of English similarity datasets. 3. 2. Literature Survey Word embeddings were first introduced in (Y. Bengio, 2003) when it was realised that learning the joint probability of sequences was not feasible due to the ‘curse of dimensionality’, i.e., at that time, the value added by an additional dimension seemed much smaller than the overhead it added in terms of computational time, and space. Since then, s"
2020.sltu-1.49,K17-1034,0,0.0157085,"ut training data, the better the embedding models. Acquiring raw corpora to be used as input training data has been a perennial problem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters"
2020.sltu-1.49,K16-1006,0,0.175393,"ata, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus to train models which is insufficient 1 for Indian languages as Wikipedia itself lacks significant number of articles or text in these languages. Although there is no cap or minimum number of documents/lines which define a usable size of a corpus for training such models, it is general"
2020.sltu-1.49,K19-1011,0,0.0114122,"ing raw corpora to be used as input training data has been a perennial problem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1)"
2020.sltu-1.49,L16-1262,0,0.0471833,"Missing"
2020.sltu-1.49,N18-1049,0,0.0591286,"Missing"
2020.sltu-1.49,D14-1162,0,0.0850276,"Missing"
2020.sltu-1.49,N18-1202,0,0.604389,"Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus to train models which is insufficient 1 for Indian languages as Wikipedia itself lacks significant number of articles or text in these languages. Although there is no cap or minimum number of documents/lines which define a usable size of a corpus for training such"
2021.calcs-1.5,P07-2045,0,0.0109553,"obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenizer (Koehn et al., 2007) and truecaser 4 scripts and Indic-nlp-library 5 to tokenize Hindi. We remove the sentences having length greater that 150 tokens and created synthetic code-mixed corpus on the resulting corpus as described earlier. The statistics of data used in the experiments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/clab/fast_align/ 32 Figure 1: An example of alignment between parallel sentence pair and generate"
2021.calcs-1.5,D18-2012,0,0.0239449,"Model Baseline Synthetic CM Test 2.45 10.09 Table 2: BLEU scores of the Baseline model and Synthetic Code-Mixed model on Development and Test sets. Source Experimental Setup We conduct the experiments on Transformer based Encoder-Decoder NMT architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warmup steps with initial learning rate of 2. We use Sentencepiece (Kudo and Richardson, 2018) with joint vocabulary size of 50K. Models are trained with OpenNMT toolkit 6 (Klein et al., 2017) with batch size of 2048 tokens till convergence and checkpoints are created after every 10,000 steps. All the checkpoints that are created during the training are averaged and considered as the best parameters for each model. During inference, beam size is set to 5. 4 Dev 2.55 11.52 Reference Output Source Reference Output Source Reference Output Who is your favorite member from the first avengers? Tumhara favorite member kaun hai first avengers mein se? first avengers se aapka favorite member ko"
2021.calcs-1.5,W18-3817,0,0.0351862,"n roman script, during the synthetic corpus creation, we transliterate the Hindi tokens from Devanagari script to Roman script. Code-Mixing (CM) is a very common phenomenon in various social media contents, product description and reviews, educational domain etc. For better understanding and ease in writing, users 2 Translation of code-mixed data has gained popularity in recent times. Menacer et al. (2019) conducted experiments on translating Arabic-English CM data to pure Arabic and/or to pure English with Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) approaches. Dhar et al. (2018) proposed an MT augmentation pipeline which takes CM sentence and determines the most dominating language and translates the ∗ 1 Related Works Equal contribution Hindi words are romanized 31 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 31–35 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_005 sides except {yk } and {xj } respectively. The obtained candidate token set is further pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token"
2021.calcs-1.5,L18-1548,1,0.792609,"f a specific language pair. We use the implementation2 of fast_align algorithm (Dyer et al., 2013) to obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenizer (Koehn et al., 2007) and truecaser 4 scripts and Indic-nlp-library 5 to tokenize Hindi. We remove the sentences having length greater that 150 tokens and created synthetic code-mixed corpus on the resulting corpus as described earlier. The statistics of data used in the experiments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/cla"
2021.calcs-1.5,N13-1073,0,0.0487045,"de-mixed corpus only and source (English) is kept as it is. The gold corpus provided by organizers is not modified in any way and also kept as it is. System Description In this section, we describe the synthetic parallel corpus creation, dataset and experimental setup of our system. 3.1 Unsupervised Synthetic Code-Mixed Corpus Creation 3.3 We utilize the existing parallel corpus to create synthetic code-mixed data. First we learn word-level alignments between source and target sentences of a given parallel corpus of a specific language pair. We use the implementation2 of fast_align algorithm (Dyer et al., 2013) to obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenize"
2021.calcs-1.5,D18-1346,0,0.0202374,"ge identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman script, we utilize Python based transliteration tool.3 This convert the Devanagari script to Roman script. We also create another version of the synthetic code-mixed corpus by replacing the two consecutive vowels with single vowel (Belinkov and Bisk, 2018). We call this version of code-mixed corpus as synthet"
2021.calcs-1.5,2020.findings-emnlp.206,1,0.7928,"MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are"
2021.calcs-1.5,P02-1040,0,0.110787,"ments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/clab/fast_align/ 32 Figure 1: An example of alignment between parallel sentence pair and generated CM sentence. In the CM sentence, the source words that are replaced are shown with red border. Corpus Synthetic CM Synthetic CM + User Patterns Gold Total Train 1,549,115 1,549,115 8,060 3,106,290 Dev 942 942 tains 960 sentences. Our model achieved BLEU (Papineni et al., 2002) score of 10.09. Table 2 shows the BLEU scores obtained from the trained models on Development and Test sets. Table 3 shows some sample translations. Table 1: Data statistics used in the experiment. Synthetic CM: Size of synthetic code-mixed data. Synthetic CM + User Patterns: Size of synthetic codemixed data with addition of user writing patterns. Gold: Size of gold standard parallel corpus provided by organizers. Train, Dev denotes Training and Development set statistics respectively. In the experiments we use only gold standard corpus as development set. 3.4 Model Baseline Synthetic CM Test"
2021.calcs-1.5,P18-1143,0,0.0229817,"while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman scr"
2021.calcs-1.5,P17-4012,0,0.0360608,"e-Mixed model on Development and Test sets. Source Experimental Setup We conduct the experiments on Transformer based Encoder-Decoder NMT architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warmup steps with initial learning rate of 2. We use Sentencepiece (Kudo and Richardson, 2018) with joint vocabulary size of 50K. Models are trained with OpenNMT toolkit 6 (Klein et al., 2017) with batch size of 2048 tokens till convergence and checkpoints are created after every 10,000 steps. All the checkpoints that are created during the training are averaged and considered as the best parameters for each model. During inference, beam size is set to 5. 4 Dev 2.55 11.52 Reference Output Source Reference Output Source Reference Output Who is your favorite member from the first avengers? Tumhara favorite member kaun hai first avengers mein se? first avengers se aapka favorite member kon hai? I think it was a robotic shark, but am not sure. me sochta hoon voh robotic shark thi, but"
2021.calcs-1.5,N19-1044,0,0.0173279,"r pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token xj is replaced with aligned target token yk . The generated code-mixed sentence is in the form: CM = {x1 , x2 , ..., yk , yl , ..., xm }. Figure 1 shows an example of English-Hindi code-mixed sentence generated through this method. remaining words into that language. The resulting sentence will be in one single language and can be translated to other language with the existing MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input"
2021.calcs-1.5,K19-1026,0,0.0175694,"hata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman script, we utilize Python based transliteration tool.3 This convert the Devanagari script to Roman script. We also create another version of the synthetic code-mixed corpus by replacing the two consecutive vowe"
2021.calcs-1.5,2020.emnlp-main.208,0,0.0291341,"52-056-4_005 sides except {yk } and {xj } respectively. The obtained candidate token set is further pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token xj is replaced with aligned target token yk . The generated code-mixed sentence is in the form: CM = {x1 , x2 , ..., yk , yl , ..., xm }. Figure 1 shows an example of English-Hindi code-mixed sentence generated through this method. remaining words into that language. The resulting sentence will be in one single language and can be translated to other language with the existing MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an"
2021.eacl-main.299,N01-1016,0,0.233466,"eech. They include filler pauses such as uh and um, word repetitions, irregular elongations, discourse markers, conjunctions, and restarts. For example, the disfluent sentence “well we’re actually uh we’re getting ready” has its fluent form as, “we’re getting ready”. Here, the words highlighted in green, blue and red refer to discourse, filler and restart disfluencies, respectively. Disfluencies in the text can alter its syntactic and semantic structure, thereby adversely affecting the performance of downstream NLP tasks such as information extraction, summarization, translation, and parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004). These tasks also employ pretrained language models that are typically trained ∗ Joint first authors • We cast the problem of disfluency correction as one of translation from disfluent to fluent text and we propose an unsupervised transformer-based encoder-decoder model for disfluency correction. • We compare and contrast an unsupervised and semi-supervised approach for disfluency correction, where the latter has access to a small amount of parallel text. We also implement fully-supervised methods as a skyline and show how our models come very close in performance"
2021.eacl-main.299,2013.iwslt-papers.12,0,0.13885,"Missing"
2021.eacl-main.299,W10-4343,0,0.0281696,"' Pred 1' 512 512 Shared Transduction Parameters Disﬂuent Domain (a) Style-transfer BOS DE Pred 1 DE 1024 512 1024 512 (b) Use of domain embedding Figure 1: Illustration of (a) Style transfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014"
2021.eacl-main.299,D14-1181,0,0.00245652,"f input sentences. The sum of token-level cross-entropy losses between the input and the reconstructed output serves as the reconstruction loss. Borrowing from prior work on unsupervised style transfer model (He et al., 2020), the decoder is conditioned on a domain embedding that specifies the direction of translation. In this work, we employ two types of embeddings: A vanilla binary domain embedding that takes a bit as input to indicate whether the input text is fluent or disfluent and a classifier-based domain embedding. The latter is obtained from a trained standalone CNN-based classifier (Kim, 2014) that predicts the disfluency type of a disfluent input sentence. (Here, we as3422 sume that disfluency type labels are available for the disfluent sentences in our training data.) The classifier’s penultimate layer acts as our classifier embedding, which is further used to condition the decoder. We hypothesize that additional information about disfluency types via the classifier-based embedding might help guide the process of disfluency correction better. Furthermore, similar to the noise models adopted by (He et al., 2020; Lample et al., 2017), a randomly sampled noisy version of every sente"
2021.eacl-main.299,2020.acl-main.346,0,0.0141018,"e of domain embedding Figure 1: Illustration of (a) Style transfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation systems, several works translate fluent utterances from disfluent spe"
2021.eacl-main.299,2020.findings-emnlp.186,0,0.0367492,"e of domain embedding Figure 1: Illustration of (a) Style transfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation systems, several works translate fluent utterances from disfluent spe"
2021.eacl-main.299,2020.bea-1.16,0,0.0324131,"ransfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation systems, several works translate fluent utterances from disfluent speech (Salesky et al., 2018; Ansari et al., 2020; Fukuda et al.,"
2021.eacl-main.299,2020.iwslt-1.22,1,0.772397,"luency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation systems, several works translate fluent utterances from disfluent speech (Salesky et al., 2018; Ansari et al., 2020; Fukuda et al., 2020) or disfluent text (Cho et al., 2013; Saini et al., 2020; Cho et al., 2016). Most of these approaches work in a supervised setting or mitigate the lack of parallel disfluent-fluent text via data augmentation, model design, incorporating domain knowledge of the language, or using multi-lingual NMT. (Salesky et al., 2019) proposes a system for conversational speech translation with the joint removal of disfluencies. 3 Our Approach We draw inspiration from unsupervised neural machine translation models (Lample et al., 2017) and style transfer models (He et al., 2020) to design the disfluency correction model illustrated in Figure 1a. It consists of a"
2021.eacl-main.299,N19-1285,0,0.0181862,"l., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation systems, several works translate fluent utterances from disfluent speech (Salesky et al., 2018; Ansari et al., 2020; Fukuda et al., 2020) or disfluent text (Cho et al., 2013; Saini et al., 2020; Cho et al., 2016). Most of these approaches work in a supervised setting or mitigate the lack of parallel disfluent-fluent text via data augmentation, model design, incorporating domain knowledge of the language, or using multi-lingual NMT. (Salesky et al., 2019) proposes a system for conversational speech translation with the joint removal of disfluencies. 3 Our Approach We draw inspiration from unsupervised neural machine translation models (Lample et al., 2017) and style transfer models (He et al., 2020) to design the disfluency correction model illustrated in Figure 1a. It consists of a single encoder and a single decoder, used to translate in both directions, i.e., from disfluent to fluent text and vice-versa. The decoder is additionally conditioned using a domain embedding to convey the direction of translation, signifying whether the input to t"
2021.eacl-main.299,C18-1299,0,0.0148093,"n (a) Style-transfer BOS DE Pred 1 DE 1024 512 1024 512 (b) Use of domain embedding Figure 1: Illustration of (a) Style transfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 2005; Hassan et al., 2014). With the popularity of end-to-end spoken translation sys"
2021.eacl-main.299,C16-1027,0,0.0174876,"uent Emb Latent BOS' Pred 1' 512 512 Shared Transduction Parameters Disﬂuent Domain (a) Style-transfer BOS DE Pred 1 DE 1024 512 1024 512 (b) Use of domain embedding Figure 1: Illustration of (a) Style transfer model modified to use type embedding drawn from a pretrained CNN classifier. (b) Conditioning on domain embeddings in the transformers’ decoder. Pred(i) and Input(i) are the decoder’s prediction and input to the decoder at the ith time-step respectively. 2 Related work Current literature has primarily focused on disfluency detection in both speech and text in fully supervised settings (Wang et al., 2016; Georgila et al., 2010; Zayats et al., 2014; Tran et al., 2019; Wang et al., 2018; Bach and Huang, 2019; Zayats et al., 2016; Lou and Johnson, 2020a). The grammatical error correction (Omelianchuk et al., 2020) approach does not perform well on the disfluency correction tasks. In most cases, simply removing disfluencies from an utterance can render the sentence ill-formed. More meaningful and syntactically well-formed utterances are generated by performing automatic disfluency removal from speech (Kaushik et al., 2010; Lou and Johnson, 2020b) and text (Wang et al., 2010; Honal and Schultz, 20"
2021.emnlp-main.675,N16-4006,1,0.802232,"to create the multilingual task training set. Then, the base LM is fine-tuned on the multilingual task training corpus. This multilingual fine-tuning now yields a model per task, and not per task-language pair. 3.2 Script Similarity and Transliteration Languages of a language family often use similar writing systems. For example, in the IA family, on one hand, Hindi, Bhojpuri, Magahi, Marathi, Sanskrit, and Nepali are written in Devanagari script. On the other hand, Bengali, Gujarati, Punjabi, and Oriya are written in their respective scripts. As Indic languages have high lexical similarity (Bhattacharyya et al., 2016), having a universal script for all these languages allows for model to exploit cross-lingual similarities. For example, the verb term for “to go” is similar in Hindi (जाना jaanaa), Urdu ( جاناjaanaa), Gujarati (જવું javum), Punjabi (ਜਾਣਾ jaanaa), Marathi (जाने jaane), and Oriya (ଯିବାକୁ jibaku), and Bengali (যাওয়া jao)with each language morphing it in different manners. We use indic-nlp-library (Kunchukuttan et al., 2015) (for all but Urdu) and indic-trans tools (Bhat et al., 2015) (for Urdu) for transliteration to Devanagari. Traditionally, a pre-trained LM (such as mBERT) is used as base m"
2021.emnlp-main.675,2020.acl-main.747,0,0.522681,"rain two language models for Indo-Aryan language family from scratch. We utilize various tasks of IndicGLUE (Kakwani et al., 2020b) as our test-beds. 2 Related Work Multilinguality aspect has been explored in context of pre-training language models, for effective transfer from one language to other, and in multilingual fine-tuning, to an extent. 2.1 Multilingual Pre-training Multilingual LMs have enabled effective task fine-tuning across various languages. Notable examples include, multilingual BERT (mBERT)4 model trained with 104 languages, and XLM (Lample and Conneau, 2019) and XLM-RoBERTa (Conneau et al., 2020a) trained with 100 languages. In the context of Indic languages, three recent works on multilingual LMs have been IndicBERT, MuRIL, and Indic-Transformers language models. IndicBERT (Kakwani et al., 2020b) focuses on languages belonging to the Indo-Aryan and Dravidian language families along with English. Along with 17 Indic languages and English, Multilingual Representations for Indian Languages (MuRIL)5 (Khanuja et al., 2021), utilizes EnglishIndic languages parallel corpus and the Roman transliterated counterparts to train an mBERT model. Similarly, Indic-Transformers (Jain et al., 2020) p"
2021.emnlp-main.675,2020.acl-main.536,0,0.33897,"rain two language models for Indo-Aryan language family from scratch. We utilize various tasks of IndicGLUE (Kakwani et al., 2020b) as our test-beds. 2 Related Work Multilinguality aspect has been explored in context of pre-training language models, for effective transfer from one language to other, and in multilingual fine-tuning, to an extent. 2.1 Multilingual Pre-training Multilingual LMs have enabled effective task fine-tuning across various languages. Notable examples include, multilingual BERT (mBERT)4 model trained with 104 languages, and XLM (Lample and Conneau, 2019) and XLM-RoBERTa (Conneau et al., 2020a) trained with 100 languages. In the context of Indic languages, three recent works on multilingual LMs have been IndicBERT, MuRIL, and Indic-Transformers language models. IndicBERT (Kakwani et al., 2020b) focuses on languages belonging to the Indo-Aryan and Dravidian language families along with English. Along with 17 Indic languages and English, Multilingual Representations for Indian Languages (MuRIL)5 (Khanuja et al., 2021), utilizes EnglishIndic languages parallel corpus and the Roman transliterated counterparts to train an mBERT model. Similarly, Indic-Transformers (Jain et al., 2020) p"
2021.emnlp-main.675,N19-1423,0,0.0154023,"ily. Therefore, we include the following components to the approach: (1) multilingual For this study, we use the mBERT, IndicBERT, fine-tuning, (2) transliteration, and (3) language and MuRIL as existing pre-trained language modmodels. Next, we discuss these in detail. els. Additionally, we pre-train language models 8586 (from scratch) specifically for only the Indo-Aryan languages, as other LMs contains languages of other families too. Pre-training Language Model From Scratch: We choose to pre-train RoBERTa (Liu et al., 2019) transformer-based model as it has been shown to improve over BERT (Devlin et al., 2019) recently. Existing pre-trained language models are trained on original script data. For fair study of effectiveness of transliteration, we wish to pre-train separate language models on original and transliterated corpus from scratch. Our experimentation around transliteration makes existing pre-trained models (mBERT, IndicBERT, and MuRIL) somewhat incompatible. In other words, it would be akin to finetuning for an unseen language, albeit in a previously seen script. Thus, we settle upon pre-training contextual LM from scratch for this purpose. Specifically, we train two LMs from scratch, one"
2021.emnlp-main.675,2020.findings-emnlp.445,0,0.353441,"the gradation of multilinguality by incrementally adding in new languages added one-by-one building up to an all-language multilingual fine-tuning. A good approximation for language relatedness is their membership to the same language family as languages of a family often share properties such as grammar, vocabulary, etymology, and writing systems. We choose the Indo-Aryan (IA) family for the study, since constituent languages 1) include low-resource languages, 2) have similar Abugida writing system, 3) are relatively understudied, and 4) are covered in a well-defined NLP benchmark IndicGLUE (Kakwani et al., 2020b). Further, the fact that all constituent languages except one use similar Abugida writing systems (rooted in the ancient Brahmi Script3 ) presents an opportunity for script normalization via transliteration. Overall, although the general notion of language relatedness is explored, and the multilingual fine-tuning is explored in literature, the detailed 2 Fine-tuning a pre-trained model with a downstream task’s training data for multiple languages. 3 https://en.wikipedia.org/wiki/Brahmic_ scripts 8584 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, page"
2021.emnlp-main.675,W19-4203,0,0.0282428,"n that language-specific FT serves as skyline, and, in 4 https://github.com/google-research/bert/ blob/master/multilingual.md 5 8585 https://tfhub.dev/google/MuRIL/1 these set of works, pursuit has been to get zero-shot transfer from related languages(s) closer to the skyline (Wu and Dredze, 2019). 2.3 Multilingual Fine-tuning Tsai et al. (2019) perform multilingual fine-tuning of 48 languages for downstream tasks of POS tagging and morphological tagging, and find these multilingual models to be slightly poorer compared to monolingual models. For morphological tagging and lemmatization tasks, Kondratyuk (2019) makes similar observation regarding poor performance for the model fine-tuned with 66 languages in multilingual setting compared to monolingual fine-tuning (although, a second stage of perlanguage fine-tuning yields superior performance). These findings indicate that arbitrary collection of languages may not be suitable for improving downstream task performance; and that, a principled approach for selecting a set of languages may be preferable for multilingual fine-tuning. To this end, we hypothesize that language relatedness should be an important aspect to consider while selecting a languag"
2021.emnlp-main.675,2020.findings-emnlp.83,0,0.0275824,"It is understood that a multilingual model gains cross-lingual understanding from sharing of layers that allows the alignment of representations among languages; to the extent that large overlap of the vocabulary between the languages is not required to bridge the alignment (Conneau et al., 2020b; Wang et al., 2019). This property facilitates, zero-shot transfer between two related languages (e.g. Hindi and Urdu) reasonably well (Pires et al., 2019). Performance for zero-shot transfer further improves when multilingual model is further aligned by utilizing parallel word or sentence resources (Kulshreshtha et al., 2020). Usually, the low-resource language members in a multilingual LM benefit by presence of related languages (Liu et al., 2020). Further, it is likely that presence of unrelated languages do not aid the multilingual training, but rather may lead to negative interference rooted in conflicting gradients (Wang et al., 2020b) or yield substantially poor transfer between unrelated languages (e.g. English and Japanese) (Pires et al., 2019). A recent work by Dolicki and Spanakis (2021) focuses on establishing the connection between the effectiveness of zero-shot transfer and the linguistic feature of s"
2021.emnlp-main.675,2020.sltu-1.49,1,0.736003,"d Indic-Transformers language models. IndicBERT (Kakwani et al., 2020b) focuses on languages belonging to the Indo-Aryan and Dravidian language families along with English. Along with 17 Indic languages and English, Multilingual Representations for Indian Languages (MuRIL)5 (Khanuja et al., 2021), utilizes EnglishIndic languages parallel corpus and the Roman transliterated counterparts to train an mBERT model. Similarly, Indic-Transformers (Jain et al., 2020) presents monolingual LMs for Hindi, Bengali, and Telugu. Recently, various types of word-embeddings are also trained for each language (Kumar et al., 2020; Kakwani et al., 2020b). These approaches focus on multilingual pretraining of models. This means that once a multilingual LM is pre-trained, it is fine-tuned per task separately for each language. 2.2 Language Transfer It is understood that a multilingual model gains cross-lingual understanding from sharing of layers that allows the alignment of representations among languages; to the extent that large overlap of the vocabulary between the languages is not required to bridge the alignment (Conneau et al., 2020b; Wang et al., 2019). This property facilitates, zero-shot transfer between two re"
2021.emnlp-main.675,L18-1548,1,0.793343,"Missing"
2021.emnlp-main.675,N15-3017,1,0.794182,"en in Devanagari script. On the other hand, Bengali, Gujarati, Punjabi, and Oriya are written in their respective scripts. As Indic languages have high lexical similarity (Bhattacharyya et al., 2016), having a universal script for all these languages allows for model to exploit cross-lingual similarities. For example, the verb term for “to go” is similar in Hindi (जाना jaanaa), Urdu ( جاناjaanaa), Gujarati (જવું javum), Punjabi (ਜਾਣਾ jaanaa), Marathi (जाने jaane), and Oriya (ଯିବାକୁ jibaku), and Bengali (যাওয়া jao)with each language morphing it in different manners. We use indic-nlp-library (Kunchukuttan et al., 2015) (for all but Urdu) and indic-trans tools (Bhat et al., 2015) (for Urdu) for transliteration to Devanagari. Traditionally, a pre-trained LM (such as mBERT) is used as base model, which is fine-tuned for a downstream task for a specific language (monolingual). In this work, we aim to evaluate the role of script and language relatedness in multilingual fine-tuning by employing the Indo-Aryan lan3.3 Language Models guage family. Therefore, we include the following components to the approach: (1) multilingual For this study, we use the mBERT, IndicBERT, fine-tuning, (2) transliteration, and (3) la"
2021.emnlp-main.675,2020.acl-main.703,0,0.0168882,"a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental. 1 Introduction1 Transformer-based (Vaswani et al., 2017) language models (LM) have been proven to be extremely ∗ Equal contribution 1 https://github.com/IBM/ indo-aryan-language-family-model useful in variety of natural language processing (NLP) tasks. Some of the most notable models are GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and BART (Lewis et al., 2020). To fine-tune a pre-trained language model for downstream tasks has become a de facto approach in recent literature. We empirically study whether (and to what extent) do the related languages accentuate the performance of models in downstream tasks with multilingual fine-tuning2 in comparison to monolingual fine-tuning. To understand the quantitative advantage of including languages gradually, we explore the gradation of multilinguality by incrementally adding in new languages added one-by-one building up to an all-language multilingual fine-tuning. A good approximation for language relatedne"
2021.emnlp-main.675,2020.tacl-1.47,0,0.0157978,"sentations among languages; to the extent that large overlap of the vocabulary between the languages is not required to bridge the alignment (Conneau et al., 2020b; Wang et al., 2019). This property facilitates, zero-shot transfer between two related languages (e.g. Hindi and Urdu) reasonably well (Pires et al., 2019). Performance for zero-shot transfer further improves when multilingual model is further aligned by utilizing parallel word or sentence resources (Kulshreshtha et al., 2020). Usually, the low-resource language members in a multilingual LM benefit by presence of related languages (Liu et al., 2020). Further, it is likely that presence of unrelated languages do not aid the multilingual training, but rather may lead to negative interference rooted in conflicting gradients (Wang et al., 2020b) or yield substantially poor transfer between unrelated languages (e.g. English and Japanese) (Pires et al., 2019). A recent work by Dolicki and Spanakis (2021) focuses on establishing the connection between the effectiveness of zero-shot transfer and the linguistic feature of source and target languages; interestingly, they observe that the effectiveness of zero-shot transfer is a function of downstr"
2021.emnlp-main.675,2021.ccl-1.108,0,0.019057,"Missing"
2021.emnlp-main.675,P19-1493,0,0.345351,"tilingual pretraining of models. This means that once a multilingual LM is pre-trained, it is fine-tuned per task separately for each language. 2.2 Language Transfer It is understood that a multilingual model gains cross-lingual understanding from sharing of layers that allows the alignment of representations among languages; to the extent that large overlap of the vocabulary between the languages is not required to bridge the alignment (Conneau et al., 2020b; Wang et al., 2019). This property facilitates, zero-shot transfer between two related languages (e.g. Hindi and Urdu) reasonably well (Pires et al., 2019). Performance for zero-shot transfer further improves when multilingual model is further aligned by utilizing parallel word or sentence resources (Kulshreshtha et al., 2020). Usually, the low-resource language members in a multilingual LM benefit by presence of related languages (Liu et al., 2020). Further, it is likely that presence of unrelated languages do not aid the multilingual training, but rather may lead to negative interference rooted in conflicting gradients (Wang et al., 2020b) or yield substantially poor transfer between unrelated languages (e.g. English and Japanese) (Pires et al"
2021.emnlp-main.675,D19-6132,0,0.0148542,"ion of languages may not be suitable for improving downstream task performance; and that, a principled approach for selecting a set of languages may be preferable for multilingual fine-tuning. To this end, we hypothesize that language relatedness should be an important aspect to consider while selecting a language set for multilingual fine-tuning. Pires et al. (2019) briefly explore language set selection based on topological features (syntactic word order). Wang et al. (2020b) explores multilingual fine-tuning in strictly bilingual settings. Taking the language relatedness in considerations, Tran and Bisazza (2019) show that joint fine-tuning with four European language is better than fine-tuning with only English in the specific task of universal dependency parsing. Unfortunately, it doesn’t provide comparison with monolingual fine-tuning of all constituent languages. We observe that there is a void regarding the systematic analysis to understand how a presence of related languages in the multilingual fine-tuning affects the performance on the target language. 3 Methodology 3.1 Multilingual Fine-Tuning As opposed to traditional monolingual fine-tuning for a downstream task, in multilingual fine-tuning"
2021.emnlp-main.675,D19-1374,0,0.0123141,"tiveness of zero-shot transfer and the linguistic feature of source and target languages; interestingly, they observe that the effectiveness of zero-shot transfer is a function of downstream task, in addition to the languages themselves. The general understanding has been that language-specific FT serves as skyline, and, in 4 https://github.com/google-research/bert/ blob/master/multilingual.md 5 8585 https://tfhub.dev/google/MuRIL/1 these set of works, pursuit has been to get zero-shot transfer from related languages(s) closer to the skyline (Wu and Dredze, 2019). 2.3 Multilingual Fine-tuning Tsai et al. (2019) perform multilingual fine-tuning of 48 languages for downstream tasks of POS tagging and morphological tagging, and find these multilingual models to be slightly poorer compared to monolingual models. For morphological tagging and lemmatization tasks, Kondratyuk (2019) makes similar observation regarding poor performance for the model fine-tuned with 66 languages in multilingual setting compared to monolingual fine-tuning (although, a second stage of perlanguage fine-tuning yields superior performance). These findings indicate that arbitrary collection of languages may not be suitable for imp"
2021.emnlp-main.675,2020.emnlp-main.359,0,0.198327,"erty facilitates, zero-shot transfer between two related languages (e.g. Hindi and Urdu) reasonably well (Pires et al., 2019). Performance for zero-shot transfer further improves when multilingual model is further aligned by utilizing parallel word or sentence resources (Kulshreshtha et al., 2020). Usually, the low-resource language members in a multilingual LM benefit by presence of related languages (Liu et al., 2020). Further, it is likely that presence of unrelated languages do not aid the multilingual training, but rather may lead to negative interference rooted in conflicting gradients (Wang et al., 2020b) or yield substantially poor transfer between unrelated languages (e.g. English and Japanese) (Pires et al., 2019). A recent work by Dolicki and Spanakis (2021) focuses on establishing the connection between the effectiveness of zero-shot transfer and the linguistic feature of source and target languages; interestingly, they observe that the effectiveness of zero-shot transfer is a function of downstream task, in addition to the languages themselves. The general understanding has been that language-specific FT serves as skyline, and, in 4 https://github.com/google-research/bert/ blob/master/"
2021.emnlp-main.675,D19-1077,0,0.0477673,"Missing"
2021.emnlp-main.789,L16-1079,0,0.0279456,"scale (Likert, 1932). 2 Related Work Most of the previous work on computational humour has been towards the detection of humour. Smaller joke formats like one-liners which have just a single line of context, have been used (Hetzron, 1991). Language models like BERT are used for generating sentence embeddings, which have been shown to outperform other architectures in humour detection on short texts (Annamoradnejad, 2020). Since humour depends on how the speaker’s voice changes, the audio features, and language features have been used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time a"
2021.emnlp-main.789,W18-3502,0,0.0218029,"guage features have been used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time and money and has been one of the noted issues for creating humour datasets. Yang et al. (2019a,b) used time-aligned user comments for generating automated humour labels for multimodal humour identification tasks and found good agreement with manually annotated data. However, none of the previously existing datasets are created with standup comedy clips. 3 Dataset Acquisition and Pre-processing In this section, we describe the creation of our multi-modal dataset and the manual evaluation performed with"
2021.emnlp-main.789,2021.ccl-1.108,0,0.0320744,"Missing"
2021.emnlp-main.789,D19-1211,0,0.0340777,"Missing"
2021.emnlp-main.789,D14-1162,0,0.0852428,"d as input to separate Bi-LSTM layers followed by separate, Dense layers (Graves, Alex and Fernán- models can process sequences of token length 512; thus, we employ them for the entire transcript of dez, Santiago and Schmidhuber, Jürgen, 2005) as shown in Figure 1. The output from these two path- each ∼ 2 minute clip. We sum the output of the final 4 layers from these models to obtain a clip ways is then concatenated and fed to a classifier that outputs one-hot encoding of the 5-point rating. embedding (Alammar, 2018). As baseline textual features, we use GloVe em4.2 Muting Laughter beddings (Pennington et al., 2014). For obtaining Before extracting audio features, we remove the textual features, we experiment with BERTbase , audience laughter and isolate the speaker’s voice BERTlarge , XLM, DistilBERT, RoBERTabase and from each clip. Retaining the audience laughter RoBERTalarge to generate text embeddings (Demay enable a neural network to utilize it and predict vlin et al., 2018; Lample and Conneau, 2019; Sanh a score without using information from the text et al., 2019; Liu et al., 2019). 10075 4.5 Methodology The audio features and textual features are fed as input to the network for obtaining an outpu"
2021.emnlp-main.789,D19-1372,0,0.0205635,"een used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time and money and has been one of the noted issues for creating humour datasets. Yang et al. (2019a,b) used time-aligned user comments for generating automated humour labels for multimodal humour identification tasks and found good agreement with manually annotated data. However, none of the previously existing datasets are created with standup comedy clips. 3 Dataset Acquisition and Pre-processing In this section, we describe the creation of our multi-modal dataset and the manual evaluation performed with the help of human annota"
2021.findings-acl.256,D18-1178,0,0.0278593,"Missing"
2021.findings-acl.256,P98-1013,0,0.448964,"P ROTEST −−→ I NTENTIONALLY ACT • Using: the child frame presupposes the parent Uses frame, e.g., P ROTEST −−→ TAKING SIDES • Subframe: the child frame is a subevent of Subframe a complex parent event, e.g., T RIAL −−−−−→ V ERDICT Along with each frame relation, FrameNet also consists of relations between FEs of parent-child frames. Following are illustrative examples for FE relations for the above frame relations: • P ROTEST:P ROTESTER ALLY ACT:AGENT . • P ROTEST:P ROTESTER FrameNet RDP (Recoverable Deleted Predicates) https://framenet.icsi.berkeley.edu I NTENTION - Uses −−→ TAK - Subframe (Baker et al., 1998) is a taxonomy based on Fillmore’s theory of Frame Semantics. This theory claims that most words’ meanings can be inferred based on a semantic frame: a conceptual structure that denotes an abstract event, relation, or entity and the involved participants. For example, the concept of questioning involves a person asking a question (S PEAKER), person/people begin questioned A DDRESSEE, the content of the question M ESSAGE, and so on. In FrameNet, such a concept is 2 Is-A −−→ ING SIDES :C OGNIZER • T RIAL :J UDGE −−−−−→ V ERDICT:J UDGE FrameNet2 1 Relations 3.1.2 Mappings FrameNet data provides t"
2021.findings-acl.256,S13-2025,0,0.0468044,"Missing"
2021.findings-acl.256,P19-1568,0,0.0168413,"not be effective. We implement these models in PyTorch (Paszke et al., 2017). We initialize the word embedding layer with Google’s pre-trained embeddings3 and initialize the frame embedding layer with random values, in one case, for baseline, and pre-trained frame embedding, in another case. We use the same architecture to train another model for FE prediction, replace the frame embedding layer with an FE embedding layer, and candidates FEs are the FEs from all candidate frames. We take all FEs as a candidate set if no such mapping is found. 4.2 Frame and Frame Element Embeddings Inspired by Kumar et al. (2019)’s approach for the task of Word Sense Disambiguation (WSD), we propose a similar approach to perform NC interpretation. Our approach uses the definition of entities (along with the relations) to learn entity embeddings and relation embeddings. It uses an encoder (Bi-LSTM) to encode the definition of an entity and uses encoded representation as an embedding of the entity for ConvE. During the training, it also optimizes both: the encoder and ConvE. After the training, the encoding of definitions is taken as entity embeddings. We train ConvE twice to get frame and frame element embeddings separ"
2021.findings-acl.256,P98-1015,0,0.311648,"ed Work A relation between the components of a noun compound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forw"
2021.findings-acl.256,W17-2618,0,0.0118194,"tation is performed by one of the authors and hence does not warrant discussion on the interannotator agreement. However, please allow us to point out that our annotations are still manually performed by a human, which begets the consideration of these annotations to be gold-standard. The author chose the examples from Tratz and Hovy (2010)’s dataset randomly. During the annotation process, we found some difficulties because of the coverage issue of the FrameNet. The wordto-frame mapping in FrameNet has a coverage issue, and it has been widely reported in the literature (Pavlick et al., 2015; Botschen et al., 2017). We categorize the coverage issues into the following: No Candidate Frames: The word-to-frame mapping returned no candidate frame. In some cases, we could find a frame with manual effort (ref. Table 1). However, despite manual efforts, some cases, we could not find an appropriate frame all the time (e.g., star autograph, employee misconduct, etc.). No Suitable Frame in the Candidate Set: In this set, word-to-frame mapping retrieved candidate frames, but none of the candidates was found to be appropriate. For example, candidate frames for heat returned by the mapping are: C AUSE TEMPERATURE CH"
2021.findings-acl.256,W15-0122,0,0.0900705,"s of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This network takes concatenated embeddings of component nouns as an input and predicts one of the labels from the Tratz and Hovy (2010)’s label set. Fares et al. (2018) used a similar feed-forward network to predict two types of relations. This network, however, shares initial layers and separates output layers for each label type. NC interpretation via paraphrasing is another methodology that contains approaches such as prepositional and free paraphrasing"
2021.findings-acl.256,P16-3011,0,0.0135742,"e cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This network takes conca"
2021.findings-acl.256,N04-1016,0,0.159611,"dings of component nouns as an input and predicts one of the labels from the Tratz and Hovy (2010)’s label set. Fares et al. (2018) used a similar feed-forward network to predict two types of relations. This network, however, shares initial layers and separates output layers for each label type. NC interpretation via paraphrasing is another methodology that contains approaches such as prepositional and free paraphrasing. Prepositional paraphrasing, i.e., paraphrasing using a preposition, for example, student protest: “protest by student(s)”, is a relatively well-attended problem (Lauer, 1995; Lapata and Keller, 2004; Ponkiya et al., 2018b). All the above approaches proposed for prepositional paraphrasing use the fixed-set of eight prepositions proposed by Lauer (1995). The other set of approaches, i.e., free paraphrasing, however, has not received much attention. Apart from two SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013), it does not have much literature available. A recent study (Ponkiya et al., 2020) expresses paraphrasing as a “fill-in-theblank” problem, and utilizes pre-trained language models, for the task of noun-compound interpretation. 3 Foundations Levi (1978) performed a lingu"
2021.findings-acl.256,H94-1111,0,0.823659,"Missing"
2021.findings-acl.256,P15-2067,0,0.0385092,"Missing"
2021.findings-acl.256,2020.findings-emnlp.386,1,0.716195,"aphrasing. Prepositional paraphrasing, i.e., paraphrasing using a preposition, for example, student protest: “protest by student(s)”, is a relatively well-attended problem (Lauer, 1995; Lapata and Keller, 2004; Ponkiya et al., 2018b). All the above approaches proposed for prepositional paraphrasing use the fixed-set of eight prepositions proposed by Lauer (1995). The other set of approaches, i.e., free paraphrasing, however, has not received much attention. Apart from two SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013), it does not have much literature available. A recent study (Ponkiya et al., 2020) expresses paraphrasing as a “fill-in-theblank” problem, and utilizes pre-trained language models, for the task of noun-compound interpretation. 3 Foundations Levi (1978) performed a linguistic study to understand how noun compounds are generated. They call such compounds nominal compounds. This theory puts nominal compounds into two categories, 2902 based on the compounding process, as 1. Predicate Deletion: Here, a predicate between the components is dropped to create a compound. For example, apple pie is a “pie made from apple.” The predicate made from is dropped in this case. Similarly, fo"
2021.findings-acl.256,L18-1489,1,0.506606,"d from the semantics of the individual noun units present. 2901 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2901–2911 August 1–6, 2021. ©2021 Association for Computational Linguistics From a relation representation perspective, noun compounds are interpreted in two ways: via labelling and paraphrasing. Labelling involves assigning an abstract semantic relation from a predefined set, for example, orange juice: M ADE O F, hillside home: L OCATION, etc. There are many inventories of predefined semantic relations. We use the FrametNet based labels proposed by Ponkiya et al. (2018a). As per their convention, the head noun of a compound invokes the frame, and the modifier noun fits in one of the frame elements of the invoked frame, vide ‘board approval’ in the abstract. There are more than 11,000 FEs in FrameNet, and we have about 1900 training examples. Thus, the average number of examples for each label is quite small, and many labels do not have a training example. In summary, the contributions of this paper are three-fold: 1. We embed FrameNet entities in a continuous space, perform prediction in the continuous space to generalize over unseen labels, and show perfor"
2021.findings-acl.256,C18-1155,1,0.399286,"d from the semantics of the individual noun units present. 2901 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2901–2911 August 1–6, 2021. ©2021 Association for Computational Linguistics From a relation representation perspective, noun compounds are interpreted in two ways: via labelling and paraphrasing. Labelling involves assigning an abstract semantic relation from a predefined set, for example, orange juice: M ADE O F, hillside home: L OCATION, etc. There are many inventories of predefined semantic relations. We use the FrametNet based labels proposed by Ponkiya et al. (2018a). As per their convention, the head noun of a compound invokes the frame, and the modifier noun fits in one of the frame elements of the invoked frame, vide ‘board approval’ in the abstract. There are more than 11,000 FEs in FrameNet, and we have about 1900 training examples. Thus, the average number of examples for each label is quite small, and many labels do not have a training example. In summary, the contributions of this paper are three-fold: 1. We embed FrameNet entities in a continuous space, perform prediction in the continuous space to generalize over unseen labels, and show perfor"
2021.findings-acl.256,P10-1070,0,0.084251,"ompound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This networ"
2021.findings-acl.256,C94-2125,0,0.607722,"rom http://www.cfilt.iitb.ac.in/nc-dataset. 2 Related Work A relation between the components of a noun compound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is s"
2021.findings-acl.256,P07-3013,0,0.137977,"Missing"
2021.inlg-1.39,N18-2008,0,0.0902917,"ction IV, we describe the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) design"
2021.inlg-1.39,W17-5534,0,0.0151217,"having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The au"
2021.inlg-1.39,N19-1374,0,0.0361565,"Missing"
2021.inlg-1.39,P17-1059,0,0.0344962,"Missing"
2021.inlg-1.39,P18-3020,0,0.0161891,"l setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic s"
2021.inlg-1.39,D18-1071,0,0.0175659,"e the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized re"
2021.inlg-1.39,P16-1094,0,0.174332,"employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016"
2021.inlg-1.39,D16-1127,0,0.242056,"employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016"
2021.inlg-1.39,I17-1099,0,0.0171313,"e used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with co"
2021.inlg-1.39,W04-1013,0,0.024961,"to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when the gradient norm is over 5. To reduce data sparsity, all the numbers and names are replaced with &lt;number&gt; and &lt;person&gt;. Automatic Evaluation Metrics: In order to assess the model at the emotional and grammatical level, we present the results using the traditional automatic metrics. Perplexity(Chen et al., 1998) is stated to test our proposed framework at the content level. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the ability of the generated response for capturing the correct information. BLEU measures the n-grams overlap between the generated response and the gold response, and has become a standard measure for comparing task-oriented dialog systems. It is used to measure the content preservation in the generated responses. We report Distinct-1 and Distinct-2 metrics that measure the distinct n-grams in the generated responses and are scaled with respect to the total number of generated tokens to avoid repetitive and boring responses (Li et al., 2016b). To measure the emotional content in"
2021.inlg-1.39,D15-1166,0,0.0231398,"he last hidden representation hi|Pm |(i.e. the representation at the EOS token) as the persona representation of the given speaker. Therefore, the final persona representation of the utterance Pm is: hpi = hi|Pm |+ p0i (5) Emotion controlled Decoder: To generate the next textual response with the given emotion information we employ a RNN decoder as shown in Figure 1. We employ GRU for generating the response in a sequential manner based on the context hidden representation from both the transformers, and the words decoded previously. We use the input feeding decoding along with the attention (Luong et al., 2015) mechanism for enhancing the performance of the model. Using the decoder state hdec d,t as the query vector, we apply self-attention on the hidden representation of the utterance-level encoder. The decoder state, persona information and the context vector are concatenated and used to calculate a final distribution of the probability 356 over the output tokens. the basic hierarchical encoder-decoder framework. (vii) Trans: Basic transformer network without persona, sentiment and emotion information. (viii) Trans + E + P: The transformer encoders along with persona encoder and emotion informatio"
2021.inlg-1.39,P19-1542,0,0.0155032,"18, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The authors in (Madotto et al., 2019) used a meta-learning framework to include persona information in the generated responses. Similarly, the authors in (Yavuz et al., 2019) employed a hierarchical pointer network for generating personabased responses. The authors in (Song et al., 2019) used persona information to generate diverse responses by employing conditional variational autoencoder. Our present work differs from these existing works (that made use of the PersonaChat dataset) in a sense that we intend to use the persona information while g"
2021.inlg-1.39,D18-1298,0,0.0439632,"Missing"
2021.inlg-1.39,P02-1040,0,0.109339,"as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when the gradient norm is over 5. To reduce data sparsity, all the numbers and names are replaced with &lt;number&gt; and &lt;person&gt;. Automatic Evaluation Metrics: In order to assess the model at the emotional and grammatical level, we present the results using the traditional automatic metrics. Perplexity(Chen et al., 1998) is stated to test our proposed framework at the content level. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the ability of the generated response for capturing the correct information. BLEU measures the n-grams overlap between the generated response and the gold response, and has become a standard measure for comparing task-oriented dialog systems. It is used to measure the content preservation in the generated responses. We report Distinct-1 and Distinct-2 metrics that measure the distinct n-grams in the generated responses and are scaled with respect to the total number of generated tokens to avoid repetitive and boring responses (Li et al., 2016b). To measure t"
2021.inlg-1.39,P19-1050,0,0.0181672,"wd workers who were instructed to play the part of a given persona. In over 10,981 dialogues, this dataset comprises of 164,356 utterances and has a collection of 1,155 personas, each consisting of at least four personality texts. There are 1,016 dialogues in the testing set and 200 never before seen personas. As the dataset is not labeled with emotions, we use the emotion annotated version of the dataset used in (Firdaus et al., 2020). Dataset Preparation: As sentiment and emotions are highly co-related we annotate the PersonaChat dataset using the emotion information in a similar manner as (Poria et al., 2019). As emotions such as excited, grateful, joyful, caring, hopeful, faithful, impressed have a positive undertone hence we automatically label the utterances having these emotion labels as positive sentiment. Similarly for emotions such as angry, sad, annoyed, disgusted, terrified, furious, disappointed, jealous has a negative undertone hence are labelled as negative sentiment. For the other emotion labels such as surprise, proud, nostalgic, guilty, confident, prepared, sentimental that can either be positive, neutral or negative depending on the utterance and the context we resort to manual ann"
2021.inlg-1.39,P19-1534,0,0.0136906,"g with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence fr"
2021.inlg-1.39,2020.acl-main.7,0,0.0236516,"ersona and sentiment while generating emotional responses compared to the existing baselines. The rest of the paper is structured as follows. In Section II, we present a brief survey of the related work. In Section III, we explain the proposed methodology. In Section IV, we describe the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed"
2021.inlg-1.39,N18-1186,0,0.0231067,"dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to"
2021.inlg-1.39,D18-1428,0,0.0243644,"arial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Ma"
2021.inlg-1.39,D17-1065,0,0.0607417,"Missing"
2021.inlg-1.39,W19-5917,0,0.0176674,", 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The authors in (Madotto et al., 2019) used a meta-learning framework to include persona information in the generated responses. Similarly, the authors in (Yavuz et al., 2019) employed a hierarchical pointer network for generating personabased responses. The authors in (Song et al., 2019) used persona information to generate diverse responses by employing conditional variational autoencoder. Our present work differs from these existing works (that made use of the PersonaChat dataset) in a sense that we intend to use the persona information while generating emotional"
2021.inlg-1.39,P18-1205,0,0.228837,"nts and decreasing breakdowns in conversations (Martinovski and Traum, 2003). Moreover, these agents should also have the capability to generate personalized responses conforming to the personal interests and unique needs of different users while presenting a consistent personality to gain the user’s trust and confidence. Hence, the primary motivation of our current work lies in generating responses that are engaging, emotionally appropriate, and also integrates the personal interests of the user. Lately, researchers have started focusing on incorporating personality information on chit-chat (Zhang et al., 2018) and goal-oriented (Joshi et al., 2017; Luo et al., 2019) conversational systems. Due to the lack of persona data sets, the authors created a PersonaChat dataset in (Zhang et al., 2018), where the individual personality data is represented in a few texts for open-domain chit-chat dialogue systems. We present an example from the dataset in Table 1, from which it is obvious that the speakers are able to retain the persona knowledge when communicating with each other. This helps to make the dialogue engaging and also makes it easier to build trust and credibility with the users (Shum et al., 2018"
2021.inlg-1.39,P19-1366,0,0.0163002,"mpathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song"
2021.ltedi-1.29,W14-3914,0,0.0272537,"ages by training transformer models specifically for Indian languages. Indic BERT (Kakwani et al., 2020) and MuRIL (https://tfhub.dev/google/ MuRIL/1) are two such transformer-based language models. We plan to tackle the Hope speech discovery for the English, Tamil, and Malayalam language by obtaining representation from multilingual transformer models. Tamil and Malayalam are Dravidian languages locally spoken in the states of Tamil Nadu and Kerala respectively on Indian territory. For a country like India, where people speak many languages, code-mixing is fairly common (Barman et al., 2014; Bali et al., 2014; Gupta et al., 2018). The dataset for this task is code-mixed such as tag, inter-sentential, and intra-sentential (Chakravarthi, 2020). The shared task was launched as a part of the first workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-EACL-2021) (Chakravarthi and Muralidaran, 2021). It was conducted for English, Tamil, and Malayalam language categories. The task was to classify a given piece of text scraped from YouTube comments into one of the three possible categories. These three possible categories or labels were: • Label 1: Hope Speech • Label 2: Not hope spee"
2021.ltedi-1.29,W14-3915,0,0.0278095,"lated to Indian languages by training transformer models specifically for Indian languages. Indic BERT (Kakwani et al., 2020) and MuRIL (https://tfhub.dev/google/ MuRIL/1) are two such transformer-based language models. We plan to tackle the Hope speech discovery for the English, Tamil, and Malayalam language by obtaining representation from multilingual transformer models. Tamil and Malayalam are Dravidian languages locally spoken in the states of Tamil Nadu and Kerala respectively on Indian territory. For a country like India, where people speak many languages, code-mixing is fairly common (Barman et al., 2014; Bali et al., 2014; Gupta et al., 2018). The dataset for this task is code-mixed such as tag, inter-sentential, and intra-sentential (Chakravarthi, 2020). The shared task was launched as a part of the first workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-EACL-2021) (Chakravarthi and Muralidaran, 2021). It was conducted for English, Tamil, and Malayalam language categories. The task was to classify a given piece of text scraped from YouTube comments into one of the three possible categories. These three possible categories or labels were: • Label 1: Hope Speech • Lab"
2021.ltedi-1.29,2020.peoples-1.5,0,0.134577,"it comes to hope speech detection there has not been much work done but recently the NLP community has started showing interest in this area. In a work by Palakodety et al. (2019), they have analyzed YouTube comments and performed the task of hope speech detection to identify hostilitydiffusing content. Here the authors have not taken other aspects like equality, diversity, and inclusion into account. Chakravarthi et al. (2020) did work in Indian languages where they manually annotated the YouTube comments for Tamil and Malayalam languages for performing sentiment analysis. In a similar work, Chakravarthi (2020) released the dataset consisting of YouTube comments with hope and not-hope speech annotation. Hate speech is a well-researched area related to Hope speech. According to the survey done by Schmidt and Wiegand (2017), automatic hate detection was needed due to a large number of people using the net and the massive scale with which the web is growing. Zhang and Luo (2018) used deep neural networks for the task and they were able to outperform the best performing method by up to 5 percentage points in macro-average F1-score. Multilingual BERT (Pires et al., 2019) is a variant of BERT (Devlin et a"
2021.ltedi-1.29,2020.sltu-1.25,0,0.0377834,"t of work that is being done to remove the negativity from the web but Hope speech detection focuses to spread positivity by detecting content that is encouraging, positive, and supportive. When it comes to hope speech detection there has not been much work done but recently the NLP community has started showing interest in this area. In a work by Palakodety et al. (2019), they have analyzed YouTube comments and performed the task of hope speech detection to identify hostilitydiffusing content. Here the authors have not taken other aspects like equality, diversity, and inclusion into account. Chakravarthi et al. (2020) did work in Indian languages where they manually annotated the YouTube comments for Tamil and Malayalam languages for performing sentiment analysis. In a similar work, Chakravarthi (2020) released the dataset consisting of YouTube comments with hope and not-hope speech annotation. Hate speech is a well-researched area related to Hope speech. According to the survey done by Schmidt and Wiegand (2017), automatic hate detection was needed due to a large number of people using the net and the massive scale with which the web is growing. Zhang and Luo (2018) used deep neural networks for the task"
2021.ltedi-1.29,2021.ltedi-1.8,0,0.0275213,"obtaining representation from multilingual transformer models. Tamil and Malayalam are Dravidian languages locally spoken in the states of Tamil Nadu and Kerala respectively on Indian territory. For a country like India, where people speak many languages, code-mixing is fairly common (Barman et al., 2014; Bali et al., 2014; Gupta et al., 2018). The dataset for this task is code-mixed such as tag, inter-sentential, and intra-sentential (Chakravarthi, 2020). The shared task was launched as a part of the first workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-EACL-2021) (Chakravarthi and Muralidaran, 2021). It was conducted for English, Tamil, and Malayalam language categories. The task was to classify a given piece of text scraped from YouTube comments into one of the three possible categories. These three possible categories or labels were: • Label 1: Hope Speech • Label 2: Not hope speech • Label 3: Not in intended language The rest of the paper is paper is organized as follows. In Section 2, we provide the dataset details and statistics. Section 3 consists of our system description and architecture details. In Section 4, we describe our experimental setups and report our results. We conclud"
2021.ltedi-1.29,2020.acl-main.747,0,0.0944135,"Missing"
2021.ltedi-1.29,N19-1423,0,0.0334491,"rthi (2020) released the dataset consisting of YouTube comments with hope and not-hope speech annotation. Hate speech is a well-researched area related to Hope speech. According to the survey done by Schmidt and Wiegand (2017), automatic hate detection was needed due to a large number of people using the net and the massive scale with which the web is growing. Zhang and Luo (2018) used deep neural networks for the task and they were able to outperform the best performing method by up to 5 percentage points in macro-average F1-score. Multilingual BERT (Pires et al., 2019) is a variant of BERT (Devlin et al., 2019) that has been heavily used by the NLP community. Pires et al. (2019) in their work showed that multilingual rep193 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 193–196 April 19, 2021. ©2020 Association for Computational Linguistics resentation by multilingual BERT handles cross linguality without being explicitly trained for it. It also handles transfer across scripts and to codeswitching fairly well. Conneau et al. (2020) proposed another variant of the BERT model called XLM-RoBERTa by pre-training multilingual models at scale. There h"
2021.ltedi-1.29,K18-1012,1,0.829987,"ansformer models specifically for Indian languages. Indic BERT (Kakwani et al., 2020) and MuRIL (https://tfhub.dev/google/ MuRIL/1) are two such transformer-based language models. We plan to tackle the Hope speech discovery for the English, Tamil, and Malayalam language by obtaining representation from multilingual transformer models. Tamil and Malayalam are Dravidian languages locally spoken in the states of Tamil Nadu and Kerala respectively on Indian territory. For a country like India, where people speak many languages, code-mixing is fairly common (Barman et al., 2014; Bali et al., 2014; Gupta et al., 2018). The dataset for this task is code-mixed such as tag, inter-sentential, and intra-sentential (Chakravarthi, 2020). The shared task was launched as a part of the first workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-EACL-2021) (Chakravarthi and Muralidaran, 2021). It was conducted for English, Tamil, and Malayalam language categories. The task was to classify a given piece of text scraped from YouTube comments into one of the three possible categories. These three possible categories or labels were: • Label 1: Hope Speech • Label 2: Not hope speech • Label 3: Not in"
2021.ltedi-1.29,2020.findings-emnlp.445,0,0.0632716,"Missing"
2021.ltedi-1.29,W18-5113,0,0.0201697,"Introduction The prominence of web-based media is expanding quickly because it is being used to create and share content, even by those who are ignorant of online media. Several web platforms allow users to add textual feedback on non-textual content, such as images, photos, animations, etc. With millions of videos posted by its users and billions of comments on all these videos, YouTube is undoubtedly the most famous of them. Online social media comments/posts have been examined to identify and avoid the propagation of negativity using strategies such as detecting abusive language detection (Lee et al., 2018) and hate speech (Schmidt and Wiegand, 2017). There is a lot of work that is being done to remove the negativity from the web but Hope speech detection focuses to spread positivity by detecting content that is encouraging, positive, and supportive. When it comes to hope speech detection there has not been much work done but recently the NLP community has started showing interest in this area. In a work by Palakodety et al. (2019), they have analyzed YouTube comments and performed the task of hope speech detection to identify hostilitydiffusing content. Here the authors have not taken other asp"
2021.ltedi-1.29,P19-1493,0,0.019468,"ment analysis. In a similar work, Chakravarthi (2020) released the dataset consisting of YouTube comments with hope and not-hope speech annotation. Hate speech is a well-researched area related to Hope speech. According to the survey done by Schmidt and Wiegand (2017), automatic hate detection was needed due to a large number of people using the net and the massive scale with which the web is growing. Zhang and Luo (2018) used deep neural networks for the task and they were able to outperform the best performing method by up to 5 percentage points in macro-average F1-score. Multilingual BERT (Pires et al., 2019) is a variant of BERT (Devlin et al., 2019) that has been heavily used by the NLP community. Pires et al. (2019) in their work showed that multilingual rep193 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 193–196 April 19, 2021. ©2020 Association for Computational Linguistics resentation by multilingual BERT handles cross linguality without being explicitly trained for it. It also handles transfer across scripts and to codeswitching fairly well. Conneau et al. (2020) proposed another variant of the BERT model called XLM-RoBERTa by pre-tra"
2021.ltedi-1.29,W17-1101,0,0.0253228,"based media is expanding quickly because it is being used to create and share content, even by those who are ignorant of online media. Several web platforms allow users to add textual feedback on non-textual content, such as images, photos, animations, etc. With millions of videos posted by its users and billions of comments on all these videos, YouTube is undoubtedly the most famous of them. Online social media comments/posts have been examined to identify and avoid the propagation of negativity using strategies such as detecting abusive language detection (Lee et al., 2018) and hate speech (Schmidt and Wiegand, 2017). There is a lot of work that is being done to remove the negativity from the web but Hope speech detection focuses to spread positivity by detecting content that is encouraging, positive, and supportive. When it comes to hope speech detection there has not been much work done but recently the NLP community has started showing interest in this area. In a work by Palakodety et al. (2019), they have analyzed YouTube comments and performed the task of hope speech detection to identify hostilitydiffusing content. Here the authors have not taken other aspects like equality, diversity, and inclusion"
2021.mtsummit-loresmt.17,W14-4012,0,0.185796,"Missing"
2021.mtsummit-loresmt.17,N16-1101,0,0.0218262,"arning techniques in which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019) introduced a transfer learning technique based on a pivot language in which a pivot language is used to pre-train the encoder and decoder of a NMT model, which are then used to initialize the encoder and decoder of the final NMT model which is then fine-tuned on the low resource language pair. Multi-lingual NMT models (Zoph and Knight, 2016; Firat et al., 2016; Johnson et al., 2017) that can translate to or from multiple languages have shown performance improvements in the case of low resource language pairs when the system also includes high resource language pairs. 3 Approaches In this section, we discuss the various techniques that we have used to implement our English-Marathi and Marathi-English MT systems. 3.1 Baseline Model In our Baseline English-Marathi and Marathi-English MT models, we train a NMT system using the given English-Marathi parallel corpus for 1600 epochs and we save the model for every 200 epochs starting from 200, to test the"
2021.mtsummit-loresmt.17,Q17-1024,0,0.0277985,"which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019) introduced a transfer learning technique based on a pivot language in which a pivot language is used to pre-train the encoder and decoder of a NMT model, which are then used to initialize the encoder and decoder of the final NMT model which is then fine-tuned on the low resource language pair. Multi-lingual NMT models (Zoph and Knight, 2016; Firat et al., 2016; Johnson et al., 2017) that can translate to or from multiple languages have shown performance improvements in the case of low resource language pairs when the system also includes high resource language pairs. 3 Approaches In this section, we discuss the various techniques that we have used to implement our English-Marathi and Marathi-English MT systems. 3.1 Baseline Model In our Baseline English-Marathi and Marathi-English MT models, we train a NMT system using the given English-Marathi parallel corpus for 1600 epochs and we save the model for every 200 epochs starting from 200, to test the performance. 3.2 Back-"
2021.mtsummit-loresmt.17,D19-1080,0,0.0200805,"ch et al. (2016) introduced the technique of back-translation in which monolingual data is used to create augmented pseudo-parallel data. Sen et al. (2018) used Statistical Machine Translation (SMT) system by extracting phrases generated during SMT training and using them along with the training data for NMT systems. Zoph et al. (2016) introduced a transfer learning techniques in which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019) introduced a transfer learning technique based on a pivot language in which a pivot language is used to pre-train the encoder and decoder of a NMT model, which are then used to initialize the encoder and decoder of the final NMT model which is then fine-tuned on the low resource language pair. Multi-lingual NMT models (Zoph and Knight, 2016; Firat et al., 2016; Johnson et al., 2017) that can translate to or from multiple languages have shown performance improvements in the case of low resource language pairs when the system also includes high resource language pairs. 3 Approaches In this sect"
2021.mtsummit-loresmt.17,2021.mtsummit-loresmt.11,0,0.0609237,"Missing"
2021.mtsummit-loresmt.17,N19-4009,0,0.0156312,"as a part of this work. 4.1 Dataset Type of Data Number of sentences Parallel Monolingual 20,933 21,902 Table 1: Dataset We used the English-Marathi parallel corpus provided by the LoResMT 2021 organizers, which consisted of 20,933 English-Marathi parallel sentences. Further, for our back-translation experiment we used the Marathi monolingual corpus provided by the LoResMT 2021 organizers which consisted of 21,902 Marathi sentences. 4.2 Training Setup For all of the NMT systems discussed in this paper, we have used the transformer-based architecture which we have implemented using the fairseq Ott et al. (2019) library. This transformer-based architecture consisted of 6 encoder layers and 6 decoder layers. The number of encoder and decoder attention heads used were 4 each. We used encoder and decoder embedding dimension of 512 each. For training the system, the optimizer used was Adam optimizer with betas (0.9, 0.98). The inverse square root learning rate scheduler was used with initial learning rate of 5e-4 and 4,000 warm-up updates. The criterion used was label smoothed cross entropy with label smoothing of 0.1. The dropout probability value of 0.3 was used. 5 Results and Analysis Model English-Ma"
2021.mtsummit-loresmt.17,P02-1040,0,0.113882,"aseline-1200 Baseline-1400 Baseline-1600 Back-translation 11 10.4 — 10.6 10.5 10.7 10.5 10.8 12.2 16.8 17.1 17.2 17.2 16.6 16.3 16.2 — — Table 2: BLEU scores of English-Marathi language pair where for a model named Baseline-X, X represents the number of epochs for which the model was trained. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 160 Table 2 shows the results of the different techniques used to implement the MT systems for the English-Marathi language pair. We used BLEU (Papineni et al., 2002) metric to measure the performance of the MT systems. The baseline English-Marathi system produced a BLEU score of 11 and the baseline Marathi-English System produced a BLEU score of 17.2. We observe that the English-Marathi and Marathi-English NMT were trained using the same English-Marathi parallel data, the Marathi-English system produced higher BLEU scores than that produced by the English-Marathi system. We also observe that the English-Marathi system gives the best BLEU score after 400 epochs and after that the scores decrease and fluctuate between a small range. The MarathiEnglish model"
2021.mtsummit-loresmt.17,P16-1009,0,0.0695951,"Network (RNN) based approaches (Cho et al., 2014; Sutskever et al., 2014). But Recurrent Neural Network based architectures were not able to capture long term dependencies in long sentences. In order to overcome this problem, Attention (Bahdanau et al., 2014) mechanism was introduced. The Attention-based RNN architecture still suffered from problems like longer training time because of their sequential nature. Later Transformer architecture (Vaswani et al., 2017) was introduced which improved the performance of the NMT systems and also lead to faster training due to its non-sequential nature. Sennrich et al. (2016) introduced the technique of back-translation in which monolingual data is used to create augmented pseudo-parallel data. Sen et al. (2018) used Statistical Machine Translation (SMT) system by extracting phrases generated during SMT training and using them along with the training data for NMT systems. Zoph et al. (2016) introduced a transfer learning techniques in which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019"
2021.mtsummit-loresmt.17,N16-1004,0,0.0265595,"ntroduced a transfer learning techniques in which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019) introduced a transfer learning technique based on a pivot language in which a pivot language is used to pre-train the encoder and decoder of a NMT model, which are then used to initialize the encoder and decoder of the final NMT model which is then fine-tuned on the low resource language pair. Multi-lingual NMT models (Zoph and Knight, 2016; Firat et al., 2016; Johnson et al., 2017) that can translate to or from multiple languages have shown performance improvements in the case of low resource language pairs when the system also includes high resource language pairs. 3 Approaches In this section, we discuss the various techniques that we have used to implement our English-Marathi and Marathi-English MT systems. 3.1 Baseline Model In our Baseline English-Marathi and Marathi-English MT models, we train a NMT system using the given English-Marathi parallel corpus for 1600 epochs and we save the model for every 200 epochs starting f"
2021.mtsummit-loresmt.17,D16-1163,0,0.0235171,"ill suffered from problems like longer training time because of their sequential nature. Later Transformer architecture (Vaswani et al., 2017) was introduced which improved the performance of the NMT systems and also lead to faster training due to its non-sequential nature. Sennrich et al. (2016) introduced the technique of back-translation in which monolingual data is used to create augmented pseudo-parallel data. Sen et al. (2018) used Statistical Machine Translation (SMT) system by extracting phrases generated during SMT training and using them along with the training data for NMT systems. Zoph et al. (2016) introduced a transfer learning techniques in which a parent NMT model is initially trained on a high resource language pair and then the parameters of this parent model are used to initialize a child model, which is then trained on a low resource language pair. Kim et al. (2019) introduced a transfer learning technique based on a pivot language in which a pivot language is used to pre-train the encoder and decoder of a NMT model, which are then used to initialize the encoder and decoder of the final NMT model which is then fine-tuned on the low resource language pair. Multi-lingual NMT models"
2021.mtsummit-research.2,P10-1088,0,0.619744,"learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, interactive paradigm of SMT was introduced in Gonz´alez-Rubio et al. (2012); Gonz´alez-Rubio and Casacuberta (2014). Later, the NMT became more prominent and efficient in the interactive paradigm of MT (Knowles and Koehn, 2016; Peris et al., 2017). Peris and Casacuberta (2018) explored the application of active learning and IMT on the NMT model. They performed the experiments over the attention based encoder-decoder NMT model (Bah"
2021.mtsummit-research.2,E14-1042,0,0.264435,"ments using this proposed INMT model. 2 Related Work In a case, where an MT model is not providing high quality translation due to low resource or out-of-domain scenarios, it could be beneficial to update the model with new samples while preserving the previous knowledge too. There has been some works which deal with the large input data streams but generally adopt the incremental learning approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and"
2021.mtsummit-research.2,E12-1025,0,0.867306,"Missing"
2021.mtsummit-research.2,N09-1047,0,0.53437,"17) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, interactive paradigm of SMT was introduced in Gonz´alez-Rubio et al. (2012); Gonz´alez-Rubio and Casacuberta (2014). Later, the NMT became more prominent and efficient in the interactive paradigm of MT (Knowles and Koehn, 2016; Peris et al., 2017). Peris and Casacuberta (2018) explored the application of active learning and IMT on the NMT model. They performed the experiments over the attention"
2021.mtsummit-research.2,P19-3020,0,0.178652,"Missing"
2021.mtsummit-research.2,2020.amta-research.9,0,0.319239,"ation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments were based on the Transformer NMT model Vaswani et al. (2017). We used 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up steps. We used the BPE (Sennrich et al., 2016) with a vocabulary size of 40K. Models were trained with OpenNMT toolkit4 (Klein et al., 2020) with batch size of 2,048 tokens till convergence and checkpoints were created after every 10,000 steps. During inference, beam size is set to 5. We measured BLEU (calculated with multi-bleu.pl script) (Papineni et al., 2002) of the trained models on the test sets. 7 Results and Analysis We evaluate the impact of the proposed sampling techniques for active learning in NMT in two different ways. Firstly, we test whether the proposed techniques help the NMT model to improve its translation performance in terms of the BLEU score. Secondly, in order to see whether the proposed techniques are able"
2021.mtsummit-research.2,2016.amta-researchers.9,0,0.620653,"in automatic translation workflows by employing an iterative collaborative strategy with its two most important Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 10 Figure 1: A pipeline showing the flow of data through sampling module, model updation through active learning. components, the human agent and the MT engine. As of today, NMT models (Bahdanau et al., 2015; Vaswani et al., 2017) represent state-of-the-art in MT research. This has led researchers to test interactive-predictive protocol on NMT too. Papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003). So use of interactive NMT (INMT) for output sample correction can significantly reduce the overall translation time and active learning strategy can use human corrected samples for adapting the underlying NMT model so that in future, the model does not repeat previous errors and improves the translation quality. The contributions of our current work are stated as follows: • We propose term based (NEC) and quality based (QE and Sim) sampling techniques tha"
2021.mtsummit-research.2,W04-3250,0,0.203057,"Missing"
2021.mtsummit-research.2,2005.mtsummit-papers.11,0,0.309802,"-th target word. Note that, 1 the fraction |x| is equivalent to the mean of the attention weights of the word yi . Finally, The kurtosis values for all the target words are used to obtain the attention distraction score. 5 Dataset We carried out experiments on three language pairs using three benchmark datasets. Table 2 shows the statistics of training, development and test sets used for our experiments. In order to measure performance of the proposed sampling techniques, we use different domain datasets for training and testing. For German-English and Spanish-English, we use Europarl corpus (Koehn, 2005) for training and News-Commentary (NC) corpus for testing. This gives us a clear indication whether the translation models trained over Europarl corpus are able to adapt over the sampled examples from NC corpus using active learning. Similarly, for English-Hindi translation, we use the IITB corpus (Kunchukuttan et al., 2018) for training which is a combination of sentences from government sites, ted talks, administration books etc. As for evaluation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments w"
2021.mtsummit-research.2,N03-1017,0,0.210173,"al Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 10 Figure 1: A pipeline showing the flow of data through sampling module, model updation through active learning. components, the human agent and the MT engine. As of today, NMT models (Bahdanau et al., 2015; Vaswani et al., 2017) represent state-of-the-art in MT research. This has led researchers to test interactive-predictive protocol on NMT too. Papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003). So use of interactive NMT (INMT) for output sample correction can significantly reduce the overall translation time and active learning strategy can use human corrected samples for adapting the underlying NMT model so that in future, the model does not repeat previous errors and improves the translation quality. The contributions of our current work are stated as follows: • We propose term based (NEC) and quality based (QE and Sim) sampling techniques that provide us with the ideal source samples which are first post-edited using interactive NMT (INMT) and then used to update the Transformer"
2021.mtsummit-research.2,L18-1548,1,0.831213,". Table 2 shows the statistics of training, development and test sets used for our experiments. In order to measure performance of the proposed sampling techniques, we use different domain datasets for training and testing. For German-English and Spanish-English, we use Europarl corpus (Koehn, 2005) for training and News-Commentary (NC) corpus for testing. This gives us a clear indication whether the translation models trained over Europarl corpus are able to adapt over the sampled examples from NC corpus using active learning. Similarly, for English-Hindi translation, we use the IITB corpus (Kunchukuttan et al., 2018) for training which is a combination of sentences from government sites, ted talks, administration books etc. As for evaluation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments were based on the Transformer NMT model Vaswani et al. (2017). We used 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up"
2021.mtsummit-research.2,N10-1062,0,0.430942,"n terms of token replacements using this proposed INMT model. 2 Related Work In a case, where an MT model is not providing high quality translation due to low resource or out-of-domain scenarios, it could be beneficial to update the model with new samples while preserving the previous knowledge too. There has been some works which deal with the large input data streams but generally adopt the incremental learning approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model le"
2021.mtsummit-research.2,2020.eamt-1.11,0,0.181939,"riteria. For every input sentence, this tool takes two inputs which are source sentence and translation of the source sentence generated by the initial NMT model and gives us the estimated HTER score. For a test sentence Si in S where (1 ≤ i ≤ |S|) (|S |= number of sentences in S), quality estimation (QE) pre-trained model takes Si and its generated translation Ti , and returns the corresponding HTER score HT ERi . 4.3 Sentence Similarity (SS) Here, we calculate the similarity between the source sentence and its round trip translation (source-to-target and again target-to-source translation) (Moon et al., 2020). We explore the similarity based sampling criteria since the quality of the round trip translation depends on the two intermediate translations i.e. forward translation (source-to-target) and back-translation (target-to-source). In case of a weak NMT model (i.e. MT system that does not generate high quality translations; e.g. say in low resource scenario or translating out-of-domain data), it is unlikely that a generated round-trip translation would be closer to the source sentence. As for the RTT setup, we had to train forward- and back-translation models. In this case, a low similarity scor"
2021.mtsummit-research.2,W04-3225,0,0.804296,"approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, i"
2021.mtsummit-research.2,J16-1004,0,0.692805,"Missing"
2021.mtsummit-research.2,P02-1040,0,0.109148,"6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up steps. We used the BPE (Sennrich et al., 2016) with a vocabulary size of 40K. Models were trained with OpenNMT toolkit4 (Klein et al., 2020) with batch size of 2,048 tokens till convergence and checkpoints were created after every 10,000 steps. During inference, beam size is set to 5. We measured BLEU (calculated with multi-bleu.pl script) (Papineni et al., 2002) of the trained models on the test sets. 7 Results and Analysis We evaluate the impact of the proposed sampling techniques for active learning in NMT in two different ways. Firstly, we test whether the proposed techniques help the NMT model to improve its translation performance in terms of the BLEU score. Secondly, in order to see whether the proposed techniques are able to reduce the human efforts (number of token correction required) in correcting the hypothesis, we compare the performance of the proposed 4 https://opennmt.net/ Proceedings of the 18th Biennial Machine Translation Summit Vir"
2021.mtsummit-research.2,2021.mtsummit-research.2,1,0.0530913,"Missing"
2021.mtsummit-research.2,D19-1410,0,0.0225896,"e may be different from the original source sentence but semantically similar to it, which is not captured by surface level metrics such as BLEU. In fact, we need information about the semantics of both source and back translation. ‘Similarity based on sentence embedding’ (Simemb ) as the name itself suggests, this sampling technique uses a cosine similarity measure based on sentence embeddings. For every input sentence, two embeddings are generated: 1) embedding of the source sentence and 2) embedding of the RTTed sentence of the source sentence. These embeddings are generated using S-BERT 1 Reimers and Gurevych (2019). Sentences having the least similarity scores in the block are sampled and supervised by the user. 4.3.2 Similarity based on Edit distance between sentences (Simf uzzy ) This similarity is a surface level similarity method and it does not take into account the semantics of the source and back translated sentences. In this sampling technique the similarity measure/score is based on the ‘levenshtein-distance’ between the source sentence and the round-trip translation of the source sentence. For every test sentence the similarity score (Simf uzzy ) between the sentence and round-trip translation"
2021.mtsummit-research.2,P16-1162,0,0.474635,"een the source sentence and the round-trip translation of the source sentence. For every test sentence the similarity score (Simf uzzy ) between the sentence and round-trip translation is calculated using ‘fuzzywuzzy’ toolkit2 which is based on the levenshtein-distance and generates a score between 0-100 (0 and 100 are the lowest and highest similarity level). The sentences having the least score in the block are considered for supervision. 4.4 Named Entity Counting (NEC) The NMT model suffers with the vocabulary restriction problem due to the limitation over the decoder side vocabulary size (Sennrich et al., 2016). Named entities (NEs) are open vocabularies and it is not possible for the NMT model to have all the NEs in the decoder vocabulary. Therefore, we considered presence of NEs as one of the sampling criteria. In other words, we took inability of the NMT model to translate the NEs perfectly into account for sampling. We count the NE tokens in each source sample of the incoming inference data and the sentences having the most number of NE tokens in the block are considered as “difficult to translate” by the NMT model, and hence filtered for supervision. We use Spacy3 named entity recognizer (NER)"
2021.naacl-main.322,W14-0130,1,0.864171,"Missing"
2021.naacl-main.322,N16-1102,0,0.0411193,"Missing"
2021.naacl-main.322,2020.winlp-1.10,0,0.0361127,"arios, monolingual models can give competitive results compared to cross-lingual models, a result that is supported by research in other tasks such as morphological inflection (Anastasopoulos and Neubig, 2019). Additionally, in the low resource setting, additional features are an important source of information. Even PoS tags benefit the training process. 6.1 Areas of improvement The model currently does not exploit any linguistic knowledge available to improve its performance. Incorporating morphological rules or using bilingual knowledge to create transfer models could grant accuracy gains (Gebreselassie et al., 2020; Faruqui et al., 2015). Moreover, transformers have been shown to improve performance on character level tasks which would be applicable method here (Wu et al., 2020). Another potential area of improvement could be the usage of different data hallucination techniques like in Shcherbakov et al. (2016), which uses phonetics instead of relying on characters for predictions. 7 Ethical Considerations equitable distribution of technology and quality of life across the globe. References Mubashir Ali, Shehzad Khalid, and Muhammad Haseeb Aslam. 2017. Pattern based comprehensive urdu stemmer and short"
2021.naacl-main.322,C12-3034,0,0.0213156,"eadily available. For languages which for Konkani and Kashmiri (Rajan et al., 2020; have no linguistic work available, espeIslam et al., 2018). cially on morphology or in languages where Techniques like Porter stemmer are indeed the computational realization of linguistic quick solutions, but they are suited only for rules is complex and cumbersome, machine alphabetic script languages, like English, and learning based lemmatizers are the way to not abugida, like Bengali (Ali et al., 2017), or go. In this paper, we devote our attention to lemmatisation for low resource, morphoabjad, like Urdu (Kansal et al., 2012), script logically rich scheduled Indian languages languages. Moreover, creating stemmers reusing neural methods. Here, low resource quires different language specific stemming almeans only a small number of word forms gorithms. This requirement of language speare available. We perform tests to analyse cific measures comes in the way of scaling the the variance in monolingual models’ perforenterprise of creating stemmers for the hunmance on varying the corpus size and condreds and thousands of languages that exist textual morphological tag data for training. We show that monolingual approaches"
2021.naacl-main.322,W19-4226,0,0.0221181,"raining examples samples to train on and show a competitive performance against cross-lingual models in the same setting. Neubig (2019) for more details and explanations about the two-step attention process and Cohn et al. (2016) for more details regarding structural bias. 2 4 Experiments Related work In Zeman et al. (2018), lemmatisation was performed for small treebanks exploiting the common annotation standard across all languages, and the same task was implicit in Nivre et al. (2017). Recently, there has been a shift to extremely low resource settings with the SIGMORPHON 2019 shared task (McCarthy et al., 2019) focusing on cross-lingual learning. However, their task focuses on the reverse direction: given a lemma and a set of morphological features, generate a target inflected form. 3 Models 4.1 Data From the SIGMORPHON 2019 shared task, we collect language data from the multilingual morphological inflection task for Bengali, Hindi, Kannada, Sanskrit, Telugu, and Urdu. Out of these, Telugu is the only one that does not have a large data set (inflected word forms). We use the same task categorization of high or low resource languages as SIGMORPHON. Each training sample is a triplet: (inflected word,"
2021.naacl-main.322,U16-1009,0,0.0174905,"formation. Even PoS tags benefit the training process. 6.1 Areas of improvement The model currently does not exploit any linguistic knowledge available to improve its performance. Incorporating morphological rules or using bilingual knowledge to create transfer models could grant accuracy gains (Gebreselassie et al., 2020; Faruqui et al., 2015). Moreover, transformers have been shown to improve performance on character level tasks which would be applicable method here (Wu et al., 2020). Another potential area of improvement could be the usage of different data hallucination techniques like in Shcherbakov et al. (2016), which uses phonetics instead of relying on characters for predictions. 7 Ethical Considerations equitable distribution of technology and quality of life across the globe. References Mubashir Ali, Shehzad Khalid, and Muhammad Haseeb Aslam. 2017. Pattern based comprehensive urdu stemmer and short text classification. IEEE Access, 6:7374–7389. Antonios Anastasopoulos and Graham Neubig. 2019. Pushing the limits of low-resource morphological inflection. arXiv preprint arXiv:1908.05838. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual rep"
2021.naacl-main.322,P19-1148,0,0.0371872,"Missing"
2021.naacl-main.322,2021.eacl-main.163,0,0.0242457,"Missing"
2021.naacl-main.322,K18-2001,0,0.0188022,"rs contributed equally to this work 4088 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4088–4094 June 6–11, 2021. ©2021 Association for Computational Linguistics setting with as few as 100 training examples samples to train on and show a competitive performance against cross-lingual models in the same setting. Neubig (2019) for more details and explanations about the two-step attention process and Cohn et al. (2016) for more details regarding structural bias. 2 4 Experiments Related work In Zeman et al. (2018), lemmatisation was performed for small treebanks exploiting the common annotation standard across all languages, and the same task was implicit in Nivre et al. (2017). Recently, there has been a shift to extremely low resource settings with the SIGMORPHON 2019 shared task (McCarthy et al., 2019) focusing on cross-lingual learning. However, their task focuses on the reverse direction: given a lemma and a set of morphological features, generate a target inflected form. 3 Models 4.1 Data From the SIGMORPHON 2019 shared task, we collect language data from the multilingual morphological inflection"
2021.naacl-main.456,S17-2126,0,0.0585515,"Missing"
2021.naacl-main.456,D17-1169,0,0.0269257,"tion the tweet acts. For expressive TAs such as “exIdentification of speech acts is one of the prelimi- pression"", “request"", “threat"" etc., the tweeter’s nary means of determining the communicative in- sentiment and emotion can aid in classifying true tent or pragmatics of a speaker (for example, state- communicative intent and vice-versa. ment, request, question etc.). This is true for diaAdditionally, multi-modal inputs, i.e., the comlogue system, speech transcription, social media bination of text and other nonverbal cues (emojis such as Twitter, MySpace etc. Twitter is one of in tweets) (Felbo et al., 2017) help create reliable the leading micro-blogging services. By 2019, 330 classification models aiding the identification of million users were active monthly and 500 mil- emotional state and sentiment of the tweeter which lion tweets were sent per day1 . Identification of in turn help in determining correct TAs. tweet acts (TAs- speech acts in Twitter) is highly In this paper, we leverage the relationships as beneficial for Twitter as well as tweeters. For Twitdelineated above to predict TAs of tweets in a multiter, it helps decipher a particular subject in terms modal framework. In this multi-"
2021.naacl-main.456,D09-1130,0,0.0462097,"elation of the tweeter. In (Cerisara et al., 2018), SS-Twitter (Thelwall et al., 2012) etc. However, authors proposed a LSTM based study for jointly we chose to use SemEval-2018 dataset for further optimizing SA and TAC in a decentralized social investigation of our task at hand. The reason bemedia platform called Mastodon. However, they hind this choice was that most of the ER datasets modelled their task as a multi-party conversation were annotated with only six Eckman’s (Ekman, pretty different in essence to that of Twitter anal- 1999) or eight Plutchik’s (Plutchik, 1980) emotion ysis. In (Jeong et al., 2009), authors presented a categories. Whereas SemEval-2018 dataset consemi-supervised approach to identify speech acts tains tweets annotated with multi-label 11 emotion in emails and different forums. These works, how- categories which aids the diversity of the problem ever, use datasets that comprise of face-to-face or statement. Intuitively, it was indeed possible to go telephone data that can not directly aid in advanc- the other way round and search for Twitter dataset ing work on endless data in electronic mode such as annotated with TAs such as (Zhang et al., 2011), 5728 Tweet And it pisses"
2021.naacl-main.456,S18-1001,0,0.0193272,"the literature for the latimportance of identification of tweet acts and es- est SA and ER dataset for Twitter in order to gather tablished it to be one of the elementary steps for potentially emotionally rich tweets to explore its detection of rumours in Twitter. In (Saha et al., impact on TAC. Initially, we came across several 2020c), authors proposed an attention based model SA and ER datasets for Twitter such as (Oleri built on top of the Transformer for predicting TAs. and Karagoz, 2016), (Mohammad and Kiritchenko, In (Saha et al., 2020a), authors proposed a capsule 2018), SemEval-2018 (Mohammad et al., 2018), based network built on top of BERT for TAC. All BTD (Wang et al., 2012), TEC (Mohammad, 2012), these works utilized only the textual modality to CBET (Shahraki and Zaiane, 2017), STS-Gold (Moidentify TAs without any sentiment or emotional hammad and Turney, 2013), STS (Go et al., 2009), correlation of the tweeter. In (Cerisara et al., 2018), SS-Twitter (Thelwall et al., 2012) etc. However, authors proposed a LSTM based study for jointly we chose to use SemEval-2018 dataset for further optimizing SA and TAC in a decentralized social investigation of our task at hand. The reason bemedia platfo"
2021.naacl-main.456,L18-1030,0,0.0485504,"Missing"
2021.naacl-main.456,D14-1162,0,0.0856125,"Missing"
2021.naacl-main.456,P17-1001,0,0.0517252,"Missing"
2021.naacl-main.456,2020.acl-main.402,1,0.90301,"peech acts has been studied extensively for dialogue conversations starting from early 2000’s with (Stolcke et al., 2000) being one of the benchmark works where the authors presented varieties of approaches such as Hidden Markov Models, Neural Networks and Decision Trees to identify dialogue acts on a benchmark dialogue data known as the Switchboard (SWBD) (Godfrey et al., 1992) dataset. In (Saha et al., 2021), authors studied the role of emotion in identifying dialogue acts for a dyadic conversation by considering thee textual and the audio modality of the utterances in the conversation. In (Saha et al., 2020b), authors proposed studying the role of emotion in determining dialogue acts on a dyadic and multi-party conversational dataset in a multi-modal framework (incorporating text, audio and video). However, tweets are unstructured and noisy communications with spelling mistakes, random coinages with limitations in expression because of character constraint per tweet. This makes it very different from face-to-face or other conversations. There exist plenty of works which address the task of TAC as a standalone problem. In (Zhang et al., 2011), (Vosoughi and Roy, 2016), authors proposed Machine Le"
2021.naacl-main.456,S12-1033,0,0.0297496,"r Twitter in order to gather tablished it to be one of the elementary steps for potentially emotionally rich tweets to explore its detection of rumours in Twitter. In (Saha et al., impact on TAC. Initially, we came across several 2020c), authors proposed an attention based model SA and ER datasets for Twitter such as (Oleri built on top of the Transformer for predicting TAs. and Karagoz, 2016), (Mohammad and Kiritchenko, In (Saha et al., 2020a), authors proposed a capsule 2018), SemEval-2018 (Mohammad et al., 2018), based network built on top of BERT for TAC. All BTD (Wang et al., 2012), TEC (Mohammad, 2012), these works utilized only the textual modality to CBET (Shahraki and Zaiane, 2017), STS-Gold (Moidentify TAs without any sentiment or emotional hammad and Turney, 2013), STS (Go et al., 2009), correlation of the tweeter. In (Cerisara et al., 2018), SS-Twitter (Thelwall et al., 2012) etc. However, authors proposed a LSTM based study for jointly we chose to use SemEval-2018 dataset for further optimizing SA and TAC in a decentralized social investigation of our task at hand. The reason bemedia platform called Mastodon. However, they hind this choice was that most of the ER datasets modelled th"
2021.naacl-main.456,J00-3003,0,0.15382,"Missing"
2021.smm4h-1.15,N19-1423,0,0.0389777,"., 2021). Our team participated in the first subtask that classifies tweets with Adverse Drug Effect (ADE) mentions. Our best performing model utilizes BERTweet followed by a single layer of BiLSTM. The system achieves an F-score of 0.45 on the test set without using any supplementary resources such as Part-of-Speech tags, dependency tags, or knowledge from medical dictionaries. 1 2. Strip ’#’ from hashtags in tweets 3. Drop user-mentions and URLs 4. Lowercase all words We used emoji1 package to translate emoji to text string. 3 We explore three BERT-based models for classification: (i) BERT (Devlin et al., 2019), (ii) RoBERTa (Liu et al., 2019), and (iii) BERTweet (Nguyen et al., 2020). We pass the input through our BERTbased models to get token representations. To compute the sentence representations, we consider two cases - i) [CLS] token (fine-tuning) ii) we pass token representations without [CLS] and [SEP] through a single layer BiLSTM and concatenate the forward and backward context. The sentence representation is passed through a fully connected neural network layer followed by a sigmoid activation to predict probabilities. To tackle class imbalance, we experiment with oversampling, undersampl"
2021.smm4h-1.15,2020.emnlp-demos.2,0,0.119582,"with Adverse Drug Effect (ADE) mentions. Our best performing model utilizes BERTweet followed by a single layer of BiLSTM. The system achieves an F-score of 0.45 on the test set without using any supplementary resources such as Part-of-Speech tags, dependency tags, or knowledge from medical dictionaries. 1 2. Strip ’#’ from hashtags in tweets 3. Drop user-mentions and URLs 4. Lowercase all words We used emoji1 package to translate emoji to text string. 3 We explore three BERT-based models for classification: (i) BERT (Devlin et al., 2019), (ii) RoBERTa (Liu et al., 2019), and (iii) BERTweet (Nguyen et al., 2020). We pass the input through our BERTbased models to get token representations. To compute the sentence representations, we consider two cases - i) [CLS] token (fine-tuning) ii) we pass token representations without [CLS] and [SEP] through a single layer BiLSTM and concatenate the forward and backward context. The sentence representation is passed through a fully connected neural network layer followed by a sigmoid activation to predict probabilities. To tackle class imbalance, we experiment with oversampling, undersampling, and addition of perclass penalties in the objective function. For over"
2021.wat-1.26,D18-1549,0,0.0160477,"related languages. 1 2 Introduction 2.1 Neural Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016) has become a de-facto for automatic translation of language pairs. NMT systems with Transformer (Vaswani et al., 2017) based architectures have achieved competitive accuracy on data-rich language pairs like English-French. However, NMT systems are datahungry, and only a few pairs of languages have abundant parallel data. For low resource setting, techniques like transfer learning (Zoph et al., 2016) and utilization of monolingual data in an unsupervised setting (Artetxe et al., 2018; Lample et al., 2017, 2018) have shown support for increasing the translation accuracy. Multilingual Neural Machine Translation is an ideal setting for low resource MT (Lakew et al., 2018) since it allows sharing of encoder-decoder parameters, word embeddings, and joint or separate vocabularies. It also enables zero-shot translations, i.e., translating between language pairs that were not seen during training (Johnson et al., 2017a). Related work Neural Machine Translation Neural Machine Translation architectures consist of encoder layers, attention layers, and decoder layers. NMT framework t"
2021.wat-1.26,Q17-1010,0,0.0391215,"mesh et al., 2021). 3 System overview In this section, we describe the details of the submitted systems to MultiIndicMT task at WAT2021. We report results for four types of models: 218 • Bilingual: Trained only using parallel data for a particular language pair (bilingual models). 1 https://github.com/anoopkunchukuttan/ indic_nlp_library • All-En: Multilingual many-to-one system trained using all available parallel data of all language pairs. of all languages into the same script, hence the choice of Devnagari as a common script is arbitrary. We use fastBPE3 to learn BPE (Byte pair encoding) (Bojanowski et al., 2017). For bilingual models, we use 60000 BPE codes over the combined tokenized data of both languages. The number of BPE codes is set to 100000 for All-En, and 80000 for DR-En and IA-En. • IA-En: Multilingual many-to-one system trained using Indo-Aryan languages from the provided parallel data. • DR-En: Multilingual many-to-one system trained using Dravidian languages from the provided parallel data. 4.3 To train our multilingual models, we use shared encoder-decoder transformer architecture. To handle the lexical gap between Indic languages in multilingual models, we convert the data of all Indic"
2021.wat-1.26,2020.coling-tutorials.3,0,0.0146361,"(Sennrich et al., 2016a) are used in almost all NMT models. Pivoting (Cheng et al., 2017) and Transfer Learning (Zoph et al., 2016) have leveraged the language relatedness by indirectly providing the model with more parallel data from related language pairs. 2.2 Multilingual Neural Machine Translation Multilingual NMT trains a single model utilizing data from multiple language-pairs to improve the performance. There are different approaches to incorporate multiple language pairs in a single system, like multi-way NMT, pivot-based NMT, transfer learning, multi-source NMT and, multilingual NMT (Dabre et al., 2020). Multilingual NMT came into picture because many languages share certain amount of vocabulary and share some structural similarity. These languages together can be utilized to improve the performance of NMT systems. In this paper, our focus is to analyze the performance of multi-source NMT. The simplest approach is to share the parameters of NMT model across multiple language pairs. These kinds of systems work better if languages are related to each other. In Johnson et al. (2017b), the encoder, decoder, and attention are shared for the training of multiple language pairs and a target languag"
2021.wat-1.26,N16-1101,0,0.0201385,"tain amount of vocabulary and share some structural similarity. These languages together can be utilized to improve the performance of NMT systems. In this paper, our focus is to analyze the performance of multi-source NMT. The simplest approach is to share the parameters of NMT model across multiple language pairs. These kinds of systems work better if languages are related to each other. In Johnson et al. (2017b), the encoder, decoder, and attention are shared for the training of multiple language pairs and a target language token is added at the beginning of target sentence while decoding. Firat et al. (2016) utilizes a shared attention mechanism to train multilingual models. Recently many approaches have been proposed, where monolingual data of multiple languages is utilized to pre-train a single model using different objectives like masked language modeling and denoising (Lample and Conneau, 2019; Song et al., 2019; Lewis et al., 2020; Liu et al., 2020). Multilingual pre-training followed by multilingual finetuning has also proven to be beneficial (Tang et al., 2020). 2.3 Language Relatedness Telugu, Tamil, Kannada, and Malayalam are Dravidian languages whose speakers are predominantly found in"
2021.wat-1.26,2020.findings-emnlp.445,0,0.0649613,"Missing"
2021.wat-1.26,P07-2045,0,0.0119638,"Missing"
2021.wat-1.26,J82-2005,0,0.674827,"Missing"
2021.wat-1.26,2020.acl-main.703,0,0.0612652,"Missing"
2021.wat-1.26,2020.tacl-1.47,0,0.0443476,"Missing"
2021.wat-1.26,P16-1162,0,0.0571976,"neural network, layer normalization, and residual connections. The decoder in the transformer has an additional encoder-attention layer that attends to the output states of the transformer encoder. 217 Proceedings of the 8th Workshop on Asian Translation, pages 217–223 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics NMT is data-hungry, and only a few pairs of languages have abundant parallel data. In recent years, NMT has been accompanied by several techniques to improve the performance of both low & high resource language pairs. Back-translation (Sennrich et al., 2016b) is used to augment the parallel data with synthetically generated parallel data by passing monolingual datasets to the previously trained models. Currently, NMT systems also perform on-the-fly back-translation to train the model simultaneously. Tokenization methods like Byte Pair Encoding (Sennrich et al., 2016a) are used in almost all NMT models. Pivoting (Cheng et al., 2017) and Transfer Learning (Zoph et al., 2016) have leveraged the language relatedness by indirectly providing the model with more parallel data from related language pairs. 2.2 Multilingual Neural Machine Translation Mult"
2021.wat-1.26,P16-1009,0,0.0243976,"neural network, layer normalization, and residual connections. The decoder in the transformer has an additional encoder-attention layer that attends to the output states of the transformer encoder. 217 Proceedings of the 8th Workshop on Asian Translation, pages 217–223 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics NMT is data-hungry, and only a few pairs of languages have abundant parallel data. In recent years, NMT has been accompanied by several techniques to improve the performance of both low & high resource language pairs. Back-translation (Sennrich et al., 2016b) is used to augment the parallel data with synthetically generated parallel data by passing monolingual datasets to the previously trained models. Currently, NMT systems also perform on-the-fly back-translation to train the model simultaneously. Tokenization methods like Byte Pair Encoding (Sennrich et al., 2016a) are used in almost all NMT models. Pivoting (Cheng et al., 2017) and Transfer Learning (Zoph et al., 2016) have leveraged the language relatedness by indirectly providing the model with more parallel data from related language pairs. 2.2 Multilingual Neural Machine Translation Mult"
2021.wat-1.26,D16-1163,0,0.118517,"nce by training a multilingual NMT system using languages of the same family, i.e., related languages. 1 2 Introduction 2.1 Neural Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016) has become a de-facto for automatic translation of language pairs. NMT systems with Transformer (Vaswani et al., 2017) based architectures have achieved competitive accuracy on data-rich language pairs like English-French. However, NMT systems are datahungry, and only a few pairs of languages have abundant parallel data. For low resource setting, techniques like transfer learning (Zoph et al., 2016) and utilization of monolingual data in an unsupervised setting (Artetxe et al., 2018; Lample et al., 2017, 2018) have shown support for increasing the translation accuracy. Multilingual Neural Machine Translation is an ideal setting for low resource MT (Lakew et al., 2018) since it allows sharing of encoder-decoder parameters, word embeddings, and joint or separate vocabularies. It also enables zero-shot translations, i.e., translating between language pairs that were not seen during training (Johnson et al., 2017a). Related work Neural Machine Translation Neural Machine Translation architect"
2021.wat-1.28,W14-4012,0,0.148208,"Missing"
2021.wat-1.28,P15-1166,0,0.142901,"troduction In recent years, the Neural Machine Translation (NMT) systems (Vaswani et al., 2017; Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014) have consistently outperformed the Statistical Machine Translation (SMT) (Koehn, 2009) systems. One of the major problems with NMT systems is that they are data hungry, which means that they require a large amount of parallel data to give better performance. This becomes a very challenging task while working with low-resource language pairs for which a very less amount of parallel corpora is available. Multilingual NMT (MNMT) systems (Dong et al., 2015; Johnson et al., 2017) alleviate this issue by using the phenomenon of transfer learning among related languages, which are the languages that are related by genetic and contact relationships. (Kunchukuttan and Bhattacharyya, 2020) have shown that the lexical and orthographic similarity among languages can be utilized to improve translation quality between Indic languages when limited parallel corpora is available. Another advantage of using MNMT systems is that they support zero-shot translation, that is, translation among two languages for which no parallel corpora is available during train"
2021.wat-1.28,N16-1101,0,0.0373509,"this paper, we describe the two MNMT systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task (Nakazawa et al., 2021) as team ’CFILT’, namely one-to-many for English to Indic languages and many-to-one for Indic languages to English. This task covers 10 Indic languages which are Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu. 2 Related Work Dong et al. (2015) was the first to introduce MNMT. The authors used a one-to-many model where a separate decoder and an attention mechanism was used for each target language. Firat et al. (2016) extended this to a many-to-many setting using a shared attention mechanism. In Zoph and Knight (2016) a multi-source translation approach was proposed where multiple encoders were used, each having a separate attention mechanism. Lee et al. (2017) proposed a CNN-based character level approach where a single encoder was shared across all the source languages. A second line of work on MNMT uses a single shared encoder and decoder (Ha et al., 2016; Johnson et al., 2017) irrespective of the number of languages on the source or the target side. An 233 Proceedings of the 8th Workshop on Asian Trans"
2021.wat-1.28,D10-1092,0,0.0757509,"Missing"
2021.wat-1.28,P09-5002,0,0.0186512,"lish. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration. 1 Introduction In recent years, the Neural Machine Translation (NMT) systems (Vaswani et al., 2017; Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014) have consistently outperformed the Statistical Machine Translation (SMT) (Koehn, 2009) systems. One of the major problems with NMT systems is that they are data hungry, which means that they require a large amount of parallel data to give better performance. This becomes a very challenging task while working with low-resource language pairs for which a very less amount of parallel corpora is available. Multilingual NMT (MNMT) systems (Dong et al., 2015; Johnson et al., 2017) alleviate this issue by using the phenomenon of transfer learning among related languages, which are the languages that are related by genetic and contact relationships. (Kunchukuttan and Bhattacharyya, 202"
2021.wat-1.28,Q17-1026,0,0.0206279,"c languages to English. This task covers 10 Indic languages which are Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu. 2 Related Work Dong et al. (2015) was the first to introduce MNMT. The authors used a one-to-many model where a separate decoder and an attention mechanism was used for each target language. Firat et al. (2016) extended this to a many-to-many setting using a shared attention mechanism. In Zoph and Knight (2016) a multi-source translation approach was proposed where multiple encoders were used, each having a separate attention mechanism. Lee et al. (2017) proposed a CNN-based character level approach where a single encoder was shared across all the source languages. A second line of work on MNMT uses a single shared encoder and decoder (Ha et al., 2016; Johnson et al., 2017) irrespective of the number of languages on the source or the target side. An 233 Proceedings of the 8th Workshop on Asian Translation, pages 233–237 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics en-bn en-gu en-hi en-kn en-ml en-mr en-or en-pa en-ta en-te ALT Bible-uedin CVIT-PIB IITB 3.0 MTEnglish2Odia NLPC OdiEnCorp 2.0 Open"
2021.wat-1.28,N19-4009,0,0.0226828,"ystem architecture details model. In our one-to-many model, we used the transformer architecture with a single encoder and a single shared decoder. The encoder used the English vocabulary and the decoder used a shared vocabulary of all the Indic languages. In our manyto-one model, we used the transformer architecture with a single shared encoder and a single decoder. Here the encoder used a shared vocabulary of all the Indic languages and English vocabulary is used for the decoder. In both of these MNMT models, we prepended a language specific token to the input sentence. We used the fairseq (Ott et al., 2019) library for implementing the multilingual systems. For training, we used Adam optimizer with betas ’(0.9,0.98)’. The initial learning rate used was 0.0005 and the inverse square root learning rate scheduler was used with 4000 warm-up updates. The dropout probability value used was 0.3 and the criterion used was label smoothed cross entropy with label smoothing of 0.1. We used an update frequency, that is, after how many batches the backward pass is performed, of 8 for the multilingual models and 4 for the bilingual baseline models. During decoding we used the beam search algorithm with a beam"
2021.wat-1.28,P02-1040,0,0.109674,"Missing"
2021.wat-1.28,P16-1162,0,0.0345475,"s as those performed in WAT 2021. The many-to-one results are the official evaluation results provided by the organizers of WAT 2021.(bn:Bengali, gu:Gujarati, hi:Hindi, kn:Kannada, ml:Malayalam, mr:Marathi, or:Oriya, pa:Punjabi, ta:Tamil, te:Telugu) language pairs along with the number of parallel sentences. The validation and test sets have 1,000 and 2,390 sentences, respectively and are 11-way parallel. 8000 merge operations are used for learning the BPE codes of the one-to-many, many-to-one and bilingual baseline models, respectively. 4.4 4.3 Preprocessing We used Byte Pair Encoding (BPE) (Sennrich et al., 2016) technique for data segmentation, that is, break up the words into sub-words. This technique is especially helpful for Indic languages as they are morphologically rich. Separate vocabularies are used for the source and target side languages. For training the one-to-many and many-to-one models, the data of all the 10 Indic languages is combined before learning the BPE codes. 48000, 48000 and Baseline Models The baseline MT models are bilingual MT models based on the vanilla transformer architecture. We have trained 20 separate bilingual MT models, 10 for English to each Indic language and 10 fo"
2021.wat-1.28,N16-1004,0,0.0221781,"An Indic Language Multilingual Task (Nakazawa et al., 2021) as team ’CFILT’, namely one-to-many for English to Indic languages and many-to-one for Indic languages to English. This task covers 10 Indic languages which are Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu. 2 Related Work Dong et al. (2015) was the first to introduce MNMT. The authors used a one-to-many model where a separate decoder and an attention mechanism was used for each target language. Firat et al. (2016) extended this to a many-to-many setting using a shared attention mechanism. In Zoph and Knight (2016) a multi-source translation approach was proposed where multiple encoders were used, each having a separate attention mechanism. Lee et al. (2017) proposed a CNN-based character level approach where a single encoder was shared across all the source languages. A second line of work on MNMT uses a single shared encoder and decoder (Ha et al., 2016; Johnson et al., 2017) irrespective of the number of languages on the source or the target side. An 233 Proceedings of the 8th Workshop on Asian Translation, pages 233–237 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computationa"
2021.wat-1.29,N19-1388,0,0.0761137,"paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively"
2021.wat-1.29,2020.findings-emnlp.223,0,0.140666,"between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indi"
2021.wat-1.29,W19-5308,0,0.0178398,"a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingual 117,757 125,647 156,605 79,433 82,026 120,362 103,876 90,916 91,324 111,749 109,480 Table 1: Language wise training set sizes in terms of number of sentences. Parallel: Parallel corpus"
2021.wat-1.29,2020.coling-tutorials.3,0,0.0360101,"Missing"
2021.wat-1.29,D17-1158,0,0.0179712,"nnada (KN) Malayalam (ML) Marathi (MR) Odia (OR) Punjabi (PA) Tamil (TA) Telugu (TE) English (EN) Improving the quality of NMT models with monolingual data is a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingual 117,757 125,647 156,605"
2021.wat-1.29,N16-1101,0,0.0544852,"3.79 respectively. 1 Introduction In this paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each oth"
2021.wat-1.29,2020.wmt-1.66,0,0.17654,"submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when comp"
2021.wat-1.29,D10-1092,0,0.20704,"as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indic data. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Multilingual Machine Translation enabled the ability to deploy a single model for multiple language pairs without training multiple models. Dong et al. (2015) proposes a multi-task learning framework to translate one source languag"
2021.wat-1.29,P17-4012,0,0.0421798,"idden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as described in Section 3.2. After creating synthetic parallel corpora, baseline models are further trained for another 200,000 steps 8 on combined synthetic and clean parallel corpora (see Table 2). Finally, all checkpoints that are created by the model using the combined corpora are averaged 9 and consi"
2021.wat-1.29,P15-1166,0,0.227793,"of 8.51, 6.25 and 3.79 respectively. 1 Introduction In this paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are"
2021.wat-1.29,P18-1007,0,0.0229244,"training set after merging clean corpus with synthetic back-translated corpus. XX → EN: Indic-toEnglish model. EN → XX: English-to-Indic model. trained on the Transformer architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as des"
2021.wat-1.29,D18-2012,0,0.0203765,"corpus with synthetic back-translated corpus. XX → EN: Indic-toEnglish model. EN → XX: English-to-Indic model. trained on the Transformer architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as described in Section 3.2. After creating synthetic"
2021.wat-1.29,I17-2050,0,0.0153507,"e XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The model"
2021.wat-1.29,P02-1040,0,0.110046,"e suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indic data. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Multilingual Machine Translation enabled the ability to deploy a single model for multiple language pairs without training multiple models. Dong et al. (2015) proposes a multi-task learning framework t"
2021.wat-1.29,Y18-3012,1,0.881082,"tion Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works"
2021.wat-1.29,P16-1009,0,0.206728,"training time. Aharoni et al. (2019) show that multilingual NMT models are capable of handling large number of language pairs. Freitag and Firat (2020) proposes that the use of multi-way alignment information will improve the translation quality of language pairs for which training data is scarce in multilingual settings. 3 This section describes datasets, preprocessing and experimental setup of our models. 3.1 In this work, we follow Johnson et al. (2017) method to train multilingual NMT models. We romanize Indic data and use it to train our models. We also follow back-translation approach (Sennrich et al., 2016) to create synthetic parallel data. We report the results of the models which are trained on combined synthetic and clean parallel corpus. Datasets We use MultiIndicMT parallel corpus 2 consisting of following languages: Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu and English. It contains the parallel corpora for 10 Indic languages which are translated into English. We also use PMI monolingual corpus 3 to generate synthetic data with back-translation (Sennrich et al., 2016) approach. Table 1 shows the data sizes of corpora used in the experiments. Develo"
2021.wat-1.29,D16-1160,0,0.0194489,"ati (GU) Hindi (HI) Kannada (KN) Malayalam (ML) Marathi (MR) Odia (OR) Punjabi (PA) Tamil (TA) Telugu (TE) English (EN) Improving the quality of NMT models with monolingual data is a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingua"
2021.wat-1.29,D16-1163,0,0.0198568,"ne for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic"
ar-etal-2012-cost,C10-1063,1,\N,Missing
ar-etal-2012-cost,D11-1100,1,\N,Missing
ar-etal-2012-cost,P96-1042,0,\N,Missing
ar-etal-2012-cost,P02-1053,0,\N,Missing
ar-etal-2012-cost,W02-1011,0,\N,Missing
ar-etal-2012-cost,W10-1808,0,\N,Missing
bellare-etal-2004-generic,J98-3005,0,\N,Missing
bellare-etal-2004-generic,W97-0703,0,\N,Missing
bellare-etal-2004-generic,P00-1038,0,\N,Missing
bellare-etal-2004-generic,C96-2166,0,\N,Missing
bellare-etal-2004-generic,W01-0100,0,\N,Missing
C08-1068,W98-1005,0,0.342414,"ose languages. We introduce UIT (universal intermediate transcription) for the same pair on the basis of their common phonetic repository in such a way that it can be extended to other languages like Arabic, Chinese, English, French, etc. We describe a transliteration model based on FST and UIT, and evaluate it on Hindi and Urdu corpora. 1 Introduction Transliteration is mainly used to transcribe a word written in one language in the writing system of the other language, thereby keeping an approximate phonetic equivalence. It is useful for MT (to create possible equivalents of unknown words) (Knight and Stall, 1998; Paola and Sanjeev, 2003), cross-lingual information retrieval (Pirkola et al, 2003), the development of multilingual resources (Yan et al, 2003) and multilingual text and speech processing. Inter-dialectal translation without lexical changes is quite useful and sometimes even necessary when the dialects in question use different scripts; it can be achieved by transliteration alone. That is the case of HUMT (Hindi-Urdu Machine Transliteration) where each word has to be transliterated from Hindi to Urdu and vice versa, irrespective of its © 2008. Licensed under the Creative Commons Attribution"
C08-1068,P06-1143,0,0.0126056,"Conference on Computational Linguistics (Coling 2008), pages 537–544 Manchester, August 2008 2 HUMT 3 There exist three languages at the border between India and Pakistan: Kashmiri, Punjabi and Sindhi. All of them are mainly written in two scripts, one being a derivation of the Persio-Arabic script and the other being Devanagari script. A person using the Persio-Arabic script cannot understand the Devanagari script and vice versa. The same is true for Hindi and Urdu which are varieties or dialects of the same language, called Hindustani by Platts (1909). PMT (Punjabi Machine Transliteration) (Malik, 2006) was a first effort to bridge this scriptural divide between the two scripts of Punjabi namely Shahmukhi (a derivation of Perio-Arabic script) and Gurmukhi (a derivation of Landa, Shardha and Takri, old Indian scripts). HUMT is a logical extension of PMT. Our HUMT system is generic and flexible such that it will be extendable to handle similar cases like Kashmiri, Punjabi, Sindhi, etc. HUMT is also a special type of machine transliteration like PMT. A brief account of Hindi and Urdu is first given for unacquainted readers. 2.1 Hindi The Devanagari (literally “godly urban”) script, a simplified"
C08-1068,W04-1613,0,0.115926,"هh] = [ ﮐﻬkʰ], [ بb] + [ هh] = ﺑﻬ [bʰ], [ لl] + [ هh] = [ ﻟﻬlʰ], etc. The UIT mapping for aspirated consonants is given in Table 2. Xì y6[6Ei ÌòâF¯ ÌÐ y636G6[6 zEegEZ [ʊrḓu pɑkɪstɑn ki qɔmi zubɑn hæ] (Urdu is the National Language of Pakistan.) 538 Hindi भ [ ﺑﻬbʰ] Urdu UIT b_h Hindi हर [ رهrʰ] Urdu UIT r_h फ [ ﭘﻬpʰ] p_h ढ़ [ ڑهɽʰ] r`_h थ [ ﺗﻬṱʰ] t_d_h ख [ ﮐﻬkʰ] k_h ठ [ ﭨﻬʈʰ] t`_h घ [ ﮔﻬgʰ] g_h झ [ ﺟﻬʤʰ] d_Z_h लह [ ﻟﻬlʰ] l_h छ [ ﭼﻬʧʰ] t_S_h मह [ ﻣﻬmʰ] m_h ध [ دهḓʰ] d_d_h नह [ ﻧﻬnʰ] n_h ढ Urdu contains 10 vowels and 7 of them have nasalized forms (Hussain, 2004; Khan, 1997). Urdu vowels are represented using four long vowels (Alef Madda ()ﺁ, Alef ()ا, Vav ( )وand Choti Yeh ( ))ﯼand three short vowels (Arabic Fatha – Zabar َ-, Arabic Damma – Pesh ُ- and Arabic Kasra – Zer ِ-). Vowel representation is contextsensitive in Urdu. Vav ( )وand Choti Yeh ( )ﯼare also used as consonants. Hamza ( )ءis a place holder between two successive vowel sounds, e.g. in [ ﮐﻤﺎﺋﯽkəmɑi] (earning), Hamza ( )ءseparates the two vowel sounds Alef ([ )اɑ] and Choti Yeh ([ )ﯼi]. Noonghunna ( )ںis used as nasalization marker. Analysis and mapping of Hindi"
C08-1068,J98-4003,0,\N,Missing
C08-1068,Y06-1050,0,\N,Missing
C08-2007,W07-1104,0,0.0539931,"Missing"
C08-2007,P06-1101,0,0.0215253,"Missing"
C10-1063,E09-1006,0,0.0556241,"Missing"
C10-1063,C96-1005,0,0.55873,"Missing"
C10-1063,P07-1005,0,0.101757,"Missing"
C10-1063,D09-1048,1,0.570253,"curacy. 555 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 555–563, Beijing, August 2010 cost of annotation. Finally, we propose a measure for cost-benefit analysis which identifies the optimal point of balance between these three related entities, viz., cross-linking, sense annotation and accuracy of disambiguation. The remainder of this paper is organized as follows. In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of Khapra et al. (2009) on parameter projection for multilingual WSD. Section 5 is on the economics of multilingual WSD. In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets. In section 7 we present a strategy for injecting hard-to-disambiguate cases from the target language using selective sampling. In section 8 we introduce a measure for cost-benefit analysis for calculating the value for money in terms of accuracy, annotation effort and lexicon building effort. In section 9 we describe the experimental setup. In section 10 we present the results followed by disc"
C10-1063,W04-0834,0,0.784073,"er’s intuition. We extend their work by addressing the following question on the economics of annotation, lexicon building and performance: • Is there an optimal point of balance between the annotation effort and the lexicon building (i.e. manual cross-linking) effort at which one can be assured of best value for money in terms of accuracy? Word Sense Disambiguation (WSD) is one of the most widely investigated problems of Natural Language Processing (NLP). Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004) and knowledge based approaches (Mihalcea, 2005). HowTo address the above question we first propose a probabilistic cross linking model to eliminate the effort of manually cross linking words within the source and target language synsets and calibrate the resultant trade-off in accuracy. Next, we show that by injecting examples for most frequent hard-to-disambiguate words from the target domain one can achieve higher accuracies at optimal Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on mu"
C10-1063,H05-1052,0,0.289278,"of annotation, lexicon building and performance: • Is there an optimal point of balance between the annotation effort and the lexicon building (i.e. manual cross-linking) effort at which one can be assured of best value for money in terms of accuracy? Word Sense Disambiguation (WSD) is one of the most widely investigated problems of Natural Language Processing (NLP). Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004) and knowledge based approaches (Mihalcea, 2005). HowTo address the above question we first propose a probabilistic cross linking model to eliminate the effort of manually cross linking words within the source and target language synsets and calibrate the resultant trade-off in accuracy. Next, we show that by injecting examples for most frequent hard-to-disambiguate words from the target domain one can achieve higher accuracies at optimal Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on multilingual WSD has shown that it is possible to leverage the annotation work done for WS"
C10-1063,P96-1006,0,\N,Missing
C10-2040,A00-1031,0,0.187331,"Missing"
C10-2040,J95-4004,0,0.280771,"Missing"
C10-2040,W99-0633,0,0.0524016,"Missing"
C10-2040,A94-1024,0,0.180209,"Missing"
C10-2040,W96-0213,0,0.293228,"Missing"
C10-2040,P06-2100,1,0.863614,"Missing"
C10-2040,A92-1018,0,\N,Missing
C10-2040,H92-1023,0,\N,Missing
C10-2091,kang-choi-2000-automatic,0,0.0523799,"n to objective evaluations. 1 Introduction Transliteration refers to phonetic translation across two languages with different writing systems, such as Arabic to English (Arbabi et al., 1994; Stall and Knight, 1998; Al-Onaizan and Knight, 2002; AbdulJaleel and Larkey, 2003). Most prior work on transliteration has been done for MT of English, Arabic, Japanese, Chinese, Korean, etc., for CLIR (Lee and Choi., 1998; Jeong et al., 1999; Fujii and Ishikawa, 2001; Sakai et al., 2002; Pirkola et al., 2003; Virga and Khudanpur, 2003; Yan et al., 2003), and for the development of multilingual resources (Kang and Choi, 2000; Yan, Gregory et al., 2003). The terms transliteration and transcription are often used as generic terms for various processes like transliteration, transcription, romanization, transcribing and technography (Halpern, 2002). In general, the speech processing community uses the term transcription to denote a process of conversion from the script or writing system to the sound (phonetic representation). For examPushpak Bhattacharyya IIT Bombay pb@iitb.ac.in ple, the transcription of the word “love” in the International Phonetic Alphabet (IPA) is [ləv]. While the text processing community uses t"
C10-2091,J94-3001,0,0.0919442,"4: System Architecture of fintie-state scriptural translation ज़ [ ذz] z1 म [ مm] m 5.1 र [ رr] r न [ نn] n उ [ ڑɽ] r` व [ وv] v ज़ [ زz] z ह [ ﮦh] h ज़ [ ژʒ] Z य [ ﯼj] j स [ سs] s त [ ةṱ] t_d2 श [ شʃ] S ण - [ɳ] n` Both conversions of the source language text into the UIT encoded text and from the UIT encoded text into the target language text are regular relations on strings. Moreover, regular relations are closed under serial composition and a finite set of conversion relations when applied to each other’s output in a specific order, also defines a regular expression (Kaplan and Kay, 1994). Thus we model the conversions from the source language to UIT and from UIT to the target language as finite-state transducers. These translational transducers can be deterministic and nondeterministic. Character Mappings: Table 5 shows regular relations for converting Hindi and Urdu aspirated consonants into UIT. ष ◌ं S1 ~ [ شʃ] [ ںŋ] Table 4: UIT encodings of Urdu non-aspirated consonants 5 tokens using the UIT to target language conversion transducer from the repertoire of Transducers. Finally, Text Generator generates the target language text from the translated target language tokens"
C10-2091,W09-3536,1,0.818499,"ndard code page for Shahmukhi does not exist. Similar problems also exist for the Kashmiri and Seraiki languages. 3.3 Absence of necessary information There are cases where the necessary and indispensable information for scriptural translation are missing in the source text. For example, the first word [ دﻧﻴﺎḓʊnɪjɑ] (world) of the example sentence of Figure 1 misses crucial diacritical information, mandatory to perform Urdu to Hindi scriptural translation. Like in Arabic, diacritical marks are part of the Urdu writing system but are sparingly used in writings (Zia, 1999; Malik et al., 2008; Malik et al., 2009). Figure 2(a) shows the example word without diacritical marks and its wrong Hindi conversion according to conversion rules (explained later). The Urdu community can understand the word in its context or without the context because people are tuned to understand the Urdu text or word without diacritical marks, but the Hindi conversion of Figure 2(a) is not at all acceptable or readable in the Hindi community. Figure 2(b) shows the example word with diacritical marks and its correct Hindi conversion according to conversion rules. Similar problems also arise for the other Indo-Pak languages. The"
C10-2091,C08-1068,1,0.794573,"and till date, a standard code page for Shahmukhi does not exist. Similar problems also exist for the Kashmiri and Seraiki languages. 3.3 Absence of necessary information There are cases where the necessary and indispensable information for scriptural translation are missing in the source text. For example, the first word [ دﻧﻴﺎḓʊnɪjɑ] (world) of the example sentence of Figure 1 misses crucial diacritical information, mandatory to perform Urdu to Hindi scriptural translation. Like in Arabic, diacritical marks are part of the Urdu writing system but are sparingly used in writings (Zia, 1999; Malik et al., 2008; Malik et al., 2009). Figure 2(a) shows the example word without diacritical marks and its wrong Hindi conversion according to conversion rules (explained later). The Urdu community can understand the word in its context or without the context because people are tuned to understand the Urdu text or word without diacritical marks, but the Hindi conversion of Figure 2(a) is not at all acceptable or readable in the Hindi community. Figure 2(b) shows the example word with diacritical marks and its correct Hindi conversion according to conversion rules. Similar problems also arise for the other In"
C10-2091,W98-1005,0,\N,Missing
C10-2091,W02-1206,0,\N,Missing
C10-2091,W02-0505,0,\N,Missing
C10-2091,knight-al-onaizan-1998-translation,0,\N,Missing
C12-1113,P05-2008,0,0.0287044,"Missing"
C12-1113,D11-1015,0,0.068417,"Missing"
C12-1113,I11-1038,0,0.339016,"ing discourse coherent structures and found different kinds of crossed dependencies. In the work, Contextual Valence Shifters (Polanyi et al., 2004), the authors investigate the effect of intensifiers, negatives, modals and connectors that changes the prior polarity or valence of the words and brings out a new meaning or perspective. They also talk about pre-suppositional items and irony and present a simple weighting scheme to deal with them. Somasundaran et al. (2009) and Asher et al. (2008) discuss some discourse-based supervised and unsupervised approaches to opinion analysis. Zhou et al. (2011) present an approach to identify discourse relations as identified by RST. Instead of depending on cue-phrase based methods to identify discourse relations, they leverage it to adopt an unsupervised approach that would generate semantic sequential representations (SSRs) without cue phrases. Taboada et al. (2008) leverage discourse to identify relevant sentences in the text for sentiment analysis. However, they narrow their focus to adjectives alone in the relevant portions of the text while ignoring the remaining parts of speech of the text. Most of these discourse based works make use of a di"
C12-1113,C08-2002,0,\N,Missing
C12-1113,J11-2001,0,\N,Missing
C12-1113,D11-1100,1,\N,Missing
C12-1113,P09-1077,0,\N,Missing
C12-1113,P06-2079,0,\N,Missing
C12-1113,C10-2005,0,\N,Missing
C12-1113,J05-2005,0,\N,Missing
C12-1113,W02-1011,0,\N,Missing
C12-1113,P11-2102,0,\N,Missing
C12-1113,pak-paroubek-2010-twitter,0,\N,Missing
C12-1113,D07-1010,0,\N,Missing
C12-1113,P11-4022,1,\N,Missing
C12-1114,P04-1036,0,\N,Missing
C12-2008,D08-1014,0,0.0167976,"synset-based features perform better than word-based features for sentiment analysis. Here, we carry out our study on two widely spoken Indian languages: Hindi and Marathi. These languages belong to the Indo-Aryan subgroup of the Indo-European language family. For these two languages, we first verify the superiority of sense-based features over word-based features for SA. Thereafter we proceed to verify the efficacy of the sense-based approach for cross-lingual sentiment analysis for these two languages. This work differs from existing works(Brooke et al., 2009; Wan, 2009; Wei and Pal, 2010; Banea et al., 2008) on CLSA in two aspects: (i) our focus is not necessarily to use a resource-rich language to help a resource-scarce language but can be applied to any two languages which share a common sense space (by using WordNets with matching synset identifiers); (ii) our work is an alternative to MT-based cross-lingual sentiment analysis for languages which do not have an MT system between them. 2 Background Study: Word Senses for SA In our previous work (Balamurali et al., 2011), we showed that word senses act as better features than lexeme-based features for document level SA. We termed this feature sp"
C12-2008,bhattacharyya-2010-indowordnet,1,0.681629,"used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets in the source language (Ls ) WordNet. As a consequence, corresponding concepts in Ls and L t have the same synset (concept) identifier. Our work leverages this fact, and uses WordNet senses as features for building a classifier in L t r ain . The document to be tested for polarity is preprocessed by replacing words in this document with the corresponding synset identifiers. This step eliminates the distinction between L t r ain and L t est as far as the document is concerned"
C12-2008,R09-1010,0,0.168504,"lamurali et al., 2011) where we showed that WordNet synset-based features perform better than word-based features for sentiment analysis. Here, we carry out our study on two widely spoken Indian languages: Hindi and Marathi. These languages belong to the Indo-Aryan subgroup of the Indo-European language family. For these two languages, we first verify the superiority of sense-based features over word-based features for SA. Thereafter we proceed to verify the efficacy of the sense-based approach for cross-lingual sentiment analysis for these two languages. This work differs from existing works(Brooke et al., 2009; Wan, 2009; Wei and Pal, 2010; Banea et al., 2008) on CLSA in two aspects: (i) our focus is not necessarily to use a resource-rich language to help a resource-scarce language but can be applied to any two languages which share a common sense space (by using WordNets with matching synset identifiers); (ii) our work is an alternative to MT-based cross-lingual sentiment analysis for languages which do not have an MT system between them. 2 Background Study: Word Senses for SA In our previous work (Balamurali et al., 2011), we showed that word senses act as better features than lexeme-based featur"
C12-2008,N10-1120,0,0.0497777,"Missing"
C12-2008,W02-1011,0,0.0159694,"Missing"
C12-2008,schulz-etal-2010-multilingual,0,0.0970841,"Missing"
C12-2008,P09-1027,0,0.839269,"an opinion in a text. Though the majority of the work in SA is for English, there has been work in other languages as well such as Chinese, Japanese, German and Spanish (Seki et al., 2007; Nakagawa et al., 2010; Schulz et al., 2010). To perform SA on these languages, cross-lingual approaches are often used due to the lack of annotated content in these languages. In Cross-Lingual Sentiment Analysis (CLSA), the training corpus in one language (call it L t r ain ) is used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets"
C12-2008,P10-2048,0,0.800561,"in a text. Though the majority of the work in SA is for English, there has been work in other languages as well such as Chinese, Japanese, German and Spanish (Seki et al., 2007; Nakagawa et al., 2010; Schulz et al., 2010). To perform SA on these languages, cross-lingual approaches are often used due to the lack of annotated content in these languages. In Cross-Lingual Sentiment Analysis (CLSA), the training corpus in one language (call it L t r ain ) is used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets in the source langu"
C12-2008,D11-1100,1,\N,Missing
C12-2023,C94-1087,0,0.0143373,"Missing"
C12-2023,J95-3006,0,0.464557,"Missing"
C12-2023,W10-3604,1,0.854743,"Missing"
C12-3013,2012.freeopmt-1.4,0,0.152824,"Missing"
C12-3013,W06-3205,0,0.0330114,"Missing"
C12-3013,W09-4615,0,\N,Missing
C12-3013,W05-0617,0,\N,Missing
C12-3013,J01-2001,0,\N,Missing
C12-3013,J11-2002,0,\N,Missing
C12-3031,S07-1068,0,0.187128,"ides only a single algorithm and does not enable the user to compare the performance of different algorithms. Our system is a one-stop-shop for comparing several algorithms (including UKB, IMS and SenseRelate) with minimum computational and manual overhead for the user. We would like to mention that internally our system uses the implementations provided by UKB, IMS and SenseRelate and hence it would provide the same results as obtained by independently downloading and using these systems. Apart from UKB, IMS and SenseRelate, our system also provides an implementation for McCarthy’s approach (Koeling and McCarthy, 2007) and IWSD (Khapra et al., 2010). 3 System Details Figure 1 shows the main interface of our system. We first provide an overview of the system introducing the inputs which it expects followed by explaining the online output viewer, which is an interesting feature of our system. We also provide details about the mechanism with which new algorithms can be easily added to the system. Kindly refer to the figure while reading this section. 3.1 Interface Design To support various web browsers on different operating systems, we have designed the web interface using standard open technologies. The inte"
C12-3031,P05-3014,0,0.390396,"his system demonstration, we explain the system which powers our interface. The paper is organized as follows: We describe the existing, publicly available systems in section 2. In section 3 we provide technical details about our system. Section 4 summarizes the evaluation results on 3 standard datasets. Section 5 concludes the paper presenting the salient points of our system and some future enhancements for our system. 2 Related Work There are a few algorithms for which the implementation is publicly available. These include UKB (Agirre et al., 2009), IMS (Zhong and Ng, 2010), SenseLearner (Mihalcea and Csomai, 2005) and SenseRelate (Patwardhan et al., 2005). However, most of these have one or more of the overheads listed above. For example, UKB is currently available only for linux platforms. Further, the user needs to download and install these systems separately and run them on her/his machine which increases the computational cost. SenseLearner has an online interface, but in contrast to our system, it provides only a single algorithm and does not enable the user to compare the performance of different algorithms. Our system is a one-stop-shop for comparing several algorithms (including UKB, IMS and S"
C12-3031,P05-3019,0,0.785467,"-the-art algorithms. There is no overhead for the user, all (s)he needs is a web browser and the input file which may be sense tagged. Further, we also make provision for the developers of the new algorithms to integrate their algorithm in our system. This can be done by implementing a java interface exposed by us and upload the class file on our web-page. Some of the important aspects of our system are as follows: 1. Collection of several approaches - Users can obtain results for state-of-the-art approaches like IMS (Zhong and Ng, 2010), PPR (Agirre et al., 2009), knowledge based approaches (Patwardhan et al., 2005) etc, for an easy comparison of all approaches on a single dataset. 2. Parallel execution of several algorithms - The user can choose to run multiple algorithms in parallel, over the same dataset. The associated overhead of scheduling jobs and managing system resources is handled by the server and the user is exempted of these hassles. 3. Minimum supervision - After submitting his/her request, the end user can continue with their work without having to constantly monitor the task. Our interface notifies the user when the results are available. Once the users is notified, (s)he needs to downloa"
C12-3031,P10-4014,0,0.478735,"ve developed an online system, which allows the user to run several state-of-the-art algorithms. There is no overhead for the user, all (s)he needs is a web browser and the input file which may be sense tagged. Further, we also make provision for the developers of the new algorithms to integrate their algorithm in our system. This can be done by implementing a java interface exposed by us and upload the class file on our web-page. Some of the important aspects of our system are as follows: 1. Collection of several approaches - Users can obtain results for state-of-the-art approaches like IMS (Zhong and Ng, 2010), PPR (Agirre et al., 2009), knowledge based approaches (Patwardhan et al., 2005) etc, for an easy comparison of all approaches on a single dataset. 2. Parallel execution of several algorithms - The user can choose to run multiple algorithms in parallel, over the same dataset. The associated overhead of scheduling jobs and managing system resources is handled by the server and the user is exempted of these hassles. 3. Minimum supervision - After submitting his/her request, the end user can continue with their work without having to constantly monitor the task. Our interface notifies the user w"
C16-1047,L16-1429,1,0.902536,"Missing"
C16-1047,bakliwal-etal-2012-hindi,0,0.0468051,"ling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark datasets and other resources/tools in Indian languages. In our work, we focus on sentiment analysis in Hindi, the official language of India and the fourth most spoken language all over in the world. We make use of benchmark datasets released as part of a shared task on sentiment analysis in Indian languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have created a dataset for aspe"
C16-1047,C12-2008,1,0.309392,"etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark datasets and other resources/tools in Indian languages. In our work, we focus on sentiment analysis in Hindi, the official language of India and the fourth most spoken language all over in the world. We make use of benchmark datasets released as part of a shared task on sentiment analysis in Indian languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have cre"
C16-1047,C14-1008,0,0.0230143,"2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In this work, we do not use the tra"
C16-1047,P97-1023,0,0.0611487,"Missing"
C16-1047,C04-1200,0,0.0321456,"00,000 tweets per minute2 . At the same time more than 26K user reviews are posted on Yelp, an online user review portal. This tremendous amount of semi-structured data poses a great challenge in its efficient processing for any specific purpose. Sentiment analysis for web generated content e.g. tweets and online reviews, is a cumbersome problem mainly due to its unstructured and noisy nature (e.g. gr8, g8 etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the sc"
C16-1047,D14-1181,0,0.00304412,"dings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In thi"
C16-1047,S13-2053,0,0.0329837,"nute2 . At the same time more than 26K user reviews are posted on Yelp, an online user review portal. This tremendous amount of semi-structured data poses a great challenge in its efficient processing for any specific purpose. Sentiment analysis for web generated content e.g. tweets and online reviews, is a cumbersome problem mainly due to its unstructured and noisy nature (e.g. gr8, g8 etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark dat"
C16-1047,S14-2004,0,0.151622,"languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have created a dataset for aspect based sentiment analysis This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 https://www.domo.com/learn/data-never-sleeps-2 2 http://aci.info/2014/07/12/the-data-explosion-in-2014-minute-by-minute-infographic/ License details: http:// 482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activ"
C16-1047,D15-1303,0,0.0122283,"its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In this work, we do not use the traditional pipeline of CNN (c.f. Section 2.1) for sentiment classification. Rather, we learn sentiment features through CNN, which we call as ‘sentiment-embedded vector’. Parallely, a multi-objective optimization (MOO) based framework using Genetic Algorithm (GA) (Deb et al., 2002) is employed to derive optimized features for the respective optimization functions. In the final step, we augment the sentiment-embedded vector with the optimized feature set to form ‘sentiment augmented optimized vector’. This vector is used as the feature for sentiment classifica"
C16-1047,S15-2078,0,0.0382749,"astic) and online product reviews (sentence-level and aspect-level), across two different languages viz. Hindi and English for sentence-level as well as aspect-level sentiment analysis. Experiments show that the proposed hybrid deep learning architecture is highly efficient for sentiment analysis in multiple domains for Hindi. To the best of our knowledge, this is the very first attempt of using such a hybrid deep learning model for sentiment analysis, especially in less-resource languages. For English, we use the benchmark dataset of SemEval-2015 shared task on sentiment analysis in Twitter (Rosenthal et al., 2015) and SemEval-2014 shared task on aspect based sentiment analysis (Pontiki et al., 2014). 2 Methodology Logistic regression (LR) (or Softmax regression for multi-class classification) and SVM are two algorithms that often produce comparable results. However, SVM has an edge over LR if the data is not linearly separable, i.e. SVM with non-linear kernel performs better than LR (Pochet and Suykens, 2006). Also, LR focuses on maximizing the likelihood and is prone to over-fitting. However, SVM finds a linear hyperplane by projecting input data into higher dimension and generalizes well. We incorpor"
C16-1047,P06-1134,0,0.08803,"Missing"
C16-1287,C12-2008,1,0.936825,"t Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serve the purpose. Machine learning methods have been used and evaluated on different sets of languages (Boiy and Moens, 2009; Seki et al., 2010; Seki et al., 2008), which involve many constraints and manual functionalities. In addition, strategies to build a multilingual corpus (Schulz et al., 2010) have been devised which, quite frankly, are not possible for large number of languages, owing to the diversity involved. In this paper, we present a simple approach to multilingual sentiment classification which: (a) poses minimal restrictions on the language or the text genre to be used, (b)"
C16-1287,R09-1010,0,0.0265097,"d Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serve the purpose. Machine learning methods have been used and evaluated on different sets of languages (Boiy and Moens, 2009; Seki et al., 2010; Seki et al., 2008), which involve many constraints and manual functionalities. In addition, strategies to build a multilingual corpus (Schulz et al., 2010) have been devised which, quite frankly, are not possible for large number of languages, owing to the diversity involved. In this paper, we present a simple approach to multilingual sentiment classification which: (a) poses minimal restrictions on the lan"
C16-1287,J81-4005,0,0.71504,"Missing"
C16-1287,esuli-sebastiani-2006-sentiwordnet,0,0.0235411,"overcome this hurdle, one way is to make use of some list of frequently used sentiment-polar words. The idea is to familiarize the network with the fact that the text “This may be nice but I do not like it” reflects negative opinion but “nice” portrays positive sentiment. This improves the chances of correct prediction, as now the network is able to learn that the word nice is generally used for positive opinion but when used in some particular context (say, with discourse particle but or negation element not), it displays negative sentiment. Several rich English resources like SentiWordNet (Esuli and Sebastiani, 2006) or list of positive/negative English vocabulary (Hu and Liu, 2004) are publicly available. We propose to make use of this information for supervised sentiment classification in a multilingual scenario, by augmenting sentiment bearing words with their polarities to the annotated training corpora. 3 Proposed Method We use a deep learning model like Convolutional neural network as our classification system. We use randomly initialized word-vectors and no other resources in our baseline model. For example, a simple CNN-Rand (Non-static) model (Kim, 2014) can be used to work as our baseline. The p"
C16-1287,W09-0421,0,0.0179603,"Missing"
C16-1287,P14-1062,0,0.0383149,"Missing"
C16-1287,D14-1181,0,0.218149,"nglish. Subsequently, rich English resources like SentiWordNet (Esuli and Sebastiani, 2006) and pre-trained word embeddings (Mikolov et al., 2013a) are available publicly. However, lack of rich resources and annotated corpora in other languages like Dutch, Russian or Hindi makes it difficult to analyze texts with as good accuracy as that in English. Deep learning models have achieved astonishing results in several fields like Speech Recognition and Computer vision, and have shown promising results when used for several NLP tasks (like Convolutional Neural Networks for Sentence Classification (Kim, 2014), LSTMs for tweet classification (Wang et al., 2015) and Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet sc"
C16-1287,N13-1090,0,0.0169701,"ollobert et al., 2011) assume importance, since they are well-known to have no such text dependent constraints. 2.2 Why English word-embeddings? Deep learning NLP models require word representations (containing context information) as input. One way to do so is to randomly initialize the word vectors and trust the sentiment classification model itself to learn the word representations, besides the network parameters. However, this requires a large annotated corpus, which is difficult to obtain in most languages. The other way is to train a suitable deep learning model (Collobert et al., 2011; Mikolov et al., 2013a) on a raw corpus in that language and then use the obtained embeddings of these in-language words as input to the sentiment classification model. However, learning context-rich word embeddings in any language requires large datasets, generally of the order of billions of words, thereby eating up a lot of time as well as system resources. Hence, the pre-trained word embeddings of English prove to be useful, which have already been obtained by training suitable models on billions of data. Their context-rich information can be utilized to compensate for the small size of the available corpora i"
C16-1287,W02-1011,0,0.0317285,"that in English. Deep learning models have achieved astonishing results in several fields like Speech Recognition and Computer vision, and have shown promising results when used for several NLP tasks (like Convolutional Neural Networks for Sentence Classification (Kim, 2014), LSTMs for tweet classification (Wang et al., 2015) and Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serve the purpose. Machine learning methods have been used and evaluated on different sets of languages (Boiy and Moens, 2009; Seki et al., 2010; Seki et al., 2008), which involve many constraints and manual functionalities."
C16-1287,schulz-etal-2010-multilingual,0,0.0249027,"yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serve the purpose. Machine learning methods have been used and evaluated on different sets of languages (Boiy and Moens, 2009; Seki et al., 2010; Seki et al., 2008), which involve many constraints and manual functionalities. In addition, strategies to build a multilingual corpus (Schulz et al., 2010) have been devised which, quite frankly, are not possible for large number of languages, owing to the diversity involved. In this paper, we present a simple approach to multilingual sentiment classification which: (a) poses minimal restrictions on the language or the text genre to be used, (b) has minimal demands for preprocessing tools, and (c) shows no aversion to the small size of datasets or inadequacy of resources in any language. Our approach uses deep learning models for sentiment classification in a given language by borrowing word-embeddings and word polarities from the rich cousin En"
C16-1287,D13-1170,0,0.0071428,"embeddings (Mikolov et al., 2013a) are available publicly. However, lack of rich resources and annotated corpora in other languages like Dutch, Russian or Hindi makes it difficult to analyze texts with as good accuracy as that in English. Deep learning models have achieved astonishing results in several fields like Speech Recognition and Computer vision, and have shown promising results when used for several NLP tasks (like Convolutional Neural Networks for Sentence Classification (Kim, 2014), LSTMs for tweet classification (Wang et al., 2015) and Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serv"
C16-1287,P09-1027,0,0.0402583,"ral Networks for Sentence Classification (Kim, 2014), LSTMs for tweet classification (Wang et al., 2015) and Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculator (Brooke et al., 2009), and Wordnets (Balamurali et al., 2012) to serve the purpose. Machine learning methods have been used and evaluated on different sets of languages (Boiy and Moens, 2009; Seki et al., 2010; Seki et al., 2008), which involve many constraints and manual functionalities. In addition, strategies to build a multilingual corpus (Schulz et al., 2010) have been devised which, quite frankly, are not possible for large number of languages, owing to the diversity involved. In this paper, w"
C16-1287,P15-1130,0,0.0828088,"like SentiWordNet (Esuli and Sebastiani, 2006) and pre-trained word embeddings (Mikolov et al., 2013a) are available publicly. However, lack of rich resources and annotated corpora in other languages like Dutch, Russian or Hindi makes it difficult to analyze texts with as good accuracy as that in English. Deep learning models have achieved astonishing results in several fields like Speech Recognition and Computer vision, and have shown promising results when used for several NLP tasks (like Convolutional Neural Networks for Sentence Classification (Kim, 2014), LSTMs for tweet classification (Wang et al., 2015) and Recursive Deep Models for Sentiment Analysis (Socher et al., 2013)). A key feature of deep learning models, which seems to attract NLP researchers, is their lack of demand for manual feature engineering, unlike other classical machine learning algorithms (SVM, etc.) (Pang et al., 2002). Multilingual Sentiment Analysis has been a challenging, yet an important area of research since a long time, mainly involving other NLP tools like Sentence-level Machine translation (Joshi et al., 2010; Wan, 2009), Machine Translation with SentiWordNet scores (Denecke, 2008), semantic orientation calculato"
C16-1287,P10-2048,0,0.0553059,"Missing"
C18-1042,D14-1181,0,0.00359966,"the most semantically similar question is identified by comparing the unseen question with the existing set of questions. The question, which is closest to the unseen question can be retrieved as a possible semantically similar question. Thus, accurate semantic question matching can significantly improve a QA system. In the recent past, several deep learning based models such as recurrent neural networks (RNNs), convolution neural network (CNN), gated recurrent units (GRUs) etc. have been explored to obtain representation at the word (Mikolov et al., 2013; Pennington et al., 2014), sentence (Kim, 2014) and paragraph (Zhang et al., 2017) level. In the proposed semantic question matching framework, we use attention based neural network models to generate question vectors. We create a hierarchical taxonomy by considering different types and subtypes in such a way that questions having similar answers belong to the same taxonomy class. We propose and train a deep learning based question classifier network to classify the taxonomy classes. The taxonomy information is helpful in taking a decision on semantic similarity between them. For example, the questions ‘How do scientists work?’ and ‘Where"
C18-1042,N16-1153,0,0.112881,"Missing"
C18-1042,P11-1143,0,0.0115573,"researchers in very recent times (M`arquez et al., 2015; Nakov et al., 2016). It solves the problem of question starvation in cQA forums by providing a semantically similar question which has already been answered. In literature, there have been attempts to address the problem of finding the most similar match to a given question, for e.g. Burke et al. (1997) and Mlynarczyk and Lytinen (2005). Wang et al. (2009) have presented syntactic tree based matching for finding semantically similar questions. ‘Similar question retrieval’ has been modeled using various techniques such as topic modeling (Li and Manandhar, 2011), knowledge graph representation (Zhou et al., 2013) and machine translation (Jeon et al., 2005). Semantic kernel based similarity methods for QA have also been proposed in (Filice et al., 2016; Croce et al., 2017; Croce et al., 2011). Answer selection in QA forums is similar to the question similarity task. In recent times, researchers have been investigating DL-based models for answer selection (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; Feng et al., 2015). Most of the existing works either focus on better representations for questions or linguistic information associated with the q"
C18-1042,C02-1150,0,0.348075,"Missing"
C18-1042,S15-2047,0,0.060731,"Missing"
C18-1042,D14-1162,0,0.083547,"bypassed. For each unseen question, the most semantically similar question is identified by comparing the unseen question with the existing set of questions. The question, which is closest to the unseen question can be retrieved as a possible semantically similar question. Thus, accurate semantic question matching can significantly improve a QA system. In the recent past, several deep learning based models such as recurrent neural networks (RNNs), convolution neural network (CNN), gated recurrent units (GRUs) etc. have been explored to obtain representation at the word (Mikolov et al., 2013; Pennington et al., 2014), sentence (Kim, 2014) and paragraph (Zhang et al., 2017) level. In the proposed semantic question matching framework, we use attention based neural network models to generate question vectors. We create a hierarchical taxonomy by considering different types and subtypes in such a way that questions having similar answers belong to the same taxonomy class. We propose and train a deep learning based question classifier network to classify the taxonomy classes. The taxonomy information is helpful in taking a decision on semantic similarity between them. For example, the questions ‘How do scienti"
C18-1042,P15-2116,0,0.0707142,"Missing"
C18-1155,W04-0404,0,0.054324,"ange juice. Noun compound interpretation deals with uncovering such hidden relations. Noun compounds are usually interpreted in two ways: labelling and paraphrasing. Labelling involves assigning a semantic relation to a noun compound e.g., student protest: AGENT, orange juice: M ADE O F, etc.. These relations come from a set of a predefined taxonomy of semantic relations (Lauer, 1995; Warren, 1978; Barker and Szpakowicz, 1998; Girju et al., 2003; Tratz and Hovy, 2010; Ponkiya et al., 2018). Such detailed, fine-grained information can be useful for downstream tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), etc. Unfortunately, there is a lack of standard taxonomy. There is no consensus on which set of labels should be uniformly used. On the other hand, paraphrasing involves rewriting the noun compound as a paraphrase which conveys its meaning explicitly, e.g., orange juice: “juice made from orange” or “juice with orange flavour”. Generic paraphrasing has been relatively less pursued (Butnariu et al., 2009; Hendrickx et al., 2013). Prepositional paraphrasing – paraphrasing using only prepositions,"
C18-1155,P98-1015,0,0.834739,"l-defined meaning when written together. For example, orange juice, colon cancer, research paper submission, paper submission deadline, etc. The fact that “juice is made from orange” is hidden in orange juice. Noun compound interpretation deals with uncovering such hidden relations. Noun compounds are usually interpreted in two ways: labelling and paraphrasing. Labelling involves assigning a semantic relation to a noun compound e.g., student protest: AGENT, orange juice: M ADE O F, etc.. These relations come from a set of a predefined taxonomy of semantic relations (Lauer, 1995; Warren, 1978; Barker and Szpakowicz, 1998; Girju et al., 2003; Tratz and Hovy, 2010; Ponkiya et al., 2018). Such detailed, fine-grained information can be useful for downstream tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), etc. Unfortunately, there is a lack of standard taxonomy. There is no consensus on which set of labels should be uniformly used. On the other hand, paraphrasing involves rewriting the noun compound as a paraphrase which conveys its meaning explicitly, e.g., orange juice: “juice made from orange” or “ju"
C18-1155,W09-2416,0,0.0746887,"Missing"
C18-1155,W15-0122,0,0.434812,", prep, n2 i. In an alternative model to handle sparsity, she used the following: X P (t1 |prep)P (t2 |prep) prep∗ = arg max prep t1∈cats(n1 ),t2 ∈cats(n2 ) where t1 and t2 represent particular concepts from the sets of concepts cats(n1 ) and cats(n2 ), respectively. These sets come from Roget’s thesaurus. She also used a lexical frequency based approach in the above formula using pattern searching from Grolier’s encyclopedia. Lapata and Keller (2004) implemented the same lexical based model, but they used Altavista search engine and BNC corpus for computing the probability factors. Recently, Dima and Hinrichs (2015) proposed a feed-forward neural network based system for noun compound interpretation via labelling. Their network takes concatenated word-vectors of components of the noun compound as input, and predicts one of the labels from the Tratz and Hovy (2010)’s label set. We also adapt their system to our problem setting for a comparison. 3 Why Prepositional Paraphrasing of Noun Compounds? Uncovering a hidden relation or ellipsis from a construct is an important problem in NLP. For instance, when a customer searches for 15 inch laptop, he/she is actually searching for a laptop with 15-inch display."
C18-1155,N03-1011,0,0.00779654,"n together. For example, orange juice, colon cancer, research paper submission, paper submission deadline, etc. The fact that “juice is made from orange” is hidden in orange juice. Noun compound interpretation deals with uncovering such hidden relations. Noun compounds are usually interpreted in two ways: labelling and paraphrasing. Labelling involves assigning a semantic relation to a noun compound e.g., student protest: AGENT, orange juice: M ADE O F, etc.. These relations come from a set of a predefined taxonomy of semantic relations (Lauer, 1995; Warren, 1978; Barker and Szpakowicz, 1998; Girju et al., 2003; Tratz and Hovy, 2010; Ponkiya et al., 2018). Such detailed, fine-grained information can be useful for downstream tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), etc. Unfortunately, there is a lack of standard taxonomy. There is no consensus on which set of labels should be uniformly used. On the other hand, paraphrasing involves rewriting the noun compound as a paraphrase which conveys its meaning explicitly, e.g., orange juice: “juice made from orange” or “juice with orange flav"
C18-1155,P07-1072,0,0.154663,"ds, Lauer (1995) and Girju et al. (2005) have proposed datasets. These datasets contain noun compounds annotated with correct preposition. Lauer (1995)’s dataset is not publicly available. Thus, we extracted test samples from her thesis (Lauer, 1995). We dropped 118 samples out of 400 samples, as they had been marked as non-prepositional, and use the remaining 282 samples for our test experiments. Do note that this dataset is too small for effective learning. We have included it only for the sake of completeness. Similarly, Girju et al. (2005)’s dataset is not available online2 . Thus, we use Girju (2007)’s dataset (which has cross-lingual information along with English noun compounds) and extract relevant information to create a dataset. We extracted 832 samples. As our experiments are for Lauer (1995)’s set of 8 prepositions, we dropped examples annotated with prepositions that do not belong to this set. We use the remaining 805 examples for our experiments. In addition to the above two datasets, we prepared a dataset by manually annotating noun compounds with Lauer (1995)’s prepositions. For annotation, we use 1,779 noun compounds (without labels) from Kim and Baldwin (2005)’s dataset. Each"
C18-1155,S13-2025,0,0.660119,"Missing"
C18-1155,D16-1104,1,0.830207,"di-to-English translation (Kulkarni et al., 2012). Thus, prepositional paraphrasing is a useful first step towards noun compound interpretation. Deep learning has made tremendous progress in various fields. One of the significant contributions of deep learning in NLP is word embeddings. They are dense real-valued representations of words learnt in an unsupervised manner. Their use has advanced the state in many applications (word sense disambiguation - Rothe and Sch¨utze (2015), named entity recognition - Lample et al. (2016), sentiment classification - Tang et al. (2014), sarcasm detection - Joshi et al. (2016), etc.). Word embeddings enable words to share statistical strength. For instance, pattern learnt for the word hotel could be used to a good extent for the word motel, since they are semantically similar (which is elicited by the similarity between their corresponding word embeddings). This motivates us to investigate the use of word embeddings for noun compound interpretation. Another significant contribution of deep learning to the field of NLP is the introduction of Encoder Decoder architectures (Bahdanau et al., 2015) for different tasks involving sequences. In such models, an input sequen"
C18-1155,I05-1082,0,0.823815,"ngo”, etc. Prepositions are ambiguous, and they have their own selectional preferences (Zapirain et al., 2013). Once, a preposition and its corresponding sense are known, the space of verb+preposition constructions that can be used for paraphrasing is severely reduced (illustrated in Table 1), thereby making the task of verb+preposition paraphrasing easy. Thus we believe prepositional paraphrasing to be an important step towards verb+preposition paraphrasing of noun compounds. Noun compound interpretation via labelling is another avenue where prepositional paraphrasing is useful. We annotated Kim and Baldwin (2005)’s dataset with the correct paraphrasing prepositions. A comparison of preposition vs. the annotated semantic relation (Table 2) shows that knowing preposition for a noun compound helps the system in identifying the finer semantic relations. For instance, 29 samples in the annotated data have from preposition. Following is the distribution of those samples: 1829 Correct Preposition Intended Meaning from Source from Purpose Possible Verb+Prep Constructions Noun Compounds Example Verb+Prep Paraphrases resulted from, produced from, grown in provided during forest product farm product bonds yield"
C18-1155,C12-1091,0,0.158564,"sition This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1827 Proceedings of the 27th International Conference on Computational Linguistics, pages 1827–1836 Santa Fe, New Mexico, USA, August 20-26, 2018. (VERB+PREP) paraphrasing. For example, if the preposition used in paraphrase is at, the verbs that go along with mainly be of type L OCATION. Thus, it can aid verb+preposition paraphrasing. Moreover, uncovering of a preposition is enough for some NLP application like Hindi-to-English translation (Kulkarni et al., 2012). Thus, prepositional paraphrasing is a useful first step towards noun compound interpretation. Deep learning has made tremendous progress in various fields. One of the significant contributions of deep learning in NLP is word embeddings. They are dense real-valued representations of words learnt in an unsupervised manner. Their use has advanced the state in many applications (word sense disambiguation - Rothe and Sch¨utze (2015), named entity recognition - Lample et al. (2016), sentiment classification - Tang et al. (2014), sarcasm detection - Joshi et al. (2016), etc.). Word embeddings enabl"
C18-1155,N16-1030,0,0.0571807,"rasing. Moreover, uncovering of a preposition is enough for some NLP application like Hindi-to-English translation (Kulkarni et al., 2012). Thus, prepositional paraphrasing is a useful first step towards noun compound interpretation. Deep learning has made tremendous progress in various fields. One of the significant contributions of deep learning in NLP is word embeddings. They are dense real-valued representations of words learnt in an unsupervised manner. Their use has advanced the state in many applications (word sense disambiguation - Rothe and Sch¨utze (2015), named entity recognition - Lample et al. (2016), sentiment classification - Tang et al. (2014), sarcasm detection - Joshi et al. (2016), etc.). Word embeddings enable words to share statistical strength. For instance, pattern learnt for the word hotel could be used to a good extent for the word motel, since they are semantically similar (which is elicited by the similarity between their corresponding word embeddings). This motivates us to investigate the use of word embeddings for noun compound interpretation. Another significant contribution of deep learning to the field of NLP is the introduction of Encoder Decoder architectures (Bahdana"
C18-1155,N04-1016,0,0.771281,"e, T Pprep is the number of true-positives and F Pp is the number of false-positives for a preposition prep. Nprep is the number of instances with preposition prep in the test set, and N is the total number of instances in a test-set. To compare the results with previous work, we report micro-averaged accuracy5 . The following formula computes micro-accuracy: Accuracy = 6 Number of correcly classified instances Total number of instances in test-test Results and Analysis We compare the performance of our approach (referred to as NC-LSTM hereafter) with the performance reported by Lauer (1995), Lapata and Keller (2004), and Girju (2007) and performance computed for Dima and Hinrichs (2015)’s approach (referred to as NC-FFN hereafter) on different datasets. We first compare NC-LSTM with NC-FFN. Table 3 shows that NC-LSTM performs comparably, if not better, than NC-FNN. Also, NC-LSTM easily outperforms NC-FFN by a significant margin when the network parameters are further tuned with a portion of the dataset. Thus, this also shows the importance of fine-tuning. This is easily explained by the fact that the original training data was extracted from the web, and is noisy in nature. Dataset Lauer (1995) Girju (20"
C18-1155,P07-3013,0,0.6305,"Missing"
C18-1155,L18-1489,1,0.656101,"on cancer, research paper submission, paper submission deadline, etc. The fact that “juice is made from orange” is hidden in orange juice. Noun compound interpretation deals with uncovering such hidden relations. Noun compounds are usually interpreted in two ways: labelling and paraphrasing. Labelling involves assigning a semantic relation to a noun compound e.g., student protest: AGENT, orange juice: M ADE O F, etc.. These relations come from a set of a predefined taxonomy of semantic relations (Lauer, 1995; Warren, 1978; Barker and Szpakowicz, 1998; Girju et al., 2003; Tratz and Hovy, 2010; Ponkiya et al., 2018). Such detailed, fine-grained information can be useful for downstream tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), etc. Unfortunately, there is a lack of standard taxonomy. There is no consensus on which set of labels should be uniformly used. On the other hand, paraphrasing involves rewriting the noun compound as a paraphrase which conveys its meaning explicitly, e.g., orange juice: “juice made from orange” or “juice with orange flavour”. Generic paraphrasing has been relativel"
C18-1155,P02-1032,0,0.0183398,"s and some analysis, followed by a conclusion and future work. 2 Background and Related Work A relation between the components of a noun compound can be represented in either of the following two ways: (1) Labelling: assigning a relation from a predefined set of semantic relations (e.g., apple juice: M ADE O F), or (2) Paraphrasing: using a paraphrase to convey the underlying semantic relation (e.g., apple juice: “juice extracted from an apple” or “juice with apple flavor”). Over the years, many sets of semantic relations have been proposed (Levi, 1978; Warren, 1978; Lauer, ´ S´eaghdha, 2007; Rosario et al., 2002; Barker and Szpakowicz, 1998; Vanderwende, 1994; 1995; O 1828 Tratz and Hovy, 2010; Ponkiya et al., 2018). Most of these sets have been created with the assumption that most predicates are of the form verb+preposition. For example, The C AUSED -B Y relation in Levi (1978), or the U SER /R ECIPIENT +T HING U SED /R ECEIVED relation in Tratz and Hovy (2010) which paraphrases to thing used by user. One may observe that these labels are motivated from verb+preposition constructions. We believe that preposition uncovering will help in the verb+preposition paraphrasing of noun compounds. A system f"
C18-1155,P15-1173,0,0.0573253,"Missing"
C18-1155,S14-2033,0,0.0243809,"enough for some NLP application like Hindi-to-English translation (Kulkarni et al., 2012). Thus, prepositional paraphrasing is a useful first step towards noun compound interpretation. Deep learning has made tremendous progress in various fields. One of the significant contributions of deep learning in NLP is word embeddings. They are dense real-valued representations of words learnt in an unsupervised manner. Their use has advanced the state in many applications (word sense disambiguation - Rothe and Sch¨utze (2015), named entity recognition - Lample et al. (2016), sentiment classification - Tang et al. (2014), sarcasm detection - Joshi et al. (2016), etc.). Word embeddings enable words to share statistical strength. For instance, pattern learnt for the word hotel could be used to a good extent for the word motel, since they are semantically similar (which is elicited by the similarity between their corresponding word embeddings). This motivates us to investigate the use of word embeddings for noun compound interpretation. Another significant contribution of deep learning to the field of NLP is the introduction of Encoder Decoder architectures (Bahdanau et al., 2015) for different tasks involving s"
C18-1155,P10-1070,0,0.912405,"ple, orange juice, colon cancer, research paper submission, paper submission deadline, etc. The fact that “juice is made from orange” is hidden in orange juice. Noun compound interpretation deals with uncovering such hidden relations. Noun compounds are usually interpreted in two ways: labelling and paraphrasing. Labelling involves assigning a semantic relation to a noun compound e.g., student protest: AGENT, orange juice: M ADE O F, etc.. These relations come from a set of a predefined taxonomy of semantic relations (Lauer, 1995; Warren, 1978; Barker and Szpakowicz, 1998; Girju et al., 2003; Tratz and Hovy, 2010; Ponkiya et al., 2018). Such detailed, fine-grained information can be useful for downstream tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2015), question answering (Ahn et al., 2005), text entailment (Nakov, 2013), etc. Unfortunately, there is a lack of standard taxonomy. There is no consensus on which set of labels should be uniformly used. On the other hand, paraphrasing involves rewriting the noun compound as a paraphrase which conveys its meaning explicitly, e.g., orange juice: “juice made from orange” or “juice with orange flavour”. Generic paraphra"
C18-1155,C94-2125,0,0.779554,"uture work. 2 Background and Related Work A relation between the components of a noun compound can be represented in either of the following two ways: (1) Labelling: assigning a relation from a predefined set of semantic relations (e.g., apple juice: M ADE O F), or (2) Paraphrasing: using a paraphrase to convey the underlying semantic relation (e.g., apple juice: “juice extracted from an apple” or “juice with apple flavor”). Over the years, many sets of semantic relations have been proposed (Levi, 1978; Warren, 1978; Lauer, ´ S´eaghdha, 2007; Rosario et al., 2002; Barker and Szpakowicz, 1998; Vanderwende, 1994; 1995; O 1828 Tratz and Hovy, 2010; Ponkiya et al., 2018). Most of these sets have been created with the assumption that most predicates are of the form verb+preposition. For example, The C AUSED -B Y relation in Levi (1978), or the U SER /R ECIPIENT +T HING U SED /R ECEIVED relation in Tratz and Hovy (2010) which paraphrases to thing used by user. One may observe that these labels are motivated from verb+preposition constructions. We believe that preposition uncovering will help in the verb+preposition paraphrasing of noun compounds. A system for automatic uncovering of the predicate can be"
C18-1237,D15-1075,0,0.0270996,"s are relevant to a context. We reserve Temporality to be explored in a later work. 5.1 Embedding and Sentence Encoder The task of Novelty Detection requires high-level understanding and reasoning about semantic relationships within texts. Textual Entailment or Natural Language Inference is one such task which exhibits such complex semantic interactions. Following from (Conneau et al., 2017) we therefore employ a sentence encoder based on a bi-directional Long Short Term Memory (LSTM) architecture with max pooling, trained on the large-scale Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). SNLI entries supposedly captures rich semantic associations between the text pairs (entailment/inference relationships between premise and hypothesis). The output of our sentence encoder is 2 https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/data/corpus http://www.iitp.ac.in/ ai-nlp-ml/resources.html 4 We call our document vector relative, as we desire to encode the relative new information of a target document w.r.t. its relevant source document(s) 3 2805 Figure 1: RDV-CNN framework for Novelty Detection. Generic SNLI Training (Conneau et al., 2017). The sentence e"
C18-1237,J81-4005,0,0.700949,"Missing"
C18-1237,D17-1070,0,0.268875,"m has already seen the designated source documents for a particular event, it is to judge whether an incoming on-topic document is novel or not. The semantic nature of the dataset makes it an ideal candidate for our experiments. 5 Proposed Model Having an acceptable benchmark dataset avaialable at our end, instead of handcrafting the similarity/divergence based features,we try to learn feature representations from a target document with respect to the source document(s) using a Convolutional Neural Network (CNN). Our proposed model is based on a recent sentence embedding paradigm proposed by (Conneau et al., 2017). We leverage their idea and create a representation of the relevant target document relative to the designated source document(s) and call it as the Relative Document Vector (RDV)4 . We then train a Convolutional Neural Network (CNN) with the RDV of the target documents in the similar line of (Kim, 2014), and finally classify a document as novel or non-novel with respect to its source documents (Figure 1). Although there are document embedding models available, our method is specifically tailored to address the relativity and diversity criteria which is fundamental to the definition of novelt"
C18-1237,L18-1559,1,0.92083,"new information to be labeled as novel with respect to a set of source documents. The source document set could be seen as the memory of the reader which stores known information. Document-level novelty detection is a crucial problem and finds application in diverse domains of language processing such as information retrieval, document summarization, predicting impact of scholarly articles, etc. It is a complex problem that comprehends lexical, syntactic, semantic and pragmatic levels of texts in conjunction with certain characteristics like relevance, diversity, relativity, and temporality (Ghosal et al., 2018). Literature methods for novelty detection based on lexical similarity, divergence features, and information retrieval measures, although proved effective for sentence level but could not address the document-level comprehension needs. Our understanding of the problem led us to an assertion that a deep neural network might be able to extract the text subtleties involved in understanding a document’s novelty. To the best of our knowledge this is the very first attempt to address document-level novelty detection without the involvement of any hand-crafted features. Our approach achieves signific"
C18-1237,D14-1181,0,0.0266799,"f handcrafting the similarity/divergence based features,we try to learn feature representations from a target document with respect to the source document(s) using a Convolutional Neural Network (CNN). Our proposed model is based on a recent sentence embedding paradigm proposed by (Conneau et al., 2017). We leverage their idea and create a representation of the relevant target document relative to the designated source document(s) and call it as the Relative Document Vector (RDV)4 . We then train a Convolutional Neural Network (CNN) with the RDV of the target documents in the similar line of (Kim, 2014), and finally classify a document as novel or non-novel with respect to its source documents (Figure 1). Although there are document embedding models available, our method is specifically tailored to address the relativity and diversity criteria which is fundamental to the definition of novelty. Here T1 is the target document whose state of novelty is to be determined against the source document(s) S1 , S2 , ...SM i.e. to say the objective is to automatically figure out whether T1 is novel or not once the machine has already seen/scanned S1 , S2 , ...SM . Our model assumes that the documents a"
C18-1237,N13-1090,0,0.0143391,"s of the concatenation of the two sentence embeddings corresponding to tk and sij , their absolute element-wise difference, and their element-wise product (Mou et al., 2016). The first heuristic follows the most standard procedure of the Siamese architecture, while the latter two are certain measures of ”similarity” or ”closeness”. Thus, the Relative Sentence Vector (RSV) corresponding to a target sentence tk is represented as : RSVk = [ak , bij , |ak − bij |, ak ∗ bij ] where comma (,) refers to the column vector concatenation. This representation is inspired from the word embedding studies (Mikolov et al., 2013) where the linear offset of vectors is seen to capture semantic relationships between the two words. (Mou et al., 2016) successfully leveraged this idea for modeling sentence-pair relationships which we alleviate to model documents. Thus for each target sentence tk we compute the RSV and aggregate them to form the Relative Document Vector (RDV) of target document Tj with respect to the source documents(s) Si . Aggregation is realized as a slot filling task to shape the document matrix8 or RDV of dimension N X 4D where N is the number of sentences in a target document (padded when necessary) an"
C18-1237,P16-2022,0,0.0141123,"n the target document with one of the source sentences. bij → ak where ak has the max. cosine similarity with bij . 5.3 Aggregator module This module aggregates the mappings produced in the comparator module to generate a document matrix. The mapping of a target sentence tk to its closest source sentence sij is rendered by constructing a feature vector that captures the relation between the source and the target. This feature vector consists of the concatenation of the two sentence embeddings corresponding to tk and sij , their absolute element-wise difference, and their element-wise product (Mou et al., 2016). The first heuristic follows the most standard procedure of the Siamese architecture, while the latter two are certain measures of ”similarity” or ”closeness”. Thus, the Relative Sentence Vector (RSV) corresponding to a target sentence tk is represented as : RSVk = [ak , bij , |ak − bij |, ak ∗ bij ] where comma (,) refers to the column vector concatenation. This representation is inspired from the word embedding studies (Mikolov et al., 2013) where the linear offset of vectors is seen to capture semantic relationships between the two words. (Mou et al., 2016) successfully leveraged this idea"
C18-1237,H05-1090,0,0.029064,"was to highlight the relevant sentences that contain novel information, given a topic and an ordered list of relevant documents. Some interesting works in TREC were based on the sets of terms (Zhang et al., 2003a; Zhang et al., 2003b), term translations (Collins-Thompson et al., 2002), Principal Component Analysis (PCA) vectors (Ru et al., 2004), Support Vector Machine (SVM) classification (Tomiyama et al., 2004) etc. Similar works relied on named entities (Gabrilovich et al., 2004; Li and Croft, 2005; Zhang and Tsai, 2009), language models (Zhang et al., 2002; Allan et al., 2003), contexts (Schiffman and McKeown, 2005), etc. Next came the novelty sub tracks of Recognizing Textual Entailment-Text Analytics Conference (RTE-TAC) 6 and 7 (Bentivogli et al., 2011) where Textual Entailment was viewed as one close neighbor to sentence level novelty detection. At the document level an interesting work was carried out by (Yang et al., 2002) via topical classification of on-line document streams and then detecting novelty of documents in each topic exploiting the named entities. Another work by (Zhang et al., 2002) viewed novelty as an opposite characteristic to redundancy and proposed a set of five redundancy measur"
C18-1237,H05-1014,0,0.0664507,"he Topic Detection and Tracking (TDT) (Wayne, 1997) evaluation campaigns where the concern was to detect new events or First Story Detection (FSD) with respect to online news streams. Techniques mostly involved grouping the news stories into clusters and then measuring the belongingness of an incoming story to any of the clusters based on some preset similarity threshold. Some notable contributions from TDT are (Allan et al., 1998; Yang et al., 2002; Allan et al., 2000; Yang et al., 1998). The task gained prominence in the novelty tracks of Text Retrieval Conferences (TREC) from 2002 to 2004 (Soboroff and Harman, 2005; Harman, 2002; Soboroff and Harman, 2003; Clarke et al., 2004) although the focus was sentence-level novelty detection. The goal of these tracks was to highlight the relevant sentences that contain novel information, given a topic and an ordered list of relevant documents. Some interesting works in TREC were based on the sets of terms (Zhang et al., 2003a; Zhang et al., 2003b), term translations (Collins-Thompson et al., 2002), Principal Component Analysis (PCA) vectors (Ru et al., 2004), Support Vector Machine (SVM) classification (Tomiyama et al., 2004) etc. Similar works relied on named en"
D09-1048,C96-1005,0,0.775156,"Missing"
D09-1048,W04-0834,0,0.628522,"Missing"
D09-1048,P97-1009,0,0.0853609,"Missing"
D09-1048,H05-1052,0,0.122839,"nguage to another. Section 7 describes two WSD algorithms which combine various parameters for do459 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP main-specific WSD. Experiments and results are presented in sections 8 and 9. Section 10 concludes the paper. 2 Related work Knowledge based approaches to WSD such as Lesk‟s algorithm (Michael Lesk, 1986), Walker‟s algorithm (Walker D. & Amsler R., 1986), conceptual density (Agirre Eneko & German Rigau, 1996) and random walk algorithm (Mihalcea Rada, 2005) essentially do Machine Readable Dictionary lookup. However, these are fundamentally overlap based algorithms which suffer from overlap sparsity, dictionary definitions being generally small in length. Supervised learning algorithms for WSD are mostly word specific classifiers, e.g., WSD using SVM (Lee et. al., 2004), Exemplar based WSD (Ng Hwee T. & Hian B. Lee, 1996) and decision list based algorithm (Yarowsky, 1994). The requirement of a large training corpus renders these algorithms unsuitable for resource scarce languages. Semi-supervised and unsupervised algorithms do not need large amou"
D09-1048,P96-1006,0,0.615896,"Missing"
D09-1048,W97-0209,0,0.231629,"Missing"
D09-1048,P94-1013,0,0.0429486,"Missing"
D09-1048,P95-1026,0,0.472283,"Missing"
D09-1048,J93-2003,0,\N,Missing
D11-1100,D09-1020,0,0.229494,"benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on hu1082 man annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularl"
D11-1100,P04-1035,0,0.0423852,"ose are limited to affective classes. This restricts the size of the feature space to a limited set of labels. As opposed to this, we construct feature vectors that map to a larger sense-based space. In order to do so, we use synset offsets as representation of sense-based features. Akkaya et al. (2009), Martn-Wanton et al. (2010) and Carrillo de Albornoz et al. (2010) perform sentiment classification of individual sentences. However, we consider a document as a unit of sentiment classification i.e. our goal is to predict a document on the whole as positive or negative. This is different from Pang and Lee (2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that can be achieved only in a sense-based space. 3 Features based on WordNet Senses In their original form, documents are said to be in lexical space since they consist of words. When the words are replaced by their corresponding senses, the resultant document is said to be in semantic space. WordNet 2.1 (Fellbaum, 1"
D11-1100,W02-1011,0,0.0298014,"address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary 1081 classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semant"
D11-1100,N04-3012,0,0.0435663,"Missing"
D11-1100,R09-1067,0,0.0838769,"ords senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularly help understanding metaphors in these sentences. Our work differs in terms of the corpus and document sizes in addition to generating a general purpose classifier. Another supervised approach of creating an emotional intensity classifier using concepts as features has been reported by Carrillo de Albornoz et al."
D11-1100,P02-1053,0,0.0127258,"of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary 1081 classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervi"
D11-1100,P06-1134,0,0.0426931,"the similaritybased replacement technique using WordNet synsets in section 4. Our experiments have been described in section 5. In section 6, we present our results and related discussions. Section 7 analyzes some of the causes for erroneous classification. Finally, section 8 concludes the paper and points to future work. 2 Related Work This work studies the benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on hu1082 man annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources u"
D12-1012,X98-1004,0,0.080891,"s on SystemT and AQL (Section 3) and define the target language for our induction algorithm and the notion of rule complexity (Section 4). We then present our approach for inducing CD and CR rules, and discuss induction biases that would favor interpretability (Section 5), and discuss the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples1 . However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextu"
D12-1012,P10-1014,1,0.836172,"s that the induced rules have good accuracy and low complexity according to our complexity measure. SystemT Dataset ACE2002 ACE 2005 CoNLL 2003 Enron Generic 57.8 57.32 64.15 76.53 Fβ=1 Customized 82.2 88.95 91.77 85.29 Table 1: Quality of generic vs. customized rules. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chiticariu et al., 2010b) respectively. Rule-based systems are widely used in enterprise settings due to their explainability. Rules are transparent, which leads to better explainability of errors. One can easily identify the cause of a false positive or negative, and refine the rules without affecting other correct result"
D12-1012,D10-1098,1,0.920912,"Missing"
D12-1012,D08-1003,1,0.939845,"items in the WHERE clause), or the size of the head of the query (e.g., items in the SELECT clause). As such, our notion of complexity is rather coarse, and we shall discuss its shortcomings in detail in Section 6.2. However, we shall show that the complexity score significantly reduces the search space of our induction techniques leading to Phase name Basic Features AQL statements Phase 1 (Clustering and RLGG) Phase 2 (Propositional Rule Learning) Consolidation select extract select, union all, minus consolidate, union all Prescription Off-the-shelf, Learning using prior work (Riloff, 1993; Li et al., 2008) Bottom-up learning (LGG), Top-down refinement RIPPER, Lightweight Rule Induction Rule Type Basic Features Definition Manually identified consolidation rules, based on domain knowledge Consolidation rules Development of Candidate Rules Candidate Rules Filtering Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the phase and the corresponding type of rule in manual rule development. simpler and smaller extractors, and therefore constitutes a good basis for more comprehensive measures of interpretability in the future. 5 Inductio"
D12-1012,W03-0419,0,0.261988,"Missing"
D15-1017,P13-1101,0,0.0142366,"n. other hand, when we extract sentences first, an important subjective sentence may fail to be selected, simply because it is long. The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences. We then, use partial enumeration based greedy algorithm (Khuller et al., 1999), which gives performance guarantee of (1 − e−1 ) ≈ 0.632 (Sviridenko, 2004). The performance guarantee reported is better than simple greedy algorithm, used by Lin and Bilmes (2010) as their proof is erroneous (Morita et al., 2013). Further, the same greedy algorithm, which was used again in Lin and Bilmes (2011) gives only performance guarantee of 12 (1 − e1 ) ≈ 0.316 (Khuller et al., 1999). The rest of the paper is as follows - in the next section, we look at previous work and establish further motivation for our work. Following that, we build the theory and formulate suitable objectives for opinion summarization task. In the final section, we present results based on implementation and testing of the functions. Experimental results show that the functions outperform the-stateof-the-art methods. 2 Previous Work Automa"
D15-1017,C10-2105,0,0.0171486,"or Computational Linguistics. erage (SMAC), builds a summary that trades-off between DIVERSITY, maximally covering important aspects and MISMATCH, matching the overall sentiment of the entity along with high INTENSITY. The third model, called sentiment-aspect match (SAM), not only attempts to cover important aspects, but cover them with appropriate sentiment using KL-Divergence function. Here, INTENSITY and DIVERSITY in the first two models are linear monotone submodular functions, while KL-Divergence function i.e. relative entropy in last model, unlike entropy is not monotone submodular. In (Nishikawa et al., 2010b), a more sophisticated summarization technique was proposed, which generates a traditional text summary by selecting and ordering sentences taken from multiple reviews, considering both informativeness and readability of the final summary. The readability score in this paper would have been linear monotone submodular function, if the negative polarity was not penalizing. In (Nishikawa et al., 2010a), the authors further studied this problem using an integer linear programming formulation. On the other hand, Lin et al .(2011) treated the task of generic summarization as monotone submodular fu"
D15-1017,esuli-sebastiani-2006-sentiwordnet,0,0.104862,"es the subjectivity property. L(S) = i word∈j where F (S) , total utility of summary is given as a linear combination of L(S), relevance and A(S), subjective coverage of aspects. F (S) = αL(S) + βA(S) X Here Pi ; i = 1...K is a partition of the ground set V (i.e., ∪i Pi = V ), which contains sentences pertaining to different distinct aspects. wi are the weights of the partitions, based on the corresponding aspects. sj is the subjective score of the sentence j in summary. The subjective score sj is calculated using sentiwordnet as sum of the positive score ∈ [0, 1] and negative score ∈ [0, 1] (Esuli and Sebastiani, 2006). (1) i∈S S ∗ ∈ argmaxS⊂V F (S) s.t. X (4) A2 (S) = X i (5) j∈S min( X sj , λi ) ∗ wi (8) j∈(Pi ∩S) 3. A3 : Polarity Partitioned Budget-additive Function Here, wi,j > 0 measures the similarity between th i and j th sentences and ci (S) measures the similarity of summary with the document. Since, A(S), subjective coverage of aspects has to be modeled as monotone submodular function, it has been formulated as : In previous formulation we have not considered the polarity of the sentences. For example, if an aspect have many positive sentences with more intensity but few negative 172 5. A5 : Polar"
D15-1017,W04-1017,0,0.0310148,"would have been linear monotone submodular function, if the negative polarity was not penalizing. In (Nishikawa et al., 2010a), the authors further studied this problem using an integer linear programming formulation. On the other hand, Lin et al .(2011) treated the task of generic summarization as monotone submodular function maximization. Further, they argued that monotone non-decreasing submodular functions are an ideal class of functions to investigate for document summarization. They also show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova, 2004; Riedhammer et al., 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. Since many authors either in summarization or opinion summarization have used functions similar to submodular functions as objective, we can take this fact as testament to the value of submodular functions for opinion summarization. other hand, when we extract sentences first, an important subjective sentence may fail to be selected, simply because it is long. The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences"
D15-1017,P10-2060,0,0.0155479,"or Computational Linguistics. erage (SMAC), builds a summary that trades-off between DIVERSITY, maximally covering important aspects and MISMATCH, matching the overall sentiment of the entity along with high INTENSITY. The third model, called sentiment-aspect match (SAM), not only attempts to cover important aspects, but cover them with appropriate sentiment using KL-Divergence function. Here, INTENSITY and DIVERSITY in the first two models are linear monotone submodular functions, while KL-Divergence function i.e. relative entropy in last model, unlike entropy is not monotone submodular. In (Nishikawa et al., 2010b), a more sophisticated summarization technique was proposed, which generates a traditional text summary by selecting and ordering sentences taken from multiple reviews, considering both informativeness and readability of the final summary. The readability score in this paper would have been linear monotone submodular function, if the negative polarity was not penalizing. In (Nishikawa et al., 2010a), the authors further studied this problem using an integer linear programming formulation. On the other hand, Lin et al .(2011) treated the task of generic summarization as monotone submodular fu"
D15-1017,P04-1035,0,0.825058,"they often have the following parts: 1. Plot - Description of the story, which is factual in nature 2. Critique - Opinion about the movie, which is sentiment bearing Clearly, opinion summary to be generated will have a trade-off between the two opposing parts - subjective critique and objective plot. Our goal is to strike a balance through linear combination of suitable submodular functions in our paper. Joint models of relevance and subjectivity have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approach Pang and Lee (2004), which first generate candidate subjective sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we select subjective sentences first, the sentiment as well information content may become redundant for a particular aspect. On the Introduction Sentiment Analysis is often addressed as a classification task, which aims at determining the sentiment of a word, sentence, paragraph or a document as a whole into p"
D15-1017,W02-1011,0,0.0211927,"ctive sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we select subjective sentences first, the sentiment as well information content may become redundant for a particular aspect. On the Introduction Sentiment Analysis is often addressed as a classification task, which aims at determining the sentiment of a word, sentence, paragraph or a document as a whole into positive, negative or neutral classes (Pang et al., 2002). Summarization, on the other hand is the task of aggregating and representing information content from a single document or multiple documents in a brief and fluent manner. Due to the explosive growth of data, fine grained sentiment analysis as well as summarization on the whole chunk of data can be a very time-consuming task. Sentiment Analysis also requires filtering of text portions as either objective (factual information) or subjective (expressing some sentiment or opinion) during pre-processing and then, classifying the subjective extracts as positive or negative. 169 Proceedings of the"
D15-1017,E09-1059,0,0.391375,"the-art methods. 2 Previous Work Automatically generating opinion summaries from large review text corpora has long been studied in both information retrieval and natural language processing. In (Pang and Lee, 2004), a mincut-based algorithm was proposed to classify each sentence as being subjective or objective. The purpose of this work was to remove objective sentences from reviews to improve document level sentiment classification. Interestingly, the cut functions are symmetrical and submodular, and the problem of finding min-cut is equivalent to minimizing a symmetric submodular function. Lerman et al. (2009) proposed three different models - sentiment match (SM), sentiment match + aspect coverage (SMAC) and sentiment-aspect match (SAM) to perform summarization of reviews of a product. The first model is called sentiment match (SM), which extracts sentences so that the average sentiment of the summary is as close as possible to the average sentiment rating of reviews of the entity i.e. low MISMATCH but with high sentiment INTENSITY. The second model, called sentiment match + aspect cov3 3.1 Theoretical Background Introduction to Submodular Functions A submodular function is a set function (f : 2V"
D15-1017,P06-2084,0,0.0148909,"ing graph. Summary is generated with sentences having more vertex score (Mihalcea and Tarau, 2004). Results We use ROUGE (Lin, 2004) for evaluating the content of summaries. We have used the 200 test documents that are manually summarized as gold standard data for ROUGE evaluation. For figuring out the sentiment correlation between manual and system generated summaries, we trained Naive Bayes sentiment classifier (Pang et al., 2002) on training data using bag of words approach with features as unigrams and bigrams and then, using minimum Pearson’s chi-square score of 3 for feature extraction (Pecina and Schlesinger, 2006) before calculating the sentiment. The measure of sentiment preservation is calculated as Pearson correlation between the sentiment score of the document and the corresponding summary sentiment, both calculated by the Naive Bayes sentiment classifier while the measure of coverage of information content is given by ROUGE-1 and ROUGE-2 f-scores. Mathematically, Covariance(X, Y ) std.dev(X) ∗ std.dev(Y ) (13) Here, random variable X is the sentiment score of the document sample and random variable Y is the sentiment score of the corresponding summary sample. For 200 documents, it will be [(X1 , Y"
D15-1017,N10-1134,0,0.142244,"ue of submodular functions for opinion summarization. other hand, when we extract sentences first, an important subjective sentence may fail to be selected, simply because it is long. The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences. We then, use partial enumeration based greedy algorithm (Khuller et al., 1999), which gives performance guarantee of (1 − e−1 ) ≈ 0.632 (Sviridenko, 2004). The performance guarantee reported is better than simple greedy algorithm, used by Lin and Bilmes (2010) as their proof is erroneous (Morita et al., 2013). Further, the same greedy algorithm, which was used again in Lin and Bilmes (2011) gives only performance guarantee of 12 (1 − e1 ) ≈ 0.316 (Khuller et al., 1999). The rest of the paper is as follows - in the next section, we look at previous work and establish further motivation for our work. Following that, we build the theory and formulate suitable objectives for opinion summarization task. In the final section, we present results based on implementation and testing of the functions. Experimental results show that the functions outperform t"
D15-1017,P11-1052,0,0.451694,"ay fail to be selected, simply because it is long. The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences. We then, use partial enumeration based greedy algorithm (Khuller et al., 1999), which gives performance guarantee of (1 − e−1 ) ≈ 0.632 (Sviridenko, 2004). The performance guarantee reported is better than simple greedy algorithm, used by Lin and Bilmes (2010) as their proof is erroneous (Morita et al., 2013). Further, the same greedy algorithm, which was used again in Lin and Bilmes (2011) gives only performance guarantee of 12 (1 − e1 ) ≈ 0.316 (Khuller et al., 1999). The rest of the paper is as follows - in the next section, we look at previous work and establish further motivation for our work. Following that, we build the theory and formulate suitable objectives for opinion summarization task. In the final section, we present results based on implementation and testing of the functions. Experimental results show that the functions outperform the-stateof-the-art methods. 2 Previous Work Automatically generating opinion summaries from large review text corpora has long been s"
D15-1017,W04-1013,0,0.0196913,"es classifier (Pang et al., 2002) trained on imdb corpus to predict the sentiment of a sentence and document. X minS⊂V (|senti(V ) − senti(j)|) (14) j∈S 4. Baseline-4/TEXTRANK : TextRank summarizer is based on Graph based unsupervised algorithm. Graph is constructed by creating a vertex for each sentence in the document and edges between vertices based on the number of words two sentences (of vertices) have in common and then, ranking them by applying PageRank to the resulting graph. Summary is generated with sentences having more vertex score (Mihalcea and Tarau, 2004). Results We use ROUGE (Lin, 2004) for evaluating the content of summaries. We have used the 200 test documents that are manually summarized as gold standard data for ROUGE evaluation. For figuring out the sentiment correlation between manual and system generated summaries, we trained Naive Bayes sentiment classifier (Pang et al., 2002) on training data using bag of words approach with features as unigrams and bigrams and then, using minimum Pearson’s chi-square score of 3 for feature extraction (Pecina and Schlesinger, 2006) before calculating the sentiment. The measure of sentiment preservation is calculated as Pearson corre"
D15-1017,W04-3252,0,\N,Missing
D15-1300,P98-1013,0,0.217373,"menon as a base, we deduce that words which satisfy case-1 tend to be high intensity words while words satisfying case-2 are low intensity words. Hence, we conclude that high intensity words (case-1) have higher cosine similarity with each other than with low or medium intensity words (case-2). Therefore, cosine similarity with a high intensity word can be used to obtain intensity ordering for remaining words of the category. 3 Data and Resources This section gives an overview of the corpus and lexical resources used in our approach. Semantic Categories: We worked with frames of FrameNet-1.5 (Baker et al., 1998). A frame 1 The semantic bleaching phenomenon in words was reported in US edition of New York Times: http: //www.nytimes.com/2010/07/18/magazine/ 18onlanguage-anniversary.html?_r=0 2521 Rating Definition 0 Totally painful, unbearable picture 1 Poor Show ( dont waste your money) 2 Average Movie 3 Excellent show, look for it 4 A must see film Size 179 standard intensity level for the word. To compute agreement among five annotators, we used fleiss’ kappa, and obtained a score of 0.61. 1057 888 1977 905 Table 1: Review ratings with their definitions and number of reviews. represents a semantic p"
D15-1300,Q13-1023,0,0.219962,"Missing"
D15-1300,W10-1205,0,0.0330467,"Missing"
D15-1300,P93-1023,0,0.702114,"Missing"
D15-1300,P97-1023,0,0.81355,"Missing"
D15-1300,D13-1169,0,0.110561,"Missing"
D15-1300,S13-2053,0,0.0349863,"Missing"
D15-1300,P04-1035,0,0.0972104,"Missing"
D15-1300,P05-1015,0,0.425182,"2 Average Movie 3 Excellent show, look for it 4 A must see film Size 179 standard intensity level for the word. To compute agreement among five annotators, we used fleiss’ kappa, and obtained a score of 0.61. 1057 888 1977 905 Table 1: Review ratings with their definitions and number of reviews. represents a semantic property and contains words bearing the property. We explored the FrameNet data manually and found 52 frames (semantic categories) with polar semantic properties. Intensity Annotated Corpus: To identify a high intensity word for a semantic category, we use a movie review corpus2 (Pang and Lee, 2005) of 5006 files. Each review is rated on a scale of 0 to 4, where 0 indicates an unbearable movie and 4 represents a must see film. Table 1 describes the meanings of the rating scores with the count of reviews in each rating. We can infer that increase in rating corresponds to increase in positive intensity and decrease in negative intensity. Sentiment Lexicon: To identify the polarity orientation of words, we use a list of positive (2006) and negative (4783) words3 (Liu, 2010). We manually assign polarities to universally polar words like enduring, creditable and nonsensical, which are missing"
D15-1300,W02-1011,0,0.0312174,"Missing"
D15-1300,C10-1103,0,0.0751267,"he accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I to assign intensity score to a review r: P3 P3 P N i=1 i ∗ Ci − i=1 i ∗ Ci I(r) = (2) P3 P 3 ∗ ( i=1 CiP + 3i=1 CiN ) where CiP and CiN respectively represent"
D15-1300,E14-4023,0,0.270144,"Missing"
D15-1300,I13-1076,1,0.868543,"(word) = P5 5 i=1 i ∗ Ci P ∗ 5i=1 Ci (1) where Ci is the count of the word in i-star reviews. 2.2 Need Significant Occurrence of A Word The W N P I formula gives a corpus based result, hence can give biased scores for words which occur less frequently in the corpus. For example, in our movie review data-set, the word substandard occurs only 3 times in the corpus, and these occurrences happen to be in 1-star and 2-star reviews only. Hence, the W N P I formula assigns a higher score to substandard. To avoid such a bias, we integrate W N P I formula with Chi-Square test. Sharma and Bhattacharyya (2013) used ChiSquare test to find significant polar words in a domain. We use the same categorical Chi-Square test in our work. 2.3 How to Get Intensity Clue for All Words? A combination of W N P I formula and ChiSquare test cannot assign intensity scores to adjectives, which are not present in the corpus. To overcome this data sparsity problem, we restrict the use of W N P I formula to identify the most intense word in each category. We explore precomputed context vectors of words, presented by Mikolov et al. (2011) (Section 3), to assign intensity levels to remaining words of the semantic categor"
D15-1300,P02-1053,0,0.13858,"Missing"
D15-1300,H05-2018,0,0.246499,"Missing"
D15-1300,H05-1044,0,0.277237,"Missing"
D15-1300,C98-1013,0,\N,Missing
D15-1300,P13-1041,1,\N,Missing
D16-1104,W14-2608,0,0.0423397,"): We first compute similarity scores for all pairs of words (except stop words). We then return four feature values per sentence.3 : 2. Gonz´alez-Ib´anez et al. (2011a): They propose two sets of features: unigrams and dictionary-based. The latter are words from a lexical resource called LIWC. We use words from LIWC that have been annotated as emotion and psychological process words, as described in the original paper. • • • • Maximum score of most similar word pair Minimum score of most similar word pair Maximum score of most dissimilar word pair Minimum score of most dissimilar word pair 3. Buschmeier et al. (2014): In addition to unigrams, they propose features such as: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment words followed by an exclamation mark or question mark, (d) Positive/Negative Sentiment Scores followed by ellipsis (represented by a ‘...’), (e) Punctuation, (f) Interjections, and (g) Laughter expressions. For example, in case of the first feature, we consider the most similar word to every word in the sentence, and the corresponding similarity scores. These most similar word scores for each word are"
D16-1104,D15-1116,0,0.214199,"Missing"
D16-1104,P11-2102,0,0.245391,"Missing"
D16-1104,P15-2124,1,0.893523,"ight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection. 1 Can word embedding-based features when augmented to features reported in prior work improve the performance of sarcasm detection? Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Linguistic studies show that the notion of context incongruity is at the heart of sarcasm (Ivanko and Pexman, 2003). A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013). However, techniques to extract these patterns rely on sentiment-bearing words and may not capture nuanced forms of sarcasm. Consider the sentence ‘With a sense of humor like that, you could make a living as a garbage man anywhere in the country.1 ’ The speaker makes a subtle, contemptuous remark about the 1 All examples in this paper are actual instances from our dataset. To the best of our knowledge, this is the first attempt that uses word embedding-based features to detect sarcasm. In this respect, the paper makes a simple increment to state-of-the-art but opens up a"
D16-1104,W15-2905,1,0.883789,"Missing"
D16-1104,W07-0101,0,0.125028,"and ‘apple’ with similarity score of 0.1414. The sarcasm in this sentence can be understood only in context of the complete conversation that it is a part of. 3. Metaphors in non-sarcastic text: Figurative language may compare concepts that are not directly related but still have low similarity. Consider the nonsarcastic quote ‘Oh my love, I like to vanish in you like a ripple vanishes in an ocean - slowly, silently and endlessly’. Our system incorrectly predicts this as sarcastic. 8 Related Work Early sarcasm detection research focused on speech (Tepperman et al., 2006) and lexical features (Kreuz and Caucci, 2007). Several other features have been proposed 1010 Conclusion This paper shows the benefit of features based on word embedding for sarcasm detection. We experiment with four past works in sarcasm detection, where we augment our word embedding-based features to their sets of features. Our features use the similarity score values returned by word embeddings, and are of two categories: similarity-based (where we consider maximum/minimum similarity score of most similar/dissimilar word pair respectively), and weighted similarity-based (where we weight the maximum/minimum similarity scores of most si"
D16-1104,P14-2050,0,0.0405182,"Features given in paper X + S+WS (i.e., weighted and unweighted similarity features) 1008 67.2 64.6 67.6 67 78.8 75.2 51.2 52.8 72.53 69.49 58.26 59.05 using embeddings from Word2Vec We experiment with four types of word embeddings: 1. LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word embeddings based on LSA5 . The vocabulary size is 100,000. 2. GloVe: We use pre-trained vectors avaiable from the GloVe project6 . The vocabulary size in this case is 2,195,904. 3. Dependency Weights: We use pre-trained vectors7 weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015. Experiment Setup 1. Features given in paper X F Table 2: Performance of unigrams versus our similarity-based features These are computed similar to unweighted similarity features. We create a dataset consisting of quotes on GoodReads 4 . GoodReads describes itself as ‘the world’s largest site for readers and book recommendations.’ The website also allows users to post quotes from books. These quotes are snippets from books labeled by the user with tags of their choice. We download quotes with the tag ‘sarcastic’ as sarcastic quotes, and the ones with ‘philosop"
D16-1104,W13-1605,0,0.0630179,"Missing"
D16-1104,D13-1066,0,0.438285,"Missing"
D16-1104,P14-2084,0,0.0583793,"Missing"
D16-1104,P15-1100,0,0.0518009,"Missing"
D16-1196,N16-4006,1,0.288206,"riable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. 1 Introduction Related languages exhibit lexical and structural similarities on account of sharing a common ancestry (Indo-Aryan, Slavic languages) or being in prolonged contact for a long period of time (Indian subcontinent, Standard Average European linguistic areas) (Bhattacharyya et al., 2016). Translation between related languages is an important requirement due to substantial government, business and social communication among people speaking these languages. However, most of these languages have few parallel corpora resources, an important requirement for building good quality SMT systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi"
D16-1196,N12-1047,0,0.0882306,"s – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph"
D16-1196,P10-1048,0,0.244431,"Missing"
D16-1196,E14-4029,0,0.0372769,"the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph-segmenters and our implementation of orthographic syllabification are made available as part of the Indic NLP Library1 . Evaluation: We use BLEU (Papineni et al., 2002) and Le-BLEU (Virpioja and Grönroos, 2015) for evaluation. Le-BLEU does fuzzy matches of words and hence is suitable for evaluating SMT systems that perform transfor"
D16-1196,P06-2025,0,0.0317112,"present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thought of as an approximate syllable with the onset and nucleus, but no coda. While true syllabification is hard, orthographic syllabification can be easily done. Atreya et al. (2016) and Ekbal et al. (2006) have shown that the OS is a useful unit for transliteration involving Indian languages. We show that orthographic syllable-level trans1912 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1912–1917, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics lation significantly outperforms character-level and strong word-level and morpheme-level baselines over multiple related language pairs (Indian as well as others). Character-level approaches have been previously shown to work well for language pairs with high lexical si"
D16-1196,P07-2045,0,0.00772487,"sisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morpholog"
D16-1196,N15-3017,1,0.711478,"y et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph-segmenters and our implementation of orthographic syllabification are made available as part of the Indic NLP Library1 . Evaluation: We use BLEU (Papineni et al., 2002) and Le-BLEU (Virpioja and Grönroos, 2015) for evaluation. Le-BLEU does"
D16-1196,W95-0115,0,0.0765572,"ravidian). These languages have been in contact for a long time, hence there are many lexical and grammatical similarities among them, leading to the subcontinent being considered a linguistic area (Emeneau, 1956). Specifically, there is overlap between the vocabulary of these languages to varying degrees due to cognates, language contact and loanwords from Sanskrit (throughout history) and English (in recent times). Table 2 lists the languages involved in the experiments and provides an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level. All these language have a rich inflectional morphology with Dravidian languages, and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus mono"
D16-1196,P12-2059,0,0.186929,"uations are retained and a special word boundary marker character (_) is introduced to indicate word boundaries as shown here: W: O: राजू , घराबाहेर जाऊ नको . रा जू _ , _ घ रा बा हे र _ जा ऊ _ न को _ . For all units of representation, we trained phrasebased SMT (PBSMT) systems. Since related languages have similar word order, we used distance based distortion model and monotonic decoding. For character and orthographic syllable level models, we use higher order (10-gram) languages models since data sparsity is a lesser concern due to small vocabulary size (Vilar et al., 2007). As suggested by Nakov and Tiedemann (2012), we used word-level tuning for character and orthographic syllable level models by post-processing n-best lists in each tuning step to calculate the usual word-based BLEU score. While decoding, the word and morpheme level systems will not be able to translate OOV words. Since the languages involved share vocabulary, we transliterate the untranslated words resulting in the post-edited systems WX and MX corresponding to the systems W and M respectively. Following decoding, we used a simple method to regenerate words from sub-word level units: Since we represent word boundaries using a word boun"
D16-1196,quasthoff-etal-2006-corpus,0,0.335578,", and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliterat"
D16-1196,W12-5611,0,0.120413,"e have a rich inflectional morphology with Dravidian languages, and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et"
D16-1196,R13-1088,0,0.470971,"rrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thought of as an approximate syllable with the onset and nucleus, but no coda. While true syllabification is hard, orthographic syllabification can be easily done. Atreya"
D16-1196,2009.eamt-1.3,0,0.615229,"systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS c"
D16-1196,E12-1015,0,0.719407,"similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thou"
D16-1196,W07-0705,0,0.651908,"ing good quality SMT systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basi"
D16-1196,W15-3052,0,0.178484,"Missing"
D17-1057,S17-2089,0,0.0197875,"Missing"
D17-1057,C16-1047,1,0.0808105,"current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging the syntactic and semantic richness of the two distributed word representation through a stacked denoising autoencoder; and c) build a state-of-the-art model for sentiment analysis in financial domain. 2 Figure 1: MLP based ensemble architecture. A. Convolution Neural Network (CNN): Literature suggests that CNN architecture had been successfully applied for sentiment analysis at various level (Kim, 2014; Akhtar et al., 2016; Singhal and Bhattacharyya, 2016). Most of these works involve classification tasks, however, we adopt CNN architecture for solving the regression problem. Our proposed system employs a convolution layer followed by a max pool layer, 2 fully connected layers and an output layer. We use 100 different filters while sliding over 2, 3 and 4 words at a time. We employ all these filters in parallel. Proposed Methodology We propose a Multi-Layer Perceptron based ensemble approach to leverage the goodness of various supervised systems. We develop three deep neural network architecture based models, v"
D17-1057,D14-1181,0,0.0031271,"ons of the current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging the syntactic and semantic richness of the two distributed word representation through a stacked denoising autoencoder; and c) build a state-of-the-art model for sentiment analysis in financial domain. 2 Figure 1: MLP based ensemble architecture. A. Convolution Neural Network (CNN): Literature suggests that CNN architecture had been successfully applied for sentiment analysis at various level (Kim, 2014; Akhtar et al., 2016; Singhal and Bhattacharyya, 2016). Most of these works involve classification tasks, however, we adopt CNN architecture for solving the regression problem. Our proposed system employs a convolution layer followed by a max pool layer, 2 fully connected layers and an output layer. We use 100 different filters while sliding over 2, 3 and 4 words at a time. We employ all these filters in parallel. Proposed Methodology We propose a Multi-Layer Perceptron based ensemble approach to leverage the goodness of various supervised systems. We develop three deep neural network archite"
D17-1057,W14-4012,0,0.04212,"Missing"
D17-1057,S17-2152,0,0.0291858,"Missing"
D17-1057,S17-2138,0,0.0483805,"WE-GLV CNN C3 FWE-W2V CNN C4 FWE-GLV CNN C5 DAWE CNN Long short term memory (LSTM) L1 PWE-W2V LSTM L2 PWE-GLV LSTM L3 FWE-W2V LSTM L4 FWE-GLV LSTM L5 DAWE LSTM Gated Recurrent Unit (GRU) G1 PWE-W2V GRU G2 PWE-GLV GRU G3 FWE-W2V GRU G4 FWE-GLV GRU G5 DAWE GRU Feature - SVR F1 Tf-idf + Lexicon + Vader F2 Tf-idf + Lexicon + Vader + PWE-W2V F3 Tf-idf + Lexicon + Vader + PWE-GLV F4 Tf-idf + Lexicon + Vader + FWE-W2V F5 Tf-idf + Lexicon + Vader + FWE-GLV F6 Tf-idf + Lexicon + Vader + DAWE Ensemble E1 C4 + L3 + G5 + F6 (MLP) E2 C1 + L5 + G1 + F6 (MLP) systems (ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al., 2017)) which were the best performing systems at SemEval-2017 shared task 5. ECNU reported to have obtained cosine similarity of 0.777 in microblog as compared to 0.797 cosine similarity of our proposed system, whereas, for news headlines Fortia-FBK reported cosine similarity of 0.745. ECNU employed several regressors on top of optimized feature set obtained through hill climbing algorithm. For the final prediction, authors averaged the predictions of different regressors. Fortia-FBK trained a CNN with the assistance of sentiment lexicons for predicting the sentiment score. It should be noted that"
D17-1057,S13-2053,0,0.0104051,"which can efficiently learn long-term dependencies. A key difference of GRU with LSTM is that, GRU’s 541 recurrent state is completely exposed at each time step in contrast to LSTM’s recurrent state which controls its recurrent state. Thus, comparably GRUs have lesser parameters to learn and training is computationally efficient. We use two GRU layers on top of each other having 100 neurons in each. This was followed by 2 fully connected layers and an output layer. cons. These are NRC (Hashtag Context, Hashtag Sentiment, Sentiment140, Sentiment140 Context) lexicons (Kiritchenko et al., 2014; Mohammad et al., 2013) which associate a positive or negative score to a token. Following features are extracted for each of these: i) positive, negative and net count. ii) maximum of positive and negative scores. iii) sum of positive, negative and net scores. - Vader Sentiment: Vader sentiment (Gilbert, 2014) score is a rule-based method that generates a compound sentiment score for each sentence between -1 (extreme negative) and +1 (extreme positive). It also produces ratio of positive, negative and neutral tokens in the sentence. We obtain score and ratio of each instance in the datasets and use as feature for t"
D17-1057,D14-1162,0,0.0937113,"re 1. and implicit sentiment in the financial text. An application of multiple regression model was developed by (Oliveira et al., 2013). In this paper, we propose a novel Multi-Layer Perceptron (MLP) based ensemble technique for fine-grained sentiment analysis. It combines the outputs of four systems, one is feature-driven supervised model and the rest three are deep learning based. We further propose to develop an enhanced word representation by learning through a stacked denoising autoencoder network (Vincent et al., 2010) using word embeddings of Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models. For evaluation purpose we use datasets of SemEval-2017 ‘Fine-Grained Sentiment Analysis on Financial Microblogs and News’ shared task (Keith Cortis and Davis, 2017). The dataset comprises of financial short texts for two domains i.e. microblog messages and news headlines. Comparisons with the state-of-the-art models show that our system produces better performance. The main contributions of the current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging t"
D17-1057,C16-1287,1,0.898554,"Missing"
D17-1057,H05-1044,0,0.0279265,"ting the sentiment score in the continuous range of -1 to +1. - Word Tf-Idf: Term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a corpus. We consider tf-idf weighted counts of continuous sequences of ngrams (n=2,3,4,5) at a time. - Lexicon Features: Sentiment lexicons are widely utilized resources in the field of sentiment analysis. Its application and effectiveness in sentiment prediction task had been widely studied. We employ two lexicons i.e. Bing Liu opinion lexicon (Ding et al., 2008) and MPQA (Wilson et al., 2005) subjectivity lexicon for news headlines domain. First we compile a comprehensive list of positive and negative words form these lexicons and then extract the following lexicon driven features. Agreement Score : This score indicates the polarity of the sentence i.e. whether the sentence takes a polar or neutral stance. If the agreement score is 1 then it implies that the instance is of having either high positive or negative sentiment whereas, a 0 agreement score indicates a mixed or disharmony in the positive and negative sentiment implying the sentence is not polar (Rao and Srivastava, 2012)"
D17-1058,Q13-1023,0,0.184166,"Missing"
D17-1058,J11-2001,0,0.00698776,"er, most of the work addressed the problem of finding polarity orientation of the adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wilson et al., 2005; Fahrni and Klenner, 2008; Dragut et al., 2010; Taboada and Grieve, 2004; Baccianella et al., 2010). The task of ranking polar words has received much attention recently due to the vital role of word’s intensity in several real world applications. Most of the literature on intensity ranking consists of manual approaches or corpus-based approaches. Affective Norms (Warriner et al., 2013), SentiStrength (Thelwall et al., 2010), SoCAL (Taboada et al., 2011), and LABMT (Dodds et al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such publicly available sentiment intensity lexicons which are manually created. Corpus-based approaches follow the assumption that the polarity of a new word can be inferred from the corpus (Hatzivassiloglou and McKeown, 1993; Kiritchenko et al., 2014; De Melo and Bansal, 2013). Corpus-based approaches require a huge amount of data, otherwise they suffer from the data sparsity problem. None of the these approaches considers the concept of semantics of adjectives, assuming one single intensity scale"
D17-1058,P93-1023,0,0.623883,"much attention recently due to the vital role of word’s intensity in several real world applications. Most of the literature on intensity ranking consists of manual approaches or corpus-based approaches. Affective Norms (Warriner et al., 2013), SentiStrength (Thelwall et al., 2010), SoCAL (Taboada et al., 2011), and LABMT (Dodds et al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such publicly available sentiment intensity lexicons which are manually created. Corpus-based approaches follow the assumption that the polarity of a new word can be inferred from the corpus (Hatzivassiloglou and McKeown, 1993; Kiritchenko et al., 2014; De Melo and Bansal, 2013). Corpus-based approaches require a huge amount of data, otherwise they suffer from the data sparsity problem. None of the these approaches considers the concept of semantics of adjectives, assuming one single intensity scale for all adjectives. Ruppenhofer et al., (2014) made the first attempt in this direction. They provided ordering among polar adjectives that bear the same semantics using a corpus-based approach. On the contrary, Sharma et al., (2015) used publicly available embeddings (word2vec) of words to assign intensity to words. Le"
D17-1058,P97-1023,0,0.158732,"e able to separate the words like love and hate to the opposite ends of the spectrum. Tang et al., (2014) proposed a method to learn sentiment specific word embeddings from tweets with emoticons as distantsupervised corpora without any manual annotation. Specifically, they developed three neural networks to effectively incorporate the supervision from sentiment polarity of text in their loss functions. Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the work addressed the problem of finding polarity orientation of the adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wilson et al., 2005; Fahrni and Klenner, 2008; Dragut et al., 2010; Taboada and Grieve, 2004; Baccianella et al., 2010). The task of ranking polar words has received much attention recently due to the vital role of word’s intensity in several real world applications. Most of the literature on intensity ranking consists of manual approaches or corpus-based approaches. Affective Norms (Warriner et al., 2013), SentiStrength (Thelwall et al., 2010), SoCAL (Taboada et al., 2011), and LABMT (Dodds et al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such public"
D17-1058,P14-1146,0,0.246311,"resent a semi-supervised approach to establish a continuous intensity ranking among polar adjectives having the same semantics. Essentially, our approach is a refinement of the work done by Sharma et al., (2015). They also built a system that generates intensity of the words that bear the same semantics; however, their system considers only three discrete intensity levels, viz., low, medium and high. The important feature of our approach is that it uses Sentiment Specific Word Embeddings (SSWE). SSWE are an enhancement to the normal word embeddings with respect to the sentiment analysis task (Tang et al., 2014). SSWE capture syntactic, semantic as well as sentiment information, unlike normal word embeddings (word2vec and GloVe), which capture only syntactic and semantic information. Our Contribution: We propose an approach that generates a continuous (finer) intensity ranking among polar words, which belong to the same semantic category. In addition, we show that SSWE produce a significantly better intensity ranking scale than word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which do not capture sentiment information of the words. The remaining paper is organized as follows. Sect"
D17-1058,N16-1095,0,0.022235,"of the adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wilson et al., 2005; Fahrni and Klenner, 2008; Dragut et al., 2010; Taboada and Grieve, 2004; Baccianella et al., 2010). The task of ranking polar words has received much attention recently due to the vital role of word’s intensity in several real world applications. Most of the literature on intensity ranking consists of manual approaches or corpus-based approaches. Affective Norms (Warriner et al., 2013), SentiStrength (Thelwall et al., 2010), SoCAL (Taboada et al., 2011), and LABMT (Dodds et al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such publicly available sentiment intensity lexicons which are manually created. Corpus-based approaches follow the assumption that the polarity of a new word can be inferred from the corpus (Hatzivassiloglou and McKeown, 1993; Kiritchenko et al., 2014; De Melo and Bansal, 2013). Corpus-based approaches require a huge amount of data, otherwise they suffer from the data sparsity problem. None of the these approaches considers the concept of semantics of adjectives, assuming one single intensity scale for all adjectives. Ruppenhofer et al., (2014) made the first attempt in this direct"
D17-1058,H05-2018,0,0.0600629,"o the opposite ends of the spectrum. Tang et al., (2014) proposed a method to learn sentiment specific word embeddings from tweets with emoticons as distantsupervised corpora without any manual annotation. Specifically, they developed three neural networks to effectively incorporate the supervision from sentiment polarity of text in their loss functions. Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the work addressed the problem of finding polarity orientation of the adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wilson et al., 2005; Fahrni and Klenner, 2008; Dragut et al., 2010; Taboada and Grieve, 2004; Baccianella et al., 2010). The task of ranking polar words has received much attention recently due to the vital role of word’s intensity in several real world applications. Most of the literature on intensity ranking consists of manual approaches or corpus-based approaches. Affective Norms (Warriner et al., 2013), SentiStrength (Thelwall et al., 2010), SoCAL (Taboada et al., 2011), and LABMT (Dodds et al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such publicly available sentiment intensity l"
D17-1058,P05-1015,0,0.150356,"t attempt in this direction. They provided ordering among polar adjectives that bear the same semantics using a corpus-based approach. On the contrary, Sharma et al., (2015) used publicly available embeddings (word2vec) of words to assign intensity to words. Learning of word embeddings does not require annotated (labeled) corpus. 4 Data and Resources In this work, we have used the 52 polar semantic categories from the FrameNet data.2 FrameNet1.5 (Baker et al., 1998) is a lexical resource which groups words based on their semantics.3 We also used a star-rated movie review corpus of 5006 files (Pang and Lee, 2005) to extract the pivot for each semantic category.4 Though our approach uses a corpus, its use is limited to identification of pivot. Intensity ranking of other words of the semantic category is derived by exploiting the cosineThe embeddings used in our work are sentiment specific word embeddings. Integration of sentiment information of a word with syntactic and semantic information makes our approach more accurate for fine-grained sentiment intensity ranking of words. 2 Sharma et al., (2015) also have presented their results using the same 52 polar semantic categories of the FrameNet data. 3 A"
D17-1058,D14-1162,0,0.0806466,"ddings (SSWE). SSWE are an enhancement to the normal word embeddings with respect to the sentiment analysis task (Tang et al., 2014). SSWE capture syntactic, semantic as well as sentiment information, unlike normal word embeddings (word2vec and GloVe), which capture only syntactic and semantic information. Our Contribution: We propose an approach that generates a continuous (finer) intensity ranking among polar words, which belong to the same semantic category. In addition, we show that SSWE produce a significantly better intensity ranking scale than word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which do not capture sentiment information of the words. The remaining paper is organized as follows. Section 2 describes the previous work related to intensity ranking task. Section 3 describes the different word embeddings explored in the paper. Section 4 gives the description of the data and the resources. Section 5 provides details of the gold Identification of intensity ordering among polar (positive or negative) words which have the same semantics can lead to a finegrained sentiment analysis. For example, master, seasoned and familiar point to different intensity levels, though they al"
D17-1058,E14-4023,0,0.62652,"al., 2011), Best–Worst Scaling (Kiritchenko and Mohammad, 2016) are a few such publicly available sentiment intensity lexicons which are manually created. Corpus-based approaches follow the assumption that the polarity of a new word can be inferred from the corpus (Hatzivassiloglou and McKeown, 1993; Kiritchenko et al., 2014; De Melo and Bansal, 2013). Corpus-based approaches require a huge amount of data, otherwise they suffer from the data sparsity problem. None of the these approaches considers the concept of semantics of adjectives, assuming one single intensity scale for all adjectives. Ruppenhofer et al., (2014) made the first attempt in this direction. They provided ordering among polar adjectives that bear the same semantics using a corpus-based approach. On the contrary, Sharma et al., (2015) used publicly available embeddings (word2vec) of words to assign intensity to words. Learning of word embeddings does not require annotated (labeled) corpus. 4 Data and Resources In this work, we have used the 52 polar semantic categories from the FrameNet data.2 FrameNet1.5 (Baker et al., 1998) is a lexical resource which groups words based on their semantics.3 We also used a star-rated movie review corpus o"
D17-1058,I13-1076,1,0.896658,"Missing"
D17-1058,D15-1300,1,0.724841,"t Bearing Word Embeddings Raksha Sharma, Arpan Somani, Lakshya Kumar, Pushpak Bhattacharyya Dept. of Computer Science and Engineering IIT Bombay, India raksha,somani,lakshya,pb@cse.iitb.ac.in Abstract respectively. Identification of intensity ranking among the words which have the same semantics can facilitate such a fine-grained sentiment analysis as exemplified in 1(a), 1(b) and 1(c).1 In this paper, we present a semi-supervised approach to establish a continuous intensity ranking among polar adjectives having the same semantics. Essentially, our approach is a refinement of the work done by Sharma et al., (2015). They also built a system that generates intensity of the words that bear the same semantics; however, their system considers only three discrete intensity levels, viz., low, medium and high. The important feature of our approach is that it uses Sentiment Specific Word Embeddings (SSWE). SSWE are an enhancement to the normal word embeddings with respect to the sentiment analysis task (Tang et al., 2014). SSWE capture syntactic, semantic as well as sentiment information, unlike normal word embeddings (word2vec and GloVe), which capture only syntactic and semantic information. Our Contribution:"
D17-1058,baccianella-etal-2010-sentiwordnet,0,\N,Missing
D17-1058,P98-1013,0,\N,Missing
D17-1058,C98-1013,0,\N,Missing
D19-1566,W18-3301,0,0.0474043,"ical correlation analysis approach (En-SLDCCA) to learn the multi-modal shared feature representation. Tzirakis et al. (2017) introduced a Long Short Term Memory (LSTM) based end-to-end multi-modal emotion recognition system in which convolutional neural network (CNN) and a deep residual network are used to capture the emotional content for various styles of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Network (TFN) to capture the inter-modality and intra-modality dynamics between the multi-modalities (i.e., text, visual, and acoustic). These works did not take contextual information into account. Poria et al. (2017b) i"
D19-1566,W12-3701,0,0.026074,"distinct features of the input modalities, i.e., text, acoustic and visual; (2) We employ a Context-aware Attention Module (CAM) that identifies and assigns the weights to the neighboring utterances based on their contributing features. It exploits the interactive representations of pairwise modalities to learn the attention weights, and (3) We present new state-of-the-arts for five benchmark datasets for both sentiment and emotion predictions. 2 Related Work Different reviews in (Arevalo et al., 2017; Poria et al., 2016, 2017b; Ghosal et al., 2018; Morency et al., 2011a; Zadeh et al., 2018a; Mihalcea, 2012; Lee et al., 2018; Tsai et al., 2018) suggest that multi-modal sentiment and emotion analysis are relatively new areas as compared to uni-modal analysis. Feature selection (fusion) is a challenging and important task for any multi-modal analysis. Poria et al. (2016) proposed a multi-kernel learning based feature selection method for multimodal sentiment and emotion recognition. A convolutional deep belief network (CDBN) is proposed in (Ranganathan et al., 2016) to learn salient multi-modal features of low-intensity expressions of emotions, whereas Lee et al. (2018) introduced a convolutional"
D19-1566,W14-4012,0,0.0408248,"Missing"
D19-1566,P13-1096,0,0.381682,"Missing"
D19-1566,D18-1382,1,0.554739,"e (IIM) that aims to learn the interaction among the diverse and distinct features of the input modalities, i.e., text, acoustic and visual; (2) We employ a Context-aware Attention Module (CAM) that identifies and assigns the weights to the neighboring utterances based on their contributing features. It exploits the interactive representations of pairwise modalities to learn the attention weights, and (3) We present new state-of-the-arts for five benchmark datasets for both sentiment and emotion predictions. 2 Related Work Different reviews in (Arevalo et al., 2017; Poria et al., 2016, 2017b; Ghosal et al., 2018; Morency et al., 2011a; Zadeh et al., 2018a; Mihalcea, 2012; Lee et al., 2018; Tsai et al., 2018) suggest that multi-modal sentiment and emotion analysis are relatively new areas as compared to uni-modal analysis. Feature selection (fusion) is a challenging and important task for any multi-modal analysis. Poria et al. (2016) proposed a multi-kernel learning based feature selection method for multimodal sentiment and emotion recognition. A convolutional deep belief network (CDBN) is proposed in (Ranganathan et al., 2016) to learn salient multi-modal features of low-intensity expressions of emo"
D19-1566,P17-1081,0,0.0333009,"nce in (Patwardhan, 2017). Similar work on feature-level fusion based on self- attention mechanism is reported in (Hazarika et al., 2018). Fu et al. (2017) introduced an enhanced sparse local discriminative canonical correlation analysis approach (En-SLDCCA) to learn the multi-modal shared feature representation. Tzirakis et al. (2017) introduced a Long Short Term Memory (LSTM) based end-to-end multi-modal emotion recognition system in which convolutional neural network (CNN) and a deep residual network are used to capture the emotional content for various styles of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Netw"
D19-1566,W18-3304,0,0.0298946,"Missing"
D19-1566,P18-1208,0,0.317135,"Missing"
D19-1566,P17-1142,0,0.0295202,"fine the following setups for our experiments. • Two-class (pos and neg) classification: MOSEI, MOSI, ICT-MMMO, and MOUD. • Three-class (pos, neu, and neg) classification: YouTube. • Five-class (strong pos, weak pos, neu, weak neg, and strong neg) classification: MOSEI. • Seven-class (strong pos, moderate pos, weak pos, neu, weak neg, moderate neg, and strong neg) classification: MOSEI and MOSI. • Intensity prediction: MOSEI and MOSI. 4.3 Experiments We implement our proposed model on the Pythonbased Keras deep learning library. As the evaluation metric, we employ accuracy (weighted accuracy (Tong et al., 2017)) and F1-score for the classification problems, while for the intensity prediction task, we compute Pearson correlation scores and mean-absolute-error (MAE). We evaluate our proposed CIA model on five benchmark datasets i.e., MOUD, MOSI, YouTube, ICT-MMMO, and MOSEI. For all the datasets, we perform grid search to find the optimal hyperparameters (c.f. Table 4). Though we push for a generic hyper-parameter configuration for all datasets, in some cases, a different choice of the parameter has a significant effect. Therefore, we choose different parameters for different datasets for our experime"
D19-1566,D17-1115,0,0.0196845,"of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Network (TFN) to capture the inter-modality and intra-modality dynamics between the multi-modalities (i.e., text, visual, and acoustic). These works did not take contextual information into account. Poria et al. (2017b) introduced an Long Short Term Memory (LSTM) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances. In another work, Poria et al. (2017c) proposed a user opinion based framework to combine all the multi-modal inputs (i.e., visual, acoustic, and textual) by applying a multi-ker"
E17-1077,D14-1200,0,0.246959,"o-end relation extraction : i) identifying boundaries of entity mentions, ii) identifying entity types of these mentions and iii) identifying appropriate semantic relation for each pair of mentions. We refer to this model as AWP-NN, i.e. All Word Pairs model using Neural Networks. Here, annotations of all these three sub-tasks can be represented by assigning an appropriate label to each pair of words. It is not necessary to assign label to all possible word pairs; rather ith word is paired with j th word only when j ≥ i. AWP-NN model is motivated from the table representation idea proposed by Miwa and Sasaki (2014) but differs significantly from it in following ways: 1. boundary identification is modelled with the help of a special relation type (WEM) instead of BIO (Begin, Inside, Other) encoding or BILOU (Begin, Inside, Last, Unit, Other) encoding 2. neural network model for prediction of appropriate label for each word pair instead of structured prediction Labels predicted by the AWP-NN model for each word pair can then be used to construct the end-to-end relation extraction output as described in tables 1 and 2. Consider the example sentence from Section 2. Table 3 shows true annotations of all word"
E17-1077,H05-1091,0,0.449571,"ne output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004. 1 Introduction The task of relation extraction (RE) deals with identifying whether any pre-defined semantic relation holds between a pair of entity mentions in the given sentence. Pure relation extraction techniques (Zhou et al., 2005; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Qian et al., 2008) assume that for a sentence, gold-standard entity 1 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-edt-v4.2.6.pdf 2 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-rdc-v4.3.2.PDF 818 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 818–827, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Entity Mention His sister Mary Jones United Kingdom Boundaries (0, 0) (1, 1) (2, 3) (7, 8) Entity Type PER PER PER GPE Entity Mention Pair His, sist"
E17-1077,P11-1056,0,0.314314,"for MLN inference. The value of K (maximum length of dependency path, see Figure 1) was set to be 4, hence all word pairs having length of dependency path more than 4 were assumed to have NULL label. ACE 2004 dataset (Doddington et al., 2004) is the most widely used dataset5 for reporting relation extraction performance. We use this dataset to demonstrate effectiveness of our approach for endto-end relation extraction using AWP-NN model and MLN inference. We perform 5-fold crossvalidation on this dataset where the folds are formed at the document level. We follow the same assumptions made by (Chan and Roth, 2011; Li and Ji, 2014; Pawar et al., 2016), which are - ignore the DISC relation, do not consider implicit relations (resulting due to intra-sentence coreferences) as false positives and use coarse-level entity and relation types. Direction of Relations: Out of 6 coarse-level relation types that we are considering, we need not model direction for relation types PER-SOC, GPE-AFF and ART. Because in case of these relations, given the entity types of their arguments, the direction of relation is not necessary or becomes implicit. As PER-SOC is a social relation between two PER entity mentions, the di"
E17-1077,C08-1088,0,0.105996,"del by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004. 1 Introduction The task of relation extraction (RE) deals with identifying whether any pre-defined semantic relation holds between a pair of entity mentions in the given sentence. Pure relation extraction techniques (Zhou et al., 2005; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Qian et al., 2008) assume that for a sentence, gold-standard entity 1 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-edt-v4.2.6.pdf 2 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-rdc-v4.3.2.PDF 818 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 818–827, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Entity Mention His sister Mary Jones United Kingdom Boundaries (0, 0) (1, 1) (2, 3) (7, 8) Entity Type PER PER PER GPE Entity Mention Pair His, sister His, Mary Jones s"
E17-1077,doddington-etal-2004-automatic,0,0.47206,"om pb@cse.iitb.ac.in Abstract mentions (i.e. boundaries as well as types) in it are known. In contrast, end-to-end relation extraction deals with plain sentences without assuming any knowledge of entity mentions in them. The task of end-to-end relation extraction consists of three sub-tasks: i) identifying boundaries of entity mentions, ii) identifying entity types of these mentions and iii) identifying appropriate semantic relation for each pair of mentions. First two sub-tasks correspond to the Entity Detection and Tracking task defined by the the Automatic Content Extraction (ACE) program (Doddington et al., 2004) and the third sub-task corresponds to the Relation Detection and Characterization (RDC) task. ACE standard defined 7 entity types1 : PER (person), ORG (organization), LOC (location), GPE (geopolitical entity), FAC (facility), VEH (vehicle) and WEA (weapon). It also defined 7 coarse level relation types2 : EMP-ORG (employment), PERSOC (personal/social), PHYS (physical), GPEAFF (GPE affiliation), OTHER-AFF (PER/ORG affiliation), ART (agent-artifact) and DISC (discourse). Traditionally, the three sub-tasks of end-to-end relation extraction are carried out serially in a “pipeline” fashion. In thi"
E17-1077,C02-1151,0,0.403793,"yment), PERSOC (personal/social), PHYS (physical), GPEAFF (GPE affiliation), OTHER-AFF (PER/ORG affiliation), ART (agent-artifact) and DISC (discourse). Traditionally, the three sub-tasks of end-to-end relation extraction are carried out serially in a “pipeline” fashion. In this case, the errors in any sub-task affect subsequent sub-tasks. Another disadvantage of this “pipeline” approach is that it allows only one-way information flow, i.e. the knowledge about entities is used for identifying relations but not vice versa. Hence to overcome this problem, several approaches (Roth and Yih, 2004; Roth and Yih, 2002; Singh et al., 2013; Li and Ji, 2014) were proposed which carried out these subtasks jointly rather than in “pipeline” manner. We propose a new approach which combines End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a “pipeline” fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly results in better pe"
E17-1077,N07-1015,0,0.225706,"e also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004. 1 Introduction The task of relation extraction (RE) deals with identifying whether any pre-defined semantic relation holds between a pair of entity mentions in the given sentence. Pure relation extraction techniques (Zhou et al., 2005; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Qian et al., 2008) assume that for a sentence, gold-standard entity 1 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-edt-v4.2.6.pdf 2 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-rdc-v4.3.2.PDF 818 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 818–827, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Entity Mention His sister Mary Jones United Kingdom Boundaries (0, 0) (1, 1) (2, 3) (7, 8) Entity Type PER PER PER GPE Ent"
E17-1077,W04-2401,0,0.641924,"es2 : EMP-ORG (employment), PERSOC (personal/social), PHYS (physical), GPEAFF (GPE affiliation), OTHER-AFF (PER/ORG affiliation), ART (agent-artifact) and DISC (discourse). Traditionally, the three sub-tasks of end-to-end relation extraction are carried out serially in a “pipeline” fashion. In this case, the errors in any sub-task affect subsequent sub-tasks. Another disadvantage of this “pipeline” approach is that it allows only one-way information flow, i.e. the knowledge about entities is used for identifying relations but not vice versa. Hence to overcome this problem, several approaches (Roth and Yih, 2004; Roth and Yih, 2002; Singh et al., 2013; Li and Ji, 2014) were proposed which carried out these subtasks jointly rather than in “pipeline” manner. We propose a new approach which combines End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a “pipeline” fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly"
E17-1077,W10-2924,0,0.0212855,"ntence level. Related Work There have been multiple lines of research for jointly modelling and extracting entities and relations. Integer Linear Programming (ILP) based approaches (Roth and Yih, 2004; Roth and Yih, 2007) were the earliest ones. Here, various local decisions are associated with suitable “cost” values and they are represented using an integer linear program. The optimal solution to this integer linear program provides the best global output. Another significant lines of research were Probabilistic Graphical Models (Roth and Yih, 2002; Singh et al., 2013), Card-pyramid parsing (Kate and Mooney, 2010) and Structured Prediction (Li and Ji, 2014; Li et al., 2014; Miwa and Sasaki, 2014). Four previous approaches (Miwa and Sasaki, 2014; Li and Ji, 2014; Pawar et al., 2016; Miwa and Bansal, 2016) are the most similar to our approach in the sense that they all address the problem of end-to-end relation extraction without assuming gold-standard entity mention boundaries like the earlier approaches. Our idea of labelling “all word pairs” is similar to the table representation idea of Miwa and Sasaki (2014). The major difference is that they identify boundaries of mentions through BIO encoding of l"
E17-1077,P14-1038,0,0.435756,"physical), GPEAFF (GPE affiliation), OTHER-AFF (PER/ORG affiliation), ART (agent-artifact) and DISC (discourse). Traditionally, the three sub-tasks of end-to-end relation extraction are carried out serially in a “pipeline” fashion. In this case, the errors in any sub-task affect subsequent sub-tasks. Another disadvantage of this “pipeline” approach is that it allows only one-way information flow, i.e. the knowledge about entities is used for identifying relations but not vice versa. Hence to overcome this problem, several approaches (Roth and Yih, 2004; Roth and Yih, 2002; Singh et al., 2013; Li and Ji, 2014) were proposed which carried out these subtasks jointly rather than in “pipeline” manner. We propose a new approach which combines End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a “pipeline” fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly results in better performance. We propose a single, joint"
E17-1077,D14-1198,0,0.0458451,"Missing"
E17-1077,P05-1053,0,0.203299,"ation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004. 1 Introduction The task of relation extraction (RE) deals with identifying whether any pre-defined semantic relation holds between a pair of entity mentions in the given sentence. Pure relation extraction techniques (Zhou et al., 2005; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Qian et al., 2008) assume that for a sentence, gold-standard entity 1 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-edt-v4.2.6.pdf 2 www.ldc.upenn.edu/sites/www.ldc. upenn.edu/files/english-rdc-v4.3.2.PDF 818 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 818–827, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Entity Mention His sister Mary Jones United Kingdom Boundaries (0, 0) (1, 1) (2, 3) (7, 8) Entity Ty"
E17-1077,P16-1105,0,0.511376,"s of one or both the mentions are identified incorrectly. 3. entity+relation extraction: It is end-to-end relation extraction which includes boundary identification, entity type classification and relation type classification. Here, correct relation label for an entity mention pair is counted as a true positive only if boundaries and entity types of both the mentions are identified correctly. It can be observed in the table 5 that end-toend relation extraction performance of our AWPNN model is better than all the 4 previous approaches (Chan and Roth, 2011; Li and Ji, 2014; Pawar et al., 2016; Miwa and Bansal, 2016) on the ACE 2004 dataset. However, the AWP-NN+MLN approach which uses MLN inference to revise AWP-NN predictions during decoding, achieves the best performance. • ..the fisheries section of the Gulf Coast Research Laboratory.. • ..company that owned Road & Track.. Here, EMP-ORG relation exists between ORG entity mentions fisheries section and Gulf Coast Research Laboratory. Whereas, EMPORG-R relation holds between that and Road & Track. Hence, we consider 9 distinct relation types: EMP-ORG, EMP-ORG-R, PHYS, PHYS-R, OTHER-AFF, OTHER-AFF-R, PER-SOC, 5 Implementation details 6 279 instances of ty"
E17-1109,grouin-2014-biomedical,0,0.0252279,"utperform the dictionary based methods. Some of the recent works in BNER includes the unsupervised model as proposed in (Zhang and Elhadad, 2013), and the system based on CRF (Li et al., 2015a). A two-phase approach based on semi-Markov CRF is proposed in (Yang and Zhou, 2014). In the first phase boundaries of entities are identified while in the second phase semantic labeling is performed to label the detected entities. A CRF based system has been proposed by (Tang et al., 2015), where in the first step boundaries of NEs are identified and in the second step appropriate labels are assigned. (Grouin, 2014) performed experiments on the i2b2/VA-2010 challenge dataset to detect bacteria and biotopes names. They developed a model based on CRFs. An unsupervised approach is proposed in (Han et al., 2016) that made use of clustering based active learning. They have used Shared Nearest Neighbor (SNN) clustering technique. The work reported in (Li et al., 2015a), authors have proposed a parallel CRF algorithm (MapReduce CRF) which provides a mechanism to minimise the time taken for CRF learning. They showed that the proposed approach outperforms other traditional models in terms of time and efficiency."
E17-1109,W04-1219,0,0.045718,"ation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account t"
E17-1109,W02-0301,0,0.116279,"to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these fe"
E17-1109,W04-1213,0,0.0899452,"m. The challenges as of these kinds are the primary causes behind the low accuracies of the systems developed for entity extraction in biomedi1159 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1159–1170, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics cal text. The research challenges have been addressed in the literature including in some sharedtask challenges, such as JNLPBA (Joint Workshop on Natural Language Processing in Biomedicine and its Applications) in 2004 (Kim et al., 2004) and BioCreative (Critical Assessment for Information Extraction in Biology Challenge) II GM (gene mention) subtask in 2007 (Smith et al., 2008). Over the years several benchmark corpora have been created that do not conform to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised"
E17-1109,I05-1057,0,0.0287701,"er domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account to the problem of data sparsity. In the recent past, there has been huge interest in using large unlabeled corpus to generate word representation feat"
E17-1109,W16-5102,0,0.0181422,"ed function that can efficiently solve the full NER task. The work proposed in (Tohidi et al., 2014) aims to improve the performance of entity extraction using statistical character-based syntax similarity (SCSS) algorithm. This algorithm computes the similarity between the identified candidate entities and a known set of well-known NEs. This set of NEs is created by extracting the most frequently occurring NEs in the GENIA V3.0 corpus. In recent times deep learning based approaches such as Recurrent Neural Network and Bi-directional LSTM have also used for entity extraction(Li et al., 2015b; Limsopatham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) et"
E17-1109,N04-1043,0,0.0608219,"portant role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene expression (Ding and Peng, 2005), face recognition (Seal et al., 2015) and signal processing (Ala"
E17-1109,W04-1221,0,0.0914003,"ed for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account to the problem of data sparsity. In the recent past, there has been huge interest in using large unlabeled corpus to generate word"
E17-1109,P13-1045,0,0.0532197,"t al., 2015b; Limsopatham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene"
E17-1109,P10-1040,0,0.079586,"tham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene expression (Ding and P"
E17-1109,W04-1216,0,0.055746,"orpora have been created that do not conform to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Lan"
I08-1067,J90-2002,0,0.782551,"Missing"
I08-1067,N04-1014,0,0.00857381,"ng these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods require a parser for the target language and, in some cases, the source language too. Yamada and Knight (2001) propose a model that transforms target language parse trees to source language strings by applying reordering, insertion, and translation operations at each node of the tree. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves signiﬁcant improvements over a phrasebased baseline model for Japanese-English translation. Recently, various preprocessing approaches have been proposed for handling syntax within SMT. These algorithms attempt to reconcile the wordorder differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. Nießen and Ney (2004) propose some restructuring steps for German-English SMT. Popovi"
I08-1067,2005.mtsummit-papers.35,0,0.191748,"Missing"
I08-1067,D07-1091,0,0.0149464,"coding cycles. Nießen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for Spanish-English and SerbianEnglish translation. Collins et al. (2006) propose German clause restructuring to improve GermanEnglish SMT. The use of morphological information for SMT has been reported in (Nießen and Ney, 2004) and (Popovic and Ney, 2006). The detailed experiments by Nießen and Ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data. Recent work by Koehn and Hoang (2007) pro514 poses factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model. Our work uses a preprocessing approach for incorporating syntactic information within a phrasebased SMT system. For incorporating morphology, we use a simple sufﬁx removal program for Hindi and a morphological analyzer for English. These aspects are described in detail in the next section. 3 3.1 Syntactic & Morphological Information for English-Hindi SMT 3.2 Phrase-Based SMT: the Baseline Given a source sentence f , SMT chooses as"
I08-1067,N03-1017,0,0.19042,"to the techniques used for incorporating syntactic and morphological information within this system. Experimental results are discussed in section 4. Section 5 concludes the paper with some directions for future work. 2 Related Work Statistical translation models have evolved from the word-based models originally proposed by Brown et al. (1990) to syntax-based and phrase-based techniques. The beginnings of phrase-based translation can be seen in the alignment template model introduced by Och et al. (1999). A joint probability model for phrase translation was proposed by Marcu and Wong (2002). Koehn et al. (2003) propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the IBM models (Brown et al., 1990). Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods require a parser for the target language and, in some cases, the source language too. Yamada and Knight (2001) pr"
I08-1067,W02-1018,0,0.0135127,"hed brieﬂy, leading up to the techniques used for incorporating syntactic and morphological information within this system. Experimental results are discussed in section 4. Section 5 concludes the paper with some directions for future work. 2 Related Work Statistical translation models have evolved from the word-based models originally proposed by Brown et al. (1990) to syntax-based and phrase-based techniques. The beginnings of phrase-based translation can be seen in the alignment template model introduced by Och et al. (1999). A joint probability model for phrase translation was proposed by Marcu and Wong (2002). Koehn et al. (2003) propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the IBM models (Brown et al., 1990). Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods require a parser for the target language and, in some cases, the source language too. Yamada"
I08-1067,P04-1083,0,0.0126583,"shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods require a parser for the target language and, in some cases, the source language too. Yamada and Knight (2001) propose a model that transforms target language parse trees to source language strings by applying reordering, insertion, and translation operations at each node of the tree. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves signiﬁcant improvements over a phrasebased baseline model for Japanese-English translation. Recently, various preprocessing approaches have been proposed for handling syntax within SMT. These algorithms attempt to reconcile the wordorder differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. Nießen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) re"
I08-1067,P01-1067,0,0.0606018,"(2002). Koehn et al. (2003) propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the IBM models (Brown et al., 1990). Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods require a parser for the target language and, in some cases, the source language too. Yamada and Knight (2001) propose a model that transforms target language parse trees to source language strings by applying reordering, insertion, and translation operations at each node of the tree. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves signiﬁcant improvements over a phrasebased baseline model for Japanese-English translation. Recently, various preprocessing approaches have been proposed for handling syntax within SMT. These algorithms attempt to reconcile the wordorder differences between the source a"
I08-1067,N01-1006,0,0.00834066,"e model, we compared various n-gram models, and found trigram models with modiﬁed Kneser-Ney smoothing to be the best performing (Chen and Goodman, 1998). One language model was learnt from the Hindi part of the 5000 sentence training corpus. The larger monolingual Hindi corpus was used to learn another language model. The SRILM toolkit 4 was used for the language modeling experiments. The development corpus was used to set weights for the language models, the distortion model, the phrase translation model etc. using minimum error rate training. Decoding was performed using Pharaoh 5 . fnTBL (Ngai and Florian, 2001) was used to POS tag the English corpus, and Bikel’s parser was used for parsing. The reordering program was written using the perl module Parse::RecDescent. We evaluated the various techniques on the following criteria. For the objective criteria (BLEU and mWER), two reference translations per sentence were used. • BLEU (Papineni et al., 2001): This measures 4 5 http://www.speech.sri.com/projects/srilm/ http://www.isi.edu/licensed-sw/pharaoh/ • SSER (subjective sentence error rate) (Nießen et al., 2000): This is calculated using human judgements. Each sentence was judged by a human evaluator"
I08-1067,niessen-etal-2000-evaluation,0,0.014715,"etc. using minimum error rate training. Decoding was performed using Pharaoh 5 . fnTBL (Ngai and Florian, 2001) was used to POS tag the English corpus, and Bikel’s parser was used for parsing. The reordering program was written using the perl module Parse::RecDescent. We evaluated the various techniques on the following criteria. For the objective criteria (BLEU and mWER), two reference translations per sentence were used. • BLEU (Papineni et al., 2001): This measures 4 5 http://www.speech.sri.com/projects/srilm/ http://www.isi.edu/licensed-sw/pharaoh/ • SSER (subjective sentence error rate) (Nießen et al., 2000): This is calculated using human judgements. Each sentence was judged by a human evaluator on the following ﬁve-point scale, and the SSER was calculated as described in (Nießen et al., 2000). 0 1 2 3 4 Nonsense Roughly understandable Understandable Good Perfect Again, the lower the SSER, the better the translation. Table 4 shows the results of the evaluation. We ﬁnd that using syntactic preprocessing brings substantial improvements over the baseline phrasebased system. While the impact of morphological information is not seen in the BLEU and mWER scores, the subjective scores reveal the effect"
I08-1067,J04-2003,0,0.160784,"ertion, and translation operations at each node of the tree. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves signiﬁcant improvements over a phrasebased baseline model for Japanese-English translation. Recently, various preprocessing approaches have been proposed for handling syntax within SMT. These algorithms attempt to reconcile the wordorder differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. Nießen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for Spanish-English and SerbianEnglish translation. Collins et al. (2006) propose German clause restructuring to improve GermanEnglish SMT. The use of morphological information for SMT has been reported in (Nießen and Ney, 2004) and (Popovic and Ney, 2006). The detailed experiments by Nießen and Ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data. Recent work by Koehn and Hoang (2007) pro514 poses"
I08-1067,W99-0604,0,0.0321651,". Section 3 describes our approach – ﬁrst, the phrase-based baseline system is sketched brieﬂy, leading up to the techniques used for incorporating syntactic and morphological information within this system. Experimental results are discussed in section 4. Section 5 concludes the paper with some directions for future work. 2 Related Work Statistical translation models have evolved from the word-based models originally proposed by Brown et al. (1990) to syntax-based and phrase-based techniques. The beginnings of phrase-based translation can be seen in the alignment template model introduced by Och et al. (1999). A joint probability model for phrase translation was proposed by Marcu and Wong (2002). Koehn et al. (2003) propose certain heuristics to extract phrases that are consistent with bidirectional word-alignments generated by the IBM models (Brown et al., 1990). Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003). Syntax-based models use parse-tree representations of the sentences in the training data to learn, among other things, tree transformation probabilities. These methods requ"
I08-1067,P03-1021,0,0.0309641,"of collected phrase pairs, the phrase translation probability is calculated by relative frequency: count(f , e) φ(f |e) =  f count(f, e) Lexical weighting, which measures how well words within phrase pairs translate to each other, validates the phrase translation, and addresses the problem of data sparsity. The language model p(e) used in our baseline system is a trigram model with modiﬁed Kneser-Ney smoothing (Chen and Goodman, 1998). The weights for the various components of the model (phrase translation model, language model, distortion model etc.) are set by minimum error rate training (Och, 2003). Syntactic Information As mentioned in section 2, phrase-based models have emerged as the most successful method for SMT. These models, however, do not handle syntax in a natural way. Reordering of phrases during translation is typically managed by distortion models, which have proved not entirely satisfactory (Collins et al., 2006), especially for language pairs that differ a lot in terms of word-order. We use a preprocessing approach to get over this problem, by reordering the English sentences in the training and test corpora before the SMT system kicks in. This reduces, and often eliminat"
I08-1067,2001.mtsummit-papers.68,0,0.0927563,"Missing"
I08-1067,P02-1040,0,\N,Missing
I08-1067,P06-2100,1,\N,Missing
I08-1067,P05-1066,0,\N,Missing
I08-1067,J08-3004,0,\N,Missing
I11-1078,C10-1063,1,0.826027,"Missing"
I11-1078,C96-1005,0,0.519315,"n pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility"
I11-1078,E09-1005,0,0.072917,"-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking at the counts of its tr"
I11-1078,S10-1013,0,0.0582003,"Missing"
I11-1078,apidianaki-2008-translation,0,0.01889,"presence. Further, in a bilingual setting where parameters need to be ported from one language to another, it is important to associate labels with the clusters induced from the graph partitions so that these clusters can be aligned across languages. This is a difficult proposition and does not fall under the purview of WSI. Hence, in this work we stick to dictionary defined senses as opposed to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need parallel corpora and relies only on in-domain corpora Monolingual approaches to Word Sense Disambiguation are abundant ranging fro"
I11-1078,J94-4003,0,0.150752,"ion (especially in an all-words scenario) for resource constrained languages such as Hindi and Marathi which have very poor web presence. Further, in a bilingual setting where parameters need to be ported from one language to another, it is important to associate labels with the clusters induced from the graph partitions so that these clusters can be aligned across languages. This is a difficult proposition and does not fall under the purview of WSI. Hence, in this work we stick to dictionary defined senses as opposed to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need paral"
I11-1078,P02-1033,0,0.413982,"ially for some of the resource deprived languages. In this context, “Disambiguation by Translation” is a popular paradigm which tries to obviate the need for sense annotated corpora without compromising on accuracy. Such algorithms rely on the frequently made observation that a word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense ma"
I11-1078,C02-1058,0,0.18181,"Note that every word in the Marathi synset is considered to be a translation of the corresponding words in the Hindi synset. Thus, the Marathi words mulgaa, porgaa and por are translations of the Hindi word ladakaa and so on. These synsetspecific translations play a very important role in our work as explained in the next section. from two languages. However, their approach is semi-supervised in contrast to our approach which is unsupervised. Further, they focus on the more specific task of Word Translation Disambiguation (WTD) as opposed to our work which focuses on the broader task of WSD. Kaji and Morimoto (2002) proposed an unsupervised bilingual approach which aligns statistically significant pairs of related words in language L1 with their cross-lingual counterparts in language L2 using a bilingual dictionary. This approach is based on two assumptions (i) words which are most significantly related to a target word provide clues about the sense of the target word and (ii) translations of these related words further reinforce the sense distinctions. The translations of related words thus act as cross-lingual clues for disambiguation. This algorithm when tested on 60 polysemous words (using English as"
I11-1078,D09-1048,1,0.922468,"word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense marked corpus for one of the two languages. In this work, we focus on scenarios where no sense marked corpus is available in either language. Our method requires only untagged in-domain corpora from the two languages. Given such bilingual in-domain corpora (non-parallel) the cou"
I11-1078,W04-0834,0,0.817414,"utions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same dom"
I11-1078,J04-1001,0,0.0261917,"to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need parallel corpora and relies only on in-domain corpora Monolingual approaches to Word Sense Disambiguation are abundant ranging from supervised, 696 Table 1 shows the structure of MultiDict, with one example row standing for the concept of boy. The first column is the pivot describing a concept with a unique ID. The subsequent columns show the words expressing the concept in respective languages (in the example table, English, Hindi and Marathi). The pivot language to which other languages link is Hindi. This approach of"
I11-1078,P04-1036,0,0.374066,"sed and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking at the counts of its translations in different"
I11-1078,P96-1006,0,0.834947,"the sense distributions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belong"
I11-1078,P03-1058,0,0.20434,"esource deprived languages. In this context, “Disambiguation by Translation” is a popular paradigm which tries to obviate the need for sense annotated corpora without compromising on accuracy. Such algorithms rely on the frequently made observation that a word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense marked corpus for on"
I11-1078,H05-1052,0,0.237448,"urrent state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking"
I11-1078,C04-1192,0,0.0628159,"Missing"
I11-1152,W09-0434,0,0.0202729,"auses, but not for non-finite clauses. In our experiments in English-Hindi translation with an SMT system (DTM2), on a test corpus containing around 850 sentences with manually annotated clause boundaries, BLEU improves to 20.4 from the baseline score of 19.4. This statistically significant improvement is also confirmed by subjective (human) evaluation. We also report preliminary work on automatically identifying the kind of clause boundaries appropriate for enforcing reordering constraints. 1 Introduction It has been recognized widely that reordering is the Achilles’ heel of most SMT models (Birch et al., 2009; Hoang and Koehn, 2009), especially when applied to languages with large syntactic differences. In our experiments in EnglishHindi SMT, we observe that it is quite frequent in multi-clause sentences for phrases to move out of their respective clauses due to incorrect reordering. While such mistakes can be avoided by restricting reordering over clause boundaries, this paper demonstrates that such a strategy works well only for finite clauses, and not for non-finite clauses. Clause-wise or part-by-part translation has been a standard approach in traditional transfer-based systems. In Systran, a"
I11-1152,P05-1033,0,0.0968886,"Missing"
I11-1152,N10-1140,0,0.0462483,"Missing"
I11-1152,2007.mtsummit-papers.33,0,0.0360202,"ed in the the Stanford Machine Translation project (Wilks, 1973). Chandrasekar (1994) applies a sentence simplification method to machine translation, where sentences are split at conjunctions, relative pronouns, etc., before translation. Rao et al. (2000) describe a clause-wise translation strategy within an EnglishHindi transfer-based MT system. In the context of SMT, Koehn and Knight (2003) use a dedicated noun phrase (NP) translation subsystem to obtain significant improvements in German-English translation. Other similar work includes (Watanabe et al., 2003) for Japanese-English SMT and (Hewavitharana et al., 2007) for Arabic-English. Sudoh et al. (2010) propose methods to perform clauselevel alignment of the parallel corpus, and to translate clauses (all clauses identified by a syntactic parser) as a unit to improve long-distance reordering in a specialized domain – EnglishJapanese translation of research paper abstracts in the medical domain. While our approach draws from many of the above, it is novel in the following ways: (i) We provide an analysis of different clause types in translation – we show that only some kinds of clauses benefit from the use of reordering constraints, and (ii) We demonstra"
I11-1152,E09-1043,0,0.0242911,"on-finite clauses. In our experiments in English-Hindi translation with an SMT system (DTM2), on a test corpus containing around 850 sentences with manually annotated clause boundaries, BLEU improves to 20.4 from the baseline score of 19.4. This statistically significant improvement is also confirmed by subjective (human) evaluation. We also report preliminary work on automatically identifying the kind of clause boundaries appropriate for enforcing reordering constraints. 1 Introduction It has been recognized widely that reordering is the Achilles’ heel of most SMT models (Birch et al., 2009; Hoang and Koehn, 2009), especially when applied to languages with large syntactic differences. In our experiments in EnglishHindi SMT, we observe that it is quite frequent in multi-clause sentences for phrases to move out of their respective clauses due to incorrect reordering. While such mistakes can be avoided by restricting reordering over clause boundaries, this paper demonstrates that such a strategy works well only for finite clauses, and not for non-finite clauses. Clause-wise or part-by-part translation has been a standard approach in traditional transfer-based systems. In Systran, as described by Hutchins"
I11-1152,N07-1008,0,0.0131062,"in the complement position, the specifier position, or the adjunct position. We may also classify clauses based on whether they play an adjectival, nominal, or adverbial role in the sentence. The focus of the present work, however, is only on the finiteness aspect. 4 Experiments In this section, we first describe briefly our baseline system and our approach to handling clauses within this system. We then summarize the datasets used and our evaluation methodology, before describing the results of various experiments. 4.1 Approach The baseline system we use is DTM2 (a direct translation model) (Ittycheriah and Roukos, 2007). The word-alignments were done using an HMM aligner. We used the best-performing parameter setting in the decoder 1 . A beam search decoder (Ittycheriah and Roukos, 2007) similar to other phrase-based decoders (Tillmann and Ney, 2003) is used for translation. The reordering restriction is applied by treating the relevant clause-boundaries as barriers, and putting a hard constraint on reordering across 1 Specifically, the skip-length (distortion limit) was set to 8. Lower skip-lengths led to much poorer performance. For example, with a skip-length of 4, the BLEU score dropped by around 2 point"
I11-1152,2001.mtsummit-papers.68,0,0.019044,"ause boundaries identified in three ways: (i) manually (section 4.3.1), (ii) automatically using a constituency parser (section 4.3.2), and (iii) automatically using a CRFbased clause-boundary classifer using part-ofspeech and parser features (section 4.3.3). 4.2 Data and Evaluation The system was trained on a parallel corpus with 289k sentences (combining the LDC EnglishHindi parallel texts with internal datasets) consisting of various domains including news, and tested on 844 sentences. The language model was trained on around 1.5 million sentences. Automatic evaluation was done using BLEU (Papineni et al., 2001) with a single reference translation per sentence. Statistical significance of the test results with BLEU was computed using paired bootstrap resampling (Koehn, 2004). Subjective evaluation was performed on hundred randomly selected multi-clause sentences, using a five-point scale. 4.3 4.3.1 Results Manual annotation: finite vs. non-finite clauses For this experiment, the sentences in the test sets were manually annotated with finite and non-finite clause boundaries. The results in table 1 indicate that reordering constraints around finite clauses work much better than around non-finite clause"
I11-1152,W10-1762,0,0.0899885,"ct (Wilks, 1973). Chandrasekar (1994) applies a sentence simplification method to machine translation, where sentences are split at conjunctions, relative pronouns, etc., before translation. Rao et al. (2000) describe a clause-wise translation strategy within an EnglishHindi transfer-based MT system. In the context of SMT, Koehn and Knight (2003) use a dedicated noun phrase (NP) translation subsystem to obtain significant improvements in German-English translation. Other similar work includes (Watanabe et al., 2003) for Japanese-English SMT and (Hewavitharana et al., 2007) for Arabic-English. Sudoh et al. (2010) propose methods to perform clauselevel alignment of the parallel corpus, and to translate clauses (all clauses identified by a syntactic parser) as a unit to improve long-distance reordering in a specialized domain – EnglishJapanese translation of research paper abstracts in the medical domain. While our approach draws from many of the above, it is novel in the following ways: (i) We provide an analysis of different clause types in translation – we show that only some kinds of clauses benefit from the use of reordering constraints, and (ii) We demonstrate significant improvements using this s"
I11-1152,J03-1005,0,0.0294832,"n the finiteness aspect. 4 Experiments In this section, we first describe briefly our baseline system and our approach to handling clauses within this system. We then summarize the datasets used and our evaluation methodology, before describing the results of various experiments. 4.1 Approach The baseline system we use is DTM2 (a direct translation model) (Ittycheriah and Roukos, 2007). The word-alignments were done using an HMM aligner. We used the best-performing parameter setting in the decoder 1 . A beam search decoder (Ittycheriah and Roukos, 2007) similar to other phrase-based decoders (Tillmann and Ney, 2003) is used for translation. The reordering restriction is applied by treating the relevant clause-boundaries as barriers, and putting a hard constraint on reordering across 1 Specifically, the skip-length (distortion limit) was set to 8. Lower skip-lengths led to much poorer performance. For example, with a skip-length of 4, the BLEU score dropped by around 2 points compared to a skip-length of 8 barriers – that is, during decoding, if a new hypothesis requires reordering over a barrier, the hypothesis is discarded. We experiment with clause boundaries identified in three ways: (i) manually (sec"
I11-1152,W01-0708,0,0.0342226,"bracketing guidelines – S, S-BAR, SQ, SBARQ, and SINV). The results were negative (table 2 – row titled parser), indicating that straightforward use of a parser is not sufficient to help in identifying clauses suitable for reordering constraints. The column titled ACI accuracy has the F-measure for automatic clause-boundary identification measured over the entire test corpus. 4.3.3 Automatic clause identification using a CRF classifier We annotated a set of 1500 English sentences with finite-clause boundaries, and used this to train a CRF-based clause-boundary classifier (Ram and Devi, 2008; Tjong et al., 2001). Unigram and bigram word features, unigram, bigram and trigram POS features, and the POS of the following verb group were used in the model (Kashioka et al., 2003) 2 . We see (table 2) that a reasonable gain is obtained using the classifier, though not nearly as much as with manual annotation. 5 Discussion The subjective evaluation scores reveal that only a few translations degrade in quality when reordering constraints are used with finite clauses (largely due to fluency issues of the kind described in section 3). Table 3 shows the number of translations that improved (i.e., the average adeq"
I11-1152,P03-1039,0,0.0249815,"nted in a preprocessing step. Similar methods were used in the the Stanford Machine Translation project (Wilks, 1973). Chandrasekar (1994) applies a sentence simplification method to machine translation, where sentences are split at conjunctions, relative pronouns, etc., before translation. Rao et al. (2000) describe a clause-wise translation strategy within an EnglishHindi transfer-based MT system. In the context of SMT, Koehn and Knight (2003) use a dedicated noun phrase (NP) translation subsystem to obtain significant improvements in German-English translation. Other similar work includes (Watanabe et al., 2003) for Japanese-English SMT and (Hewavitharana et al., 2007) for Arabic-English. Sudoh et al. (2010) propose methods to perform clauselevel alignment of the parallel corpus, and to translate clauses (all clauses identified by a syntactic parser) as a unit to improve long-distance reordering in a specialized domain – EnglishJapanese translation of research paper abstracts in the medical domain. While our approach draws from many of the above, it is novel in the following ways: (i) We provide an analysis of different clause types in translation – we show that only some kinds of clauses benefit fro"
I11-1152,2003.mtsummit-papers.29,0,0.0588785,"is not sufficient to help in identifying clauses suitable for reordering constraints. The column titled ACI accuracy has the F-measure for automatic clause-boundary identification measured over the entire test corpus. 4.3.3 Automatic clause identification using a CRF classifier We annotated a set of 1500 English sentences with finite-clause boundaries, and used this to train a CRF-based clause-boundary classifier (Ram and Devi, 2008; Tjong et al., 2001). Unigram and bigram word features, unigram, bigram and trigram POS features, and the POS of the following verb group were used in the model (Kashioka et al., 2003) 2 . We see (table 2) that a reasonable gain is obtained using the classifier, though not nearly as much as with manual annotation. 5 Discussion The subjective evaluation scores reveal that only a few translations degrade in quality when reordering constraints are used with finite clauses (largely due to fluency issues of the kind described in section 3). Table 3 shows the number of translations that improved (i.e., the average adequacy and fluency score increased) due to clause-based translation, and the number which degraded, among the hundred sentences taken up for subjective evalua2 Adding"
I11-1152,P03-1040,0,0.0333089,"anslation has been a standard approach in traditional transfer-based systems. In Systran, as described by Hutchins and Somers (1992), conjunct and relative clauses were segmented in a preprocessing step. Similar methods were used in the the Stanford Machine Translation project (Wilks, 1973). Chandrasekar (1994) applies a sentence simplification method to machine translation, where sentences are split at conjunctions, relative pronouns, etc., before translation. Rao et al. (2000) describe a clause-wise translation strategy within an EnglishHindi transfer-based MT system. In the context of SMT, Koehn and Knight (2003) use a dedicated noun phrase (NP) translation subsystem to obtain significant improvements in German-English translation. Other similar work includes (Watanabe et al., 2003) for Japanese-English SMT and (Hewavitharana et al., 2007) for Arabic-English. Sudoh et al. (2010) propose methods to perform clauselevel alignment of the parallel corpus, and to translate clauses (all clauses identified by a syntactic parser) as a unit to improve long-distance reordering in a specialized domain – EnglishJapanese translation of research paper abstracts in the medical domain. While our approach draws from ma"
I11-1152,W04-3250,0,0.0625266,"clause-boundary classifer using part-ofspeech and parser features (section 4.3.3). 4.2 Data and Evaluation The system was trained on a parallel corpus with 289k sentences (combining the LDC EnglishHindi parallel texts with internal datasets) consisting of various domains including news, and tested on 844 sentences. The language model was trained on around 1.5 million sentences. Automatic evaluation was done using BLEU (Papineni et al., 2001) with a single reference translation per sentence. Statistical significance of the test results with BLEU was computed using paired bootstrap resampling (Koehn, 2004). Subjective evaluation was performed on hundred randomly selected multi-clause sentences, using a five-point scale. 4.3 4.3.1 Results Manual annotation: finite vs. non-finite clauses For this experiment, the sentences in the test sets were manually annotated with finite and non-finite clause boundaries. The results in table 1 indicate that reordering constraints around finite clauses work much better than around non-finite clauses. 1353 improved 36 35 17 19 finite (manual) finite (auto) non-finite (manual) finite + non-finite (manual) degraded 8 17 10 11 Table 3: Number of translations improv"
I11-1152,P08-1114,0,0.0591404,"Missing"
I11-1152,P02-1040,0,\N,Missing
I13-1076,C12-1036,0,0.226409,"Missing"
I13-1076,W06-1642,0,0.142993,"Missing"
I13-1076,W03-1017,0,0.313989,"Missing"
I13-1076,P02-1053,0,0.0194288,"ppear in a particular type of documents, frequently. If the word has positive sentiment, then it will occur more frequently in positive documents. Consider the following example. In this paper, we focus on finding sentiment words for the movie domain with their polarity as positive or negative. Finding movie domain specific polar words is an appealing task for several reasons. First, providing polarity information about movie reviews is a useful service. Its proof is the popularity of several film review websites 5 . Second, movie reviews are harder to classify than reviews of other products (Turney, 2002) and so is the classification of sentiment words. Our data contains 1000 positive and 1000 negative reviews, all written before 20046 . Movie reviews are accompanied by plot descriptions and plot is not a part of the reviewer’s opinion of the movie. So presence of polar words in plot description can mislead the Chi-Square test. To solve this problem, we clean the corpus by removing the plot description from reviews, before giving it as an input to the Chi-Square test. Cleaning of the corpus is done automatically by finding patterns for plot description in movie reviews. In this paper, we perfo"
I13-1076,H05-1044,0,0.382064,"Missing"
I13-1076,W02-1011,0,\N,Missing
I13-1093,W02-0602,0,0.0364063,"litting (the split is formally defined later) that generate the minimum number of stems (viz., 2) from the above word list is {boy+φ, boy+s, moss+φ, moss+es }, where φ is the null suffix. The minimum stem set is {boy, moss}. Any other splitting, say mosse+s will increase the number of stems. Maximum a posteriori model (Creutz and Lagus, 2007) is a generalization of the Linguistica model, in the sense of being a recursive MDL. This probabilistic approach is more suitable for languages with more than one suffix. Stochastic transducer based model (Clark, 2001) and generative probabilistic model (Snover et al., 2002) are other relevant probabilistic models for stemming. 1 Introduction 774 International Joint Conference on Natural Language Processing, pages 774–780, Nagoya, Japan, 14-18 October 2013. A Markov Random Field by Dreyer (2009) is also a useful probabilistic approach related to unsupervised morphology. separates a word in to its correct stem and correct suffix. E.g., the boy+s is the correct split of boys. Splitset: a set of splits obtained from the whole input word list. For every word in the word list, exactly one split will be there in the splitset. In other words, splitset = { t + x |t · x ∈"
I13-1093,N07-1020,0,0.335875,"x = x′ }. E.g., {bo+ys, girl+φ, play+ing} is a splitset of {boys, girl, playing}. The correct splitset is defined as the set of correct splits of all the words from the given word list. The correct splitset of {boys, girl, playing} is {boy+s, girl+φ, play+ing}. Graph based model (Johnson and Martin, 2003), lazy learning based model (van den Bosch and Daelemans, 1999), clustering based same stem identification model (Hammarstr¨om, 2006a; Hammarstr¨om, 2006b), ParaMor system for paradigm learning (Monson et al., 2008) and full morpheme segmentation and automatic induction of orthographic rules (Dasgupta and Ng, 2007; Dasgupta and Ng, 2006) are also a relevant. 3 Terminology and Notation Ts (splitset): the set of stems from the splitset. E.g., Ts ({bo+ys, girl+φ, play+ing}) = {bo, girl, play}. Let us define some terms and notations used throughout this paper. Xs (splitset): the set of suffixes from the splitset. E.g., Xs ({bo+ys, girl+φ, play+ing}) = {ys, φ, ing}. w: word W : input word list N : number of words in the input word list 4 Problem Definition X: input suffix list x: suffix candidate of w (possible suffix) The stemming problem addressed in this paper is defined as follows. Given a list of word"
I13-1093,D09-1011,0,0.0388642,"Missing"
I13-1093,J01-2001,0,0.109684,"aper, we report construction of a semi supervised stemmer that does stemming by minimizing the total number of distinct stems. The input to the system is the word list along with the legal suffix list of the language. Even if a language does not have an elaborate linguistic tradition and exhaustive body of linguistic work, the language is expected to have at least the legal suffix list for nouns and verbs. Morphology learning is one of the widely attempted problems in NLP. A recent survey by Harald Hammarstr¨om (2011) gives an overall view of unsupervised morphology learning. The Linguistica (Goldsmith, 2001) model based on minimum description length (MDL) principle is one of the benchmark works of unsupervised stemming. In the Linguistica model, the authors defined a signature structure. The objective of their stemming approach is the minimization of total description length, i.e., the description length of stem list, suffix list, signatures and corpus. To get the intuition behind our work, consider the word list {boy, boys, moss, mosses}. The splitting (the split is formally defined later) that generate the minimum number of stems (viz., 2) from the above word list is {boy+φ, boy+s, moss+φ, moss"
I13-1093,J11-2002,0,0.221031,"Missing"
I13-1093,W06-3210,0,0.0452238,"Missing"
I13-1093,N03-2015,0,0.0329169,"correct suffix. E.g., the boy+s is the correct split of boys. Splitset: a set of splits obtained from the whole input word list. For every word in the word list, exactly one split will be there in the splitset. In other words, splitset = { t + x |t · x ∈ W and for any t′ + x′ ∈ splitset, t · x = t′ · x′ → t = t′ and x = x′ }. E.g., {bo+ys, girl+φ, play+ing} is a splitset of {boys, girl, playing}. The correct splitset is defined as the set of correct splits of all the words from the given word list. The correct splitset of {boys, girl, playing} is {boy+s, girl+φ, play+ing}. Graph based model (Johnson and Martin, 2003), lazy learning based model (van den Bosch and Daelemans, 1999), clustering based same stem identification model (Hammarstr¨om, 2006a; Hammarstr¨om, 2006b), ParaMor system for paradigm learning (Monson et al., 2008) and full morpheme segmentation and automatic induction of orthographic rules (Dasgupta and Ng, 2007; Dasgupta and Ng, 2006) are also a relevant. 3 Terminology and Notation Ts (splitset): the set of stems from the splitset. E.g., Ts ({bo+ys, girl+φ, play+ing}) = {bo, girl, play}. Let us define some terms and notations used throughout this paper. Xs (splitset): the set of suffixes fr"
I13-1093,P99-1037,0,0.183551,"Missing"
I13-1122,P06-1032,0,0.218395,"Missing"
I13-1122,J93-2003,0,0.0272122,"Missing"
I13-1122,W13-1703,0,0.115008,"Missing"
I13-1122,C08-1022,0,0.0698622,"Missing"
I13-1122,P05-1033,0,0.0167735,"customized, so to say, for each error, which is a time consuming and fragile process. SMT, on the other hand, treats all errors uniformly, considering error correction as a translation problem. Secondly, problems such as reordering or word insertion are well known in machine translation. Introduction We consider grammar correction as a translation problem - translation from an incorrect sentence to a correct sentence. The correcting system is trained using a parallel corpus of incorrect and their corresponding correct sentences. The system learns SCFG (synchronous context free grammar) rules (Chiang, 2005) during translation. SCFG rules look like this:937 International Joint Conference on Natural Language Processing, pages 937–941, Nagoya, Japan, 14-18 October 2013. corpus of incorrect and correct sentences. The system starts with an alignment to obtain word to word translation probabilities. The second stage is grammar extraction using the hiero style of grammar (Chiang, 2005). Non-terminals are generalized form of phrases, i.e., all possible phrases allowed in the framework of Chiang (2005) are represented by the symbol X. There is another symbol S to start the parse tree. These rules are in"
I13-1122,P11-1094,0,0.0322745,"correction can be seen as a process of translation of incorrect sentences to correct ones. Basically the translation system needs a parallel 1 938 Here S means start of the tree 4 The yield of the tree generates few have arrived, which is the required correction. This is the essence of decoding in hierarchical machine translation. 3.2 Analysis of grammar rules extracted In this section we look at how various grammar corrections have been handled. The various types of errors handled are article choice errors, preposition errors, word-form choice errors and word insertion errors as mentioned in Park and Levy (2011). Apart from these errors, we also discuss errors due to reordering and errors due to unseen verbs which have not been implemented in previous models. Implementation The translation system being used is the Joshua Machine translation system (Li et al., 2010). The SMT based correction pipeline is a six step process in conformity with the Joshua decoder (Ganitkevitch et al., 2012). First we create the dataset in a input folder with six files such as:- 4.1 Article choice errors The article a has been replaced by the before proper nouns like a amazon and a himalayas. The grammar rules are:- 1. tra"
I13-1122,W10-1718,0,0.0167992,". This is the essence of decoding in hierarchical machine translation. 3.2 Analysis of grammar rules extracted In this section we look at how various grammar corrections have been handled. The various types of errors handled are article choice errors, preposition errors, word-form choice errors and word insertion errors as mentioned in Park and Levy (2011). Apart from these errors, we also discuss errors due to reordering and errors due to unseen verbs which have not been implemented in previous models. Implementation The translation system being used is the Joshua Machine translation system (Li et al., 2010). The SMT based correction pipeline is a six step process in conformity with the Joshua decoder (Ganitkevitch et al., 2012). First we create the dataset in a input folder with six files such as:- 4.1 Article choice errors The article a has been replaced by the before proper nouns like a amazon and a himalayas. The grammar rules are:- 1. train.incorrect- Incorrect sentences in our training corpus 2. train.correct- Correct sentences in our training corpus • X → a himalayas X1 , the himalayas X1 • X → a amazon X1 , the amazon X1 3. tune.correct- Incorrect sentences in our development set The rule"
I13-1122,C08-1109,0,0.0240526,"on system can do automatic grammar correction. Section 4 states the grammar rules that are extracted by the system automatically. In section 5, we present our experiments followed by the results in section 6. We conclude in section 7 with pointers to future work. 2 Background Initially the work that has been done in grammar correction is based on identifying grammar errors. Chodorow and Leacock (2000) used an ngram model for error detection by comparing correct ngrams with ngrams to be tested. Later, classification techniques like Maximum entropy models have been proposed (Izumi et al., 2003; Tetreault and Chodorow, 2008; Tetreault and Chodorow, 2008). These classifiers not only identify errors, but also correct them. These methods do not make use of erroneous words thus making error correction similar to the task of filling empty blanks. While in editing sentences, humans often require the information in the erroneous words for grammar correction. In other works, machine translation has been previously used for grammar correction. Brockett (2006) used phrasal based MT for noun correction of ESL students. D´esilets and Hermet (2009) translate from native language L1 to L2 and back to L1 to correct grammar in"
I13-1122,W12-3134,0,0.0126717,"this section we look at how various grammar corrections have been handled. The various types of errors handled are article choice errors, preposition errors, word-form choice errors and word insertion errors as mentioned in Park and Levy (2011). Apart from these errors, we also discuss errors due to reordering and errors due to unseen verbs which have not been implemented in previous models. Implementation The translation system being used is the Joshua Machine translation system (Li et al., 2010). The SMT based correction pipeline is a six step process in conformity with the Joshua decoder (Ganitkevitch et al., 2012). First we create the dataset in a input folder with six files such as:- 4.1 Article choice errors The article a has been replaced by the before proper nouns like a amazon and a himalayas. The grammar rules are:- 1. train.incorrect- Incorrect sentences in our training corpus 2. train.correct- Correct sentences in our training corpus • X → a himalayas X1 , the himalayas X1 • X → a amazon X1 , the amazon X1 3. tune.correct- Incorrect sentences in our development set The rules suggest that a himalayas succeeded by a phrase X1 can be replaced by the himalayas followed by the same phrase. 4. tune.i"
I13-1122,W11-2123,0,0.01407,"esting set 6. test.incorrect- Correct sentences in our testing set • X → X1 at central London, X1 in central London The pipeline starts with preprocessing the corpus, i.e., tokenisation and lowercasing followed by word alignment. The result of word alignment is stored in training.align file. Then a file, “grammar.gz” is created by joshua that stores SCFG rules using information from the training.align file and the training corpus. This process is called grammar generation and is followed by the building of the language model. For developing the language model, the Joshua MT system uses KenLM (Heafield, 2011) toolkit or BerkeleyLM. This is the end of the training process. The steps that follow in the pipeline are tuning and testing. Tuning iterates over the development set to obtain the best parameters for the translation model. At the end of tuning, the system obtains the optimized parameters that can be deployed into the translation model for testing. The testing phase translates sentences from test set to evaluate the overall BLEU score (Papineni et al., 2002). 4.3 Unknown Verb correction Lets say the training data has these sentences • He like milk → He likes milk • They hate the pollution → T"
I13-1122,P02-1040,0,0.0893496,"d grammar generation and is followed by the building of the language model. For developing the language model, the Joshua MT system uses KenLM (Heafield, 2011) toolkit or BerkeleyLM. This is the end of the training process. The steps that follow in the pipeline are tuning and testing. Tuning iterates over the development set to obtain the best parameters for the translation model. At the end of tuning, the system obtains the optimized parameters that can be deployed into the translation model for testing. The testing phase translates sentences from test set to evaluate the overall BLEU score (Papineni et al., 2002). 4.3 Unknown Verb correction Lets say the training data has these sentences • He like milk → He likes milk • They hate the pollution → They hate pollution This system will not be able to correct He hate milk, because hate needs to be corrected to hates and its grammar has no rule for hate → hates. But it has a rule for like → likes. From these two rules, the grammar extractor wont be able to derive hate → hates. This can be solved by splitting likes to like s • He like milk → He like s milk Now extractor will have a rule for this training sentence. • X → He X1 milk, He X1 s milk 939 • X → hat"
I13-1122,A00-2019,0,0.0604858,"he correct sentence is, ‘few have arrived’, the grammar rules extracted are :- The roadmap of the paper is as follows. In section 2, we discuss previous work. In section 3, we elaborate on how hierarchical machine translation system can do automatic grammar correction. Section 4 states the grammar rules that are extracted by the system automatically. In section 5, we present our experiments followed by the results in section 6. We conclude in section 7 with pointers to future work. 2 Background Initially the work that has been done in grammar correction is based on identifying grammar errors. Chodorow and Leacock (2000) used an ngram model for error detection by comparing correct ngrams with ngrams to be tested. Later, classification techniques like Maximum entropy models have been proposed (Izumi et al., 2003; Tetreault and Chodorow, 2008; Tetreault and Chodorow, 2008). These classifiers not only identify errors, but also correct them. These methods do not make use of erroneous words thus making error correction similar to the task of filling empty blanks. While in editing sentences, humans often require the information in the erroneous words for grammar correction. In other works, machine translation has b"
I13-1122,W09-2110,0,0.0510826,"Missing"
I13-1122,N06-1014,0,0.0173853,"correct ngrams with ngrams to be tested. Later, classification techniques like Maximum entropy models have been proposed (Izumi et al., 2003; Tetreault and Chodorow, 2008; Tetreault and Chodorow, 2008). These classifiers not only identify errors, but also correct them. These methods do not make use of erroneous words thus making error correction similar to the task of filling empty blanks. While in editing sentences, humans often require the information in the erroneous words for grammar correction. In other works, machine translation has been previously used for grammar correction. Brockett (2006) used phrasal based MT for noun correction of ESL students. D´esilets and Hermet (2009) translate from native language L1 to L2 and back to L1 to correct grammar in their native languages. Mizumoto (2012) also used phrase-based SMT for error correction. He used large-scale learner corpus to train his system. These translation techniques suffered from lack of good quality parallel corpora and also good translation systems. If high quality parallel corpus can be obtained, the task of grammar correction becomes easy using a powerful translation model like hierarchical phrase based machine transla"
I13-1122,P03-2026,0,\N,Missing
I13-1122,C12-2084,0,\N,Missing
I13-1122,W11-2100,0,\N,Missing
I13-1131,R09-1020,0,0.0235619,"inting to a category same as the query category are picked Introduction The ruling paradigm for Information retrieval (IR) (Manning et al., 2009) is Pseudo Relevance feedback (PRF). In PRF, an assumption is made that the top retrieved documents are relevant to the query for picking expansion terms. Zhai and Lafferty (2001) show that using pseudo relevance feedback on monolingual retrieval improves the 982 International Joint Conference on Natural Language Processing, pages 982–986, Nagoya, Japan, 14-18 October 2013. The more the relevance of D, the less is DKL (θQ |θD ). as expansion terms in Ganesh and Verma (2009). This work exploits the structure only in the form of anchor texts and category information. Techniques to disambiguate query terms based on disambiguation pages of Wikipedia are proposed in (Xu et al., 2009; Lin et al., 2010). Once disambiguated, the page is considered for picking expansion terms. Other literatures that deal with PRF based IR are (Milne. et al., 2007; Lin and Wu, 2008; Lv and Zhai, 2010; Jiang, 2011). 3 3.2 This model picks expansion terms that get combined with the query. Choosing expansion terms involves selecting a set of relevant documents and identifying terms that uniq"
I13-2006,D07-1091,0,\N,Missing
I13-2006,P07-2045,0,\N,Missing
I13-2006,N03-1017,0,\N,Missing
I17-2006,P12-1092,0,0.0548232,"r of dimensions , and going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it. 1 Introduction Word embeddings are a crucial component of modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013a; Levy and Goldberg, 2014). Word vector space models can only capture differences in meaning (Sahlgren, 2006). That is, one can infer the meaning of a word by looking at its neighbors. An isolated word on its own does not Does n (the number of pairwise equidistant words) enforce a lower bound on the number of dimensions that should be chosen for training word embeddings on the corpus? In this paper, we show that this seems to be true 31 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 31–36, c Taipei, Taiwan, November 27 – D"
I17-2006,D16-1104,1,0.840743,"f-the-shelf word embedding toolkits. While other hyperparameters have been studied to varying extents (see section 2), there are no rigorous studies on the number of dimensions that should be used while training word embeddings. They are usually decided via a rule of thumb (established as a side effect of other evaluations): use between 50 to 300, or by trial and error. This is a common thread across many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016). Depending on the corpus, its vocabulary, and the context through which the differences are elicited during training of word embedding, we are bound to obtain a certain number of words, say n, that are pairwise equidistant. Such words impose an equality constraint that the embedding algorithm has to uphold. Thus, we raise the following question: Word embeddings are a relatively new addition to the modern NLP researcher’s toolkit. However, unlike other tools, word embeddings are used in a black box manner. There are very few studies regarding various hyperparameters. One such hyperparameter is"
I17-2006,D14-1181,0,0.0073885,"n-depth analysis; many proceed with default settings that come with off-the-shelf word embedding toolkits. While other hyperparameters have been studied to varying extents (see section 2), there are no rigorous studies on the number of dimensions that should be used while training word embeddings. They are usually decided via a rule of thumb (established as a side effect of other evaluations): use between 50 to 300, or by trial and error. This is a common thread across many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016). Depending on the corpus, its vocabulary, and the context through which the differences are elicited during training of word embedding, we are bound to obtain a certain number of words, say n, that are pairwise equidistant. Such words impose an equality constraint that the embedding algorithm has to uphold. Thus, we raise the following question: Word embeddings are a relatively new addition to the modern NLP researcher’s toolkit. However, unlike other tools, word embeddings are used in a black box manner. There are"
I17-2006,P14-1023,0,0.0623768,"ds in case of 3 dimensions (low standard deviation). To further verify the distortions due to a lower than needed dimension, we make the following hypothesis: if the learning algorithm of word embeddings does not get enough dimensions, then it will fail to uphold the equality constraint. Therefore, the standard deviation of the mean of all pairwise distances will be higher. As we increase the dimension, the algorithm will get more degrees of freedom to model the equality constraint in a better way. Thus, there will be statistically significant changes in the standard deviation. Once the lower Baroni et al. (2014) claimed that neural word embeddings are better than traditional methods such as LSA, HAL, RI (Landauer and Dumais, 1997; Lund and Burgess, 1996; Sahlgren, 2005). They experimented with different settings for the number of dimensions, but their experiments were intended to evaluate the practicality of dimensions of neural embeddings as compared to their traditional methods. However, their claim was challenged by Levy et al. (2015), who showed that superiority of neural word embeddings is not due to the embedding algorithm, but due to certain design choices and hyperparameters optimizations. Wh"
I17-2006,P14-2050,0,0.0997518,"d results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it. 1 Introduction Word embeddings are a crucial component of modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013a; Levy and Goldberg, 2014). Word vector space models can only capture differences in meaning (Sahlgren, 2006). That is, one can infer the meaning of a word by looking at its neighbors. An isolated word on its own does not Does n (the number of pairwise equidistant words) enforce a lower bound on the number of dimensions that should be chosen for training word embeddings on the corpus? In this paper, we show that this seems to be true 31 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 31–36, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 3 for skip gram embedd"
I17-2006,Q15-1016,0,0.0591809,"egrees of freedom to model the equality constraint in a better way. Thus, there will be statistically significant changes in the standard deviation. Once the lower Baroni et al. (2014) claimed that neural word embeddings are better than traditional methods such as LSA, HAL, RI (Landauer and Dumais, 1997; Lund and Burgess, 1996; Sahlgren, 2005). They experimented with different settings for the number of dimensions, but their experiments were intended to evaluate the practicality of dimensions of neural embeddings as compared to their traditional methods. However, their claim was challenged by Levy et al. (2015), who showed that superiority of neural word embeddings is not due to the embedding algorithm, but due to certain design choices and hyperparameters optimizations. While they investigate different hyperparameters, they keep a consistent dimension of 500 for all different embedding models that they evaluated. Many other evaluations set the number of dimensions without any justifications (Schnabel et al., 2015; Zhai et al., 2016; Ghannay et al., 2016). Melamud et al. (2016) evaluates skip-gram word embeddings on a wide range of intrinsic and extrinsic NLP tasks. An interesting observation made b"
I17-2006,D15-1168,0,0.0774115,"Missing"
I17-2006,P14-5004,0,0.0237582,"y constraints that are broken at lower than 19 dimensions did not matter. But, for a realistic use case, one would be better off if they stick to the lower bound. Tasks We use the following intrinsic tasks for evaluation. a) Word Pair Similarity tasks are commonly used for intrinsic evaluation of word embeddings, which involve predicting similarity between a given pair of words a and b. The evaluation involves finding cosine similarity between the embeddings of a and b, and finding the spearman correlation with human annotation. We used the WS353, MEN, RW, RG65, MTurk, and SimLex999 datasets (Faruqui and Dyer, 2014) 7 Conclusion and Future Work We discussed the importance of deciding the number of dimensions for word embedding training by looking at the corpus. We motivated the idea using abstract examples and gave an algorithm for finding the lower bound. Our experiments showed that performance of word embeddings is poor, until the lower bound is reached. Thereafter, it stabilizes. Therefore, such bounds should be used to decide the number of dimensions, instead of trial and error. b) Word Analogy tasks are yet another commonly used tasks for intrinsic evaluation of word embeddings, which involve evalua"
I17-2006,L16-1046,0,0.0192927,"nded to evaluate the practicality of dimensions of neural embeddings as compared to their traditional methods. However, their claim was challenged by Levy et al. (2015), who showed that superiority of neural word embeddings is not due to the embedding algorithm, but due to certain design choices and hyperparameters optimizations. While they investigate different hyperparameters, they keep a consistent dimension of 500 for all different embedding models that they evaluated. Many other evaluations set the number of dimensions without any justifications (Schnabel et al., 2015; Zhai et al., 2016; Ghannay et al., 2016). Melamud et al. (2016) evaluates skip-gram word embeddings on a wide range of intrinsic and extrinsic NLP tasks. An interesting observation made by them is that while the performance for intrinsic tasks such as word pair similarity, etc. peaks at around 300 dimensions, the performance of extrinsic tasks peaked at around 50, and sometimes showed degradation for higher dimensions. This justifies the need for study of bounds for dimensions. As is evident from the above discussion, the analysis of the number of dimensions have not received enough attention. This paper is a contribution towards th"
I17-2006,N16-1118,0,0.0199395,"acticality of dimensions of neural embeddings as compared to their traditional methods. However, their claim was challenged by Levy et al. (2015), who showed that superiority of neural word embeddings is not due to the embedding algorithm, but due to certain design choices and hyperparameters optimizations. While they investigate different hyperparameters, they keep a consistent dimension of 500 for all different embedding models that they evaluated. Many other evaluations set the number of dimensions without any justifications (Schnabel et al., 2015; Zhai et al., 2016; Ghannay et al., 2016). Melamud et al. (2016) evaluates skip-gram word embeddings on a wide range of intrinsic and extrinsic NLP tasks. An interesting observation made by them is that while the performance for intrinsic tasks such as word pair similarity, etc. peaks at around 300 dimensions, the performance of extrinsic tasks peaked at around 50, and sometimes showed degradation for higher dimensions. This justifies the need for study of bounds for dimensions. As is evident from the above discussion, the analysis of the number of dimensions have not received enough attention. This paper is a contribution towards that direction. 32 1.5 1."
I17-2006,N13-1090,0,0.159385,"d going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it. 1 Introduction Word embeddings are a crucial component of modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013a; Levy and Goldberg, 2014). Word vector space models can only capture differences in meaning (Sahlgren, 2006). That is, one can infer the meaning of a word by looking at its neighbors. An isolated word on its own does not Does n (the number of pairwise equidistant words) enforce a lower bound on the number of dimensions that should be chosen for training word embeddings on the corpus? In this paper, we show that this seems to be true 31 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 31–36, c Taipei, Taiwan, November 27 – December 1, 2017 2017 A"
I17-2006,D15-1036,0,0.0922369,"imensions, but their experiments were intended to evaluate the practicality of dimensions of neural embeddings as compared to their traditional methods. However, their claim was challenged by Levy et al. (2015), who showed that superiority of neural word embeddings is not due to the embedding algorithm, but due to certain design choices and hyperparameters optimizations. While they investigate different hyperparameters, they keep a consistent dimension of 500 for all different embedding models that they evaluated. Many other evaluations set the number of dimensions without any justifications (Schnabel et al., 2015; Zhai et al., 2016; Ghannay et al., 2016). Melamud et al. (2016) evaluates skip-gram word embeddings on a wide range of intrinsic and extrinsic NLP tasks. An interesting observation made by them is that while the performance for intrinsic tasks such as word pair similarity, etc. peaks at around 300 dimensions, the performance of extrinsic tasks peaked at around 50, and sometimes showed degradation for higher dimensions. This justifies the need for study of bounds for dimensions. As is evident from the above discussion, the analysis of the number of dimensions have not received enough attentio"
I17-2048,N16-4006,1,0.901348,"Missing"
I17-2048,W04-3250,0,0.0228205,"he Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combining triangulated models corresponding to different pivot languages. Linear interpolation is used (Bisazza et al., 2011) for model combination. Interpolation weights are assigned to each phrase-table and the feature values for each phrase pair are interpolated using these weights as shown below: f j (¯ s, t¯) = s.t P i X i αi = 1, αi fij (¯ s, t¯) (3) αi ≥ 0 where, f j is feature j defined on the phrase pair (¯"
I17-2048,P07-2045,0,0.0100926,"Missing"
I17-2048,2011.iwslt-evaluation.18,0,0.17107,"moothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combin"
I17-2048,W16-4811,1,0.833622,"b}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer"
I17-2048,D16-1196,1,0.800917,"b}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer"
I17-2048,N12-1047,0,0.0202235,"irst translated into the pivot language, and the pivot language translation is further translated into the target language using the S-P and P-T translation models respectively. To reduce cascading errors due to pipelining, we consider the top-k source-pivot translations in the second stage of the pipeline (an approximation to expectation over all translation candidates). We used k = 20 in our experiments. The translation candidates are scored 284 as shown below: P (t|s) = k X P (t|pi )P (pi |s) et al., 2007) with grow-diag-final-and heuristic for symmetrization of alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Subwordlevel representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000) for OS and BPE-level models. This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphologi"
I17-2048,W17-4102,1,0.706625,"tion) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer to orthographic syllables and byte pair encoded units as subwords. W"
I17-2048,P07-1092,0,0.0320213,"hukuttan and Bhattacharyya, 2016b, 2017). While OS units are approximate syllables, BPE units are highly frequent character sequences, some of them representing different linguistic units like syllables, morphemes and affixes. While orthographic syllabification applies to writing systems which represent vowels (alphabets and Pivoting using related language: We use a language related to both the source and target language as the pivot language. We explore two widely used pivoting techniques: phrase-table triangulation and pipelining. Triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007) “joins” the source-pivot and pivot-target subword-level phrase-tables on the common phrases in the pivot columns, generating the pivot model’s phrasetable. It recomputes the probabilities in the new source-target phrase-table, after making a few independence assumptions, as shown below: P (t¯|¯ s) = X P (t¯|¯ p)P (¯ p|¯ s) (1) p¯ where, s¯, p¯ and t¯ are source, pivot and target phrases respectively. In the pipelining/transfer approach (Utiyama and Isahara, 2007), a source sentence is first translated into the pivot language, and the pivot language translation is further translated into the t"
I17-2048,N15-1125,1,0.752889,"ong related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity means that the languages share many words 283 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 283–289, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abugidas), BPE can be applied to text in any writing system. the restricted case of related languages). Previous work on morpheme and word-level pivot models with multiple pivot languages have reported lower translation scores than the direct model (More et al., 2015; Dabre et al., 2015). Tiedemann (2012)’s work uses a character-level model in just one language pair of the triple (source-pivot or pivot-target) when the pivot is related to either the source or target (but not both). 2 Training subword-level models: We segment the data into subwords during pre-processing and indicate word boundaries by a boundary marker ( ) as shown in the example for OS below: word: Childhood means simplicity . subword: Chi ldhoo d mea ns si mpli ci ty . For building subword-level phrase-based models, we use (a) monotonic decoding since related languages have similar word order, (b) higher ord"
I17-2048,W15-5944,1,0.935113,"xical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity means that the languages share many words 283 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 283–289, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abugidas), BPE can be applied to text in any writing system. the restricted case of related languages). Previous work on morpheme and word-level pivot models with multiple pivot languages have reported lower translation scores than the direct model (More et al., 2015; Dabre et al., 2015). Tiedemann (2012)’s work uses a character-level model in just one language pair of the triple (source-pivot or pivot-target) when the pivot is related to either the source or target (but not both). 2 Training subword-level models: We segment the data into subwords during pre-processing and indicate word boundaries by a boundary marker ( ) as shown in the example for OS below: word: Childhood means simplicity . subword: Chi ldhoo d mea ns si mpli ci ty . For building subword-level phrase-based models, we use (a) monotonic decoding since related languages have similar word"
I17-2048,P10-1048,0,0.024108,"izing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate"
I17-2048,P12-2059,0,0.100195,"Low-resource Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build"
I17-2048,P02-1040,0,0.0990581,"for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combining triangulated models corresponding to different pivot languages. Linear interpolation is used (Bisazza et al., 2011) for mode"
I17-2048,quasthoff-etal-2006-corpus,0,0.0329732,"ls. This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t a"
I17-2048,W12-5611,0,0.0295708,"Missing"
I17-2048,2009.eamt-1.3,0,0.0635171,"ology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level trans"
I17-2048,E12-1015,0,0.101031,"ce Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build"
I17-2048,W07-0705,0,0.159847,"n Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together s"
I17-2048,W15-3052,0,0.0300374,"Missing"
I17-2048,J99-1003,0,\N,Missing
I17-2048,jha-2010-tdil,0,\N,Missing
I17-2048,P07-1108,0,\N,Missing
I17-2048,bojar-etal-2014-hindencorp,0,\N,Missing
I17-2048,I08-1067,1,\N,Missing
I17-4031,D14-1181,0,0.0269852,"Tasks, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP fective in solving the problem. Hence it is challenging to build a generic model that could perform reasonably well acoross different domains and languages. In recent times, the emergence of deep learning methods have inspired researchers to develop solutions that do not require careful feature engineering. Deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) are two very popular deep learning techniques that have been successfully used in solving many sentence and document classification (Kim, 2014; Xiao and Cho, 2016) problems. We aim at developing a generic model that can be used across different languages and platforms for customer feedback analysis. The remainder of our paper is structured as follows: Section 2 offers the related literature survey for customer feedback analysis, where we discuss about the existing approaches. Section 3 describes our two proposed approaches, one based on CNN and the other based on amalgamation of RNN with CNN. Section 4 provides the detailed information about the data set used in the experiment and the experimental setup. Results, analysis and discus"
I17-4031,D14-1179,0,0.006605,"Missing"
I17-4031,W02-1011,0,0.0191959,"Missing"
I17-4031,D14-1162,0,0.0806309,"ack sentences to the same length is useful because it allows us to efficiently batch our data while training. Let a feedback sentence F consisting of ‘n’ words be the input to our model such that x = [x1 , x2 , . . . xn ] where xi is the ith word in the feedback sentence. Each token xi ∈ F is represented by its distributed representation pi ∈ Rk which is the kdimensional word vector. The distributed representation p is looked up into the word embedding matrix W which is initialized either by a random process or by some pre-trained word embeddings like Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). We then concatenate (row wise) the distributed representation pi for every ith token in the feedback F and build the feedback sentence representation matrix. The feedback sentence representation matrix p1:n can be represented as: for customer feedback analysis. 3 Network Architecture for Feedback Classification In this section we describe our proposed neural network architecture for feedback classification. We propose two variants, the first one is convolution operation inspired CNN and the second one is the amalgamation of CNN with RNN. 3.1 Feedback classification using CNN In this model a"
I17-4031,D13-1170,0,0.00497594,"Missing"
K16-1015,P10-2050,0,0.00902686,"tional random fields. Zhang et al. (2014) deal with emotion classification. Using a dataset of children’s stories manually annotated at the sentence level, they employ HMM to identify sequential structure and a classifier to predict emotion in a particular sentence. Mao and Lebanon (2006) present a isotonic CRF that predicts global and local sentiment of documents, with additional mechanism for author-specific distributions and smoothing sentiment curves. Yessenalina et al. (2010) present a joint learning algorithm for sentencelevel subjectivity labeling and document-level sentiment labeling. Choi and Cardie (2010) deal with sequence learning to jointly identify scope of opinion polarity expressions, and polarity labels. Taking inspiration from use of sequence labeling for sarcasm detection, our work takes the first step to show if sequence labeling techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of di"
K16-1015,W10-2914,0,0.127959,"ent with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 20"
K16-1015,P11-2102,0,0.111854,"Missing"
K16-1015,W15-2905,1,0.153096,"assification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 2015) present a suite of classifier features that capture different kinds of context: context related to the author, conversation, etc. The new trend in sarcasm detection is, thus, to look at additional context beyond the text where sarcasm is to be predicted. The work closest to ours is by Wang et al. (2015). They use a labeled dataset of 1500 tweets, the labels for which are obtained automatically. Due to their autom"
K16-1015,W14-2608,0,0.0951159,"from Prior Work We also compare our results with features presented in two prior works9 : 1. Features given in Gonz´alez-Ib´anez et al. (2011): These features are: (a) Interjections, (b) Punctuations, (c) Pragmatic features (where we include action words as well), (d) Sentiment lexicon-based features from LIWC (Pennebaker et al., 2001) (where they include counts of linguistic process words, positive/negative emotion words, etc.). Thus, we wish to validate our hypothesis in case of: 1. Our data-derived features as given in Section 4.1. 2. Past features from Gonz´alez-Ib´anez et al. (2011) and Buschmeier et al. (2014) as given in Section 4.2. 2. Features given in Buschmeier et al. (2014): In addition to unigrams, the features used by them are: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment Scores followed by punctuation (this includes more than one positive or negative words with an exclamation mark or question mark at the end), (d) Positive/Negative Sentiment Scores followed by ellipsis (this includes more than one positive or negative words with a ‘...’ at the end, (e) Punctuation, (f) Interjections, and (g) Laughte"
K16-1015,W07-0101,0,0.161201,"techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a r"
K16-1015,L16-1147,0,0.0280848,"Missing"
K16-1015,D10-1102,0,0.0133469,"roaches for sequence labeling in sentiment classification have been studied. Zhao et al. (2008) perform sentiment classification using conditional random fields. Zhang et al. (2014) deal with emotion classification. Using a dataset of children’s stories manually annotated at the sentence level, they employ HMM to identify sequential structure and a classifier to predict emotion in a particular sentence. Mao and Lebanon (2006) present a isotonic CRF that predicts global and local sentiment of documents, with additional mechanism for author-specific distributions and smoothing sentiment curves. Yessenalina et al. (2010) present a joint learning algorithm for sentencelevel subjectivity labeling and document-level sentiment labeling. Choi and Cardie (2010) deal with sequence learning to jointly identify scope of opinion polarity expressions, and polarity labels. Taking inspiration from use of sequence labeling for sarcasm detection, our work takes the first step to show if sequence labeling techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first ste"
K16-1015,D08-1013,0,0.0430497,"Missing"
K16-1015,P14-2084,0,0.111454,"the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 2015) present a suite of classifier features that capture different kinds of context: context related to the author, conversation, etc. The new trend in"
K16-1015,P15-1100,0,0.0664233,"be verbal: her date says, ‘They’ve taken 40 minutes to bring our appetizers’ to which the speaker responds ‘I absolutely love this restaurant’. Both these examples point to the intuition that for dialogue (i.e., data where more than one speaker participates in a discourse), conversational context is often a clue for sarcasm. For such dialogue, prior work in sarcasm detection (determining whether a text is sarcastic or not) captures context in the form of classifier features such as the topic’s probability of evoking sarcasm, or the author’s tendency to use sarcasm (Rajadesingan et al., 2015; Wallace, 2015). In this paper, we present an alternative hypothesis: sarcasm detection of dialogue is better formulated as a sequence labeling task, instead of classification task. The central message of our work is the efficacy of using sequence labeling as a learning mechanism for sarcasm detection in dialogue, and not in the set of features that we propose for sarcasm detection although we experiment with three feature sets. For our experiments, we create a manually labeled dataset of dialogues from TV series ‘Friends’. Each dialogue is considered to be a sequence of utterances, and every utterance is an"
K16-1015,P15-2124,1,\N,Missing
K16-1016,C10-2005,0,0.0497006,"wn race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010). Introduction This paper addresses the task of Sentiment Analysis (SA) - automatic detection of the sentiment polarity as positive versus negative - of usergenerated short texts and sentences. Several sentiment analyzers exist in literature today (Liu and Zhang, 2012). Recent works, such as Kouloumpis et al. (2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content. Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below. 1.1 Syntactic Challenges 1.3 Lexical Challenges Semantic and Pragmatic Challenges This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other Sentiment analyzers face the following three challe"
K16-1016,W15-2401,0,0.12339,"Missing"
K16-1016,W11-0705,0,0.0607671,"becomes an enemy to his own race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010). Introduction This paper addresses the task of Sentiment Analysis (SA) - automatic detection of the sentiment polarity as positive versus negative - of usergenerated short texts and sentences. Several sentiment analyzers exist in literature today (Liu and Zhang, 2012). Recent works, such as Kouloumpis et al. (2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content. Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below. 1.1 Syntactic Challenges 1.3 Lexical Challenges Semantic and Pragmatic Challenges This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other Sentiment analyzers fac"
K16-1016,C14-1008,0,0.144854,"Missing"
K16-1016,D09-1020,0,0.168406,"Missing"
K16-1016,W14-3010,0,0.0174291,"task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below. 3.1 Dataset 2 Dataset 1 It contains 994 text snippets with 383 positive and 611 negative examples. Out of this, 350 are sarcastic or have other forms of irony. The snippets are a collection of reviews, normalized-tweets and 158 plained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014). Table 1 presents the accuracy of the three systems. The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for existing sentiment analyzers. Hence, the selected datasets are ideal for our current experimentation that involves cognitive features. 4 verbs, nouns, adjectives and adverbs in the text. This is computed using NLTK1 . 6. Count of Named Entities (NE) i.e. Number of named entity mentions in the text. This is computed using NLTK. 7. Discourse connectors"
K16-1016,esuli-sebastiani-2006-sentiwordnet,0,0.0126573,"of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations. Moreover, a saccade is called a Regressive Saccade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment. A portion of a text is said to be skipped if it does not have any fixation. Figure 1 shows eye-movement behavior during annotation of the given sentence in dataset-1. The circles represent 3. Subjective scores (PosScore, NegScore) i.e. Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006). 4. Sentiment flip count (FLIP) i.e. Number of times words polarity changes in the text. Word polarity is determined using MPQA lexicon. 5. Part of Speech ratios (VERB, NOUN, ADJ, ADV) i.e. Ratios (proportions) of 1 159 http://www.nltk.org/ Figure 1: Snapshot of eye-movement behavior during annotation of an opinionated text. The circles represent fixations and lines connecting the circles represent saccades. Boxes represent Areas of Interest (AoI) which are words of the sentence in our case. also supported by von der Malsburg and Vasishth (2011). fixation and the line connecting the circles r"
K16-1016,D11-1100,1,0.957865,"given the context (e.g., His face fell when he was dropped from the team vs The boy fell from the bicycle, where the verb “fell” has to be disambiguated) (3) Domain Dependency, tackling words that change polarity across domains. (e.g., the word unpredictable being positive in case of unpredictable movie in movie domain and negative in case of unpredictable steering in car domain). Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006). Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of"
K16-1016,C08-1031,0,0.0135792,"and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introd"
K16-1016,W14-2609,0,0.0333978,"g the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning b"
K16-1016,P11-1015,0,0.24822,"Missing"
K16-1016,I08-1039,0,0.0272598,"artineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation c"
K16-1016,maynard-greenwood-2014-cares,0,0.0273707,"our approach before concluding the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2"
K16-1016,N13-1088,1,0.88658,"Missing"
K16-1016,P14-2007,1,0.91123,"ties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introducing Cognitive Features We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate s"
K16-1016,P13-2062,1,0.885604,"Missing"
K16-1016,P15-2124,1,0.944174,"7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches exist"
K16-1016,W14-2623,1,0.798217,"lassification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introducing Cognitive Features We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate short-text with sentiment labels. Our hypothesis is that cognitive processes in the brain are related to eye-movement activities ("
K16-1016,N16-1179,0,0.12689,"Missing"
K16-1016,W04-3253,0,0.0369816,"eatures (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla an"
K16-1016,N10-1120,0,0.0655369,"a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to spa"
K16-1016,P06-2079,0,0.140611,"eatures alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount"
K16-1016,W13-1605,0,0.0797154,"Missing"
K16-1016,P04-1035,0,0.0236932,"s Considering Dataset -1 and 2 as Test Data It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not. For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2. The statistical classifiers are based on Support Vector Machine (SVM) and N¨aive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http://www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as exEye-tracking and Sentiment Analysis Datasets We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which"
K16-1016,H05-1044,0,0.330727,"ng Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http://www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as exEye-tracking and Sentiment Analysis Datasets We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below. 3.1 Dataset 2 Dataset 1 It contains 994 text snippets with 383 positive and 6"
K16-1016,W02-1011,0,0.0277671,"and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011"
K16-1016,P13-1041,1,0.906044,"Missing"
K16-1016,P13-2149,1,0.93296,"nitive features, showing the effectiveness of cognitive features. In section 6, we discuss on the feasibility of our approach before concluding the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised"
K16-1016,D13-1066,0,0.0912234,"Missing"
K16-1016,I13-1076,1,0.926356,"Missing"
K16-1016,P06-1134,0,0.109259,"Missing"
K16-1027,W15-3902,0,0.0193051,"ing the ambiguity in the grapheme-to-phoneme mapping. 5 6 Experiments Data: We experimented on the following Indian language pairs representing two language families: Bengali→Hindi, Kannada→Hindi, Hindi→Kannada and Tamil→Kannada. Bengali (bn) and Hindi (hi) are Indo-Aryan languages, while Kannada (kn) and Tamil (ta) are Dravidian languages. We used 10k source language names as training corpus, which were collected from various sources. We evaluated our systems on the NEWS 2015 Indic dataset. We created this set from the English to Indian language training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous"
K16-1027,N09-1034,0,0.0209179,"SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We"
K16-1027,N12-1047,0,0.0499422,"Missing"
K16-1027,2010.amta-papers.12,0,0.071539,"Missing"
K16-1027,D12-1002,0,0.455144,"Missing"
K16-1027,N10-1065,1,0.749874,"Missing"
K16-1027,P06-1103,0,0.267201,"ransliteration ambiguities. SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor"
K16-1027,P06-2065,0,0.059408,"Missing"
K16-1027,P07-2045,0,0.00456309,"Missing"
K16-1027,N15-3017,1,0.80408,"Missing"
K16-1027,P02-1038,0,0.184332,"training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous transliteration features. The features are: substring transliteration probabilities, weighted average character transliteration probabilities and character language model score. The conditional probability of the target word e given the source word f is: P (e|f) = N P ∏ i=1 P (e¯i |f¯i ) = N P ∏ i=1 exp NF ∑ λj gj (f¯i , e¯i ) j=0 (11) ¯ where, fi and e¯i are source and target substrings respectively, λj and gj are feature weight and feature function respectively for feature j, N P number of substrings and N F is number of features. We synthes"
K16-1027,N09-1005,0,0.14977,"the top-1 transliterations in the synthesized, pseudo-parallel corpus; no true parallel corpus is used. Monotone decoding was performed. We used a 5-gram character language model trained with Witten-Bell smoothing on 40k names for all target languages. We ran Stage 2 for 5 iterations. For a rule-based baseline, we used the script conversion method implemented in the Indic NLP Library2 (Kunchukuttan et al., 2015) which is based on phonemic correspondences. poorly as reported in their work too. We also experimented with re-ranking the results using a unigram word based LM - our approximation to Ravi and Knight (2009)’s use of a word based LM - and its accuracy is comparable to PC_Init. The unigram LM was trained on a corpus of 185 million and 42 million tokens for hi and kn respectively. Thus, this knowledge-lite approach cannot learn a transliteration model effectively. Rule-based transliteration (Rule) performs significantly better than PC_Init. The phonetic nature of Indic scripts makes the rule-based system a stronger baseline, yet this simple approach does not ensure high accuracy transliteration. Phonetic changes like changes in manner/place of articulation, voicing, etc. make transliteration non-tr"
K16-1027,P11-1044,0,0.0232627,"verview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We need a phonemic repres"
K16-1027,P12-1049,0,0.0529545,"Missing"
K16-1027,P07-1119,0,0.0347206,"allel corpora, co-occurrence is no longer a learning signal and it is not possible to learn the character transliteration probabilities reliably. To compensate for this, we define Dirichlet priors (De ) over each character transliteration probability distributions (Θe ), which can be used to encode linguistic knowledge. This leads to our proposed EM-MAP training objective for the Mstep over the entire training set (WF ). based models, which learn substring mappings like .mba → mba, are one way to incorporate contextual information and have been shown to perform better in a supervised setting (Sherif and Kondrak, 2007). Contextual information is especially important in an unsupervised setting. 4 ∑ {∑{ ∑[ QWF (Θ) = δe,f σa,f,e ∑ + e P (e) a i=1 θfai ,ei }} + log P (e) F| ,e ) (3) j=|CF | ∀e ∈ CE , ∑ θfj ,e = 1 j=1 where, δe,f = P (e|f), σa,f,e = P (a|e, f) are conditional probabilities of the latent variables computed in the E-step. These are computed using the previous iteration’s parameter values, whose values are fixed in the current iteration. nf,e,a is the number of times characters e and f are aligned in the alignment structure a. CF and CE are the character sets of the source and target languages resp"
K16-1027,P05-1044,0,0.158297,"Missing"
K16-1027,W06-1630,0,0.0614127,"Missing"
K18-1012,2010.jeptalnrecital-court.36,0,0.0222723,"al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है"
K18-1012,P17-1055,0,0.0247585,"there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?) (Transliteration: Seattle mai baseball dal ka naam kya hai?) • Q3 :Seattle mein baseball team ka naam kya hai? (Trans: What is the name of the baseball team in Seattle?) All the three questions are same but are asked in English, Hindi and the code-mixed English-Hindi languages. It can be s"
K18-1012,W14-3902,0,0.033183,"ider the following three questions: Related Work Code-mixing refers to the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al."
K18-1012,P06-2025,1,0.666144,"Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion of SQuAD (Rajpurkar et al., 2016) dataset. We translate the English 10 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iii"
K18-1012,D14-1159,0,0.0134896,"et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?) (Transliteration: Seattle mai baseball dal ka naam kya hai?) • Q3 :Seattle mein baseball team ka naam kya hai? (Trans: What is the name of the baseball team in Seattle?) All the three questions are same but are asked in English, Hindi and the code-mixed English-Hindi languages. It can be seen that Q2 and Q3 are similar and share many false cognates (Moss, 1992) [(Seattle, िसएटल), (naam, नाम"
K18-1012,L18-1278,1,0.770245,"ode-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network"
K18-1012,bojar-etal-2014-hindencorp,0,0.19087,"di-English code mixing (CM) dataset. The CMI score of the system generated codemixed questions is 37.22. 5.2 5.3 Experimental Setup for CMQG The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser10 . The Polyglot11 Named Entity Recognizer (NER) (Al-Rfou et al., 2015) is used for named entity recognition. The lexical translation set is obtained by the lexical translation table generated as an intermediate output of Statistical Machine Translation (SMT) training by Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion o"
K18-1012,L18-1440,1,0.895377,"Missing"
K18-1012,W16-6331,1,0.816392,"Missing"
K18-1012,C18-1042,1,0.934699,"ode-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network"
K18-1012,N10-1086,0,0.0460939,"n et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बे"
K18-1012,P02-1040,0,0.10238,"11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq"
K18-1012,R15-1033,0,0.0175163,"the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a)"
K18-1012,D16-1264,0,0.278424,"egate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion of SQuAD (Rajpurkar et al., 2016) dataset. We translate the English 10 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architectur"
K18-1012,P07-2045,0,0.00479329,"HinglishQue dataset is more complex and challenging in comparison to the other Hindi-English code mixing (CM) dataset. The CMI score of the system generated codemixed questions is 37.22. 5.2 5.3 Experimental Setup for CMQG The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser10 . The Polyglot11 Named Entity Recognizer (NER) (Al-Rfou et al., 2015) is used for named entity recognition. The lexical translation set is obtained by the lexical translation table generated as an intermediate output of Statistical Machine Translation (SMT) training by Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We eva"
K18-1012,D16-1121,0,0.0792189,"Missing"
K18-1012,C02-1150,0,0.275618,"4.4 Answer-type Focused Answer Extraction The answer-type of a question provides the clues to detect the correct answer from the passage. Consider a code-mixed question Q: Kaun sa Portuguese player, Spanish club Real Madrid ke liye as a forward player khelta hai? (Trans: Which Portuguese player plays as a forward for Spanish club Real Madrid?.) The answer-type of the question Q is ‘person’. Even though the network has the capacity to capture this information up to a certain degree, it would be better if the model takes into account this information in advance while selecting the answer span. Li and Roth (2002) proposed a hierarchical question classification based on the answer-type of a question. Based on the coarse and fine classes of Li and Roth (2002), we train two separate answer-type detection networks on the Text REtrieval Conference (TREC) question classification dataset4 . First, we translate5 5952 TREC English questions into Hindi and thereafter transform the Hindi questions into the code-mixed questions by using our proposed CMQG algorithm. We train the answer-type detection network with code-mixed questions and their associated labels using the technique as discussed in (Gupta et al., 20"
K18-1012,W04-1013,0,0.0112582,"/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq network. We evaluate"
K18-1012,E17-3017,0,0.0344471,"QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq network. We evaluate the network on the manually created CMQG dataset (c.f. Section 5.1). Experimental Setup for CMQA CMQA datasets contain the words both in Roman script and English. For English, we use the fastText (Bojanowski et al., 2016) word embedding of dimension 300. We use the Hindi sentences from Bojar et al. (2014), and then transliterate it into the Roman script. These sentences are used to train the word embeddings of dim"
K18-1012,P16-1056,0,0.0215941,"(Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball"
K18-1012,D16-1013,0,0.0661766,"Missing"
K18-1012,D14-1105,0,0.0332379,"de-mixing refers to the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 201"
K18-1012,D17-1090,0,0.0426014,"Missing"
K18-1012,P17-1018,0,0.372409,"amatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?)"
khapra-etal-2014-transliteration,W10-2405,0,\N,Missing
khapra-etal-2014-transliteration,N10-1065,1,\N,Missing
khapra-etal-2014-transliteration,W10-2403,1,\N,Missing
khapra-etal-2014-transliteration,D08-1027,0,\N,Missing
khapra-etal-2014-transliteration,W09-3505,0,\N,Missing
khapra-etal-2014-transliteration,W10-0710,0,\N,Missing
khapra-etal-2014-transliteration,W10-0701,0,\N,Missing
khapra-etal-2014-transliteration,W10-2401,0,\N,Missing
khapra-etal-2014-transliteration,P12-1049,0,\N,Missing
khapra-etal-2014-transliteration,W10-0731,0,\N,Missing
khapra-etal-2014-transliteration,P11-1044,0,\N,Missing
khapra-etal-2014-transliteration,W09-3506,0,\N,Missing
khapra-etal-2014-transliteration,W10-0728,0,\N,Missing
khapra-etal-2014-transliteration,W10-0708,0,\N,Missing
khapra-etal-2014-transliteration,W09-3504,0,\N,Missing
kunchukuttan-etal-2012-experiences,E06-1032,0,\N,Missing
kunchukuttan-etal-2012-experiences,ambati-etal-2010-active,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1027,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1056,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0710,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0701,0,\N,Missing
kunchukuttan-etal-2012-experiences,D09-1030,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0734,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-1701,0,\N,Missing
kunchukuttan-etal-2014-shata,W12-3152,0,\N,Missing
kunchukuttan-etal-2014-shata,jha-2010-tdil,0,\N,Missing
kunchukuttan-etal-2014-shata,P02-1040,0,\N,Missing
kunchukuttan-etal-2014-shata,W13-2807,0,\N,Missing
kunchukuttan-etal-2014-shata,2005.mtsummit-papers.11,0,\N,Missing
kunchukuttan-etal-2014-shata,W14-0130,1,\N,Missing
kunchukuttan-etal-2014-shata,I08-1067,1,\N,Missing
kunchukuttan-etal-2014-shata,2009.mtsummit-papers.7,0,\N,Missing
L16-1098,W08-0312,0,0.0620717,"Missing"
L16-1098,kunchukuttan-etal-2014-shata,1,0.846673,"ledged bilingual Machine Translation (MT) system for any two natural languages with limited electronic resources and tools is a challenging and demanding task. Since India is rich in linguistic divergence there are many morphologically rich languages quite different from English as well as from each other, there is a large requirement for machine translation between them. Development of efficient machine translation systems using appropriate methodologies and with limited resources is a challenging task. There are many ongoing attempts to develop MT systems for Indian languages (Antony, 2013; Kunchukuttan et al., 2014; Sreelekha et al., 2014; Sreelekha et al., 2015) using both rule based and statistical approaches. There were many attempts to improve the quality of Statistical MT systems such as; using Monolingually-Derived Paraphrases(Marton et al., 2009), Using Related Resource-Rich languages (Nakov and Ng, 2012) Considering the large amount of human effort and linguistic knowledge required for developing rule based systems, statistical MT systems became a better choice in terms of efficiency. Still the statistical systems fail to handle rich morphology. Consider the English sentence, Here the system fai"
L16-1098,D09-1040,0,0.134183,"es quite different from English as well as from each other, there is a large requirement for machine translation between them. Development of efficient machine translation systems using appropriate methodologies and with limited resources is a challenging task. There are many ongoing attempts to develop MT systems for Indian languages (Antony, 2013; Kunchukuttan et al., 2014; Sreelekha et al., 2014; Sreelekha et al., 2015) using both rule based and statistical approaches. There were many attempts to improve the quality of Statistical MT systems such as; using Monolingually-Derived Paraphrases(Marton et al., 2009), Using Related Resource-Rich languages (Nakov and Ng, 2012) Considering the large amount of human effort and linguistic knowledge required for developing rule based systems, statistical MT systems became a better choice in terms of efficiency. Still the statistical systems fail to handle rich morphology. Consider the English sentence, Here the system fails to translate the verb phrase “has been sent to” together and it translated a part of the phrase “sent” as “അയചചു”{ayachu}{sent}, which is wrong in the context. The same way another verb phrase “for opening the door” has been translated par"
L16-1098,2001.mtsummit-papers.46,0,0.35441,"Missing"
L16-1098,P02-1040,0,0.101607,"Missing"
L16-1349,P98-1004,0,0.030152,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignmen"
L16-1349,P03-1012,0,0.038474,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in l"
L16-1349,P08-1112,0,0.0233195,"oarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover ali"
L16-1349,C04-1005,0,0.0538573,"ments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Sánchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the wo"
L16-1349,P10-1155,1,0.6771,"nlike so many one to one Cartesian product alignments, our approach keeps them in the same sentence, thus reducing the chances of the system learning non synonymous candidate translations. 2200 Figure 3: Parallel English-Hindi topics as generated by the topic model for the health dataset Thus, for T topics, and K top words, sentential approach results in a coarse lexical resource of T X K pseudo-parallel sentences. The coarse lexical resource for varying values of T is available freely for download. 3.1. Experiment Setup To generate the topics, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We implement the multilingual topic model in Java. Our implementation uses Gibbs sampling as described in the original paper. 3.3. Quantitative Evaluation Two human annotators evaluated the quality of the output obtained. Each word was marked as whether or not a translation in the other language was present in the same topic. The two annotators, A1 and A2, are native speakers of Hindi, and have had 15+ years of academic instruction in English. The inter-annotator agreement between them and their"
L16-1349,P07-2045,0,0.033916,"cal machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentent"
L16-1349,J00-2004,0,0.0962513,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data"
L16-1349,D09-1092,0,0.536026,"tem is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentential Approach to create pseudoparallel data ally intense alignment processes without the sentencealigned translations"
L16-1349,P00-1056,0,0.447221,"op terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daumé III (2010) use a bilingual lexical resource, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Bar"
L16-1349,J03-1002,0,0.00587388,"approach to use a coarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is p"
L16-1349,tufis-barbu-2002-lexical,0,0.0758505,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
L16-1349,J97-3002,0,0.0670303,"ment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
L16-1349,C98-1004,0,\N,Missing
L16-1369,W03-1812,0,0.0209219,"es based on the standard guidelines provided to them. We obtained 3178 compound nouns and 2556 light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available pu"
L16-1369,calzolari-etal-2002-towards,0,0.172178,"Missing"
L16-1369,C08-2007,1,0.789994,"light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available publicly. We have explored two types of MWEs: compound nouns (CNs) and light verb constructions (LVC"
L16-1369,W10-2805,0,0.0238202,"rd guidelines provided to them. We obtained 3178 compound nouns and 2556 light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available publicly. We have"
L16-1369,W06-1205,0,0.0392442,"the text data in comparison to other MWEs. The created resource can be useful for various natural language processing applications like information extraction, word sense disambiguation, machine translation, etc. Compound Nouns and Light Verb Constructions In the context of Indian languages, MWEs are quite varied and many of these are borrowed from other languages like English, Urdu, Arabic, Sanskrit, etc. For Hindi, there are limited investigations on MWE extraction. Venkatapathy et. al., (2006) worked on syntactic and semantic features for N-V collocation extraction using MaxEnt classifier. Mukerjee et al., (2006) proposed Parts-of-Speech projection from English to Hindi with corpus alignment for extracting complex predicates. Kunchukuttan et. al., (2008) presented a method for extracting compound nouns in Hindi using statistical co-occurrence. Sinha (2009) uses linguistic property of light verbs in extraction of complex predicates using Hindi-English parallel corpus. All the work mentioned above have considered only limited aspects of Hindi MWE. In this paper, we focus on creating gold standard data for CNs and LVCs. The rest of the paper is organized as follows. Section 2 gives detail about the compo"
L16-1369,I11-1024,0,0.0278613,"and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available publicly. We have explored two types of MWEs: compound nouns (CNs) and light verb constructions (LVCs), since they are used very frequently in the text da"
L16-1369,C12-1152,0,0.0161853,"1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available publicly. We have explored two types of MWEs: compound nouns (CNs) and light verb constructions (LVCs), since they are used very freq"
L16-1369,W09-2906,0,0.0178821,"the context of Indian languages, MWEs are quite varied and many of these are borrowed from other languages like English, Urdu, Arabic, Sanskrit, etc. For Hindi, there are limited investigations on MWE extraction. Venkatapathy et. al., (2006) worked on syntactic and semantic features for N-V collocation extraction using MaxEnt classifier. Mukerjee et al., (2006) proposed Parts-of-Speech projection from English to Hindi with corpus alignment for extracting complex predicates. Kunchukuttan et. al., (2008) presented a method for extracting compound nouns in Hindi using statistical co-occurrence. Sinha (2009) uses linguistic property of light verbs in extraction of complex predicates using Hindi-English parallel corpus. All the work mentioned above have considered only limited aspects of Hindi MWE. In this paper, we focus on creating gold standard data for CNs and LVCs. The rest of the paper is organized as follows. Section 2 gives detail about the compound nouns and light verb constructions. Section 3 describes the extraction process of possible MWE candidates. Section 4 gives the statistics of MWEs annotation for Hindi and Marathi. MWEs guidelines are given in Section 5 followed by discussions i"
L16-1369,W11-0816,0,0.0177809,"in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems. Keywords: Multiword Expressions, MWEs, WordNet, Hindi WordNet, Compound Nouns, Light Verb Constructions 2. 1. Introduction Recently, various approaches have been proposed for the identification and extraction of MWEs (Calzolari et al., 2002; Baldwin et al., 2003; Guevara, 2010; Al-Haj and Wintner, 2010; Kunchukuttan and Damani, 2008; Chakrabarti et al., 2008; Sinha, 2011; Singh et al., 2012; Reddy et al., 2011). The quality of such approaches depends on the use of algorithms and also on the quality of resources used. Various standard MWEs datasets1 are available for languages like English, French, German, Portuguese, etc and can be used for evaluation of MWE approaches. But for Indian languages, no such standard datasets are available publicly. Our goal is to create MWEs annotation for Indian languages viz., Hindi and Marathi and make it available publicly. We have explored two types of MWEs: compound nouns (CNs) and light verb constructions (LVCs), since the"
L16-1369,W06-1204,0,0.064605,"Missing"
L16-1429,bakliwal-etal-2012-hindi,0,0.042434,"ned level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to th"
L16-1429,D11-1100,1,0.867467,"Missing"
L16-1429,C12-2008,1,0.673646,"Missing"
L16-1429,S14-2135,0,0.0453619,"Missing"
L16-1429,W10-3207,0,0.0309648,"of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to the non-availability of benchmark datasets and the scarcity of various ot"
L16-1429,P97-1023,0,0.142173,"Neg-negative, Neu-Neutral and Con-Conflict text analytic problems. The classifier is trained with the following features: 1. Target aspect term and local context: Sentiment bearing words usually occur closer to the target aspect term. We extract target term along with its preceding and following few tokens, and use as features for training. For the proposed method we fix context window size to 5. 2. Word Bigrams: Pair of two consecutive tokens in the local context are used as features to capture the cooccurrence behavior of the tokens. 3. Semantic Orientation (SO): Semantic Orientation (SO) (Hatzivassiloglou and McKeown, 1997) is a measure of association of a token towards positive or negative sentiments and can be defined as: SO(t) = P M I(t, posRev) − P M I(t, negRev) (2) where P M I(t, posRev) stands for point-wise mutual information of a token t towards positive sentiment reviews. 4. Experiments and Evaluation As a base learning algorithm we make use of Conditional Random Field (CRF) and Support Vector Machine (SVM) for the aspect term extraction and sentiment classification tasks respectively. We use CRF++ 4 and TinySVM 5 based packages for our experiments. 4 5 http://taku910.github.io/crfpp/ http://chasen.org"
L16-1429,W13-4306,0,0.0121323,"ect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to the non-availability of"
L16-1429,S14-2004,0,0.141815,"Missing"
L16-1429,S14-2038,0,0.0110963,"raction focuses on identifying various terms that denote aspects, and the second step, i.e. sentiment classification deals with classifying the sentiments with respect to the aspect. A review sentence, therefore, may contain more than one aspect term and the sentiment associated with each. Such a fine-grained analysis provides greater insight to the sentiments expressed in the written reviews. In recent times, there have been a growing trend for sentiment analysis at the more fine-grained level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das"
L16-1429,S14-2036,0,0.00618632,"rms that denote aspects, and the second step, i.e. sentiment classification deals with classifying the sentiments with respect to the aspect. A review sentence, therefore, may contain more than one aspect term and the sentiment associated with each. Such a fine-grained analysis provides greater insight to the sentiments expressed in the written reviews. In recent times, there have been a growing trend for sentiment analysis at the more fine-grained level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages"
L16-1485,C12-2008,1,0.840336,"to beat this baseline. Our goal is to create a WFS baseline for Indian language WordNets. We focus on Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the conclusion. Hindi WordNet Hindi WordNet1 (HWN) is developed for capturing the fine grained senses of Hindi language. It consists of synse"
L16-1485,P13-2096,1,0.83396,"g baseline in English WSD. Considering both precision and recall, only 5 of 26 systems in the Senseval-3 English all-words task were able to beat this baseline. Our goal is to create a WFS baseline for Indian language WordNets. We focus on Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the"
L16-1485,N15-1132,1,0.802516,"Considering both precision and recall, only 5 of 26 systems in the Senseval-3 English all-words task were able to beat this baseline. Our goal is to create a WFS baseline for Indian language WordNets. We focus on Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the conclusion. Hindi WordNet H"
L16-1485,P13-1041,1,0.799452,"r goal is to create a WFS baseline for Indian language WordNets. We focus on Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the conclusion. Hindi WordNet Hindi WordNet1 (HWN) is developed for capturing the fine grained senses of Hindi language. It consists of synsets and semantic relations. It i"
L16-1485,P11-1057,1,0.869954,"Missing"
L16-1485,kunchukuttan-etal-2012-experiences,1,0.805126,"Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the conclusion. Hindi WordNet Hindi WordNet1 (HWN) is developed for capturing the fine grained senses of Hindi language. It consists of synsets and semantic relations. It is a part of IndoWordNet2 (Bhattacharyya, 2010) which is the most useful mu"
L16-1485,I08-1067,1,0.754337,"ge WordNets. We focus on Hindi language as the synsets of Hindi WordNet are not ranked according to the actual usage. This is because Hindi Wordnet was built using a dictionary where words were picked up according to the alphabetical order. The current statistics of HWN is given in Table 1. HWN is used in various NLP applications like Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013; Bhingardive et al., 2015), Information Retrieval (Atreya et al., 2013), Sentiment Analysis (Joshi et al., 2010; Balamurali et al., 2012; Kashyap and Balamurali, 2013), Machine Translation (Ramanathan et al., 2008; Kunchukuttan et al., 2012), etc. 1 The rest of the paper is organized as follows. Section 2 gives a detailed description of Hindi WordNet. Hindi WordNet synset ranking methodology is explained in section 3. Section 4 gives the statistics of the ranked synsets. Section 5 highlights the performance of WFS baseline on various domains. Discussion is given in section 6, followed by the conclusion. Hindi WordNet Hindi WordNet1 (HWN) is developed for capturing the fine grained senses of Hindi language. It consists of synsets and semantic relations. It is a part of IndoWordNet2 (Bhattacharyya, 2010)"
L16-1485,N03-1031,0,\N,Missing
L16-1485,I13-1131,1,\N,Missing
L16-1686,E12-1060,0,0.0200859,") cannot be assigned a POS tag. We assign an ’ABR’ tag to such abbreviations. 2.4. The purpose of this resource is to keep up with the continuously updating language of the internet. For this reason, a static resource for slang would fail its very purpose once today’s slang goes out of use. We aim to create a web-crawler which runs continuously on various user forums and bulletins. The crawler would monitor the usage of words over these websites. A new word which appears poses the probability of being a slang word. Further we also aim to use methods such as described in Cook et al. (2014) and Lau et al. (2012), to monitor if a new sense seems to be emergent. We observe that very few of the definitions provided in Urban Dictionary are close to conventional. The primary 3 http://www.reddit.com 4 http://api.urbandictionary.com/ Dynamism 3. Validation We validate our claims of a structured, usable resource by performing experiments using WSD engines. We use the Lesk and Extended Lesk algorithms, implemented through 4330 Word Text Poster Follower Gloss / Definition A short message sent using a mobile device usually through a protocol such as a short messaging service (SMS) A person who uploads a message"
L16-1686,esuli-sebastiani-2006-sentiwordnet,0,0.00620249,"h can augment the English WordNet while dealing with neologisms and slang words on the internet. We show that the general accuracy obtained by using Lesk and extended Lesk is greater than when Urban Dictionary is directly used (Swerdfeger, Online). Currently, our resource holds 3000 slang words. However, this figure is constantly being updated as described in section 2.4. above. Our resource aims to mitigate the effect of non-conventional language on the internet. We notice that several of these slang words are sentiment bearing and annotating them with sentiment scores like the SentiWordNet (Esuli and Sebastiani, 2006) would help Sentiment Analysis for data on the web. We aim to do the same as a part of our future work. Also as future work, we aim to span out to different languages. We plan to identify slang from various languages and link semantically similar slang words. For example, the word LOL (Laughing Out Loud) is synonymous to the French MDR (Morte De 4331 Rire). Such linkages could be used to aid real-time Machine Translation in both text-to-text and speech-to-text scenarios. We aim to continuously update our resource with respect to adding new slang words to our repository. We also aim to make our"
L16-1686,P10-1023,0,0.0337969,"sable as a lexical resource. Princeton WordNet or the English WordNet (Fellbaum, 1998) is an online lexical resource which can be used for various NLP applications such as WSD, Machine Translation, Information retrieval, etc. Based on English WordNet, several other WordNets like the EuroWordNet(Vossen, 1998), IndoWordNet(Bhattacharyya, 2010) and MultiWordNet(Pianta et al., 2002) were created. We create a WordNet like structure which can be utilized for the aforementioned NLP applications. Other such works which are built upon a WordNet like concept and produce augmented resources are BabelNet(Navigli et al., 2010) and FrameNet(Baker et al., 1998). VerbNet(Schuler, 2005) is another such verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to the English WordNet. ConceptNet(Liu and Singh, 2004) is a semantic network, built from nodes (or “terms”), representing words or short phrases of natural language, that labels the relationships between them. Our method helps refine the data from Urban Dictionary; using several manual changes and mappings, we normalize this to a structured WordNet like resource. We validate the usability of o"
L16-1686,P98-1013,0,\N,Missing
L16-1686,C98-1013,0,\N,Missing
L18-1049,N15-1132,1,0.92024,"spectively. We find inter-annotator multi-rater kappa agreement (Fleiss, 1971) of 0.80. We develop two models based on rules and supervised machine learning. 3.1. Rule-based Approach We define a set of generic rules which we apply for determining the temporal sense of any sentence for both Twitter and Newswire text. We apply the same set of rules for the following two cases: (i). Temporal sense of each word sense in the sentence is detected using our temporal resource. The most suitable sense of each word in the sentence is determined using an unsupervised Most Frequent Sense (MFS) algorithm (Bhingardive et al., 2015). (ii). We identify the tense of each word in a sentence using a Hindi Morphological Analyzer.7 Verbs with the tense information (past, present or future) are used for developing the rulebased system. We depict the rules in Algorithm 2. 7 Algorithm 2 Basic Steps of Rule-based Approach. 1: If majority words in a sentence belong to a particular temporal/tense category t then label it as t. 2: If the words in the sentence are equally distributed among the three classes then 2.1. Label the sentence as present if the classes are only past and present; 2.2. Label the sentence as future if the classe"
L18-1049,E14-4002,0,0.0175601,"poral word senses. The process initiates learning with a set of seed instances for each class, and then iteratively expands it following various expansion strategies. The temporal resource, Tempo-Hindi-WordNet that we build will definitely be an effective resource for the efficient temporal information access in the resource-poor languages like Hindi which is one of the widely spoken languages worldwide and one of the official languages in India. We show how this resource can be utilized for sentencelevel temporal tagging. Our present study is inspired from the prior works (Dias et al., 2014; Hasanuzzaman et al., 2014), where the authors attempted to annotate each synset of English WordNet with four temporal dimensions, namely past, present, future and atemporal. Our work differs from these existing works in terms of the following points: (i). present work attempts to build a temporal resource that can facilitate temporal information access in Hindi; (ii). new expansion strategies including word-embedding based techniques are proposed; and (iii). two approaches (i.e. rule-based and machine learning-based) for sentence-level temporality detection are developed. The present work also differs from an earlier w"
L18-1049,S13-2001,0,0.0353591,"यवःथा म भारत के लए अवसर ा है? (DijiTala arthavyavasthA meM bhArata ke lie avasara kyA hai?What are the opportunities for India in the Digital Economy?)”, “अशोक का इितहास (ashoka kA itihAsa- History of Ashoka)” need future and past related information, respectively. Here, tense related information does not help but the implicit temporal keywords ‘current’, ‘opportunities’ and ‘history’ help in finding the temporal information of the respective queries. 1.1. Motivation and Problem Definition Most of the earlier studies, for example, TempEval tasks (Verhagen et al., 2009; Verhagen et al., 2010; UzZaman et al., 2013) in the computational linguistics, have concentrated on identifying the temporal expressions, event expressions and various relations among these. These studies tried to address the temporal aspects of information with the help of linguistic constructs such as the presence of temporal expressions like before, now, after etc., document creation time (DCT), or explicit time expressions. Let us consider the following two example sentences: Sentence-I: You should live in the present. Sentence-II: She gave him a nice present. When these two sentences are subjected as input to the SUTime tagger,2 we"
L18-1049,bojar-etal-2014-hindencorp,0,0.0129835,"tic relations such as hypernym and hyponym detect connotative temporality as hypernym is a generalization and hyponym is a specialization of the synset. For example, “ वराम_काल (virAma kAla-rest period)” is the kind of “काल (kAla-period)”. Here, “काल (kAla-period)” is the hypernym and “ वराम_काल (virAma kAla- rest period)” is a hyponym. Both of these indicate temporality. As we encode both hyponyms and hypernyms, one’s presence ensures other’s inclusion through expansion. We use Word2vec tool (Mikolov et al., 2013) for generating word embedding vectors. The model is trained on Bojar’s corpus (Bojar et al., 2014) of around 44 million Hindi sentences for the training of Word2Vec using Skip-gram model with the dimension set to 200 and window size set to 7. For each content word of the synset, hyponyms, hypernyms and their glosses, we extract the corresponding vector of 200 dimension. All these vectors are averaged over to create a ‘prototype vector’. If there are m content words then the prototype vector is generated as shown in the Equation 1. ∑m i=1 W E(wi ) (1) m where, m is the number of content words in the glosses of synset, synonyms, hypernyms and hyponyms; WE(wi ) is the word embedding vector of"
L18-1049,L16-1595,1,0.886829,"Missing"
L18-1187,P16-1068,0,0.018109,"This limitation limits the utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include organization (Persing et al., 2010), prompt adher"
L18-1187,P05-1018,0,0.0843199,"Missing"
L18-1187,D13-1180,0,0.0310762,"e ASAP AEG dataset. However, most of the essays only have an overall score, not attribute-specific scores. This limitation limits the utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes"
L18-1187,D16-1115,0,0.0188267,"bute-specific scores. This limitation limits the utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include organization (Persing e"
L18-1187,K17-1017,0,0.0116378,"ll score, not attribute-specific scores. This limitation limits the utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include o"
L18-1187,P14-5010,0,0.00333636,"spaper in India), being the chief editor of the college magazine, etc. All the annotators have either studied or are studying English at a Master of Arts (MA) level. 6. 6.1. Experiments Features Used After creating the resource, we ran experiments to get some baseline results. We used the attribute independent feature set provided by Zesch et al. (2015). In addition to those features, we also made use of entity grid features described in Barzilay and Lapata (2005). Table 2 summarizes the list of features that we used in our experiments. All the features were extracted using Stanford Core NLP (Manning et al., 2014). 6 Prompt 5, with a total of 1805 essays had 105 essays in its last set. 7 http://www.thehindu.com/ 1171 Prompt ID Prompt 1 Prompt 2 Average Content Coherence Coherence Coherence Organization Length Coherence Coherence Word Choice Coherence Coherence Coherence Sentence Fluency Syntax Syntax Syntax Conventions Coherence Coherence Coherence Table 5: Results of the ablation tests using the Random Forest classifier for argumentative / persuasive essays to determine the most important feature set for each attribute in each prompt. Prompt ID Prompt 3 Prompt 4 Prompt 5 Prompt 6 Average Content Lengt"
L18-1187,P14-1144,0,0.221928,"ipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), coherence (Somasundaran et al., 2014). 4. Dataset The entire ASAP dataset has nearly 13,000 essays across 8 prompts. 6 of those 8 prompts, comprising nearly 10,400 essays, only have an overall score. 4.1. Essay Topics The following is the list of topics of the 8 prompts in the dataset: 1 www.grammarly.com www.paperrater.com 3 The dataset can be downloaded from here: https://www. kaggle.com/c/asap-aes/data. 2 1169 1. Prompt 1 - The writers had to write a letter to their local newspaper in which they stated their opinion on the effects computers have on people. 2. Prompt 2 - The writers had to"
L18-1187,D10-1023,0,0.191329,"ang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), coherence (Somasundaran et al., 2014). 4. Dataset The entire ASAP dataset has nearly 13,000 essays across 8 prompts. 6 of those 8 prompts, comprising nearly 10,400 essays, only have an overall score. 4.1. Essay Topics The following is the list of topics of the 8 prompts in the dataset: 1 www.grammarly.com www.paperrater.com 3 The dataset can be downloaded from here: https://www. kaggle.com/c/asap-aes/data. 2 1169 1. Prompt 1 - The writers had to write a letter to their local newspaper in which they stated their opinion on the effects computers have on"
L18-1187,D15-1049,0,0.0229544,"However, most of the essays only have an overall score, not attribute-specific scores. This limitation limits the utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of th"
L18-1187,C14-1090,0,0.0697415,"Missing"
L18-1187,D16-1193,0,0.162752,"utility of this dataset for predicting the scores of particular attributes of essays. Shermis and Burstein (2013) in chapter 19 (Contrasting State-of-the-Art Automatic Essay Grading Systems) of their book describe the ASAP dataset, as well as the results of current commercial AEG systems in scoring those essays. Since then, a large amount of work has been done using that dataset for evaluating the overall score of essays, from using machine learning techniques (Chen and He, 2013; Phandi et al., 2015) to deep learning systems (Dong et al., 2017; Dong and Zhang, 2016; Alikaniotis et al., 2016; Taghipour and Ng, 2016). One common feature that all the above work has in common is the fact that the essay grading dataset that they used was the ASAP AEG dataset. However, most of them (in particular the deep learning systems) are constrained by the fact that there are very few prompts to handle scoring of individual attributes. 3. Related Work While there has been a lot of work done in overall essay scoring, not much has been done with respect to scoring particular attributes of essays. Some of the attributes that have been scored include organization (Persing et al., 2010), prompt adherence (Persing and Ng, 201"
L18-1187,W15-0626,0,0.225277,"Engineering Indian Institute of Technology, Bombay {sam, pb}@cse.iitb.ac.in Abstract In this paper, we describe the creation of a resource - ASAP++ - which is basically annotations of the Automatic Student Assessment Prize’s Automatic Essay Grading dataset. These annotations are scores for different attributes of the essays, such as content, word choice, organization, sentence fluency, etc. Each of these essays is scored by an annotator. We also report the results of each of the attributes using a Random Forest Classifier using a baseline set of attribute independent features as described by Zesch et al. (2015). We release and share this resource to facilitate further research into these attributes of essay grading. Keywords: Automatic Essay Grading, Attribute-specific Essay Grading 1. Introduction Automatic essay grading (AEG) is one of the most challenging activities in natural language processing (NLP). AEG makes use of many NLP and machine learning (ML) techniques in predicting the score of an essay - a piece of text that is written by a human on a given topic (called a prompt). It has been around since the 1960s, with the first AEG system - Project Essay Grade - proposed by Ellis Page (Page, 19"
L18-1278,A97-1029,0,0.642534,"also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for entity extraction involving Indian languages was introduced in FIRE-2016 shared 1762 task (Rao and Devi, 2016). Some of the challenges for entity extraction in a code-mixed environment are as follows: • The dataset contains tweets uttera"
L18-1278,D14-1179,0,0.00987688,"Missing"
L18-1278,C96-1079,0,0.431751,"nt code-mixing in Indian languages. Hence, Indians are multilingual by adaptation and necessity, and frequently change and mix languages while writing in social media platforms. These pose additional difficulties in building automated tools for social media analytics. Named Entity Recognition (NER) is a primary task in information extraction. It aims at identifying the names of entities and classifying them into some predefined categories such as people, location, organization and product. This task can also be thought of as a two-step process, viz. entity detection and entity classification (Grishman and Sundheim, 1996). There are a significantly large body of works existing in Indian languages, but these are mostly related to the domains such as newswire. Nowadays, information extraction in microblogs has become an active research topic (Cano Basave et al., 2013), following the early experiments which showed this genre to be extremely challenging for the state-of-theart algorithms (Derczynski et al., 2015; Bontcheva et al., 2014). The shortness of micro-blogs makes them hard to interpret. The social media text normally carries less discourse information per document, and threaded structure is fragmented acr"
L18-1278,P11-1038,0,0.0284999,"rom this, short text (tweets) also exhibits more language variations, tend to be less grammatical than the longer posts, contains unorthodox capitalization, and makes use of frequent abbreviations, hashtags and emoticons. These information also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for"
L18-1278,C02-1054,0,0.165258,"d on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for entity extraction involving Indian languages was introduced in FIRE-2016 shared 1762 task (Rao and Devi, 2016). Some of the challenges for entity extraction in a code-mixed environment are as follows: • The dataset contains tweets utterance in code mixed as well as in mono-lingual text. • Introduction of a diverse set of entities, not limited to the only tra"
L18-1278,D14-1162,0,0.0823198,"Missing"
L18-1278,D11-1141,0,0.0676835,"carries less discourse information per document, and threaded structure is fragmented across multiple documents. Apart from this, short text (tweets) also exhibits more language variations, tend to be less grammatical than the longer posts, contains unorthodox capitalization, and makes use of frequent abbreviations, hashtags and emoticons. These information also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts"
L18-1413,D07-1091,0,0.430844,"nterjection) are the three types of dyootakam (Varma, 2000). Malayalam has a strong postpositional inflections with highly agglutinative suffixes (Namboodiri, 1998). These inflections carry information about tense, mood and aspect for verbs and cases (accusative, dative, etc.), gender, number, person information for nouns. Most approaches to Statistical Machine Translation, i.e., phrase based models (Koehn, Och and Marcu, 2003), syntax based models (Yamada and Knight 2001) do not allow incorporation of any linguistic information in the translation process. The introduction of factored models (Koehn and Hoang, 2007) provided this missing linguistic touch to the statistical machine translation. Factored models (Koehn and Hoang, 2007) treat each word in the corpus as vector of tokens. Each token can be any linguistic information about the word which leads to its inflection on the target side. Hence, factored models are preferred over phrase based models (Koehn, Och and Marcu, 2003) while translating from morphologically poor language to morphologically richer language. There were many attempts to improve the quality of SMT systems such as; using Monolingually-Derived Paraphrases(Marton et al., 2009), Using"
L18-1413,kunchukuttan-etal-2014-shata,1,0.859055,"hologically poor language to morphologically richer language. There were many attempts to improve the quality of SMT systems such as; using Monolingually-Derived Paraphrases(Marton et al., 2009), Using Related Resource-Rich languages (Nakov and Ng, 2012), (Minkov et. al., 2007) . In this paper, we study SMT models and the problem of sparseness and morphological complexity in the context of translation to a highly agglutinative, morphologically rich language Malayalam from English. There are many ongoing attempts to develop MT systems for Indian languages (Antony, 2013; Bharathi et. al., 1996; Kunchukuttan et al., 2014; Nair et. al., 2012; Sreelekha et al., 2013; Sreelekha et al., 2015; Sreelekha et al., 2015; Sreelekha et al., 2016; Sreelekha et al., 2018) using both rule based and statistical approaches. Even though there were many attempts to develop Machine Translation systems between English and Malayalam, the complexity of morphology, especially the word compounding phenomena and the various derivation morphology forms makes the translation quality worse. In this paper we propose a simple and effective solution to handle the morphological complexity which is based on enriching the input with various m"
L18-1413,D09-1040,0,0.0878715,"Missing"
L18-1413,P07-1017,0,0.264264,"Missing"
L18-1413,J03-1002,0,0.0278444,"Missing"
L18-1413,P02-1040,0,0.113259,"Missing"
L18-1413,L16-1098,1,0.836598,"Missing"
L18-1413,P01-1067,0,0.380936,"riyaa (verb) and bheedakam (modifier) are the three types of Vaachakam. gati (preposition), ghatakam (conjunction) and vyaakseepakam (interjection) are the three types of dyootakam (Varma, 2000). Malayalam has a strong postpositional inflections with highly agglutinative suffixes (Namboodiri, 1998). These inflections carry information about tense, mood and aspect for verbs and cases (accusative, dative, etc.), gender, number, person information for nouns. Most approaches to Statistical Machine Translation, i.e., phrase based models (Koehn, Och and Marcu, 2003), syntax based models (Yamada and Knight 2001) do not allow incorporation of any linguistic information in the translation process. The introduction of factored models (Koehn and Hoang, 2007) provided this missing linguistic touch to the statistical machine translation. Factored models (Koehn and Hoang, 2007) treat each word in the corpus as vector of tokens. Each token can be any linguistic information about the word which leads to its inflection on the target side. Hence, factored models are preferred over phrase based models (Koehn, Och and Marcu, 2003) while translating from morphologically poor language to morphologically richer lang"
L18-1424,P06-4018,0,0.0340434,"with epsilon e=0.5 and RBF kernel3 . We set C=1000 for tweets and C=1500 for snippets. We report our results on four-fold cross validation for both datasets. Note that we convert individual sentences into words. Therefore, the dataset in the case of book snippets has 6377 instances, while the one of tweets has 6610 instances. The four folds for cross-validation are created over these instances. With a word as instance, the task is binary classification: 1 indicating that the word is a sarcasm target and 0 indicating that it is not. For rules in the rule-based extractor, we use tools in NLTK (Bird, 2006), wherever necessary. We consider two baselines with which our hybrid approach is compared: 1. Baseline 1: All Objective Words: As the first baseline, we design a na¨ıve approach for our task: include all words of the sentence which are not stop words, and have neutral sentiment polarity, as the predicted sarcasm target. 2. Baseline 2: Sequence labeling has been reported for opinion target identification (Jin et al., 2009). Therefore, we use SVM-HMM (Altun et al., 2003) with default parameters as the second baseline. We report performance using two metrics: Exact Match Accuracy and Dice Score."
L18-1424,P11-2102,0,0.0396278,"Missing"
L18-1424,P15-2124,1,0.950205,"approach for sarcasm target identification. It is based on a combination of two types of extractors: one based on rules, and another consisting of a statistical classifier. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work. Keywords: Sentiment Analysis, Computational Sarcasm, Aspect Extraction 1. Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule (Source: The Free Dictionary). While several approaches have been reported for sarcasm detection (Rajadesingan et al., 2015; Joshi et al., 2015; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011), no past work, to the best of our knowledge, has attempted to identify a crucial component of sarcasm: the target of ridicule (Campbell and Katz, 2012). This is important because the sentiment of the sarcastic text needs to be attributed to this target of ridicule. Towards this motivation, we introduce ‘sarcasm target identification’: the task of extracting the target of ridicule (i.e., sarcasm target) of a sarcastic text. The input is a sarcastic text while the output is either (a) a subset of words in the sentence that point to the sarcasm"
L18-1424,D16-1104,1,0.882368,"e for future work. Sarcasm target identification can benefit natural language generation and sentiment analysis systems. Being able to recognize the entity towards which the negative sentiment was intended, a natural language generation system will have more context to generate a response. Similarly, a sentiment analysis system will be able to attribute the negative sentiment in a sarcastic text towards the correct aspect of a product or the appropriate entity. 2. Related work Computational sarcasm primarily focuses on sarcasm detection: classification of a text as sarcastic or non-sarcastic. Joshi et al. (2016a) present a survey of sarcasm detection approaches. They observe three trends in sarcasm detection: semi-supervised extraction of sarcastic patterns, use of hashtag-based supervision, and use of contextual information for sarcasm detection (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). However, to the best of our knowledge, no past work aims to identify phrases in a sarcastic sentence that indicate the target of ridicule in the sentence. Related to sarcasm target identification is sentiment target identification. Sentiment target identification deals with identifying the entit"
L18-1424,W15-2905,1,0.829118,"f the lists are empty, the output is returned as ‘Outside’. 2. Hybrid AND : In this configuration, the integrator predicts the set of words that occur in the output of both the two extractors as the sarcasm target. If the intersection of the lists is empty, the output is returned as ‘Outside’. accuracy because it accounts for missing words and extra words in the target. Let the two lists (predicted and actual) be X and Y. Dice score is given by (2X ∩ Y )/(X + Y ). 5.3. Rule R1 R2 R3 R4 R5 R6 R7 R8 R9 The idea of using two configurations OR and AND is based on a rule-based sarcasm detector by (Khattri et al., 2015). While AND is intuitive, the second configuration OR is necessary because our extractors individually may not capture all forms of sarcasm target. This is intuitive because our rules may not cover all forms of sarcasm targets. 5.2. Experiment Setup We use SVM Perf (Joachims, 2006) to train the classifiers, optimized for F-score with epsilon e=0.5 and RBF kernel3 . We set C=1000 for tweets and C=1500 for snippets. We report our results on four-fold cross validation for both datasets. Note that we convert individual sentences into words. Therefore, the dataset in the case of book snippets has 6"
L18-1424,P12-1036,0,0.0170892,"n order to achieve sentiment summarization. Lu et al. (2011) perform multi-aspect sentiment analysis using a topic model. 2676 Example Target Love when you don’t have two minutes to send me a quick text. Don’t you just love it when Microsoft tells you that you’re spelling your own name wrong. I love being ignored. He is as good at coding as Tiger Woods is at avoiding controversy. Oh, and I suppose the apple ate the cheese. you Microsoft being ignored He, Tiger Woods Outside Table 1: Examples of sarcasm targets Several other topic model-based approaches to aspect extraction have been reported (Mukherjee and Liu, 2012). To the best of our knowledge, ours is the first work that deals with sarcasm target identification. 3. that started off the day. We refer to such cases as the ‘Outside’ cases. 4. 4.1. Formulation Sarcasm is a well-known challenge to sentiment analysis (Pang et al., 2008). Consider the sarcastic sentence ‘My cell phone has an awesome battery that lasts 20 minutes’. This sentence mocks the battery of the cell phone. Aspectbased sentiment analysis deals with identifying sentiment expressed towards different aspects or dimensions of an entity. Therefore, aspect-based sentiment analysis needs to"
L18-1424,P05-1015,0,0.156733,"hat uses a word-level classifier for every word in the sentence, to predict if the word will constitute the sarcasm target). Since this is the first work in sarcasm target detection, no past work exists to be used as a baseline. Hence, we devise two baselines to validate the strength of our work. The first is a simple, intuitive baseline to show if our approach (which is computationally more intensive than this simple baseline) holds value. In absence of past work, using simple and obvious techniques to solve a problem have been considered as baselines in sentiment analysis (Tan et al., 2011; Pang and Lee, 2005). As the second baseline, we use a technique reported for sentiment/opinion target identification since sentiment target identification appears to be related to sarcasm target identification, on the surface. Our manually labeled datasets are available for download at: https://github.com/Pranav-Goel/ 1 This label is necessary because the sarcasm target may not be present as a word, as discussed in Section 2. Sarcasm-Target-Detection. Each unit consists of a piece of text (either book snippet or tweet) with the annotation as the sarcasm target where the sarcasm target is a subset of words in the"
L18-1424,J11-1002,0,0.046851,"hey observe three trends in sarcasm detection: semi-supervised extraction of sarcastic patterns, use of hashtag-based supervision, and use of contextual information for sarcasm detection (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). However, to the best of our knowledge, no past work aims to identify phrases in a sarcastic sentence that indicate the target of ridicule in the sentence. Related to sarcasm target identification is sentiment target identification. Sentiment target identification deals with identifying the entity towards which sentiment is expressed in a sentence. Qiu et al. (2011) present an approach to extract opinion words and targets collectively from a dataset. Aspect identification for sentiment has also been studied. This deals with extracting aspects of an entity (for example, color, weight, battery in case of a cell phone). Probabilistic topic models have been commonly used for the same. Titov et al. (2008) present a probabilistic topic model that jointly estimates sentiment and aspect in order to achieve sentiment summarization. Lu et al. (2011) perform multi-aspect sentiment analysis using a topic model. 2676 Example Target Love when you don’t have two minute"
L18-1424,D13-1066,0,0.163533,"Missing"
L18-1424,P08-1036,0,0.163879,"Missing"
L18-1440,D14-1179,0,0.00754919,"Missing"
L18-1440,P05-1045,0,0.0525458,"d between the query and candidate answers. ∑ VEC(ti ) × tf-idfti VEC(X) = ti ∈X (3) number of look-ups where X is query q or candidate answer sentence S, VEC(ti ) is the word vector of word ti . number of lookups represents the number of words in the question for which pre-trained word embeddings8 are available. Candidate Answer Extraction This depends on the output of question classification. For factoid question, the coarse class and finer class guide this stage to extract the appropriate entities from the candidate passage(s). We tag the candidate passage with Stanford named entity tagger (Finkel et al., 2005). We utilize the coarse class and finer class of a question to extract the suitable candidate answers. For a descriptive question, candidate answers are extracted by segmenting the relevant passage. 4.5. 2. Proximity score (PS): It calculates the length of the shortest span that covers the query contained in the candidate answer sentence. This is again normalized in the same way. Passage Retrieval The candidate passage that contains the answer(s) to the given question(s) are extracted in this stage. We exploit the Lucene’s text retrieval functionality to retrieve passage. It retrieves and rank"
L18-1440,H01-1069,0,0.304621,"Missing"
L18-1440,D14-1181,0,0.0133982,"plying that only candidate answers that are dates need to be considered. With the recent developments in deep learning, neural network models have shown promise for QA. Deep neural network being perform exceptionally well in other NLP problem. Inspired by the success of deep neural network we adapt neural network architecture to develop our question classification model. Our question classification model is based on CNN and RNN. The model comprises of Question embedding layer, Convolution layer, Recurrent layer, Softmax classification layer. Our question classification model is inspired from (Kim, 2014) and (Xiao and Cho, 2016). The input to the model is an English question. Now we describe each component of the model: Question (English): Why did Alexander marched back in 325 BC? Question (Hindi): अले जडर 325 ईसा पूव म चला गया? Answer (English): After Alexander’s last major victory in India as his forces refused to go any further. They were too tired to carry on with the Alexander’s expedition and wanted to return home. Moreover, the might of Magadhan Empire (the Nanda Rulers) also dissuaded them. Alexander marched back in 325 BC after making necessary administrative arrangement for the conq"
L18-1440,C02-1150,0,0.071864,"ge Source Preparation In this step, an information source (articles) from which answers are to be derived was set-up. We translate Hindi questions and articles into English by Google Translate5 . The complete English articles are indexed at passage level using inverted indexing mechanism. We use the Lucene6 implementation of inverted indexing. 4.2. Question Processing: The question processing (QP) step is responsible for analyzing and understanding the questions posed to the QA system. We perform question classification with the question 2 Tourism (EN):www.india.com/travel classes proposed by Li and Roth (2002). Question class proTourism (HI): https://hindi.nativeplanet.com Diseases (EN, HI): https://simple.wikipedia.org/wiki/List_of_diseases, vides us the semantic constraint on the sought-after answer. We propose a deep learning based question classification rest of the domains are curated from http://www.jagranjosh.com/ 3 The annotators are equally proficient in both the languages Total of 7120 questions (English+Hindi) for which the answer exists in either of two language documents. 4 2779 5 6 https://translate.google.com https://lucene.apache.org/ Domains Tourism History Diseases Geography Econo"
L18-1440,W04-1013,0,0.02059,"Missing"
L18-1440,forner-etal-2010-evaluating,0,0.0235479,"s about the difficulty levels associated. For better understanding and thorough analysis of various answer types, similar to Rajpurkar et al. (2016) and Trischler et al. (2016), we categorize the answers of factoid questions into 8 entities and phrases. Statistics of the answer types for English and Hindi QA pairs are provided in Table 7. An example of QA pairs formulated from a comparable articles is given in Table 1. Some examples of short descriptive QA pair from our dataset are given in Table 5. The direct comparison of our dataset with the Cross-Language Evaluation Forum (CLEF) datasets (Pamela et al., 2010) is not possible because we have created question answers pair in both language (MQA) in contrast the CLEF dataset have the question and answer pair in the different languages. However, we have shown the comparison in various terms as shown in Table 6. 4. Evaluation: Proposed Approach We develop a translation based approach for multilingual QA. As English is a resource-rich language, we translate Hindi question and articles into English. Our proposed model comprises of Knowledge Source Preparation, Question Processing and Answer Extraction, We describe the details of each component in the foll"
L18-1440,P02-1040,0,0.107844,"Missing"
L18-1440,D16-1264,0,0.0874729,"Missing"
L18-1440,W06-1908,0,0.0166324,"mplemented the web based Hindi question answer. In this work the question and answer deal with only Hindi language, if the answer was not presented in Hindi document then it was retrieved from Google. (Sekine and Grishman, 2003) proposed a question answering system for Hindi and English. The questions were created in Hindi language and the answers retrieved from Hindi newspaper in the Hindi language. These answers were then converted into the English language. In this work, an English Hindi bilingual dictionary was used to find top 20 Hindi articles which were used to find candidate answers. (Reddy and Bandyopadhyay, 2006) proposed question answering system in the Telugu language. The system was dialogue based and railway specific domain. The architecture was based on the keyword approach. The query analyzer generates the tokens and keywords. From tokens, SQL statements were generated. Using SQL query the answer was retrieved from the database. (Reddy and Bandyopadhyay, 2006) develop the question answer system in English and Punjabi language. In this work a pattern and matching algorithm was introduced to retrieve the most relevant appropriate answer from multiple sets of answers for a given question. 3. Resour"
L18-1440,N03-1033,0,0.124428,"te gate, reset get and new memory content, respectively. ci is the convolution output at time t.The final output of recurrent layer h is obtained as the concatenation of the last hidden state of forward and backward hidden states. • Softmax classification layer: Finally, the fixeddimensional vector h is fed into the softmax classification layer to compute the predictive probabilities for all the question classes (coarse or fine). 4.2.2. Query Formulation In order to form the query, we remove all the stop word, punctuation symbol from the question. We tag the question with Stanford PoS tagger (Toutanova et al., 2003). Then we concatenate all the noun, verb and adjective in the same order in which it appears in the question. 4.3. 3. N-Gram coverage score (NS): We compute the ngram coverage till n = 4. Finally, the n-gram score between a query (q) and a candidate answer sentence (S) is calculated based on the following formula. ∑ ng ∈S N GCoverage(q, S, n) = ∑ n Countcommon (ngn ) Countquery (ngn ) (1) n ∑ N GCoverage(q, S, i) ∑n N GScore(q, S) = (2) i=1 i i=1 ngn ∈q 4. Semantic Similarity Score (SS) : Query and candidate answer are represented using the semantic vectors. Cosine similarity is then computed"
L18-1442,D14-1181,0,0.00338329,"ng is fed as the input to the convolutional layer where filter F ∈ Rm×k is convoluted to the context window xi:i+m−1 of h words for each blog-post as follows. In this section we have presented the approach developed for extracting sentiments of users’ posts in medical blogs. 5.1. Network for Identifying Severity Level In this section we propose a method based on CNN that exploits sentiments from health forums (or, medical blogs) in augmentation layer. As presented in Figure-1, the proposed model has four different components which are similar to the conventional CNN components as proposed by (Kim, 2014). The first layer represents the input layer which takes a complete blog post in the form of vector representation (word embedding) and outputs a probability corresponding to the classification types. We use max-pooling over the whole blog post to obtain global features through all the filters. This pooled feature is fed into the fully connected neural network. In the output layer, we use the softmax classifier to automatically classify the post into three out(1) ci = f (F.xi:i+m−1 + b) (2) where f is non-linear function4 and b is a bias term. The feature map f is generated by applying given f"
L18-1442,W10-1915,0,0.0176194,"onducted by Biyani et al. (2013) used online cancer community user data to determine the polarity. They have adapted supervised machine learning techniques using hand-crafted features, which cover both domain-dependent as well as domain-independent features. They identified sentiments on two discourse functions such as expressive and persuasive. A supervised machine learning model (multi-nominal naive Bayes) is developed using frequency-based features. • Adverse Drug Relation: For medical domains, social media texts (corresponding to medical forums) have been utilized in the works such as DS (Leaman et al., 2010; Nikfarjam and Gonzalez, 2011; Liu and Chen, 2013), MedHelp (Yang et al., 2012) and PatientsLikeMe (Wicks et al., 2011). Non-medical social media forums like Twitter (Nikfarjam et al., 2015) have been exploited to capture adverse drug effect. With the availability of the extensive Adverse Drug Reaction (ADR) lexicons such as Side Effect Resource (SIDER)2 (Kuhn et al., 2010), Coding Symbols for a Thesaurus of Adverse Reaction Terms (COSTART), Consumer Health Vocabulary (CHV) (Zeng-Treitler et al., 2008) and Medical Dictionary for Regulatory Activities (MedDRA) (Mozzicato, 2009), some prominent"
L18-1442,R11-1019,0,0.020062,"Missing"
L18-1442,R13-1083,0,0.0439844,"Missing"
L18-1489,P98-1013,0,0.323344,"u et al., 2009; Hendrickx et al., 2013). There can be multiple paraphrase of a noun compound. Labeling: Assign a relation from predefined set of abstract relations (e.g., orange juice: S UB STANCE /M ATERIAL /I NGREDIENT). (Levi, 1978; Warren, 1978; Tratz and Hovy, 2010) Labeling is the most widely used representation in literature for noun compound. There are some attempts to paraphrase noun compounds. In between the two representation, researchers have also used scoring of template-based paraphrases for assigning abstract labels (Nakov, 2008; Nakov and Hearst, 2013). 2.1. FrameNet FrameNet (Baker et al., 1998)2 is a lexical database that shows usage of words in actual text based on annotated examples. It is based on a theory of meaning called Frame Semantics (Fillmore, 1976). The theory claims that meanings of most words can be inferred from a semantic frame: a conceptual structure that denotes the type of event, relation, or entity and the involved participants. For example, the concept of walking involves a person walking (S ELF M OVER), the PATH on which walking occurs, the D IRECTION in which the walking occurs, and so on. In FrameNet, this information is represented by a frame called S ELF M O"
L18-1489,W04-0404,0,0.207818,"ous sequences of nouns that act as a single semantic construct. They raise interesting challenges in Natural Language Processing. Without proper interpretation and paraphrasing of noun compounds, NLP methods may fail miserably at different tasks. The meaning of a noun compound is composed of the meanings of the individual constituents and the way they are semantically related. Noun compound interpretation is the task of detecting this underlying semantic relation (e.g., student protest: student ← AGENT ← protest). It is an important submodule for various NLP tasks such as machine translation (Baldwin and Tanaka, 2004; Balyan and Chatterjee, 2014), question answering (Ahn et al., 2005), etc. Noun compound interpretation can manifest itself in two settings: out-of-context interpretation and contextdependent interpretation. In out-of-context interpretation, given the noun compound, the task is either to annotate it with a relation from a semantic relation inventory (e.g., student protest: AGENT), or to produce a paraphrase (e.g., student protest: “protest carried out by student”). Any automated approaches for noun compound interpretation need a semantic relation inventory of noun-noun relations and an annota"
L18-1489,P98-1015,0,0.614817,"Missing"
L18-1489,W09-2416,0,0.501047,"Missing"
L18-1489,S13-2025,0,0.852682,"Missing"
L18-1489,I05-1082,0,0.752362,"in either KB05, OS09, or TH10. Thus, we do not consider it as a candidate noun compound. Note that a candidate noun compound can be generated from more than one frames, thereby having multiple {framei , frame elementi } labels. In that case we repeat the candidate noun compound, once for each label. Manual Phase In this phase, we check the correctness of labels (specially, frame and FE) manually. For example, consider the following annotated sentence: If the scientists are right, then a major clue about how [Entity cancer] DEVELOPST arget [P lace in children] has been found. 5. KB05: Label in Kim and Baldwin (2005)’s dataset (hereafter, KB05) (NA if not found). ´ S´eaghdha and Copestake (2009)’s 6. OS09: Label in O dataset (hereafter, OS09) (NA if not found). 7. TH10: Label in Tratz and Hovy (2010)’s dataset (hereafter, TH10) (NA if not found). 8. Type: Type of noun compound according to Levi’s theory. 4.2. Dataset Creation The previous automated phase generates children development as a candidate noun compound with the label {C OMING TO BE, P LACE}. But, in general, usage of children development means “development of children”, and not “development in children”. We simply drop such noun compounds. Then"
L18-1489,P14-5010,0,0.00455649,"Missing"
L18-1489,W04-2609,0,0.160348,"Missing"
L18-1489,E09-1071,0,0.0286245,"Missing"
L18-1489,W16-6336,1,0.726889,"five compositional SRs has been 3093 further categories in total eleven categories. They have also prepared a dataset with 1443 examples for the five coarsegrained and eleven fine-grained relations. In addition to the above-mentioned dataset for the general domain, Rosario et al. (2002) proposed an inventory of SRs for medical domain. Tratz and Hovy (2010) claims that they have a created a new inventory of semantic relations by comparing and consolidating the existing inventories. But, in contract, their inventory creation process is an iterative process to improve inter-annotator agreement. Ponkiya et al. (2016) reports many problems with this inventory. Unfortunately, these inventories are not used in actual scenarios which need an interpretation of noun compounds. For instance, Balyan and Chatterjee (2014) shows how the interpretation of NCs can help automatic machine translation (MT). But, they didn’t use any existing repository. We now propose our dataset. 4. Proposed Dataset In this version of the dataset, we generate only noun-noun compounds (noun compounds with only 2 components). 4.1. Dataset Fields Our dataset contains the following fields: 1. w1 : The first word of the noun compound. 2. w2"
L18-1489,P02-1032,0,0.597036,"pestake (2009) proposed an inventory O of SRs based on RDP (recoverable deleted predicates) of Levi (1978). Along with five SRs for compositional NCs – the meaning of the compound is composed of the meaning of the components – they proposed five more SRs for other categories like lexicalized compounds, wrongly tagged compounds. The five compositional SRs has been 3093 further categories in total eleven categories. They have also prepared a dataset with 1443 examples for the five coarsegrained and eleven fine-grained relations. In addition to the above-mentioned dataset for the general domain, Rosario et al. (2002) proposed an inventory of SRs for medical domain. Tratz and Hovy (2010) claims that they have a created a new inventory of semantic relations by comparing and consolidating the existing inventories. But, in contract, their inventory creation process is an iterative process to improve inter-annotator agreement. Ponkiya et al. (2016) reports many problems with this inventory. Unfortunately, these inventories are not used in actual scenarios which need an interpretation of noun compounds. For instance, Balyan and Chatterjee (2014) shows how the interpretation of NCs can help automatic machine tra"
L18-1489,P10-1070,0,0.796549,"per, by noun compound, we mean nounnoun compounds. For representation of the semantic relation between the components of noun compounds, there are two major ways: Paraphrasing: paraphrase a noun compound to show how the components are related (e.g., orange juice: “juice made of orange”, “a drink consisting of the juice from oranges”, etc.) (Butnariu et al., 2009; Hendrickx et al., 2013). There can be multiple paraphrase of a noun compound. Labeling: Assign a relation from predefined set of abstract relations (e.g., orange juice: S UB STANCE /M ATERIAL /I NGREDIENT). (Levi, 1978; Warren, 1978; Tratz and Hovy, 2010) Labeling is the most widely used representation in literature for noun compound. There are some attempts to paraphrase noun compounds. In between the two representation, researchers have also used scoring of template-based paraphrases for assigning abstract labels (Nakov, 2008; Nakov and Hearst, 2013). 2.1. FrameNet FrameNet (Baker et al., 1998)2 is a lexical database that shows usage of words in actual text based on annotated examples. It is based on a theory of meaning called Frame Semantics (Fillmore, 1976). The theory claims that meanings of most words can be inferred from a semantic fram"
L18-1489,C94-2125,0,0.758031,"Missing"
L18-1548,abdelali-etal-2014-amara,0,0.0794058,"l communication in Hindi and interfacing with the rest of the word via English. Hence, there is immense potential for EnglishHindi machine translation. However, the parallel corpora available in the public domain is quite limited. This work is an effort to consolidate all publicly available parallel corpora for English-Hindi as well as significantly add to the available parallel corpus through corpora collected in the course of this work. 2. Dataset The parallel corpus has been compiled from a variety of existing sources (primarily OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014)) as well as corpora developed at the Center for Indian Language Technology2 (CFILT), IIT Bombay over the years. The training corpus consists of sentences, phrases as well 1 2 as dictionary entries, spanning many applications and domains. 2.1. Corpus Details The details of the training corpus are shown in Table 1. We briefly describe the new sub-corpora we have added to the collection. For the corpora compiled from existing sources, please refer to the papers mentioned in the table. Judicial domain corpus - I contains translations of legal judgements by in-house translators with many years of"
L18-1548,W05-0909,0,0.0852849,"he dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000 iterations. We used early-stopping based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The new corpora we release are available for"
L18-1548,W14-0130,1,0.820852,"g based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The new corpora we release are available for research and non-commercial use under a Creative Commons Attribution-NonCommercial-ShareAlike License 9 . The corpora we compiled from other sources are available under their respective licenses. The sub-corpora (in the corpus distribution that we make available) are in the same order as listed in the Table 1, so they can be separa"
L18-1548,bojar-etal-2014-hindencorp,0,0.0231507,"Missing"
L18-1548,N12-1047,0,0.0293747,"In the other case, a single Unicode character represents the composite character. We choose the former representation. The normalization script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with popular off-the-shelf machine translation toolkits to provide benchmark translation accuracies for comparison. We trained phrase-base"
L18-1548,W11-2123,0,0.0369898,"zation script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with popular off-the-shelf machine translation toolkits to provide benchmark translation accuracies for comparison. We trained phrase-based Statistical Machine Translation (PBSMT) systems as well as Neural Data Preparation NMT Setup We trained a subword-level"
L18-1548,P07-2045,0,0.0122134,"th nukta can have two Unicode representations. In one case, the character and nukta are represented as two Unicode characters. In the other case, a single Unicode character represents the composite character. We choose the former representation. The normalization script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with po"
L18-1548,P13-4030,1,0.83594,"Missing"
L18-1548,P02-1040,0,0.112156,"r, containing 512 GRU units each. The dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000 iterations. We used early-stopping based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The ne"
L18-1548,I08-1067,1,0.795954,"lish-Hindi Parallel corpus version 1.0, and provided benchmark baseline SMT and NMT results on this corpus. This corpus has been used for the two shared tasks (Workshop on Asian Language Translation 2016 and 2017). The HindiEn component of the corpus has also been used for the WMT 2014 shared task. The corpus is available under a Creative Commons Licence. In future, we plan to enhance the corpus from additional sources, mostly websites of the Government of India which is still a largely untapped source of parallel corpora. We also plan to build stronger baselines like pre-ordering with PBSMT (Ramanathan et al., 2008) for English-Hindi translation, and use of synthetic corpora generated via backtranslation for NMT systems (Sennrich et al., 2016a). 6. Acknowledgements We thank past and present members of the Center for Indian Language Technology for their efforts in creating various parts of the corpora over the years: Pallabh Bhattacharjee, Kashyap Popat, Rahul Sharnagat, Mitesh Khapra, Jaya Jha, Rajita Shukla, Laxmi Kashyap, Gajanan Rane and many members of the Hindi WordNet team. We also thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information"
L18-1548,P16-1162,0,0.0295925,"d Statistical Machine Translation (PBSMT) systems as well as Neural Data Preparation NMT Setup We trained a subword-level encoder-decoder architecture based NMT system with attention (Bahdanau et al., 2015). 3474 4 5 anoopkunchukuttan.github.io/indic_nlp_library www.statmt.org/moses System SMT NMT eng-hin 5. hin-eng BLEU METEOR BLEU METEOR 11.75 12.23 0.313 0.308 14.49 12.83 0.266 0.219 Table 3: Results for Baseline Systems We used Nematus 6 (Sennrich et al., 2017) for training our NMT systems. Vocabulary: We used Byte Pair Encoding (BPE) to learn the vocabulary (with 15500 merge operations) (Sennrich et al., 2016b). We used the subword-nmt 7 tool for learning the BPE vocabulary. Since the writing systems and vocabularies of English and Hindi are separate, BPE models are trained separately. Network parameters: The network contains a single hidden encoder and decoder RNN layer, containing 512 GRU units each. The dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000"
L18-1548,E17-3017,0,0.061118,"Missing"
L18-1548,tiedemann-2012-parallel,0,0.367394,"speakers. Hence, there is a large requirement for digital communication in Hindi and interfacing with the rest of the word via English. Hence, there is immense potential for EnglishHindi machine translation. However, the parallel corpora available in the public domain is quite limited. This work is an effort to consolidate all publicly available parallel corpora for English-Hindi as well as significantly add to the available parallel corpus through corpora collected in the course of this work. 2. Dataset The parallel corpus has been compiled from a variety of existing sources (primarily OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014)) as well as corpora developed at the Center for Indian Language Technology2 (CFILT), IIT Bombay over the years. The training corpus consists of sentences, phrases as well 1 2 as dictionary entries, spanning many applications and domains. 2.1. Corpus Details The details of the training corpus are shown in Table 1. We briefly describe the new sub-corpora we have added to the collection. For the corpora compiled from existing sources, please refer to the papers mentioned in the table. Judicial domain corpus - I contains translations o"
L18-1559,P14-5010,0,0.00376227,"of few news sources : www.ndtv.com, indianexpress.com, timesofindia.indiatimes.com, indiatoday.intoday.in, thehindu.com, news18.com, firstpost.com, dnaindia.com, deccanchronicle.com, financialexpress.com, business-standard.com, sify.com, newskerala.com, mid-day.com, thedailystar.net, theweek.in, tribuneindia.com 3543 id, event id, event name, category, Document Level Annotation (DLA), number of words and sentences. We develop a semi-automatic meta file generator interface where attribute values are automatically captured from the hierarchically organized data (See Figure 2). Stanford CoreNLP (Manning et al., 2014) integrated with our interface gave us the field values for sentence and word count. We asked our annotators to provide their judgments for the DLA attribute based on the guidelines specified in the next section. 5.6. Annotation Three annotators with post-graduate level knowledge in English were involved in labeling the TAP-DLND 1.0 target documents. Having read the source document(s) we asked the annotators to annotate an incoming on-event document as non-novel or novel solely based on the information coverage in the source documents. The annotation guidelines were simple: 1. To annotate a do"
L18-1559,H05-1014,0,0.33244,"lan, 2002) evaluation campaigns where the concern was to detect new event from online news streams. Although the intention was to detect the first story or reporting of a new event from a series of news stories, the notion of novelty detection from texts came into light for the research community. Some notable approaches for New Event Detection with the TDT corpus are by (Allan et al., 1998; Yang et al., 2002; Stokes and Carthy, 2001; Franz et al., 2001; Yang et al., 1998; Allan et al., 2000; Brants et al., 2003). However, the Novelty track in Text Retrieval and Evaluation Conferences (TREC) (Soboroff and Harman, 2005) were the first to explicitly explore the concept of Novelty Detection from texts. Under the paradigm of information retrieval, given a query, the TREC experiments were designed to retrieve relevant and novel sentences from a given collection. Some notable approaches for sentence level novelty detection from the TREC exercises are by (Allan et al., 2003; Kwee et al., 2009; Li and Croft, 2005; Zhang et al., 2003; Collins-Thompson et al., 2002; Gabrilovich et al., 2004; Ru et al., 2004). Textual Entailment based sentence level novelty mining was explored in the novelty subtask of Recognizing Tex"
L18-1559,H01-1030,0,0.15739,"texts has been carried out at three levels : event level, sentence level and document level. Research in novelty mining could be traced back to the Topic Detection and Tracking (TDT) (Allan, 2002) evaluation campaigns where the concern was to detect new event from online news streams. Although the intention was to detect the first story or reporting of a new event from a series of news stories, the notion of novelty detection from texts came into light for the research community. Some notable approaches for New Event Detection with the TDT corpus are by (Allan et al., 1998; Yang et al., 2002; Stokes and Carthy, 2001; Franz et al., 2001; Yang et al., 1998; Allan et al., 2000; Brants et al., 2003). However, the Novelty track in Text Retrieval and Evaluation Conferences (TREC) (Soboroff and Harman, 2005) were the first to explicitly explore the concept of Novelty Detection from texts. Under the paradigm of information retrieval, given a query, the TREC experiments were designed to retrieve relevant and novel sentences from a given collection. Some notable approaches for sentence level novelty detection from the TREC exercises are by (Allan et al., 2003; Kwee et al., 2009; Li and Croft, 2005; Zhang et al., 2"
L18-1728,bhattacharyya-2010-indowordnet,1,0.554367,"Italian, Spanish, German, French, Czech and Estonian. Each of these wordnets is structured in the same way as the Princeton WordNet for English (Miller et al., 1990) - synsets (sets of synonymous words) and semantic relations between them. Each wordnet separately captures a language-specific information. In addition, the wordnets are linked to an Inter-Lingual-Index, which uses Princeton WordNet as a base. This index enables one to go from concepts in one language to similar concepts in any other language. Such features make this resource helpful in crosslingual NLP applications. IndoWordNet (Bhattacharyya, 2010) is a linked wordnet comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach with Hindi WordNet as a pivot, which is partially linked to English WordNet. We exploit these links to create mappings from English WordNet to wordnets of other languages. 3. Resources In this section, we describe the resources released with our work. We release two primary resources with our dataset"
L18-1728,W98-0705,0,0.362976,"1. Introduction 2. Wordnets (Fellbaum, 1998) have been useful in different Natural Language Processing applications such as Word Sense Disambiguation (TufiS¸ et al., 2004; Sinha et al., 2006), Machine Translation (Knight and Luk, 1994) etc. Linked Wordnets are extensions of wordnets. In addition to language-specific information captured in constituent wordnets, linked wordnets have a notion of an interlingual index, which connects similar concepts in different languages. Such linked wordnets have found their application in machine translation (Hovy, 1998), cross-lingual information retrieval (Gonzalo et al., 1998), etc. Given the extensive application of wordnets in different NLP applications, creation and maintenance of wordnets involve expert involvement. Such involvement is costly both in terms of time and resources. This is further amplified in case of linked wordnets, where experts need to have knowledge of multiple languages. India is a vast country with massive language diversity. According to a census in 2001, there are 122 major languages 1 , out of which, 29 have more than a million native speakers. The IndoWordNet project contains wordnets of 18 of these languages. These wordnets were create"
L18-1728,C12-3030,1,0.848256,"Missing"
L18-1728,2016.gwc-1.57,1,0.713756,"an culture. Thus, their corresponding variant is not available in the Princeton WordNet (and is not likely to be included anytime). Thus, one needs to maintain the translation/transliteration of such notions from Indian languages 3 https://wordnet.princeton.edu/wordnet/ man/wnstats.7WN.html 4 https://wordnet.princeton.edu/man/ sensemap.5WN.html 4605 to the English language as a separate bilingual mapping 5 . A similar issue arises in case of proper nouns, which should be present in an Indian lexicon but they are not present in Princeton WordNet. They are also handled using bilingual mappings (Singh et al., 2016). Some of the synsets in Indian languages are too fine-grained and have a common representation in the English language. This is why we use the principle of Hypernymy linkages for linking such concepts. We reserve a set of synset id numbers later for language specific concepts and create them to include in these wordnets, individually. These are not linked to the Princeton WordNet and hence are not included in our resource. 5. Conclusion and Future Work In this paper, we describe two resources released along with this paper. We discussed the Indian language wordnets that are part of the IndoWo"
L18-1728,C04-1192,0,0.0469016,"Missing"
mohanty-bhattacharyya-2008-lexical,C92-2094,0,\N,Missing
N10-1065,J93-2003,0,0.018145,"001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word Y = target word T = length of source word K = number of f eatures λk = f eature weight N (X) = normalization constant CRF++ 1 , an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 ≤ j ≤ i + 2 • hi and source character bigrams ( {ei−1 , ei } or {ei , ei+1 }) 3 Bridge Transliteration Systems In this section, we explore the salient questio"
N10-1065,I08-6014,0,0.0230961,"Missing"
N10-1065,2003.mtsummit-papers.17,0,0.0289296,"ents and datasets used. Section 4.3 discusses the results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conve"
N10-1065,C00-1056,0,0.0317973,"rk Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of p"
N10-1065,kang-choi-2000-automatic,0,0.138718,"usses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grap"
N10-1065,W09-3502,1,0.870678,"Missing"
N10-1065,D09-1141,0,0.0210578,"n functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X → Z Machine Translation system can be improved by using additional X → Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity. 3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word"
N10-1065,J03-1002,0,0.00379825,"3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word Y = target word T = length of source word K = number of f eatures λk = f eature weight N (X) = normalization constant CRF++ 1 , an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 ≤ j ≤ i + 2 • hi and source character bigrams ( {ei−1 , ei } or {ei , ei+1"
N10-1065,C02-1099,0,0.0547151,"d on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang"
N10-1065,W98-1005,0,0.0170358,"ocess and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available betwee"
N10-1065,I08-6006,0,0.0133337,"he results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid"
N10-1065,P07-1108,0,0.0606185,"Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X → Z Machine Translation system can be improved by using additional X → Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity."
N10-1065,P97-1017,0,\N,Missing
N13-1088,de-melo-etal-2012-empirical,0,0.0236507,"Missing"
N13-1088,I11-1078,1,0.837435,"xplicitly in the literature. NLP researchers define it according to their convenience. In our current work, we strive to unravel In this paper, we delve deep into the cognitive roles associated with sense disambiguation through the means of an eye-tracking device capturing the gaze patterns of lexicographers, during the annotation process. In-depth discussions with trained lexicographers indicate that there are multiple cognitive sub-processes driving the sense disambiguation task. The eye movement paths available from the screen recordings done during sense annotation conform to this theory. Khapra et al. (2011) points out that the accuracy of various WSD algorithms is poor on certain 733 Proceedings of NAACL-HLT 2013, pages 733–738, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Part-of-speech (POS) categories, particularly, verbs. It is also a general observation for lexicographers involved in sense annotation that there are different levels of difficulties associated with various classes of words. This fact is also reflected in our analysis on sense annotation. The data available after the eye-tracking experiments gave us the fixation times and saccades pertaini"
N13-1088,passonneau-etal-2010-word,0,0.02543,"locations during sentence reading. Their study indicates that local lexical predictability influences in decisions but not where the initial fixation lands in a word. In another work based on word grouping hypothesis and eye 734 movements during reading by Drieghe et al. (2008), the distribution of landing positions and durations of first fixations in a region containing a noun preceded by either an article or a high-frequency three-letter word were compared. Recently, some work is done on the study of sense annotation. A study of sense annotations done on 10 polysemous words was conducted by Passonneau et al. (2010). They opined that the word meanings, contexts of use, and individual differences among annotators gives rise to inter-annotation variations. De Melo et al. (2012) present a study with a focus on MASC (Manually-Annotated SubCorpus) project, involving annotations done using WordNet sense identifiers as well as FrameNet lexical units. In our current work we use eye-tracking as a tool to make findings regarding the cognitive processes connected to the human sense disambiguation procedure, and to gain a better understanding of “contextual evidence” which is of paramount importance for human annota"
N13-1088,P10-1154,0,0.0344902,"Missing"
N15-1125,W07-0702,0,0.0286083,"9) for a large number (over 25) of languages (other than Indian) including Japanese and Hindi (Japanese to Hindi translation being our focus) of approximately 30000 lines. We chose this setting because we feel that this multilingual approach is especially important for lowresource language pairs. Typically system combination methods like linear interpolation are used to combine the direct and pivot phrase tables by modifying the probabilities of phrase pairs leading to the modification of the underlying distribution which affects the resultant translation quality. The Multiple Decoding Paths (Birch and Osborne, 2007) (MDP) feature has been used 4 The construction of a multilingual corpus has already the benefit that each new language added to it will allow direct translation with a SMT system for N new language pairs. 1193 • Most works focus on obtaining pivot based phrase tables on relatively larger corpora than the ones used for the direct phrase table. We use the same corpora sizes for the pivot as well as direct tables. • We verify that Multiple Decoding Paths (MDP) feature of Moses is much more effective than plain linear interpolation, especially when more pivot languages are used together. • We sho"
N15-1125,P10-1137,1,0.846116,"ments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We first describe the pivot languages and the corpora we use. We follow this with a description of the triangulation method which we use to construct phrase tables using the pivot languages, the methods used to combine the constructed tables and then the experiments that use them. 1194 3"
N15-1125,I13-1167,0,0.0314773,"Missing"
N15-1125,W09-0431,0,0.0211025,"as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval."
N15-1125,P07-2045,0,0.00823489,"thods by: Firstly, combining the pivot based phrase tables into a single table using equation 5 (using the ratio of BLEU scores as interpolation weights) followed by using this table to support the direct phrase table by MDP. Note that the right way would be to use the BLEU scores on the tuning set but our objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) us"
N15-1125,W07-0733,0,0.0240646,"translation system. Reordering tables are supplementary and can usually be replaced by a simple distortion model. Major problems arise when source-pivot and pivot-target corpora belong to different domains leading to rather poor quality translations. Even if the individual corpora are large, one will run into domain adaptation problems. In such a scenario the availability of a small size multilingual corpus of a few thousand lines belonging to a single domain can be beneficial. The setting of this paper is: to combine two source-target phrase tables of different domains for domain adaptation (Koehn and Schroeder, 2007) but not so extensively in a pivot language scenario, especially when multiple pivots are involved (7 in our case). Our work is different from other previous works in the following ways: • We work on a realistic low resource setting for translation between Japanese and Hindi in which we use small sized multilingual corpora containing translations of a sentence in multiple languages. 1. We suppose the existence of a multilingual corpus with sentences aligned across N 4 different languages. • We focus on the impact of using a relatively large number of pivot languages (7 to be precise) to improv"
N15-1125,W04-3250,0,0.0268447,"the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set; the set of pivot languages to be used (Tables 1 and 2): 7 By Oracle sc"
N15-1125,kunchukuttan-etal-2014-shata,1,0.889032,"Missing"
N15-1125,D09-1141,0,0.0177672,"ce this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by"
N15-1125,2001.mtsummit-papers.46,0,0.15184,"rks to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We fi"
N15-1125,P03-1021,0,0.0135287,"ur objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (sour"
N15-1125,P02-1040,0,0.0937469,"so called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set;"
N15-1125,N09-2056,0,0.0204827,"hen combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related fi"
N15-1125,P14-2125,0,0.0121014,"s had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and ex"
N15-1125,E12-1055,0,0.131834,"ture work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider Eng"
N15-1125,N07-1061,0,0.736099,"he parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual parallel corpora of small sizes f"
N15-1125,P07-1108,0,0.621386,"ted to the size of the parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual paral"
N15-1125,P09-1018,0,0.362193,"airs being acquired that impact translation quality. Section 2 contains the related work. Section 3 begins with a basic description about the languages involved, followed by the corpora details and the experimental methodology. Section 4 consists of results, observations and discussions. The paper ends with conclusions and future work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots ha"
N15-1125,2009.mtsummit-papers.7,0,\N,Missing
N15-1132,P14-2131,0,0.0146427,"model. The CBOW model predicts the current word based on the surrounding context, whereas, the Skip-gram model tries to maximize the probability of a word based on other words in the same sentence (Mikolov et al., 2013). Word Embeddings for English We have used publicly available pre-trained word embeddings for English which were trained on Google News dataset2 (about 100 billion words). These word embeddings are available for around 3 million words and phrases. Each of these word embeddings have 300-dimensions. Word Embeddings for Hindi Word embeddings for Hindi have been trained on Bojar’s (2014) corpus. This corpus contains 44 million sentences. Here, the Skip-gram model is used for obtaining word embeddings. The dimensions are set as 200 and the window size as 7 (i.e. w = 7). We used the test of similarity to establish the correctness of these word embeddings. We observed that given a word and its embedding, the list of words ranked by similarity score had at the top of the list those words which were actually similar to the given word. 2.2 2 where, N is the number of words present in the sense-bag SB(wSi ) and SB(wSi ) is the sense-bag for the sense wSi which is given as, SB(wSi )"
N15-1132,D14-1110,0,0.0751396,"model. The CBOW model predicts the current word based on the surrounding context, whereas, the Skip-gram model tries to maximize the probability of a word based on other words in the same sentence (Mikolov et al., 2013). Word Embeddings for English We have used publicly available pre-trained word embeddings for English which were trained on Google News dataset2 (about 100 billion words). These word embeddings are available for around 3 million words and phrases. Each of these word embeddings have 300-dimensions. Word Embeddings for Hindi Word embeddings for Hindi have been trained on Bojar’s (2014) corpus. This corpus contains 44 million sentences. Here, the Skip-gram model is used for obtaining word embeddings. The dimensions are set as 200 and the window size as 7 (i.e. w = 7). We used the test of similarity to establish the correctness of these word embeddings. We observed that given a word and its embedding, the list of words ranked by similarity score had at the top of the list those words which were actually similar to the given word. 2.2 2 where, N is the number of words present in the sense-bag SB(wSi ) and SB(wSi ) is the sense-bag for the sense wSi which is given as, SB(wSi )"
N15-1132,J07-4005,0,0.00976281,"omain specific word embeddings, we simply have to run the word2vec program on the domain specific corpus. The domain specific word embeddings can be used to get the MFS for the domain of interest. Our approach is language independent. However, due to time and space constraints we have performed our experiments on only Hindi and English languages. 5 English We achieve good performance for English WSD on the SENSEVAL-2 dataset, whereas the performance on the SENSEVAL-3 dataset is comparatively poor. Here also, synset members alone perform badly. However, adding gloss members im1241 Related Work McCarthy et al. (2007) proposed an unsupervised approach for finding the predominant sense using an automatic thesaurus. They used WordNet similarity for identifying the predominant sense. Their approach outperforms the SemCor baseline for words Figure 2: UMFS-WE accuracy on Hindi WSD for words with various frequency thresholds in Newspaper dataset with SemCor frequency below five. Buitelaar et al. (2001) presented the knowledge based approach for ranking GermaNet synsets on specific domains. Lapata et al. (2004) worked on detecting the predominant sense of verbs where verb senses are taken from the Levin classes."
N15-1132,J04-1003,0,0.276678,"Missing"
N15-1132,bojar-etal-2014-hindencorp,0,\N,Missing
N15-3017,E14-4029,0,0.0274247,"is described by Antony, P. J. and Soman, K.P. (2011). 3.1 Transliteration Mining Statistical transliteration can address these challenges by learning transliteration divergences from a parallel transliteration corpus. For most Indian language pairs, parallel transliteration corpora are not publicly available. Hence, we mine transliteration corpora for 110 language pairs from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). Transliteration pairs are mined using the unsupervised approach proposed by Sajjad et al. (2012) and implemented in the Moses SMT system (Durrani et al., 2014). Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora. Table 1 shows the statistics of mined pairs. We mined a total of 1.69 million word pairs for 110 language pairs. We observed disparity in the counts of mined transliteration pairs across languages. Language pairs of the Indo-Aryan family from geographically contiguous regions h"
N15-3017,kunchukuttan-etal-2014-shata,1,0.843383,"in IndoWordNet3 Though this corpus does not reflect the diversity in the mined transliterations, evaluation on this corpus could be a pointer to utility of the transliteration corpus. We compare the accuracy of match for transliteration 3 http://www.cfilt.iitb.ac.in/indowordnet 84 Our work in developing the transliteration systems was initially motivated by the need for transliterating the untranslated words in SMT output. To evaluate the transliteration systems in the context of machine translation, we post-edited the phrase based system (PB-SMT) outputs of Indian language pairs provided by Kunchukuttan et al. (2014) using our transliteration systems. Each untranslated word was replaced by each of its top-1000 transliterations and the resulting candidate sentences were re-ranked using a language model. We observe a significant improvement in translation quality across language pairs, as measured by the BLEU evaluation metric. Due to space constraints, we present results for only 8 language pairs in Table 3. We observed that though the system&apos;s best transliteration is not always correct, the sentence context and the language model select the right transliteration from the top-k transliteration Lang Pair PB"
N15-3017,P12-1049,0,0.112805,"11). A summary of the challenges specific to Indian languages is described by Antony, P. J. and Soman, K.P. (2011). 3.1 Transliteration Mining Statistical transliteration can address these challenges by learning transliteration divergences from a parallel transliteration corpus. For most Indian language pairs, parallel transliteration corpora are not publicly available. Hence, we mine transliteration corpora for 110 language pairs from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). Transliteration pairs are mined using the unsupervised approach proposed by Sajjad et al. (2012) and implemented in the Moses SMT system (Durrani et al., 2014). Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora. Table 1 shows the statistics of mined pairs. We mined a total of 1.69 million word pairs for 110 language pairs. We observed disparity in the counts of mined transliteration pairs across languages. Language pairs of"
N15-3017,jha-2010-tdil,0,\N,Missing
N16-4006,P06-2112,0,\N,Missing
N16-4006,P07-1108,0,\N,Missing
N16-4006,W07-0705,0,\N,Missing
N16-4006,P15-2021,0,\N,Missing
N18-1053,C16-1047,1,0.712217,"of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work, we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM)"
N18-1053,P97-1023,0,0.380911,"tation of one word (जाता है |jAtA hai) is missing in Hindi embedding we can still find its representation in English through its translation i.e. ‘goes’. Bilingual embedding also helps in addressing the spelling variation cases. For e.g. two differently spelled words in Hindi such as ‘कि बनेशन |kambineshana’ and ‘कंबीनेशन |kaMbIneshana’ translate to an English word ‘combination’. We repeat the above process for English-French language pair to obtain two (English and French) word2vec models. We also released computed bilingual word embeddings for the research commu3. Semantic Orientation (SO) (Hatzivassiloglou and McKeown, 1997): Semantic orientation defines the association of a word w.r.t. its positivity and negativity. Semantic orientation (SO) of a word is the difference of point-wise mutual information of a word w in positive and negative reviews. We calculate the SO score of each word in the context window of size ±5 and take the cumulative SO score as the feature value. 3.3 Cross-lingual and Multi-lingual Setups We evaluate our proposed approach for two setups i.e. multi-lingual and cross-lingual setups. In multi-lingual setup, the proposed model is trained and evaluated on datasets of the same language i.e. Hi"
N18-1053,baccianella-etal-2010-sentiwordnet,0,0.0354141,"Missing"
N18-1053,bakliwal-etal-2012-hindi,0,0.0120045,"econd problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation"
N18-1053,W16-4307,0,0.0151257,"h as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has s"
N18-1053,C12-2008,1,0.91976,"lassification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The"
N18-1053,C04-1200,0,0.319706,"i.e. conflict) of the sentence ignoring critical information such as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-t"
N18-1053,C16-1152,0,0.113331,"hammad et al., 2016). 2. Approach: System (Akhtar et al., 2016a) defines classical feature driven approach while the system (Barnes et al., 2016) utilized bilingual word embeddings as feature values to train a Support Vector Machine (SVM) classifier. Rest of the systems (Akhtar et al., 2016b; Singhal and Bhattacharyya, 2016) (including the proposed one) are based on deep neural network architecture. However, the techniques employed are very much different. Akhtar et al. (2016b) is a CNN-SVM based system with the assistance of multi-objective optimized features, while Singhal and Bhattacharyya (2016) is a CNN based system that translate the source language texts into target language text (English) for training and evaluation. In comparison, our proposed method employ LSTM to solve the data sparsity problem in both multi-lingual as well as crosslingual setups. 6. Hand-crafted Features: The proposed system employs much richer set of lexicon based features than that of (Singhal and Bhattacharyya, 2016). Also, we do not augment polar words in the training instances as done in (Singhal and Bhattacharyya, 2016), rather we use sentiment scores of these lexicons as features themselves in the trai"
N18-1053,P07-2045,0,0.00578476,"ar et al., 2016b) employed mono-lingual word embeddings for training and evaluation. 3.1 Bilingual Word Embedding We employ bilingual word embeddings (Luong et al., 2015) trained on a parallel English-Hindi (and English-French) corpus. We generate a parallel corpus for Amazon product review datasets2 (consisting of approx. 7.2M sentences) using an in-house product review domain based English→Hindi (English→French) Statistical Machine Translation (SMT) system (English→Hindi: 39.5 BLEU score and English→French: 37.9 BLEU score). We employ widely used and standard machine translation tool Moses (Koehn et al., 2007) to train the phrasebased SMT system. The alignment information are obtained from the mosesdecoder (Koehn et al., 2007) during translation of the reviews. The parallel corpus along with the alignment information are used to train two (English and Hindi) 5. Data Sparsity: The system of (Akhtar et al., 2016b) does not address the problem of data sparsity, while our proposed system tries to minimize the effect of data sparsity. Our proposed system tackles the data sparsity problem by replacing the OOV word with its translated form which usually happens to be its closest neighbor in the shared vec"
N18-1053,W15-1521,0,0.0713966,"Missing"
N18-1053,P02-1053,0,0.0262187,"ll sentiment (i.e. conflict) of the sentence ignoring critical information such as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more c"
N18-1053,P05-1015,0,0.171748,"ource-poor scenario due to the absence of sufficient amount of corpus. In this work, we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the handcrafted features for the prediction. We show the efficacy of the proposed model against stateof-the-art methods in two experimental setups i.e. multi-lingual and cross-lingual. 1 Introduction Sentiment analysis (Pang and Lee, 2005) tries to automatically extract the subjective information from a user written textual content and classifies it into one of the predefined set of classes, e.g. positive, negative, neutral or conflict. Sentiment analysis performed on coarser level (i.e. document or sentence level) does not provide enough information for a user who is critical of finer details such as battery life of a laptop or service of a restaurant etc. Aspect level sentiment analysis (ABSA) (Pontiki et al., 2014) serves such a purpose, which first identifies the features (or aspects) mentioned in the text and then classifi"
N18-1053,P06-1134,0,0.145071,"Missing"
N18-1053,D14-1162,0,0.0853515,"Missing"
N18-1053,P16-1133,0,0.0362521,"Missing"
N18-1053,S14-2004,0,0.377995,"e-art methods in two experimental setups i.e. multi-lingual and cross-lingual. 1 Introduction Sentiment analysis (Pang and Lee, 2005) tries to automatically extract the subjective information from a user written textual content and classifies it into one of the predefined set of classes, e.g. positive, negative, neutral or conflict. Sentiment analysis performed on coarser level (i.e. document or sentence level) does not provide enough information for a user who is critical of finer details such as battery life of a laptop or service of a restaurant etc. Aspect level sentiment analysis (ABSA) (Pontiki et al., 2014) serves such a purpose, which first identifies the features (or aspects) mentioned in the text and then classifies it into one of the target classes. For example, the following review is for a restaurant where the writer shares her/his experience. Though s/he likes the food but certainly not happy with the service. One of the best food we had in a while but the service was very disappointing. 2 Motivation and Problem Definition Indian languages are resource-constrained in nature as there is a lack of ready availability of different qualitative lexical resources and tools. In a supervised machi"
N18-1053,C16-1287,1,0.919113,"survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in re"
N18-1061,P11-2008,0,0.0727691,"Missing"
N18-1061,P06-4018,0,0.0411422,"tional LSTMs (Schuster and Paliwal, 1997) train two LSTMs, instead of one, on the input sequence. The first on the input sequence and the second on a reversed copy of the input sequence. It is designed to capture information of the sequential dataset and maintain the contextual features from the past and the future. This can provide an additional context to the network and result in faster and even fuller learning on the problem without keeping the redundant context information. 3.2 Sentiment View of Temporal Orientation We use an existing sentiment classifier available with the NLTK toolkit (Bird, 2006) to classify the user-level tweets into positive, negative or neutral.3 Sentiment is added at the fine-grained level of the temporal orientation. Given the tweets of a user, the sentiment view of temporal orientation of that user is defined by the following equation: orientations,t (user) = |tweetss/t (user)| (1) |tweetst (user)| where, (t ∈ { past, present, or future}), and (s ∈ { positive, negative, or neutral}), in equation (1). Here, we first classify each user’s tweet into the past, present or future temporal category. Then for each temporal category, we find the percentage of each sentim"
N18-1061,chang-manning-2012-sutime,0,0.0350988,"Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in the texts. Although interesting results have been reported (UzZaman et al., 2013), but the coverage is limited to the finite number of rules they implement. The time perspective and its importance in various social science and psychological studies is well established in literature. It plays a fundamental role in our interpersonal relation influenced by cognitive process (Zimbardo and Boyd, 2015). • We introduce the sentiment dimensions in the human temporal orientation to infer the social media users’ psycho-demographic attributes on a large-scale."
N18-1061,D14-1121,0,0.0818986,"Missing"
N18-1061,P14-5010,0,0.00412872,"ucation, intelligence, optimism, and relationship using a linear regression (LR) classifier (Neter et al., 1996). 4 the filtering method, we filter out the tweets which do not contain a verb. The verb part-of-speech tag is determined using the CMU tweet-tagger (Gimpel et al., 2011). In the second pass of the filtering method, we removed the tweets having tense as past from the tweets of the present and future events. The CMU tweet-tagger does not provide verbs in different sub-categories. For this reason, we also retrieve the Part-of-Speech (PoS) tag information from the Standford PoS-tagger (Manning et al., 2014) for all the tweets to get the subcategories of verb (i.e. VB, VBD, VBG, VBN, VBP, VBZ). We observed that although Standford PoS-tagger assigned the required verb subcategories, it also incorrectly tagged some nonverbs as verbs. This is the reason why we considered only those verbs for sub-categorization which were identified (as verbs) by the CMU tweettagger. We varied the training set starting from 3K (equally distributed) to 30K and observed that the accuracy on the gold standard test set did not improve after 27K training instances. Few example tweets with the trending topics are depicted"
N18-1061,N15-1044,0,0.361498,"e current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware information access systems. In parallel, new trends have emerged in the context of the human temporal orientation (Schwartz et al., 2013; Sap et al., 2014; Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and"
N18-1061,D14-1162,0,0.0809289,"Missing"
N18-1061,D15-1063,0,0.0446812,"Missing"
N18-1061,S13-2001,0,0.0975751,"ibutes, namely age, eduction, relationship, intelligence, and optimism. Our contributions are summarised as below: • We define a way to find a novel association between the sentiment view of temporal orientation and the different psychodemographic factors of the tweet users. 2 Related Background The temporal study has recently received an increased attention in several application domains of Natural Language Processing (NLP) and Information Retrieval (IR). The introduction of the TempEval task (Verhagen et al., 2009) and the subsequent challenges i.e. TempEval-2 and -3 (Verhagen et al., 2010; UzZaman et al., 2013) in the Semantic Evaluation workshop series have clearly established the importance of time in dealing with the different NLP tasks. Alonso et al. (2011) reviewed the current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware informatio"
N18-2044,R13-1003,0,0.0169051,"online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific feature space, lea"
N18-2044,P17-1001,0,0.0828406,"Missing"
N18-2044,W14-5907,0,0.0288789,"timent analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific feature space, leading to feature redundancy. Adversarial learning (Goodfellow et al., 2014) is the process of learning a model to correctly classify both unmodified data and adversarial data through the regularization m"
N18-2044,W16-0312,0,0.0781823,"ects (medical condition and medication). The texts in bold indicates the sentiment word. uments (nurse letters, radiology reports, and discharge summaries). They also studied users self reported drug reviews on blogs (WebMD, DrugRating) to asses the possible medical sentiments. Majority of the current research in medical sentiment analysis are focused on understanding the mental health disorder, mainly depression. Several shared tasks (Losada et al., 2017; Hollingshead et al., 2017) have also been organized to study the patient health-related opinions on social media. The challenge defined in Milne et al. (2016) aims to automatically classify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies"
N18-2044,W16-0303,0,0.0193889,"ogs (WebMD, DrugRating) to asses the possible medical sentiments. Majority of the current research in medical sentiment analysis are focused on understanding the mental health disorder, mainly depression. Several shared tasks (Losada et al., 2017; Hollingshead et al., 2017) have also been organized to study the patient health-related opinions on social media. The challenge defined in Milne et al. (2016) aims to automatically classify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical se"
N18-2044,R11-1019,0,0.0205476,"sify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific"
N18-2044,R13-1082,0,0.0138959,"Deng (2015) provides the quantitative assessment of sentiment across the clinical narrative and social media sources. Towards this, they created a domain-specific corpus from MIMIC II database containing clinical doc272 Figure 2: Architecture of proposed methodology 3.1 bigram, trigram, parts of speech. Also, there has been predominant use of general sentiment lexicon, however their analysis shows that it does not help in capturing the medical sentiment. More domain specific knowledge is also embedded using medical knowledge graph such as UMLS to identify the medical condition and treatment (Sokolova et al., 2013). 3 Let us assume that a blog-text P having k sentences and word sequence w = {w1 , w2 , . . . wl } be given. The embedding layer is used to find out the vector representation xi ∈ Rd×V from a d dimensional pre-trained word embedding of vocabulary V . Each word wi ∈ w will be represented by its respective word embedding xi . The hidden units hl learned at the last time step (l) of sequence are considered as the encoding of the medical blog, P . The representations hl generated from the Eq 1 are fed to a fully connected softmax layer to generate the probability distribution over the given class"
N19-1034,D18-1382,1,0.377607,"Missing"
N19-1034,P17-1142,0,0.0269576,"Missing"
N19-1034,D17-1115,1,0.922987,"Missing"
N19-1034,P05-1015,0,0.450583,"Missing"
N19-1034,D14-1162,0,0.0825707,"rning (STL) and Multi-task (MTL) learning frameworks for the proposed approach. T: Text, V: Visual, A: Acoustic. Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works (Zadeh et al., 2018c). 5.2 Parameters Bi-GRU Dense layer Activations Optimizer Output Feature extraction We use the CMU-Multi-modal Data SDK1 for downloading and feature extraction. The dataset was pre-tokenized and a feature vector was provided for each word in an utterance. The textual, visual and acoustic features were extracted by GloVe (Pennington et al., 2014), Facets2 & CovaRep (Degottex et al., 2014), respectively. Thereafter, we compute the average of word-level features to obtain the utterance-level features. 5.3 Loss Threshold Batch Epochs Values 2×200 neurons, dropout=0.3 100 neurons, dropout=0.3 ReLu Adam (lr=0.001) Softmax (Sent) & Sigmoid (Emo) Categorical cross-entropy (Sent) Binary cross-entropy (Emo) 0.4 (F1) & 0.2 (W-Acc) for multi-label 16 50 Table 5: Model configurations Experiments We evaluate our proposed approach on the datasets of CMU-MOSEI. We use the Python based Keras library for the implementation. We compute F1score and accu"
N19-1034,P18-1208,1,0.904332,"Missing"
N19-1091,N18-2008,0,0.188843,"ent to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer ha"
N19-1091,D15-1075,0,0.0358108,"Missing"
N19-1091,D16-1127,0,0.0648497,"er is structured as follows: In section 2, we discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work dif"
N19-1091,D17-1070,0,0.0416797,"Missing"
N19-1091,D17-1230,0,0.0346807,"m these previous works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long"
N19-1091,N18-1169,0,0.058027,"encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific application of customer care. 3 rd = [ed · h1N d ] The second hierarchical layer Bi-LSTM encodes the utterance representations r1 , r2 , . . . , rD as hidden states h21 , h22 , . . . , h2D . The last hidden state h2D is the representativ"
N19-1091,D17-1090,0,0.0194825,"works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogue generation in a wa"
N19-1091,W04-1013,0,0.0169882,"Missing"
N19-1091,D15-1166,0,0.0166178,"tences which do not contain any information/ suggestions, and are purely non-informative. These may include personalized greetings and expression of appreciation, apology, empathy, assurance, or enthusiasm. Example: Sorry to hear about the trouble! (ii) Informative sentences without courteous expressions: These sentences contain the actual content of the tweet and are generally assertions, instructions, imperatives or suggestions. Example: Simply visit url name to see availability in that area! (13) Baselines We develop the following models: 1. Model-1: This is a Seq2Seq model with attention (Luong et al., 2015) and decoder conditioned on the conversational context vector c (without concatenating emotional embedding i.e. instead of Eq. 2, rd = h1N d ) 2. Model-2: This model is developed using Model-1 along with the copying mechanism of Pointer Generator Network. 3. Model-3: This model is developed using Model-2 along with emotional embeddings in the conversational context vector as in E.g., 2. # Conversation # Utterances Train 140203 179034 Valid 20032 25642 Test 40065 51238 Table 2: Dataset Statistics 4 Dataset In this section we describe the details of the dataset that we create for our experiments"
N19-1091,D17-1169,0,0.064795,". This attention distribution helps to identify the relevant encoder states necessary for the current decoding step. The representation of the encoder for this time step is an attention weighted sum of its states, called the context vector: X h∗t = αit hi (5) Conversational History Representation The conversation history C is a sequence of utterances (u1 , u2 , . . . , uD ) and each utterance ud is a sequence of words w1 , w2 , . . . , wN which are represented by their embeddings. For encoding the emotional states associated with these utterances, we use the output distribution from DeepMoji (Felbo et al., 2017) which is pre-trained on the emoji prediction task. Let the utterance ud be a sequence of sentences s1 , s2 , . . . , sN , where the nth sentence has an emotional embedding en,d . Then the emotional representation of the utterance is: n Decoder states and Attention calculation At the decoder time step t, the decoder LSTM state st is used to calculate the attention distribution over the encoder states αt : Methodology ed [i] = max en,d [i] Encoder states Another single layer unidirectional LSTM network encodes the generic response word embedding sequence to obtain the encoder hidden states hi ."
N19-1091,Q18-1027,0,0.0543472,"sappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific application of customer care. 3 rd = [ed · h1N d ] The second hierarchical layer Bi-LSTM encodes the utterance representations r1 , r2 , . . . , rD as hidden states h21 , h22 , . . . , h2D . The last hidden state h2D is the representative of the conversational"
N19-1091,P02-1040,0,0.104526,"Missing"
N19-1091,W16-3609,0,0.126114,"he dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specif"
N19-1091,N18-1012,0,0.0253443,"hile generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific ap"
N19-1091,W17-3541,0,0.0184401,"ogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In"
N19-1091,E17-1036,0,0.0248809,"discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogu"
N19-1091,P17-1061,0,0.117345,"Missing"
N19-1091,P18-1104,0,0.0807128,"tion. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the de"
N19-1091,P17-1099,0,0.0292979,"t vector c. st = LST M (st−1 , Wp [wemb (yt−1 ), h∗t−1 , c] + ˜b) (6) where, Wp and ˜b are the trainable parameters. (1) 3.4 The first bi-directional layer over any utterance ud yields the hidden states h11d , h12d , . . . , h1N d , where N is the word length of the utterance. The final representation of any utterance rd is given by the concatenation of the emotional representation as well as the last hidden state of the Bi-directional Output distribution calculation To aid the copying of words from the generic response while generating the courteous response, we use the mechanism similar to (See et al., 2017). For the pointer generator network, the model computes two distributions, one over the 853 Figure 1: Architectural Diagram of the Proposed Model. Inputs to the model: Conversation History (left), Generic Response (centre) Output: Courteous Response (right). The Conversation History is encoded by hierarchical BiLSTM to a Conversational Context vector c. The encoder encodes the Generic Response into hidden states hi . Response tokens are decoded one at a time. Attention αi , and vocabulary distributions (pvocab ) are computed, and combined using pgen to produce output distribution. Sampling it"
N19-1091,W17-2629,0,0.0171487,"works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM"
N19-1091,N18-1186,0,0.156431,"pecific metrics, both automatic and human evaluation based. The rest of the paper is structured as follows: In section 2, we discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diver"
N19-1387,W14-5105,1,0.916554,"e addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language pair. The parallel corpus b"
N19-1387,P05-1066,0,0.216311,"any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of tran"
N19-1387,Y17-1038,0,0.0240468,"ow that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowle"
N19-1387,L18-1550,0,0.0214217,"etwork: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations). In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings (Xie et al., 2018). However, publicly available bilingual embeddings for English-Indian"
N19-1387,Q19-1007,1,0.861339,"the English and Hindi training corpus respectively. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations). In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings (Xie et al., 2018). However, publicly available bilingual embeddings for English-Indian languages are not good enough for obtaining good-quality, bilingual representations (Smith et al., 2017; Jawanpuria et al., 2019) and publicly available bilingual dictionaries have limited coverage. The focus of our study is the in1 The corpus is available on request from http:// tdil-dc.in/index.php?lang=en 2 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors.md Language BLEU No Pre-Order Bengali Gujarati Marathi Malayalam Tamil 6.72 9.81 8.77 5.73 4.86 LeBLEU Pre-Ordered HT G 8.83 14.34 10.18 6.49 6.04 9.19 13.90 10.30 6.95 6.00 No Pre-Order 37.10 43.21 40.21 33.27 29.38 Pre-Ordered HT 41.50 47.36 41.49 33.69 30.77 G 42.01 47.60 42.22 35.09 31.33 Table 2: Transfer learning results for X -Hind"
N19-1387,jha-2010-tdil,0,0.0821231,", Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a"
N19-1387,K17-1024,0,0.0223941,"eedings of NAACL-HLT 2019, pages 3868–3873 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics for multilingual NMT. However, some work exists for other NLP tasks in a multilingual setting. For Named Entity Recognition (NER), Xie et al. (2018) use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition (NER) task. The approach does not show any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. P"
N19-1387,P18-3004,0,0.0468978,"Missing"
N19-1387,W18-1817,0,0.0252691,"uages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabul"
N19-1387,W04-3250,0,0.381732,"Missing"
N19-1387,L18-1548,1,0.765156,"h, datasets used, the network hyperparameters used in our experiments. Languages: We experimented with English → Hindi translation as the parent task. English is 3869 the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong"
N19-1387,kunchukuttan-etal-2014-shata,1,0.903943,"Missing"
N19-1387,Q17-1026,0,0.0287173,"anguages are related (Zoph et al., 2016; Nguyen and Chiang, 2017; Dabre et al., 2017). Zoph et al. (2016) studied the influence of language divergence between languages chosen for training the parent and the child model, and showed that choosing similar languages for training the parent and the child model leads to better improvements from transfer learning. Several studies have tried to address the lexical divergence between the source and the target languages either by using Byte Pair Encoding (BPE) as basic input representation units (Nguyen and Chiang, 2017) or character-level NMT system (Lee et al., 2017) or bilingual embeddings (Gu et al., 2018). However, the effect of word order divergence and its mitigation has not been explored. In a practical setting, it is not uncommon to have source and assisting languages with different word order. For instance, it is possible to find parallel corpora between English (SVO word order) and some Indian (SOV word order) languages, but very little parallel corpora between Indian languages. Hence, it is natural to use English as an assisting language for inter-Indian language translation. To address the word order divergence, we propose to pre-order the assi"
N19-1387,W13-2807,0,0.0175236,"der divergence on Multilingual NMT. We do not want bilingual embeddings quality or bilingual dictionary coverage to influence the experiments, rendering our conclusions unreliable. Hence, we use the above mentioned largecoverage bilingual dictionary. Pre-ordering: We use CFILT-preorder3 for prereordering English sentences. It contains two preordering configurations: (1) generic rules (G) that apply to all Indian languages (Ramanathan et al., 2008), and (2) hindi-tuned rules (HT) which improves generic rules by incorporating improvements found through error analysis of EnglishHindi reordering (Patel et al., 2013). The Hindituned rules improve translation for other English to Indian language pairs too (Kunchukuttan et al., 2014). 5 Results We experiment with two scenarios: (a) an extremely resource scarce scenario with no parallel corpus for child tasks, (b) varying amounts of parallel corpora available for child task. 5.1 No Parallel Corpus for Child Task The results from our experiments are presented in the Table 2. We report BLEU scores and LeBLEU4 3 https://github.com/anoopkunchukuttan/ cfilt_preorder 4 LeBLEU (Levenshtein Edit BLEU) is a variant of BLEU that does a soft-match of reference and outp"
N19-1387,P18-1142,0,0.0194136,"ion of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language pair. The parallel corpus between the two languages, if a"
N19-1387,I08-1067,1,0.878884,"ements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely"
N19-1387,W15-3052,0,0.0141808,"ranslation, which is also reflected in the BLEU scores and Table 4. 5.2 Parallel Corpus for Child Task We study the impact of child task parallel corpus on pre-ordering. To this end, we finetune the parent task model with the child task parallel corpus. Table 5 shows the results for Bengali-Hindi, Gujarati-Hindi, Marathi-Hindi, Malayalam-Hindi, and Tamil-Hindi translation. We observe that pre-ordering is beneficial when almost no child task corpus is available. As the child task corpus increases, the model learns the on edit distance, hence it can handle morphological variations and cognates (Virpioja and Grönroos, 2015). word order of the source language; hence, the non pre-ordering models perform almost as good as or sometimes better than the pre-ordered ones. The non pre-ordering model is able to forget the wordorder of English and learn the word order of Indian languages. We attribute this behavior of the non pre-ordered model to the phenomenon of catastrophic forgetting (McCloskey and Cohen, 1989; French, 1999) which enables the model to learn the word-order of the source language when sufficient child task parallel corpus is available. We also compare the performance of the finetuned model with the mode"
N19-1387,D18-1034,0,0.09813,"trained scenario, where there is no parallel corpus for the child task. From our experiments, we show that there is a significant increase in the translation accuracy for the unseen source-target language pair. 2 Related Work To the best of our knowledge, no work has addressed word order divergence in transfer learning 3868 Proceedings of NAACL-HLT 2019, pages 3868–3873 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics for multilingual NMT. However, some work exists for other NLP tasks in a multilingual setting. For Named Entity Recognition (NER), Xie et al. (2018) use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition (NER) task. The approach does not show any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-orderi"
N19-1387,D16-1163,0,0.213976,"rce language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by"
N19-1387,D15-1166,0,0.03224,"2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively. For representing English and o"
N19-1387,C12-1125,0,0.0179425,"the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language p"
N19-1387,I17-2050,0,0.222385,"rd order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowledge from the parent task,"
N19-2017,P14-5010,0,0.0026459,"demarcating lines separating the normal and alternate set of messages. In the example (Table 1), based on the condition of the station at which the vehicle has arrived, the step mentioned as alteranate flow or the steps 6 to 10 of the normal flow need to be followed. This branching is depicted using the altFigure 1: MSC corresponding to the use-case description in Table 1 In this paper, we describe our approach to extract MSCs from use-cases based on Open Information Extraction (OpenIE) (Mausam, 2016). OpenIE extracts structured information from a 1 In this paper, we use the Stanford CoreNLP (Manning et al., 2014) pipeline for dependency parsing. 131 Processing input using OpenIE: Use-case descriptions generally use well-written English. Over such text, generation of candidate messages requires a simple relation and argument extractor. We thus propose use of the OpenIE framework which provides tuples of the form (left argument, relation, right argument 1, right argument 2, . . .) along with confidence scores with each tuple. We first process the input use-cases through the OpenIE technique described in (Mausam et al., 2012; Mausam, 2016) and obtain a list of candidate messages. box shown in Figure 1. T"
N19-2017,W17-5912,1,0.792108,"ith multiple verbs, conjunctions and passive voice. Kof (2007a) also proposes a set of heuristics to handle missing sender or receiver of a message. A key limitation of this method is that it ignores syntactic as well as semantic relations between the verb and its corresponding sender or receiver. Hence it is possible that some of the <sender, message label, receiver> tuples may not be valid semantically. It is important to note that Kof (2007a; 2007b) do not focus on other features of MSC such as conditions, timers and altboxes. 3 Defining Actors in Software Engineering UseCases: In general (Bedi et al., 2017; Patil et al., 2018; Palshikar et al., 2019), the notion of actors in MSC is based on named entities of the types - PERSON, LOCATION and ORGANIZATION. However, in the software requirements domain, these may not be the only entities interacting with each other. This criteria is extended and is proposed to include: • PERSONS (Human SYSTEM users), ORGANIZATIONS and LOCATIONS • SYSTEMS (such as Supervisory System, Library System) • Persistent components of SYSTEMS (such as servers, databases, customer accounts) • Persistent processes in the SYSTEM (such as schedulers, daemons) • SYSTEM components"
N19-2017,D12-1048,0,0.0223572,"ts structured information from a 1 In this paper, we use the Stanford CoreNLP (Manning et al., 2014) pipeline for dependency parsing. 131 Processing input using OpenIE: Use-case descriptions generally use well-written English. Over such text, generation of candidate messages requires a simple relation and argument extractor. We thus propose use of the OpenIE framework which provides tuples of the form (left argument, relation, right argument 1, right argument 2, . . .) along with confidence scores with each tuple. We first process the input use-cases through the OpenIE technique described in (Mausam et al., 2012; Mausam, 2016) and obtain a list of candidate messages. box shown in Figure 1. The paper is organized as follows. Section 2 covers the related work; Section 3 describes the MSC extraction approach; Section 4 describes the experiments and discusses one of the use-cases in detail; Section 5 concludes the paper. 2 Related Work Feijs (2000) studies the relationship between natural language use cases and message sequence charts. He uses context-free generative grammars for natural language description of use-cases that use object-oriented system development methods. However, this study focuses on"
N19-2017,P18-2011,1,0.840609,"conjunctions and passive voice. Kof (2007a) also proposes a set of heuristics to handle missing sender or receiver of a message. A key limitation of this method is that it ignores syntactic as well as semantic relations between the verb and its corresponding sender or receiver. Hence it is possible that some of the <sender, message label, receiver> tuples may not be valid semantically. It is important to note that Kof (2007a; 2007b) do not focus on other features of MSC such as conditions, timers and altboxes. 3 Defining Actors in Software Engineering UseCases: In general (Bedi et al., 2017; Patil et al., 2018; Palshikar et al., 2019), the notion of actors in MSC is based on named entities of the types - PERSON, LOCATION and ORGANIZATION. However, in the software requirements domain, these may not be the only entities interacting with each other. This criteria is extended and is proposed to include: • PERSONS (Human SYSTEM users), ORGANIZATIONS and LOCATIONS • SYSTEMS (such as Supervisory System, Library System) • Persistent components of SYSTEMS (such as servers, databases, customer accounts) • Persistent processes in the SYSTEM (such as schedulers, daemons) • SYSTEM components (GUI elements like"
P06-2100,A92-1018,0,0.891061,"Missing"
P06-2100,J95-4004,0,0.114244,"Missing"
P06-2100,H92-1023,0,0.811288,"Missing"
P06-2100,C94-1027,0,0.0553839,"Missing"
P06-2100,P97-1032,0,0.0153778,"foreign words, derivationally morphed words, spelling variations and other unknown words) (Manning and Schutze, 2002). For English there are many POS taggers, employing machine learning techniques In this scenario, POS tagging of highly inflectional languages presents an interesting case study. Morphologically rich languages are characterized by a large number of morphemes in a single word, where morpheme boundaries are difficult to detect because they are fused together. They are typically free-word ordered, which causes fixed-context systems to be hardly adequate for statistical approaches (Samuelsson and Voutilainen, 1997). Morphology-based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Arabic (Guiassa, 2006), Czech (Hajic et al., 2001), Modern Greek (Orphanos et al., 1999) and Hungarian (Megyesi, 1999) has been tried out using a combination of hand-crafted rules and statistical learning. These systems use large amount of corpora along with morphological analysis to POS tag the texts. It may be noted that a purely rule-based or a purely stochastic approach will not be effective for such 779 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 779–786, c Sydney, July"
P06-2100,W01-0512,0,\N,Missing
P06-2100,W99-0633,0,\N,Missing
P06-2100,A94-1024,0,\N,Missing
P06-2100,P01-1035,0,\N,Missing
P09-1090,2001.mtsummit-papers.68,0,0.0291183,"Missing"
P09-1090,D07-1077,0,0.0958184,"Missing"
P09-1090,P08-1087,0,0.288639,"Missing"
P09-1090,P05-1066,0,0.0682395,"Missing"
P09-1090,2005.mtsummit-papers.35,0,0.0687092,"Missing"
P09-1090,D07-1091,0,0.0166773,"Missing"
P09-1090,de-marneffe-etal-2006-generating,0,0.0102003,"Missing"
P09-1090,P04-1083,0,0.0262115,"Missing"
P09-1090,J04-2003,0,0.085172,"Missing"
P09-1090,P03-1021,0,0.00894476,"Missing"
P09-1090,P02-1040,0,\N,Missing
P09-1090,I08-1067,1,\N,Missing
P10-1137,J03-1002,0,0.00497444,"Missing"
P10-1137,P91-1017,0,0.45202,"Missing"
P10-1137,2005.mtsummit-papers.11,0,0.0609981,"Missing"
P10-1137,P07-2045,0,0.010463,"Missing"
P10-1137,D08-1078,0,\N,Missing
P10-1155,E09-1006,0,0.0747145,"Missing"
P10-1155,martinez-agirre-2004-effect,0,0.0554789,"Missing"
P10-1155,W09-2420,0,0.0692285,"Missing"
P10-1155,P07-1007,0,0.0603341,"arthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009). To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD. We thus propose the following: In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense"
P10-1155,W00-1322,0,0.0690684,"Missing"
P10-1155,O97-1002,0,0.0334337,"Missing"
P10-1155,P03-1054,0,0.00348854,"es by pairing the target word with its top-k neighbors in the thesaurus. Each target word is then disambiguated by assigning it its predominant sense – the motivation being that the predominant sense is a powerful, hard-to-beat baseline. We implemented their method using the following steps: i (1) where, i ∈ Candidate Synsets J = Set of disambiguated words θi = BelongingnessT oDominantConcept(Si) Vi = P (Si |word) 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (20"
P10-1155,H05-1053,0,0.0936557,"apted WSD: Finding a Middle Ground between Supervision and Unsupervision Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya Indian Institute of Technology Bombay, Mumbai - 400076, India. {miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in Abstract of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target wo"
P10-1155,P98-2127,0,0.0702236,"owerful, hard-to-beat baseline. We implemented their method using the following steps: i (1) where, i ∈ Candidate Synsets J = Set of disambiguated words θi = BelongingnessT oDominantConcept(Si) Vi = P (Si |word) 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank. We"
P10-1155,P04-1036,0,0.291314,"Missing"
P10-1155,J07-4005,0,0.141616,"iddle Ground between Supervision and Unsupervision Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya Indian Institute of Technology Bombay, Mumbai - 400076, India. {miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in Abstract of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007;"
P10-1155,H93-1061,0,0.358441,"g a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD. a. For any target domain, create a small amount of sense annotated corpus. b. Mix it with an existing sense annotated corpus – from a mixed domain or specific domain – to train the WSD engine. 1 Introduction Amongst annotation tasks, sense marking surely takes the cake, demanding as it does high level This procedure tested on four adaptation scenarios, viz., (i) SemCor (Miller et al., 1993) to Tourism, (ii) SemCor to Health, (iii) Tourism to Health and (iv) Health to Tourism has consistently yielded good performance (to be explained in sections 6 and 7). The remainder of this paper is organized as follows. In section 2 we discuss previous work in the area of domain adaptation for WSD. In section 3 1532 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics we discuss three state of art supervised, unsupervised and knowledge based algorithms for WS"
P10-1155,P96-1006,0,0.288581,"Missing"
P10-1155,W04-0811,0,0.0602208,"Missing"
P10-1155,C98-2122,0,\N,Missing
P11-1057,C96-1005,0,0.525473,"Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on"
P11-1057,D09-1048,1,0.691463,"its self corpus. This is very similar to the process of co-training (Blum and Mitchell, 1998) wherein the annotated data in the two languages can be seen as two different views of the same data. Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view. The remainder of this paper is organized as follows. In section 2 we present related work. Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection. Section 4 discusses the work of Khapra et al. (2009) on parameter projection. In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping. Section 6 describes the experimental setup. In section 7 we present the results followed 562 by discussion in section 8. Section 9 concludes the paper. 2 Related Work Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagg"
P11-1057,C10-1063,1,0.75332,"recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation work done in another language (L1 ). This is achieved with the help of a novel synsetaligned multilingual dictionary which facilitates the projection of parameters learned from the Wordnet and annotated corpus of L1 to L2 . This approach thus obviates the need for collecting large amounts of annotated corpora in multiple languages by relying on sufficient annotated corpus in one resource rich language. However, in many s"
P11-1057,W04-0834,0,0.165538,"and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1 ) and Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amou"
P11-1057,J04-1001,0,0.0239784,"al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived languages have aligned Wordnets but neither has sufficient annotated data. In such cases bilingual bootstrapping can be used so that the two languages can mutually benefit from each other’s small annotated data. Li and Li (2004) proposed a bilingual bootstrapping approach for the more specific task of Word Translation Disambiguation (WTD) as opposed to the more general task of WSD. This approach does not need parallel corpora (just like our approach) and relies only on in-domain corpora from two languages. However, their work was evaluated only on a handful of target words (9 nouns) for WTD as opposed to the broader task of WSD. Our work instead focuses on improving the performance of all words WSD for two resource deprived languages using bilingual bootstrapping. At the heart of our work lies parameter projection fa"
P11-1057,P04-1036,0,0.290564,"nguage pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging"
P11-1057,H05-1052,0,0.141499,"tter than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation"
P11-1057,P96-1006,0,0.529794,"tagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1 ) and Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated da"
P11-1057,P95-1026,0,0.515652,"notated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation work done in another language (L1 ). This is achieved with the help of a novel synsetaligned multilingual dictionary which facili"
P11-4022,esuli-sebastiani-2006-sentiwordnet,0,0.0393282,"content. This is on the basis of predictions by each resource by weighting them according to their accuracies. These weights have been assigned to each resource based on experimental results. For each resource, the following scores are determined. negscore[r] = objscore[r] = m X i=1 m X i=1 m X 3.4 Resources Sentiment-based lexical resources annotate words/concepts with polarity. The completeness of these resources individually remains a question. To achieve greater coverage, we use four different sentiment-based lexical resources for C-Feel-It. They are described as follows. 1. SentiWordNet (Esuli and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It Version 2 posscore[r] = We normalize these scores to get the final positive, negative and objective pertaining to search string r. These scores are represented in form of percentage. 2. Subjectivity lexicon (Wiebe et al., 2004) i"
P11-4022,P04-1035,0,0.0911454,"Missing"
P11-4022,P05-1015,0,0.0513541,"Missing"
P11-4022,J04-3002,0,0.0110993,"and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It Version 2 posscore[r] = We normalize these scores to get the final positive, negative and objective pertaining to search string r. These scores are represented in form of percentage. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prior polarity. 3. Inquirer (Stone et al., 1966) is a list of words marked as positive, negative and neutral. We use these labels to use Inquirer resource for our prediction. 4. Taboada (Taboada and Grieve, 2004) is a word-list that gives a count of collocations with positive and negative seed words. A word closer to a positive seed word is predicted to be posit"
P11-4022,W00-1308,0,\N,Missing
P13-1041,P11-2075,0,0.0969324,"ction 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. I"
P13-1041,D11-1100,1,0.878331,"d in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recogni"
P13-1041,C12-2008,1,0.61224,"Missing"
P13-1041,D08-1014,0,0.141217,"approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of"
P13-1041,P07-1056,0,0.376814,"Missing"
P13-1041,R09-1010,0,0.119334,"11), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3.2 Approach 2: Syntagmatic Property based Clustering For this particular study, a co-occurrence based algorithm is used to create word clusters. As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the natu"
P13-1041,J92-4003,0,0.0993672,"s Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster b"
P13-1041,P11-2125,1,0.874869,"erties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser"
P13-1041,C04-1071,0,0.551503,"n 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section"
P13-1041,I08-1039,0,0.0607824,"f classifiers based on different sense-based and word-based features were compared. The results suggested that WordNet synset based features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cro"
P13-1041,J03-1002,0,0.0026508,"r the word clusterings process. For Brown clustering, an implementation by Liang (2005) was used. Cross-lingual clustering for CLSA 2 http://sanskrit.jnu.ac.in/ilci/index. jsp 416 CLSA: The same datasets used in SA are also used for CLSA. Three approaches (as described in section 4) were tested for English-Hindi and English-Marathi language pairs. To create alignments, English-Hindi and English-Marathi parallel corpora from ILCI were used. EnglishHindi parallel corpus contains 45992 sentences and English-Marathi parallel corpus contains 47881 sentences. To create alignments, GIZA++5 was used (Och and Ney, 2003). As a preprocessing step, all stop words were removed. Stemming was performed on English and Hindi whereas for Marathi data, Morphological Analyzer was used to reduce the words to their respective lemmas. All experiments were performed using C-SVM 3 http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 4 http://www.cfilt.iitb.ac. in/resources/senti/MPLC_tour_ downloaderInfo.php 5 http://www-i6.informatik.rwth-aachen. de/Colleagues/och/software/GIZA++.html Features Words WordNet Sense (Paradigmatic) Clusters (Syntagmatic) En-TD 87.02 89.13 97.45 En-PD 77.60 74.50 87.80 Hi 77.36 85.80 83.50 z Mar"
P13-1041,P11-1033,0,0.0613431,"ntiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3.2 Approach 2: Syntagmatic Property based Clustering For this particular study, a co-occurrence based algorithm is used to create word clusters. As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the nature of underlying statistics. Agglomerative clustering algorithm by Br"
P13-1041,W02-1011,0,0.0162818,"cience and Engineering, IIT Bombay Australia {kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu Abstract Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additiona"
P13-1041,N04-1043,0,0.034703,"c processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper,"
P13-1041,P07-1017,0,0.0109515,"entiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. 1 Introduction Data sparsity is the bane of Natural Language Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, c Sofia, Bulgaria, August 4-9 2013. 2013 Associ"
P13-1041,W04-3253,0,0.543596,"performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. The contributions of this paper are two fold: 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1 Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 by using automatic/manual sense disambiguation techniques. Thereafter, accuracies of classifiers based on different sense-based and word-based features were c"
P13-1041,N10-1120,0,0.0630474,"utomatic/manual sense disambiguation techniques. Thereafter, accuracies of classifiers based on different sense-based and word-based features were compared. The results suggested that WordNet synset based features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations wh"
P13-1041,P06-2079,0,0.375498,"Missing"
P13-1041,R09-1067,0,0.0105564,"of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit"
P13-1041,R11-1020,0,0.0288582,"features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Bro"
P13-1041,W12-0704,0,0.018366,"010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible rea"
P13-1041,N12-1052,0,0.011642,"Missing"
P13-1041,P10-1040,0,0.0101778,"Missing"
P13-1041,P02-1053,0,0.00870777,"llel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. The contributions of this paper are two fold: 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1 Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two"
P13-1041,P08-1086,0,0.158288,"al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification."
P13-1041,P09-1027,0,0.414442,"ed work. Section 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are use"
P13-1041,P11-2033,0,0.00516662,"mpared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the mea"
P13-1041,D11-1116,0,\N,Missing
P13-1041,P08-1068,0,\N,Missing
P13-2048,C10-1052,0,0.0297775,"l concept and most of the synsets get linked through subsumption relation. This leads to a significant amount of information loss. KYOTO project used Lexical Markup Framework (LMF) (Francopoulo et al., 2009) as a representation language. ‘LMF provides a common model for the creation and use of lexical resources, to manage the exchange of data among these resources, and to enable the merging of a large number of individual electronic resources to form extensive global electronic resources’ (Francopoulo et al., 2009). Soria et al. (2009) proposed WordNet-LMF to represent wordnets in LMF format. Henrich and Hinrichs (2010) have further modified Wordnet-LMF to accommodate lexical Figure 1: An Example of Indonet Structure 2 http://kyoto-project.eu/xmlgroup.iit. cnr.it/kyoto/index.html 269 Figure 2: LMF representation for Universal Word Dictionary 3.1 Common Concept Hierarchy (CCH) The common concept hierarchy is an abstract pivot index to link lexical resources of all languages. An element of a common concept hierarchy is defined as < sinid1 , sinid2 , ..., uwid, sumoid > where, sinidi is synset id of ith wordnet, uw id is universal word id, and sumo id is SUMO term id of the concept. Unlike ILI, the hypernymy-hy"
P13-2062,carl-2012-translog,1,0.827281,"were strongly correlated, we would have rather opted “time taken to translate” for the measurement of TDI. The reason is that “time taken to translate” is relatively easy to compute and does not require expensive setup for conducting “eye-tracking” experiments. But our experiments show that there is a weak correlation (coefficient = 0.12) between “time taken to translate” and Tp . This makes us believe that Tp is still the best option for TDI measurement. Computing TDI using eye-tracking database We obtained TDIs for a set of sentences from the Translation Process Research Database (TPR 1.0)(Carl, 2012). The database contains translation studies for which gaze data is recorded through the Translog software1 (Carl, 2012). Figure 2 presents a screendump of Translog. Out of the 57 available sessions, we selected 40 translation sessions comprising 80 sentence translations2 . Each of these 80 sentences was translated from English to three different languages, viz. Spanish, Danish and Hindi by at least 2 translators. The translators were young professional linguists or students pursuing PhD in linguistics. The eye-tracking data is noisy and often exhibits systematic errors (Hornof and Halverson, 2"
P13-2062,C96-2123,0,0.697064,"Missing"
P13-2062,W12-4906,1,0.831553,"ded through the Translog software1 (Carl, 2012). Figure 2 presents a screendump of Translog. Out of the 57 available sessions, we selected 40 translation sessions comprising 80 sentence translations2 . Each of these 80 sentences was translated from English to three different languages, viz. Spanish, Danish and Hindi by at least 2 translators. The translators were young professional linguists or students pursuing PhD in linguistics. The eye-tracking data is noisy and often exhibits systematic errors (Hornof and Halverson, 2002). To correct this, we applied automatic error correction technique (Mishra et al., 2012) followed by manually correcting incorrect gaze-toword mapping using Translog. Note that, gaze and saccadic durations may also depend on the translator’s reading speed. We tried to rule out this effect by sampling out translations for which the variance in participant’s reading speed is minimum. Variance in reading speed was calculated after taking a samples of source text for each participant and measuring the time taken to read the text. After preprocessing the data, TDI was computed for each sentence by using (2) and (3).The observed unnormalized TDI score3 ranges from 0.12 to 0.86. We norm"
P13-2062,W12-4904,0,0.0369175,"Missing"
P13-2096,C04-1162,0,0.0397123,"ork are given in the last section, section 8. X P (πL1 (S L2 )|u) · #(u) P (S L2 |v) = XuX 2 Related work L2 Si Word Sense Disambiguation is one of the hardest problems in NLP. Successful supervised WSD approaches (Lee et al., 2004; Ng and Lee, 1996) are restricted to resource rich languages and domains. They are directly dependent on availability of good amount of sense tagged data. Creating such a costly resource for all language-domain pairs is impracticable looking at the amount of time and money required. Hence, unsupervised WSD approaches (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011) attract most of the researchers. y P (πL1 (SiL2 )|y) · #(y) where, SiL2 ∈ synsetsL2 (v) u ∈ crosslinksL1 (v, S L2 ) y ∈ crosslinksL1 (v, SiL2 ) Here, • ‘#’ indicates the raw count. • crosslinksL1 (a, S L2 ) is the set of possible translations of the word ‘a’ from language L1 to L2 in the sense S L2 . • πL2 (S L1 ) means the linked synset of the sense S L1 in L2 . 3 Background Khapra et al. (2011) dealt with bilingual unsupervised WSD. It uses EM algorithm for estimating sense distributions in comparable corpora. Every polysemous word is disambiguated using th"
P13-2096,P02-1033,0,0.350264,"Missing"
P13-2096,C02-1058,0,0.624689,"Missing"
P13-2096,P10-1155,1,\N,Missing
P13-2096,P91-1017,0,\N,Missing
P13-2096,P96-1006,0,\N,Missing
P13-2096,I11-1078,1,\N,Missing
P13-2096,W04-0834,0,\N,Missing
P13-2149,esuli-sebastiani-2006-sentiwordnet,0,0.113839,"Missing"
P13-2149,P03-1054,0,0.0137796,"Missing"
P13-2149,W02-1011,0,0.0136305,"tion of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at 1 Credits The authors thank the lexicographers at Center for Indian Language Technology (CFILT) at IIT Bombay for their support for this work. 2 Introduction Although much research has been done in the field of sentiment analysis (Liu et al., 2012), thwarting and sarcasm are not addressed, to the best of our knowledge. Thwarting has been identified as a common phenomenon in sentiment analysis (Pang et al., 2002, Ohana et al., 2009, Brooke, 2009) in various forms of texts but no previous work has proposed a solution to the problem of identifying thwarting. We focus on identifying thwarting in product reviews. The definition of an opinion as specified in Liu (2012) is “An opinion is a quintuple, ( , , , , ), where is the name of an entity, is an aspect of , is the sentiment on aspect of entity , is the opinion holder, and is the time when the opinion is expressed by .” If the sentiment towards the entity or one of its important attribute contradicts the sentiment towards all other attributes, we can s"
P13-2149,N03-1033,0,\N,Missing
P13-4030,D09-1030,0,0.484519,"hrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work. 1 Introduction Crowdsourcing is no longer a new term in the domain of Computational Linguistics and Machine Translation research (Callison-Burch and Dredze, 2010; Snow et al., 2008; Callison-Burch, 2009). Crowdsourcing - basically where task outsourcing is delegated to a largely unknown Internet audience - is emerging as a new paradigm of human in the loop approaches for developing sophisticated techniques for understanding and generating natural language content. Amazon Mechanical 1 http://www.mturk.com,http://www. crowdflower.com 2 http://www.lingotek.com,http:///www. gengo.com 3 http://en.wikipedia.org/wiki/List_of_ languages_by_total_number_of_speakers 175 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175–180, c Sofia, Bulgaria, August 4-9"
P13-4030,W10-1701,0,0.0171192,"4 describes the system architecture and workflow, while Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-bas"
P13-4030,kunchukuttan-etal-2012-experiences,1,0.844063,"r interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-based merging of small chunks. This step creates small phrases 176 Figure 1: Multistage crowdsourced translation req"
P13-4030,W12-3152,0,0.0131872,"issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crowd ranking based features. However, automatic evaluation of translation quality is difficult, such automatic methods being either inaccurate or expensive. Post et al. (2012) have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations. The quality of the validations is ensured using goldstandard sentence translations. Our approach to quality control is similar to Post et al. (2012), but we work at the level of phrases. While most crowdsourcing activities for data gathering has been concerned with collecting simple annotations like relevance judgments, there has been work to explore the use of crowdsourcing for more complex tasks, of which translation is a good example. Little et al. (2010) propose that many com"
P13-4030,W10-0710,0,0.147473,"ortant aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-based merging of small chunks. This step creates small phrases 176 Figure 1: Multistage"
P13-4030,D08-1027,0,0.210716,"Missing"
P13-4030,W05-0909,0,0.0470973,"es may not map, making translation difficult. For instance, the vaala modifier in Hindi translates to a clause in English. It does not contain any tense information, therefore the tense of the English clause cannot be determined by the worker. e.g. Using TransDoop, we conducted a set of smallscale, preliminary translation experiments. We obtained translations for English-Hindi and EnglishMarathi language pairs for the Judicial and Tourism domains. For each experiment, 15 sentences were given as input to the pipeline. For evaluation, we chose METEOR, a well-known translation evaluation metric (Banerjee and Lavie, 2005). We compared the results obtained from the crowdsourcing system with a expert human translation and the output of Google Translate. We also compared two expert translations using METEOR to establish a skyline for the translation accuracy. Table 1 summarizes the results of our experiments. The translations with Quality Control and multistage pipeline are better than Google translations and translations obtained from the crowd without any quality control, as evaluated by METEOR. Multi-stage translation yields better than complete sentence translation. Moreover, the translation quality is compar"
P13-4030,P11-1122,0,0.026633,"Missing"
P13-4030,ambati-etal-2010-active,0,\N,Missing
P13-4030,W10-0701,0,\N,Missing
P14-2007,P13-2062,1,0.62415,"Missing"
P14-2007,D09-1019,0,0.0429007,"Missing"
P14-2007,carl-2012-translog,0,0.0130471,"values to a scale of 1-10 using min-max normalization. To understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2. The sentence “it is messy , uncouth , incomprehensible , vicious and absurd” has a SAC of 3.3. On the other hand, the SAC for the sarcastic sentence “it’s like an all-star salute to disney’s cheesy commercialism.” is 8.3. 2. While the annotator reads the sentence, a remote eye-tracker (Model: Tobii TX 300, Sampling rate: 300Hz) records the eyemovement data of the annotator. The eyetracker is linked to a Translog II software (Carl, 2012) in order to record the data. A snapshot of the software is shown in figure 1. The dots and circles represent position of eyes and fixations of the annotator respectively. 3. The experiment then continues in modules of 50 sentences at a time. This is to prevent fatigue over a period of time. Thus, each annotator participates in this experiment over a number of sittings. We ensure the quality of our dataset in different ways: (a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment. (b) To minimize noise due to head movements fur"
P14-2007,P05-1015,0,0.150353,"king device that measures the “duration of eye-fixations1 ”. Another attribute recorded by the eye-tracker that may have been used is “saccade duration2 ”. However, saccade duration is not significant for annotation of short text, as in our case. Hence, the SAC labels of our dataset are fixation durations with appropriate normalization. It may be noted that the eye-tracking device is used only to annotate training data. The actual prediction of SAC is done using linguistic features alone. 3.1 Eye-tracking Experimental Setup We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http://help.sentiment140. com/for-students. A total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected. We then obtain two kinds of annotation from five paid annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded 1 A long stay of the visual gaze on a single location. A rapid movement of the eyes between positions of rest on the sentence. 2 37 a fixation duration recorded for each sentenceannotator pair3 The multi-rater kappa IAA for sentiment annotation is 0.686. 3.2 Figure 1: Gaze-data recording using Transl"
P14-2007,esuli-sebastiani-2006-sentiwordnet,0,0.0691661,"Missing"
P14-2007,P13-2149,1,0.884599,"Missing"
P14-2007,C12-1055,0,0.0668226,"Missing"
P14-2007,N13-1088,1,0.300018,"Missing"
P14-2007,P11-2102,0,0.0817206,"Missing"
P14-2007,I13-1195,0,0.0644871,"Missing"
P14-2007,C96-2123,0,\N,Missing
P14-2007,C08-1031,0,\N,Missing
P14-2007,W11-1707,0,\N,Missing
P15-2100,E12-1049,0,0.0337048,"Dataset H (193 drunk, 317 sober): A separate dataset is created where drunk tweets are downloaded using drunk hashtags, as above. The set of sober tweets is collected using both the approaches above. The resultant is the held-out test set Dataset-H that contains no tweets in common with Datasets 1 and 2. The drunk tweets for Datasets 1 and 2 are the same. Figure 1 shows a word-cloud for these drunk tweets (with stop words and forms of the word ‘drunk’ removed), created using Dataset Creation We use hashtag-based supervision to create our datasets, similar to tasks like emotion classification (Purver and Battersby, 2012). The tweets are downloaded using Twitter API (https://dev. 4 This is a rigid criterion, but we observe that tweets with hyperlinks are likely to be promotional in nature. 605 Feature Unigram & Bigram (Presence) Unigram & Bigram (Count) LDA unigrams (Presence/Count) POS Ratio #Named Entity Mentions #Discourse Connectors Spelling errors Repeated characters Capitalisation Length Emoticon (Presence/Count) Sentiment Ratio Description N-gram Features Boolean features indicating unigrams and bigrams Real-valued features indicating unigrams and bigrams Stylistic Features Boolean & real-valued feature"
P15-2100,D13-1133,0,0.0262903,"witter.com/). We remove non-Unicode characters, and eliminate tweets that contain hyperlinks4 and also tweets that are shorter than 6 words in length. Finally, hashtags used to indicate drunk or sober tweets are removed so that they provide labels, but do not act as features. The dataset is available on request. As a result, we create three datasets, each using a different strategy for sober tweets, as follows: health issues, suicidal nature, criminal status, etc. (Pennebaker, 1993; Pennebaker, 1997). NLP techniques have been used in the past to address social safety and mental health issues (Resnik et al., 2013). 3 Definition and Challenges Drunk-texting prediction is the task of classifying a text as drunk or sober. For example, a tweet ‘Feeling buzzed. Can’t remember how the evening went’ must be predicted as ‘drunk’, whereas, ‘Returned from work late today, the traffic was bad’ must be predicted as ‘sober’. The challenges are: 1. More than topic categorisation: Drunktexting prediction is similar to topic categorisation (that is, classification of documents into a set of categories such as ‘news’, ‘sports’, etc.). However, Borrill et al. (1987) show that alcohol abusers have more pronounced emotion"
P15-2100,H05-1044,0,0.0102358,"sed for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user’s local times was not available, since very few users had geolocation enabled. 5 happpy. Since drunk-texting is often associated with emotional expression, we also incorporate a set of sentiment-based features. These features include: count/presence of emoticons and sentiment ratio. Sentiment ratio is the proportion of positive and negative words in the tweet. To determine positive and negative words, we use the sentiment lexicon in Wilson et al. (2005). To identify a more refined set of words that correspond to the two classes, we also estimated 20 topics for the dataset by estimating an LDA model (Blei et al., 2003). We then consider top 10 words per topic, for both classes. This results in 400 LDA-specific unigrams that are then used as features. Feature Design The complete set of features is shown in Table 1. There are two sets of features: (a) N-gram features, and (b) Stylistic features. We use unigrams and bigrams as N-gram features- considering both presence and count. Table 1 shows the complete set of stylistic features of our predic"
P15-2100,P06-4018,0,0.0189092,"Missing"
P15-2124,W14-2609,0,0.563262,"troduce inter-sentential incongruity for sarcasm detection, that expands context of a discussion forum post by including the previous post (also known as the ‘elicitor’ post) in the discussion thread. Introduction Sarcasm is defined as ‘a cutting, often ironic remark intended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity, it does so piecemeal; we take a more well-rou"
P15-2124,walker-etal-2012-corpus,0,0.128027,"ach to create a sarcasm-annotated dataset was employed in Gonzalez-Ibanez et al. (2011). As an additional quality check, a rough glance through the tweets is done, and the ones found to be wrong are removed. The hashtags mentioned above are removed from the text so that they act as labels but not as features. 2. Tweet-B (2278 tweets, 506 sarcastic): This dataset was manually labeled for Riloff et al. (2013). Some tweets were unavailable, due to deletion or privacy settings. 3. Discussion-A (1502 discussion forum posts, 752 sarcastic): This dataset is created from the Internet Argument Corpus (Walker et al., 2012) that contains manual annota• Number of sentiment incongruities: The number of times a positive word is followed by a negative word, and vice versa • Largest positive/negative subsequence: The length of the longest series of contiguous positive/negative words • Number of positive and negative words • Lexical Polarity: The polarity based purely on the basis of lexical features, as determined by Lingpipe SA system (Alias-i, 2008). Note that the ‘native polarity’ need not be correct. However, a tweet that is strongly positive on the surface is more likely to be sarcastic than a tweet that seems t"
P15-2124,W10-2914,0,0.680174,"ended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity, it does so piecemeal; we take a more well-rounded view of incongruity and place it center-stage for our work. 1 Rest of the paper is organized as follows. We first discuss related work in Section 2. We introduce context incongruity in Section 3. Feature design for explicit incongruity is presented in Section 3.1,"
P15-2124,P11-2102,0,0.679454,"ts’ as well as long ‘discussion forum posts’. • We introduce inter-sentential incongruity for sarcasm detection, that expands context of a discussion forum post by including the previous post (also known as the ‘elicitor’ post) in the discussion thread. Introduction Sarcasm is defined as ‘a cutting, often ironic remark intended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity"
P15-2124,W13-1605,0,0.460601,"Missing"
P15-2124,maynard-greenwood-2014-cares,0,0.564505,"Missing"
P15-2124,P13-2149,1,0.910944,"et of Dutch tweets that contain sarcasmrelated hashtags and implement a classifier to predict sarcasm. A recent work by ?) takes the output of sarcasm detection as an input to sentiment classification. They present a rule-based system that uses the pattern: if the sentiment of a tokenized hashtag does not agree with sentiment in rest of the tweet, the tweet is sarcastic, in addition to other rules. Our approach is architecturally similar to Tsur et al. (2010b) who use a semi-supervised pattern acquisition followed by classification. Our feature engineering is based on Riloff et al. (2013) and Ramteke et al. (2013). Riloff et al. (2013) state that sarcasm is a contrast between positive sentiment word and a negative situation. They implement a rule-based system that uses phrases of positive verb phrases and negative situations extracted from a corpus of sarcastic tweets. Ramteke et al. (2013) present a novel approach to detect thwarting: the phenomenon where sentiment in major portions of text is reversed by sentiment in smaller, conclusive portions. 3 3.1 Explicit incongruity Explicit incongruity is overtly expressed through sentiment words of both polarities (as in the case of ‘I love being ignored’ wh"
P15-2124,D13-1066,0,0.740627,"Missing"
P16-1104,W14-2609,0,0.240048,"implicated one. Giora (1995), on the other hand, define sarcasm as a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic"
P16-1104,W10-2914,0,0.661187,"a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone. With the advent of sophisticated eyetrac"
P16-1104,P15-2124,1,0.683305,"(1995), on the other hand, define sarcasm as a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features a"
P16-1104,W13-1605,0,0.0496852,"Missing"
P16-1104,maynard-greenwood-2014-cares,0,0.108701,"ntity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone. With the advent of sophisticated eyetrackers and electro/magneto-encephalographic (EEG/MEG) devices, it has been possible to delve deep into the cognitive underpinnings of sarcasm understanding. Fil"
P16-1104,P04-1035,0,0.0220621,"4 141 t 14.1 14.0 9.5 13.9 11.9 13.2 15.3 p 5.84E-39 1.71E-38 3.74E-20 1.89E-37 2.75E-28 6.79E-35 3.96E-43 Table 1: T-test statistics for average fixation duration time per word (in ms) for presence of sarcasm (represented by S) and its absence (NS) for participants P1-P7. information. 3.1 Document Description The database consists of 1,000 short texts, each having 10-40 words. Out of these, 350 are sarcastic and are collected as follows: (a) 103 sentences are from two popular sarcastic quote websites3 , (b) 76 sarcastic short movie reviews are manually extracted from the Amazon Movie Corpus (Pang and Lee, 2004) by two linguists. (c) 171 tweets are downloaded using the hashtag #sarcasm from Twitter. The 650 non-sarcastic texts are either downloaded from Twitter or extracted from the Amazon Movie Review corpus. The sentences do not contain words/phrases that are highly topic or culture specific. The tweets were normalized to make them linguistically well formed to avoid difficulty in interpreting social media lingo. Every sentence in our dataset carries positive or negative opinion about specific “aspects”. For example, the sentence “The movie is extremely well cast” has positive sentiment about the a"
P16-1104,P11-2102,0,0.127089,"Missing"
P16-1104,D13-1066,0,0.525184,"Missing"
P17-1035,D09-1020,0,0.0172231,"Missing"
P17-1035,D11-1100,1,0.83413,"haps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts. Related Work Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 20"
P17-1035,W14-2609,0,0.0663781,"s difficult for even humans to classify such texts. Related Work Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neura"
P17-1035,W15-2401,0,0.1717,"Missing"
P17-1035,W10-2914,0,0.282424,"ntiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Vea"
P17-1035,P14-2009,0,0.0046488,"sm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours"
P17-1035,C14-1008,0,0.0189745,"10), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical"
P17-1035,W16-0425,0,0.0279842,"et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours are by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze and text data for sentiment and sarcasm detection tasks. These recent advancements"
P17-1035,P11-2102,0,0.102928,"Missing"
P17-1035,maynard-greenwood-2014-cares,0,0.0220269,"Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze da"
P17-1035,P14-2007,1,0.821384,"Missing"
P17-1035,N16-1118,0,0.0392195,"Missing"
P17-1035,P15-2124,1,0.887093,"mans to classify such texts. Related Work Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcas"
P17-1035,N13-1090,0,0.00727579,"Missing"
P17-1035,D14-1181,0,0.249515,"ponents are explained below. 4.1 (2) where b ∈ R is the bias and f is the non-linear function. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H. lustration). With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to. 4 (1) Text Component The text component is quite similar to the one proposed by Kim (2014) for sentence classification. Words (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK ). As per Kim (2014), a multi-channel variant of CNN (referred to as M ULTI C HANNELT EXT) can be implemented by using two channels of embeddingsone that remains static throughout training (referred to as S TATIC T EXT), and the other one that gets updated during training (referred to as N ON S TATIC T EXT). We separately experiment with static, non-static and multi-channel varian"
P17-1035,N16-1179,0,0.0866307,"Missing"
P17-1035,P16-1104,1,0.499687,"Missing"
P17-1035,K16-1016,1,0.930106,"eatures for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features. 1 Mishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better. The hypothesis here is that human gaze activities are related to the cognitive processes in the brain that combine the “external knowledge” that the reader possesses with textual clues that she / he perceives. While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability o"
P17-1035,N10-1120,0,0.0269363,"correctly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts. Related Work Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos San"
P17-1035,P06-2079,0,0.0458124,"een incorrectly classified by M ULTI C HANNELT EXT. Example 4 is incorrectly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts. Related Work Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative"
P17-1035,N16-1082,0,0.00506596,"N are, we analyzed the features for a few example cases. Figure 4 presents some of the example test cases for the task of sarcasm detection. Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic. To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants. Output of the hidden layer after merge layer is considered as features learned by the network. We plot the features, in the form of color-bars, following Li et al. (2016) - denser colEffect of Fixation / Saccade Channels For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due 5 Effectiveness of the CNN-learned Features a standard range (Liu et al., 2015; Melamud et al., 2016) 383 1. I would like to live in Manchester, England. The transition between Manchester and death would be unnoticeable. (Sarcastic, Negative Sentiment) 2. We really did not like this camp. After a disappointing summer, we switched to another camp, and all of us much happier on all fronts! (Non Sarcastic, Negative Sentiment) 3. Helped me a lot with"
P17-1035,W13-1605,0,0.0606825,"Missing"
P17-1035,P05-1015,0,0.0528082,"-norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units during forward propagation. We set p to 0.25. Experiment Setup 5.5 We now share several details regarding our experiments below. 5.1 Dataset 5.6 Use of Pre-trained Embeddings: Initializing the embedding layer with of pretrained embeddings can be more effective than random initialization (Kim, 2014). In our experiments, we have used embeddings learned using the movie reviews with one sentence per review dataset (Pang and Lee, 2005). It is worth noting that, for a small dataset like ours, using a small data-set like the one from (Pang and Lee, 2005) helps in reducing the number model parameters resulting in faster learning of embeddings. The results are also quite close to the ones obtained using word2vec facilitated by Mikolov et al. (2013). CNN Variants With text component alone we have three variants such as S TATIC T EXT, N ON S TATIC T EXT and M ULTI C HANNELT EXT (refer to Section 4.1). Similarly, with gaze component we have variants such as F IXATION, S ACCADE and M ULTI C HAN NEL G AZE (refer to Section 4.2). Wit"
P17-1035,D15-1168,0,0.0520863,"Missing"
P17-1035,D13-1066,0,0.492543,"Missing"
P17-1035,I13-1076,1,0.89504,"Missing"
P17-1035,S14-2033,0,0.0253308,"i et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features."
P17-1035,P16-2037,0,0.00708755,"(Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi384 References tecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs. Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours are by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature e"
P18-1089,K15-1006,1,0.920471,", 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015). The existing transfer learning based domain adaptation algorithms for cross-domain classification have generally been proven useful in reducing the labeled data requirement, but they do not consider words like unpredictable that change polarity orientation across domains. Transfer (reuse) of changing polarity words affects the cross-domain performance negatively. Therefore, one cannot use transfer learning as the proverbial hammer, rather one needs to gauge what to transfer from the source domain to the target domain. In this paper, we propose that the words which Getting manually labeled da"
P18-1089,P09-1028,0,0.0327556,"). Such a polarity changing word should be assigned positive orientation in the movie domain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades. Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus (Pang et al., 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015). The existing transfer learning based domain adaptation algorithms for cross-domain classification h"
P18-1089,C12-1036,0,0.0943783,"(inconsistent) polarity across domains (Turney, 2002; Fahrni and Klenner, 2008). 968 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 968–978 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 are equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis. χ2 is a popularly used and reliable statistical test to identify significance and polarity of a word in an annotated corpus (Oakes et al., 2001; Al-Harbi et al., 2008; Cheng and Zhulyn, 2012; Sharma and Bhattacharyya, 2013). However, for an unlabeled corpus no such statistical technique is applicable. Therefore, identification of words which are significant with a consistent polarity across domains is a non-trivial task. In this paper, we present a novel technique based on χ2 test and cosine-similarity between context vector of words to identify Significant Consistent Polarity (SCP) words across domains.2 The major contribution of this research is as follows. Related Work The most significant efforts in the learning of transferable knowledge for cross-domain text classification a"
P18-1089,W02-1011,0,0.0416728,"Himanshu.S.Bhatt@Aexp.Com Abstract predictable plot), but negative in the automobile domain (unpredictable steering). Such a polarity changing word should be assigned positive orientation in the movie domain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades. Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus (Pang et al., 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt"
P18-1089,P07-1034,0,0.220815,"as a bridge to construct a bipartite graph to model the co-occurrence relationship between domainspecific words and domain-independent words. Our approach also exploits the concept of cooccurrence (Pan et al., 2010), but we measure the co-occurrence in terms of similarity between context vector of words, unlike SCL and SFA, which literally look for the co-occurrence of words in the corpus. The use of context vector of words in place of words helps to overcome the data sparsity problem (Sharma et al., 2015). Domain adaptation for sentiment classification has been explored by many researchers (Jiang and Zhai, 2007; Ji et al., 2011; Saha et al., 2011; Glorot et al., 2011; Xia et al., 2013; Zhou et al., 2014; Bhatt et al., 2015). Most of the works have focused on learning a shared low dimensional representation of features that can be generalized across different domains. However, none of the approaches explicitly analyses significance and polarity of words across domains. On the other hand, Glorot et al., (2011) proposed a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Zhou et al., (2014) also proposed a deep learning approach to le"
P18-1089,S14-2009,0,0.0157298,"ain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades. Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus (Pang et al., 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015). The existing transfer learning based domain adaptation algorithms for cross-domain classification have generally been proven useful in reducing the labeled data requirement, but they do not consid"
P18-1089,I13-1076,1,0.940927,"across domains (Turney, 2002; Fahrni and Klenner, 2008). 968 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 968–978 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 are equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis. χ2 is a popularly used and reliable statistical test to identify significance and polarity of a word in an annotated corpus (Oakes et al., 2001; Al-Harbi et al., 2008; Cheng and Zhulyn, 2012; Sharma and Bhattacharyya, 2013). However, for an unlabeled corpus no such statistical technique is applicable. Therefore, identification of words which are significant with a consistent polarity across domains is a non-trivial task. In this paper, we present a novel technique based on χ2 test and cosine-similarity between context vector of words to identify Significant Consistent Polarity (SCP) words across domains.2 The major contribution of this research is as follows. Related Work The most significant efforts in the learning of transferable knowledge for cross-domain text classification are Structured Correspondence Lear"
P18-1089,W15-5920,1,0.800572,"opose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance. 1 Introduction The choice of the words to express an opinion depends on the domain as users often use domainspecific words (Qiu et al., 2009; Sharma and Bhattacharyya, 2015). For example, entertaining and boring are frequently used in the movie domain to express an opinion; however, finding these words in the electronics domain is rare. Moreover, there are words which are likely to be used across domains in the same proportion, but may change their polarity orientation from one domain to another (Choi et al., 2009). For example, a word like unpredictable is positive in the movie domain (un1 The word ‘unpredictable’ is a classic example of changing (inconsistent) polarity across domains (Turney, 2002; Fahrni and Klenner, 2008). 968 Proceedings of the 56th Annual M"
P18-1089,D15-1300,1,0.932154,"shown significant improvement over a baseline (shift-unaware) model. SFA uses some domain-independent words as a bridge to construct a bipartite graph to model the co-occurrence relationship between domainspecific words and domain-independent words. Our approach also exploits the concept of cooccurrence (Pan et al., 2010), but we measure the co-occurrence in terms of similarity between context vector of words, unlike SCL and SFA, which literally look for the co-occurrence of words in the corpus. The use of context vector of words in place of words helps to overcome the data sparsity problem (Sharma et al., 2015). Domain adaptation for sentiment classification has been explored by many researchers (Jiang and Zhai, 2007; Ji et al., 2011; Saha et al., 2011; Glorot et al., 2011; Xia et al., 2013; Zhou et al., 2014; Bhatt et al., 2015). Most of the works have focused on learning a shared low dimensional representation of features that can be generalized across different domains. However, none of the approaches explicitly analyses significance and polarity of words across domains. On the other hand, Glorot et al., (2011) proposed a deep learning approach which learns to extract a meaningful representation"
P18-1089,J11-2001,0,0.0367528,"signed positive orientation in the movie domain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades. Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus (Pang et al., 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015). The existing transfer learning based domain adaptation algorithms for cross-domain classification have generally been proven useful in reducing the l"
P18-1089,P02-1053,0,0.0500307,"n use domainspecific words (Qiu et al., 2009; Sharma and Bhattacharyya, 2015). For example, entertaining and boring are frequently used in the movie domain to express an opinion; however, finding these words in the electronics domain is rare. Moreover, there are words which are likely to be used across domains in the same proportion, but may change their polarity orientation from one domain to another (Choi et al., 2009). For example, a word like unpredictable is positive in the movie domain (un1 The word ‘unpredictable’ is a classic example of changing (inconsistent) polarity across domains (Turney, 2002; Fahrni and Klenner, 2008). 968 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 968–978 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 are equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis. χ2 is a popularly used and reliable statistical test to identify significance and polarity of a word in an annotated corpus (Oakes et al., 2001; Al-Harbi et al., 2008; Cheng and Zhulyn, 2012; Sharma and Bhattacharyya, 201"
P18-1219,P16-1068,0,0.021795,"died by Rayner (1998), where he found that unpredictable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap"
P18-1219,P05-1018,0,0.306238,"stems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzal"
P18-1219,D16-1115,0,0.137915,"ctable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discours"
P18-1219,W17-5050,0,0.150285,"Missing"
P18-1219,P14-2007,1,0.850171,"allenge in NLP. It has been studied since Page’s seminal work on automatic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, with each paragraph denoting a single idea"
P18-1219,W15-1814,0,0.0271384,"ricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text (rather than just a sentence as done by Klerke et al. (2015)) and show that the scoring is more reliable if the reader has understood the text. 4 Features In order to predict the scores of the different properties of the text, we use the following text and gaze features. 4.1 Text-based Features We use a set of text-based features to come up with a baseline system to predict the scores for different properties. The first set of features that we use are length and count-based features, such as wo"
P18-1219,W02-0109,0,0.0151885,"features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complexity. These features were extracted using Stanford CoreNLP (Manning et al., 2014), and MorphAdorner (Burns, 2013). The third set of features that we use are stylistic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov et al., 2013). The word embeddings are 300 dimensions. We also calculate the mean and maximum similarities between the word vectors of the content words in adjacent sentences of the text, using GloVe word embeddings8 (Pennington et al., 2014). The fifth set of features that we use are language modeling features. We use the count of words that are ab"
P18-1219,P14-5010,0,0.00257896,"based features to come up with a baseline system to predict the scores for different properties. The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases6 etc. (Persing and Ng, 2015; Zesch et al., 2015). The next set of features that we use are complexity features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complexity. These features were extracted using Stanford CoreNLP (Manning et al., 2014), and MorphAdorner (Burns, 2013). The third set of features that we use are stylistic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov"
P18-1219,P13-2062,1,0.916596,"atic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, with each paragraph denoting a single idea. If the text is too long, and not split into paragraphs, one could con"
P18-1219,D14-1162,0,0.0830811,"atures are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov et al., 2013). The word embeddings are 300 dimensions. We also calculate the mean and maximum similarities between the word vectors of the content words in adjacent sentences of the text, using GloVe word embeddings8 (Pennington et al., 2014). The fifth set of features that we use are language modeling features. We use the count of words that are absent in Google News word vectors and misspelled words using the PyEnchant9 library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979). The sixth set of features are sequence features. These features are particularly useful in modeling organization (sentence and paragraph sequence similarity) (Persing et al., 2010), coherence and cohesion (PoS and lemma similarity). Pitler et al. (2010) showed that co"
P18-1219,D10-1023,0,0.06521,"Missing"
P18-1219,P15-1053,0,0.0195901,"l. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text (rather than just a sentence as done by Klerke et al. (2015)) and show that the scoring is more reliable if the reader has understood the text. 4 Features In order to pr"
P18-1219,W16-1904,1,0.815823,"e quality of a text is an interesting challenge in NLP. It has been studied since Page’s seminal work on automatic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, w"
P18-1219,C14-1090,0,0.0243358,"scribes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain a"
P18-1219,P06-2103,0,0.0478763,"years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015),"
P18-1219,D16-1193,0,0.156078,"re he found that unpredictable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for"
P18-1219,W15-0626,0,0.0909798,"ng techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text"
P18-1219,P10-1056,0,0.0813857,"Missing"
P18-2011,W97-1301,0,0.147112,"ate them using knowledge sources like Wikipedia. These in64 • Ea = {hu, vi : undirected edge, which connects nodes u and v which are headwords of aliases of the same participant }; e.g., hhim, Bonapartei Our approach has been summarized in Algorithm 1. Its input is an ULDG G(V, Ed , Ep , Ea ) for a set S of given sentences. We initialize V , Ed , Ep and Ea using any standard dependency parser, NER and coreference resolution techniques3 . volves resolution of bridging descriptions which study relationships between a definite description and its antecedent. As noted in (Vieira and Teufel, 1997; Poesio et al., 1997), bridging descriptions consider many different types of relationships between a definite description (definite generic NP) and its antecedent; e.g., synonymy, hyponymy, meronymy, events, compound nouns, etc. However, in this paper we focus on identity type of relationships only. Further, Vieira and Teufel (1997) use WordNet to identify these relationship types between definite descriptions. As described in Phase-I of algorithm 1 (Section 3), we use WordNet for a completely different purpose of identifying participant type.2 Gardent and Kow (2003) presented a corpus study of bridging definite"
P18-2011,W17-5912,1,0.427438,"An inIntroduction Identifying aliases of participants in a narrative is crucial for many NLP applications like timeline creation, question-answering, summarization, and information extraction. For instance, to answer a question (in the context of Table 1) When did Napoleon defeat the royalist rebels?, we need to identify Napoleon and the young lieutenant as aliases of Napoleon Bonaparte. Similarly, timeline for Napoleon Bonaparte will be inconsistent with the text, if the young lieutenant is not identified as an alias Napoleon Bonaparte. This will further affect any analysis of the timeline (Bedi et al., 2017). In the context of narrative analysis, we define – • A participant as an entity of type PERSON (PER), LOCATION (LOC), or ORGANIZATION (ORG). A participant has a canonical mention, ∗ When [he]A1 1 These authors contributed equally. NP with a common noun headword 63 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 63–68 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics dependent composite mention is created by recursively merging all its dependent mentions. For instance, for the phrases his par"
P18-2011,P14-2006,0,0.0571736,"Missing"
P18-2011,de-marneffe-etal-2014-universal,0,0.0313162,"Missing"
P18-2011,W03-2410,0,0.116214,"Missing"
P18-2011,D12-1114,0,0.0246045,"1. New Ea edges: hman, Bonapartei, hman, himi, & hman, Hisi are added. Newly added Ep edges are highlighted with thick, filled arrows. Participant types of man & school are changed to PER & ORG respectively; type of France is changed to OTH. stances of Predicate Schemas are then compiled into constraints in an Integer Linear Programming (ILP) based formulation to resolve coreferences. In addition to pronouns, our approach focuses on identification of common noun based aliases of a participant using MLN. MLN has been used to solve the problem of coreference resolution (Poon and Domingos, 2008; Song et al., 2012). Our work differs from them as we build upon output of off-the-shelf coreference resolution system, rather than identifying aliases/coreferences from scratch. This helps in exploiting the strengths (such as linking pronoun mentions to their antecedents) of the existing systems and overcome the weaknesses (such as resolving generic NP mentions) by incorporating additional linguistic knowledge. A more general and challenging problem inRelated Work Traditionally, alias detection restricts the focus on aliases of named entities which occur as proper nouns (Sapena et al., 2007; Hsiung et al., 2005"
P18-2011,P97-1072,0,0.327682,"dicate level and instantiate them using knowledge sources like Wikipedia. These in64 • Ea = {hu, vi : undirected edge, which connects nodes u and v which are headwords of aliases of the same participant }; e.g., hhim, Bonapartei Our approach has been summarized in Algorithm 1. Its input is an ULDG G(V, Ed , Ep , Ea ) for a set S of given sentences. We initialize V , Ed , Ep and Ea using any standard dependency parser, NER and coreference resolution techniques3 . volves resolution of bridging descriptions which study relationships between a definite description and its antecedent. As noted in (Vieira and Teufel, 1997; Poesio et al., 1997), bridging descriptions consider many different types of relationships between a definite description (definite generic NP) and its antecedent; e.g., synonymy, hyponymy, meronymy, events, compound nouns, etc. However, in this paper we focus on identity type of relationships only. Further, Vieira and Teufel (1997) use WordNet to identify these relationship types between definite descriptions. As described in Phase-I of algorithm 1 (Section 3), we use WordNet for a completely different purpose of identifying participant type.2 Gardent and Kow (2003) presented a corpus study"
P18-2011,M95-1005,0,0.574618,"oreference resolution system based on (Peng et al., 2015a,b). M is our proposed alias detection approach (Algorithm 1). Evaluation: The performance of all the approaches is evaluated at two levels: all independent participant mentions (i.e., participant detection) and their links with canonical mentions (i.e., participant linking). We use the standard F1 metric to measure performance of participant detection. For participant linking, we evaluate (Pradhan et al., 2014) the combined performance of participant mention identification and alias detection using the standard evaluation metrics, MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe) (Luo, 2005) and their average. Results: Results of the quantitative evaluation are summarized in Table 3. We observe that the proposed approach outperforms other baselines on all datasets. Dataset & Approach B1 ACEnw B2 M B1 Nap B2 M B1 BoH B2 M B1 Fas B2 M B1 Mao B2 M Participant mentions 53.1 62.9 70.2 60.5 73.9 86.4 61.7 65.6 73.5 56.8 61.6 70.3 60.1 49.1 78.9 posed approach succeeds as it exploits MLN rule CopulaConnect(x, y) ⇒ Alias(x, y). As an illustration of the proposed approach, Table 4 shows the participant mentions and the"
P18-2011,H05-1004,0,0.0685315,"lias detection approach (Algorithm 1). Evaluation: The performance of all the approaches is evaluated at two levels: all independent participant mentions (i.e., participant detection) and their links with canonical mentions (i.e., participant linking). We use the standard F1 metric to measure performance of participant detection. For participant linking, we evaluate (Pradhan et al., 2014) the combined performance of participant mention identification and alias detection using the standard evaluation metrics, MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe) (Luo, 2005) and their average. Results: Results of the quantitative evaluation are summarized in Table 3. We observe that the proposed approach outperforms other baselines on all datasets. Dataset & Approach B1 ACEnw B2 M B1 Nap B2 M B1 BoH B2 M B1 Fas B2 M B1 Mao B2 M Participant mentions 53.1 62.9 70.2 60.5 73.9 86.4 61.7 65.6 73.5 56.8 61.6 70.3 60.1 49.1 78.9 posed approach succeeds as it exploits MLN rule CopulaConnect(x, y) ⇒ Alias(x, y). As an illustration of the proposed approach, Table 4 shows the participant mentions and their corresponding canonical mentions for the example text in Table 1. Se"
P18-2011,P14-5010,0,0.00694833,"OT H Drop from Ep all outgoing edges from y foreach clique c in subgraph (V, Ea ) ⊂ G do foreach n ∈ c.nodes do n.a := earliest participant mention in c.nodes Algorithm 1: identif y participants & aliases Our algorithm modifies the input ULDG inplace by updating node labels, Ep and Ea . Figure 1 shows an example of initialized input ULDG, which gets transformed by our algorithm to the output ULDG shown in Figure 2. Phase-I: In this phase, we update participant type 2 Further details are available in Figure A.1 and Table A.2 in the supplementary material. 3 65 We use Stanford CoreNLP Toolkit (Manning et al., 2014) Predicates N ET ype(x, y) CopulaConnect(x, y) Description y is entity type of participant x Participants x and y are connected through a copula verb or a “copula-like” verb in Ed (e.g., become) Conj(x, y) Participants x and y are connected by a conjunction in Ed Dif f V erbConnect(x, y) Participants x and y are connected through a “differentiating” verb or a copula-like verb in Ed (e.g. tell) LexSim(x, y) Participants x and y are lexically similar, i.e. having low edit distance Alias(x, y) Participants x and y are aliases of each other (used as a query predicate) Hard rules Description Alias("
P18-2011,L16-1695,0,0.0170009,"Table 2: MLN Predicates and Rules of headword h of a generic NP if its WordNet hypernyms contain PER/ORG/LOC indicating synsets. We also add new Ep edges from h to dependent nodes of h using dependency relations compound, amod or det (de Marneffe et al., 2014) to get corresponding mention boundaries. The function resolveP articipantT ypeConf lict() ensures that participant types of all nodes in a single clique in Ea are same by giving higher priority to NER-induced type than WordNet-induced type. Phase-II: In this phase, we encode linguistic rules in MLN to add new Ea edges. As elaborated by Mojica and Ng (2016), MLN gives the benefits of (i) ability to employ soft constraints, (ii) compact representation, and (iii) ease of specification of domain knowledge. The predicates and key first-order logic rules are described in Table 2. Here, Alias(x, y) is the only query predicate. Others are evidence predicates, whose observed groundings are specified using G. As we use a combination of hard rules (i.e., rules with infinite weight) and soft rules (i.e., rules with finite weights), probabilistic inference in MLN is necessary to get find most likely groundings of the predicate-Alias(x, y). As the goal is to"
P18-2011,K15-1002,0,0.0364984,"Missing"
P18-2011,N15-1082,0,0.0378355,"Missing"
P18-2064,P10-2041,0,0.0684589,"ta to Spanish, the tag distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named e"
P18-2064,D11-1033,0,0.0745883,"distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the trainMu"
P18-2064,Q17-1010,0,0.0265068,"Missing"
P18-2064,D17-1038,0,0.091646,"is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the trainMultilingual learning for"
P18-2064,W02-2024,0,0.0295684,"gineering IIT Bombay, India. ‡ Microsoft AI & Research, Hyderabad, India. {rudra,pb}@cse.iitb.ac.in, ankunchu@microsoft.com Abstract Existing approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data. However, data from assisting languages can introduce a drift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions. For example, the entity China appears in training split of Spanish (primary) and English (assisting) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }. By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introd"
P18-2064,N16-1155,0,0.0520536,"opose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data. 1 Introduction Neural NER trains a deep neural network for the NER task and has become quite popular as they minimize the need for hand-crafted features and, learn feature representations from the training data itself. Recently, multilingual learning has been shown to benefit Neural NER in a resource-rich language setting (Gillick et al., 2016; Yang et al., 2017). Multilingual learning aims to improve the NER performance on the language under consideration (primary language) by adding training data from one or more assisting languages. The neural network is trained on the combined data of the primary (DP ) and the assisting languages (DA ). The neural network has a combination of languagedependent and language-independent layers, and, the network learns better cross-lingual features via these language-independent layers. ∗ This work began when the second author was a research scholar at IIT Bombay 401 Proceedings of the 56th Annual"
P18-2064,D16-1196,1,0.905778,"Missing"
P18-2064,N15-3017,1,0.82717,"ll Sub-word 89.66 89.94 German Data Selection All Primary Assisting Layers Language Language Shared Monolingual None 75.98 - English All Sub-word 76.22 79.44 76.91† 79.44 91.61† 89.10† Spanish All Sub-word 74.94 76.99 76.92† 77.45† 90.85† 90.11 Dutch All Sub-word 75.59 77.38 77.29† 77.56 SKL Italian Data Selection All SKL Table 2: F-Score for German and Italian Test data using Monolingual and Multilingual learning strategies. † indicates that the SKL results are statistically significant compared to adding all assisting language data with p-value &lt; 0.05 using two-sided Welch t-test. 3 brary3 (Kunchukuttan et al., 2015) thereby, allowing sharing of sub-word features across the Indian languages. For Indian languages, the annotated data followed the IOB format. Experimental Setup In this section we list the datasets used and the network configurations used in our experiments. 3.1 Datasets 3.2 The Table 1 lists the datasets used in our experiments along with pre-trained word embeddings used and other dataset statistics. For German NER, we use ep-96-04-16.conll to create train and development splits, and use ep-96-04-15.conll as test split. As Italian has a different tag set compared to English, Spanish and Dutc"
P18-2064,N16-1030,0,0.0699121,"s based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages. We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences. To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER. 2 Network Architecture Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature. Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs. The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models. We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments. Judicious Selection of Assisting Language Sentences For every assisting language sentence, we calculate the sentence score based on the average symmetric KL-Divergence score of o"
P18-2064,P16-1101,0,0.02011,"mple approach to select assisting language sentences based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages. We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences. To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER. 2 Network Architecture Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature. Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs. The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models. We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments. Judicious Selection of Assisting Language Sentences For every assisting language sentence, we calculate the sentence score ba"
P19-1106,D18-2029,0,0.0282081,"Missing"
P19-1106,N18-1149,0,0.161049,"Missing"
P19-1297,D13-1176,0,0.0553978,"nguage participating in training into an interlingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder’s ability to generate interlingual representation. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has become a dominant paradigm for machine translation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently beco"
P19-1297,W17-3204,0,0.0203715,"any-to-many fashion leveraging encoder’s ability to generate interlingual representation. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has become a dominant paradigm for machine translation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et"
P19-1297,P02-1040,0,0.107848,"directions using 4 languages (English, French, German and Spanish) to perform translation in 12 directions. We take En3083 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3083–3089 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics glish as the anchor language and map three nonEnglish languages’ embeddings into the English embedding space. We train the network to denoise all the four languages and back-translate between English and non-English languages. We evaluate on newstest13 and newstest14 using BLEU (Papineni et al., 2002) score. We find that the multilingual model outperforms the bilingual models by up to 1.48 BLEU points. We also find that the network learns to translate between the nonEnglish (French, German and Spanish) language pairs as well even though it does not explicitly see these pairs during training. To translate between a non-English language pair, no modification to the network is required at inference time. We also evaluate the performance of the non-English language pairs and achieve a maximum BLEU score of 13.92. 3 Background In this section, we briefly describe the basic unsupervised NMT mode"
P19-1297,P16-1009,0,0.0874549,"parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et al., 2018) or two (Artetxe et al., 2018) decoders. In supervised NMT settings, combining multiple languages to jointly train an NMT system has been found to be successful in improving the performance (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). However, to the best of our knowledge, this is the very first attempt which aims at combining multiple languages in an unsupervised NMT training. To translate between many language"
P19-1297,P18-1005,0,0.0470953,"anslation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et al., 2018) or two (Artetxe et al., 2018) decoders. In supervised NMT settings, combining multiple languages to jointly train an NMT system has been found to be successful in improving the performance (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). However, to"
P19-1516,P16-2058,0,0.0435355,"Missing"
P19-1516,P15-1033,0,0.012886,"2016). Let us the consider the input sequence to this layer is E = {e1 , e2 , . . . , en }. A convolution operation is performed over the zero-padded sequence E p . Similar to the character embedding, a set of k filter of size m are applied to the sequence. We obtain convoluted features ct at given time t for t = 1, 2, . . . , n. ct = relu(F [et− m−1 . . . et . . . et+ m−1 ]) 2 (3) 2 Then, we generate the feature vectors C 0 = [c01 , c02 . . . c0n ], by applying max pooling on C. Inspired by the success of stacked attentive RNN in solving other NLP tasks (Wu et al., 2016; Graves et al., 2013; Dyer et al., 2015; Prakash et al., 2016), we use the stacked GRU to encode the input text. The stacked GRU is an extension to GRU model that has multiple hidden GRU layers. The purpose of using multiple GRUs layers is to learn more sophisticated conditional distributions from the data (Bahdanau et al., 2015). In this work, we employ vertical stacking strategy where the output of the previous layer of GRU is fed to the highway layer and corresponding output is passed as input to the next layer of GRU. Let the number of layers in stacked GRU is L then the GRU computes the hidden state for each layer l ∈ L as fol"
P19-1516,C16-1084,0,0.12138,"es such as social media, both EMR and medical case reports offer several advantages of having complete records of patients’ medical history, treatment, conditions and the possible risk factors, and is also not restricted to the patients experiencing ADRs (Harpaz et al., 2012b). Recently, a study conducted by (Sarker and Gonzalez, 2015) utilized the data from MEDLINE case reports and Twitter. They proposed several textual features and investigated how the combination of different datasets would increase the performance of identifying ADRs. With the advancement of the neural network technique, (Huynh et al., 2016) investigated multiple neural network (NN) frameworks for ADR classification on both medical case reports and Twitter dataset. (ii) Social Media: Social media offers a very rich and viable source of information for identifying potential ADRs in a real-time. Leaman et al. (2010) conducted very first study utilizing user comments from their social media post. In total, the dataset contains 6, 890 user comments. The research shows that user comments are highly beneficial in uncovering the ADRs. Further works (Gurulingappa et al., 2012b; Benton et al., 2011; Harpaz et al., 2012a) utilized the lexi"
P19-1516,W10-1915,0,0.257382,"al Drug Administration’s Adverse Event Reporting System (FAERS) (Li et al., 2014). These systems are often under-reported, biased and delayed. To overcome the limitation of a passive reporting system, active methods to ADR monitoring continuously explores frequently updated ADR data sources (Behrman et al., 2011). The quantity and near-instantaneous nature of social media provide potential opportunities for real-time monitoring of Adverse Drug Reaction (ADR). The fact that this data is up-to-date and is generated by patients overcomes the weaknesses of traditional ADR surveillance techniques (Leaman et al., 2010). Thus, social media could complement traditional information sources for more effective pharmacovigilance studies, as well as potentially serve as an early warning system for unknown ADR, which may be important for a clinical decision. Additionally, the high statistically significant correlation (p &lt; 0.001, ρ = 0.75) between FAERS and ADRs (extracted through Twitter data) shows that Twitter is a viable pharmacovigilance data source (Freifeld et al., 2014). With the enormous amount of data generated every day, it is desirable to have an automated ADR extraction system that can ease the work of"
P19-1516,P17-1001,0,0.0174851,"order to capture the common features along the task, we utilize the above feature extractor framework which serves as a Generator model and the feed forward neural network as a Discriminator. 3.4 Task Discriminator Layer Our feature extractor layer is generating two types of features, shared and task-specific. Ideally both feature spaces should be mutually exclusive. To ensure that task-specific features of given task do not exist in the shared space, we exploit the concept of adversarial training (Goodfellow et al., 2014) into shared feature space. We follow the same method as introduced by (Liu et al., 2017) to make the shared feature space uncontaminated by the task-specific features. For achieving the aforementioned strategy, a Task Discriminator D is used to map the attention prioritized shared feature to estimate the task of its origin. In our case, Task Discriminator is a fully connected layer using a softmax layer to produce the probability distribution of the shared features belonging to any task. The shared feature extractor (c.f. 3.3) works as Generator (G) to generate shared features. The shared feature extractor is made to work in an adversarial way, preventing the discriminator from p"
P19-1516,C16-1275,0,0.0526184,"Missing"
P19-1516,E17-1014,0,0.0289778,"included in lexicons. With the emergence of annotated data, several research works have employed supervised machine learning techniques such as Support Vector Machine (SVM) (Sarker and Gonzalez, 2015), Conditional Random Field (CRF) (Nikfarjam et al., 2015) and Random Forest (Zhang et al., 2016). In recent years with the introduction of deep learning techniques, most of the studies utilize deep learning model to predict ADRs. Lee et al. (2017) developed semi-supervised deep learning model on the Twitter corpus. In particular, they used the Convolution Neural Network (CNN) for classification. Stanovsky et al. (2017) used the Recurrent Neural Network integrated with knowledge graph embedding on the CADEC corpus. Their study shows that this integration can make the model more accurate. Tutubalina and Nikolenko (2017) explored the combination of CRF and Recurrent Neural Network (RNN). Their 5236 results show that CRF can assist RNN model in capturing the context well. The most relevant work to this study is the work conducted by Chowdhury et al. (2018). They learned jointly for three tasks: binary classification, ADR labeling, and indication labeling using RNN-attentioncoverage model. 3 Methodology With our"
P19-1516,1983.tc-1.13,0,0.193168,"Missing"
P19-1540,W18-6514,0,0.260802,"from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form of knowledge 5439 bases using hierarchical encoder-decoder model. Our work is different from these existing works on MMD dataset in the sense that we incorporate position and attribute aware attention mechanism for capturing ordered information and minute details such as colour, style etc. from the image representations for more accurate response generation. Our method, unlike the previous works, make use of the MFB technique for better information fusion across different modalities. The approach that we propose to capture and i"
P19-1540,W18-5709,0,0.462084,"from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form of knowledge 5439 bases using hierarchical encoder-decoder model. Our work is different from these existing works on MMD dataset in the sense that we incorporate position and attribute aware attention mechanism for capturing ordered information and minute details such as colour, style etc. from the image representations for more accurate response generation. Our method, unlike the previous works, make use of the MFB technique for better information fusion across different modalities. The approach that we propose to capture and i"
P19-1540,P19-1648,0,0.140656,"urse-level information in the encoder. Our current work differentiates from these existing works in dialogue systems in a way that we generate the appropriate responses by capturing information from both the text and image, conditioned on the conversational history. 2.2 Multimodal Dialogue Systems With the recent shift in interdisciplinary research, dialogue systems combining different modalities (text, images, video) have been investigated for creating robust conversational agents. Dialogue generation combining information from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form"
P19-1540,W18-5712,0,0.202564,"Missing"
P19-1540,W07-0734,0,0.0236998,"s is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with respect to the following three metrics: 1. Fluency (F): The ge"
P19-1540,N16-1014,0,0.0249704,"osition and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al.; Shen et al., 2018) that generate diverse replies by capturing discourse-level information in the encoder. Our current work differentiates from these existing works in dialogue sy"
P19-1540,P16-1094,0,0.0176738,"osition and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al.; Shen et al., 2018) that generate diverse replies by capturing discourse-level information in the encoder. Our current work differentiates from these existing works in dialogue sy"
P19-1540,W04-1013,0,0.0245465,"size for all the layers is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with respect to the following"
P19-1540,D15-1166,0,0.072908,"A pre-trained VGG-19 model (Simonyan and Zisserman, 2014) is used to extract image features for all the images in a given dialogue turn. The concatenation of single image features is given as input to a single linear layer to obtain a global image context representation. In this section we firstly define the problem and then present the details of the proposed method. 3.1 (1) (7) Decoder: In the decoding stage, the decoder is another GRU that generates words sequentially conditioned on the final hidden state of the context GRU and the previously decoded words. Attention mechanism similar to (Luong et al., 2015) is incorporated to enhance the performance of the decoder GRU. The attention layer is applied to the hidden state of context encoder using decoder state dt as the query vector. The concatenation of the context vector and the decoder state is used to compute a final probability distribution over the output tokens. hd,t = GRUd (yk,t−1 , hd,t−1 ) (8) 5440 αt,m = sof tmax(hTc,m W hd,t ) (10) Attribute-aware Attention: To focus on different attributes of the image mentioned in the text, we employ attribute-aware attention. ˜ t = tanh(W˜ [hd,t ; ct ]) h h (11) αa = sof tmax(Wa T HU ), Ua = αa HU T"
P19-1540,I17-1047,0,0.378945,"jointly the first authors the information to the user is the primary objective of every response generation module. One of the running goals of AI is to bring language and vision together in building robust dialogue systems. Advances in visual question answering (VQA) (Kim et al., 2016; Xiong et al., 2016; Ben-Younes et al., 2017), and image captioning (Anderson et al., 2018; Chen et al., 2018) have ensured interdisciplinary research in natural language processing (NLP) and computer vision. Recently, several works in dialogue systems incorporating both vision and language (Das et al., 2017a; Mostafazadeh et al., 2017) have shown promising research directions. Goal oriented dialogue systems are majorly based on textual data (unimodal source). With increasing demands in the domains like retail, travel, entertainment, conversational agents that can converse by combining different modalities is an essential requirement for building the robust systems. Knowledge from different modalities carries complementary information about the various aspects of a product, event or activity of interest. By combining information from different modalities to learn better representation is crucial for creating robust dialogue"
P19-1540,P02-1040,0,0.103687,"rot and Bengio, 2010). The hidden size for all the layers is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with resp"
P19-1540,D11-1054,0,0.0484056,"rall system architecture for text generation; Right image is the baseline encoder model Context Encoder MFB Concatenation Position Concatenation Attribute aware image representation Attribute VGG Direction from left to right Position aware image representation VGG PE1 PE2 Figure 3: Proposed Multimodal Encoder with Position and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decode"
P19-1540,P15-1152,0,0.175666,"Missing"
P19-1540,N15-1020,0,0.0604202,"Missing"
P19-1540,P17-1061,0,0.0317097,"Missing"
P19-1540,P17-1018,0,0.0262307,"knowledge of every image with respect to its position is necessary so that the agent can capture the information and fulfill the objective of the customer. The lack of position information of the images in the baseline MHRED model causes quite a few errors in focusing on the right image. To alleviate this issue, we fuse position embedding of every image with the corresponding image features. The position of every image is represented by position embedding P Ei , where, P E = [P E1 , ..., P En0 ]. This information is concatenated to the corresponding image features. To compute self attention (Wang et al., 2017) we represent textual features as HU = [hU,1 , ...., hU,n ]. αp = sof tmax(Wp T HU ), Up = αp HU T (13) βa = sof tmax(Ua T Wa0 HI ), Ia = βa HI T (16) where, Wa T and Wa0 are trainable parameters. Finally, in our proposed model, as shown in Figure 3, we incorporate position-aware and attributeaware attention mechanisms to provide focused information conditioned on the text utterance. We concatenate Ua and Up vectors for the final utterance representations Uf , Ia and Ip vectors as the final image representation If . The output of the context encoder hc along with If and Uf serves as input to t"
Q18-1022,P15-1166,0,0.151875,"dels that are better at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models w"
Q18-1022,2010.iwslt-papers.7,0,0.0294356,"zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration"
Q18-1022,W15-3909,0,0.118921,"discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate trans"
Q18-1022,W16-2711,0,0.0547497,"rections. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify"
Q18-1022,N16-1101,0,0.0730592,"r at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal paramete"
Q18-1022,N16-1155,0,0.0723241,"Missing"
Q18-1022,S17-2033,0,0.030994,"er architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts o"
Q18-1022,D12-1002,0,0.0350666,"Missing"
Q18-1022,P08-1103,0,0.0451685,"tup, results and analysis. Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously"
Q18-1022,W09-3504,0,0.234378,"Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilingua"
Q18-1022,Q17-1024,0,0.446771,"g may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without increasing the model size."
Q18-1022,N10-1065,1,0.79409,"se compatible scripts resulting in a shared vocabulary. We specialize just the output layer for target languages, but share the encoder, decoder and character embeddings across languages. In this respect, we differ from Johnson et al. (2017). They share all network components across languages, but add an artificial token at the beginning of the input sequence to indicate the target language. Zeroshot Transliteration We use the multilingual models to address zeroshot transliteration. Zeroshot transliteration using bridge/pivot language has been explored for statistical machine transliteration (Khapra et al., 2010) as well as neural machine transliteration (Saha et al., 2016). Unlike previous approaches which pivot over bilingual transliteration models, we propose zeroshot transliteration that pivots over multilingual transliteration models. We also propose a direct zeroshot transliteration method, a scenario which has been explored for machine translation by Johnson et al. (2017), but not investigated previously for transliteration. In our zeroshot model, sequences from multiple source languages are mapped to a common encoder representation without the need for a parallel corpus between the source lang"
Q18-1022,P06-1103,0,0.0550269,"nguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2"
Q18-1022,W15-3912,1,0.896917,"Missing"
Q18-1022,N15-3017,1,0.810922,"he parallel corpora are roughly of the same size. Better training schedules could be explored in future. Languages: We experimented with two sets of orthographically similar languages: Indian languages: (i) Hindi (hi), Bengali (bn) from the Indo-Aryan branch of Indo-European family (ii) Kannada (kn), Tamil (ta) from the Dravidian family. We studied Indic-Indic transliteration and transliteration involving a non-Indian language (English↔Indic). We mapped equivalent characters in different Indic scripts in order to build a common vocabulary based on the common offsets of the Unicode codepoints (Kunchukuttan et al., 2015). Slavic languages: Czech (cs), Polish (pl), Slovenian (sl) and Slovak (sk). We studied Arabic↔Slavic transliteration. Arabic is a non-Slavic language (Semitic branch of Afro-Asiatic) and uses an abjad script in which vowel diacritics are omitted in general usage. The languages chosen are representative of languages spoken by some major groups of peoples en-Indic en-hi 12K en-bn 13K en-kn 10K en-ta 10K Indic-en Indic-Indic hi-en 18K bn kn ta bn-en 12K hi 3620 5085 5290 kn-en 15K bn 2720 2901 ta-en 15K kn 4216 ar-Slavic ar-cs 15K ar-pl 15K ar-sl 10K ar-sk 10K Pair Table 1: Training set statisti"
Q18-1022,K16-1027,1,0.859366,"ding the phonetic properties of the character, one bit for each value of every property. The multiplication of the phonetic feature vector with the weight matrix in the first layer generates phonetic embeddings for each character. These are inputs to the encoder. Apart from this input change, the rest of the network architecture is the same as described in Section 3.2. Experiments: We experimented with Indian languages (Indic→English and Indic-Indic transliteration). Indic scripts generally have a one-one correspondence from characters to phonemes. Hence, we use phonetic features described by Kunchukuttan et al. (2016) to generate phonetic feature vectors for characters (available via the Indic NLP Library1 ). These Indic languages are spoken by nearly a billion people and hence the use of phonetic features is useful for many of the world’s most widely spoken languages. Results and Discussion: Table 8 shows the results. We observe that phonetic feature input improves transliteration accuracy for Indic-English transliteration. The improvements are primarily due to reduction in errors related to similar consonants like (T,D), (P,B), (C,K) and the use of H for aspiration. 1 https://github.com/anoopkunchukuttan"
Q18-1022,Q17-1026,0,0.0551909,"multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are bene"
Q18-1022,W12-4810,0,0.0317588,"r its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network f"
Q18-1022,E17-3017,0,0.0348879,"ayer (stride size = 1 and SAME padding), followed by ReLU units and max pooling. We use filters of different sizes and concatenate their output to produce the encoder output. Figure 1b shows a schematic of the encoder. We chose CNN over the conventional bidirectional LSTM layer since the temporal dependencies for transliteration are mostly local, which can be handled by the CNN encoder. We observed that training and decoding are significantly faster, with little impact on accuracy. The decoder contains a layer of LSTM cells and their start state is the average of the encoder’s output vectors (Sennrich et al., 2017). set for English-Hindi, this model was used for reporting test set results for English-Hindi. We observed that this criterion performed better than choosing the model with least validation set loss over all language pairs. Parameter Sharing: The vocabulary of the orthographically similar languages (at input and/or output) is comprised of the union of character sets of all these languages. Since the character set of these languages overlaps to a large extent, we share their character embeddings too. The encoder is shared across all source languages and the decoder is shared across all target l"
Q18-1022,I08-1009,0,0.0326398,"n with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that s"
Q18-1022,P07-1015,0,0.0213722,"n To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2015) or both (Firat"
Q18-1022,D16-1163,0,0.109551,"pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without inc"
R15-1013,C86-1088,0,0.610857,"va et al., 2003). But the POS tagging produced is inaccurate due to noisy text 94 Experiments A B C MUC P 33.61 35.77 38.16 R 37.44 54.78 52.9 F 35.37 43.19 44.0 B3 P 42.72 39.14 40.02 R 50.82 58.78 58.38 F 46.36 46.98 47.44 CEAF-M P R 36.65 52.58 41.86 60.16 40.84 58.73 F 43.18 49.35 48.16 Table 3: Results (P:precision R:recall F:F-measure) Experiments A: SVM without grammatical role features B: SVM with all features C: Bayes network with all features. accuracy. There are instances of deictic phrases, where the phrase refers to an entity outside the scope of mentions defined in the discourse(Pinkal, 1986). Isolation of deictic phrases can alleviate many false alarms. Certain misclassification occurs at the clustering phase, where the wrong antecedent get selected instead of the correct one even when mention pair with the correct mention is classified as coreferent. Some mentions which are supposed to be singleton are clustered with other clusters because of their linkage with one of the mentions in the cluster. coref NE_class both_men_propnoun def_NP no_sent dem_NP m1_subj m1_obj m2_subj ﬁrst_sent root_men_bet str_match m2_obj alias same_sent Figure 1: Bayesian network structure depicting depe"
R15-1013,D09-1101,0,0.0108382,"al and nominal mentions in this forum. Resolution of these coreferences is crucial in increasing recall of relation extraction from forums. Coreference resolution identifies the real world entity, an expression is referring to (Cherry and Bergsma, 2005). Though a widely researched area, coreference resolution will have to be applied differently considering the characteristics of the text in these forums. Forum posts are generally short discourse of text where the entities mentioned are limited to the scope of a few sentences. Supervised approach has been widely used in coreference resolution (Rahman and Ng, 2009; Soon et al., 2001; Aone and Bennett, 1995; McCarthy and Lehnert, 1995). We examine the commonly used conventional features and its variants that suits this domain of text. Soon et al. and Vincent et al. have investigated an exhaustive list of features for coreference resolution. Most of these methods model this problem as classification of mention-pair as coreferent or non-coreferent. Research on coreference resolution for similar domains of text are reported. Ding et al. has discussed features for supervised approach to coreference resolution for opinion mining where the discourse of text i"
R15-1013,P95-1017,0,0.0829582,"esolution of these coreferences is crucial in increasing recall of relation extraction from forums. Coreference resolution identifies the real world entity, an expression is referring to (Cherry and Bergsma, 2005). Though a widely researched area, coreference resolution will have to be applied differently considering the characteristics of the text in these forums. Forum posts are generally short discourse of text where the entities mentioned are limited to the scope of a few sentences. Supervised approach has been widely used in coreference resolution (Rahman and Ng, 2009; Soon et al., 2001; Aone and Bennett, 1995; McCarthy and Lehnert, 1995). We examine the commonly used conventional features and its variants that suits this domain of text. Soon et al. and Vincent et al. have investigated an exhaustive list of features for coreference resolution. Most of these methods model this problem as classification of mention-pair as coreferent or non-coreferent. Research on coreference resolution for similar domains of text are reported. Ding et al. has discussed features for supervised approach to coreference resolution for opinion mining where the discourse of text is short as in forum posts (Ding and Liu, 20"
R15-1013,J01-4004,0,0.195104,"ayesian network has better performance compared to SVM with most of the evaluation metrics. with his excellent compos, where as MD never touched this raga. In Sri ragam we have plenty of compos by the trinity incl the famous Endaro Sri Ranjani is a lovely janya of K Priya with plenty of compos by both T & MD. The presence of a large number of such sentences containing potential relations present, make coreference resolution unavoidable for information extraction from these forums. The process of checking whether two expressions are coreferent to each other is termed as coreference resolution (Soon et al., 2001). The well-known discussion forum on Carnatic music Rasikas.org, is taken for our study. Enrolled with a good number of music loving users, the forum discusses many relevant topics on Carnatic music providing valued information. Sordo et al. evaluated information extraction from the same forum using contextual information (Sordo et al., 2012). Integration of natural language processing methods yields better coverage for the extracted relations. Largely the entities are mentioned using pronominal and nominal mentions in this forum. Resolution of these coreferences is crucial in increasing recal"
R15-1013,W05-0612,0,0.0107704,"loving users, the forum discusses many relevant topics on Carnatic music providing valued information. Sordo et al. evaluated information extraction from the same forum using contextual information (Sordo et al., 2012). Integration of natural language processing methods yields better coverage for the extracted relations. Largely the entities are mentioned using pronominal and nominal mentions in this forum. Resolution of these coreferences is crucial in increasing recall of relation extraction from forums. Coreference resolution identifies the real world entity, an expression is referring to (Cherry and Bergsma, 2005). Though a widely researched area, coreference resolution will have to be applied differently considering the characteristics of the text in these forums. Forum posts are generally short discourse of text where the entities mentioned are limited to the scope of a few sentences. Supervised approach has been widely used in coreference resolution (Rahman and Ng, 2009; Soon et al., 2001; Aone and Bennett, 1995; McCarthy and Lehnert, 1995). We examine the commonly used conventional features and its variants that suits this domain of text. Soon et al. and Vincent et al. have investigated an exhausti"
R15-1013,W08-1301,0,0.101792,"Missing"
R15-1013,N03-1033,0,0.0477462,"and the posts in the thread discuss the title of the thread. Table 2 shows statistics of annotated forum posts. The annotated data is made available in CoNLL format. Test CoNLL files for validation are also created from the same content by automating mention detection. 4.2 Evaluation Mention Detection Mention detection identifies entity boundaries. A rule based chunker is deployed to extract mentions limiting the extraction to predefined partof-speech tag patterns which are identified from observations on annotated mentions. We depend on Stanford POS tagger for getting POS tags of the corpus(Toutanova et al., 2003). But the POS tagging produced is inaccurate due to noisy text 94 Experiments A B C MUC P 33.61 35.77 38.16 R 37.44 54.78 52.9 F 35.37 43.19 44.0 B3 P 42.72 39.14 40.02 R 50.82 58.78 58.38 F 46.36 46.98 47.44 CEAF-M P R 36.65 52.58 41.86 60.16 40.84 58.73 F 43.18 49.35 48.16 Table 3: Results (P:precision R:recall F:F-measure) Experiments A: SVM without grammatical role features B: SVM with all features C: Bayes network with all features. accuracy. There are instances of deictic phrases, where the phrase refers to an entity outside the scope of mentions defined in the discourse(Pinkal, 1986). I"
R15-1013,C10-1031,0,0.0164828,"nd Bennett, 1995; McCarthy and Lehnert, 1995). We examine the commonly used conventional features and its variants that suits this domain of text. Soon et al. and Vincent et al. have investigated an exhaustive list of features for coreference resolution. Most of these methods model this problem as classification of mention-pair as coreferent or non-coreferent. Research on coreference resolution for similar domains of text are reported. Ding et al. has discussed features for supervised approach to coreference resolution for opinion mining where the discourse of text is short as in forum posts (Ding and Liu, 2010). Hendrickx et al. experimented their coreference resolution 2 Knowledge Source for Coreference Resolution Features are computed for a mention pair comprising of potential antecedent mention and anaphoric mention. We make use of a subset of conventional features including the features described in (Soon et al., 2001). String matching (STR MATCH) and alias (ALIAS) features check for compatibility between the mention with regard to string similarity. These features depend on fuzzy string match92 ing to bypass spelling differences. Same sentence (SAME SENT) feature checks if both the mentions are"
R15-1013,uryupina-2006-coreference,0,0.0182941,"comments and blog data targeting opinion mining (Hendrickx and Hoste, 2009). Coreference resolution in this domain is restricted to resolve coreferential relations between entities within a discourse of a post. We follow a supervised approach with mention-pair model, learning to identify two mentions are coreferent or not. Mention pairs are constructed from the annotated mentions from the posts. Along with standard set of proven features, grammatical role features and its proposed variants are found to contribute to increase in accuracy. Grammatical role features (Kong et al., 2010; Ng, 2007; Uryupina, 2006) extracted from the dependency parse are intended to capture the characteristics of the human process of coreference resolution, getting the grammatical role of a mention in the corresponding sentence and thus obtaining the relation between the mentions in the pair. Semantic compatibility is a crucial feature in coreference resolution, exploiting named entity (NE) class of mentions. To satisfy the requirements of our domain, NE classes are extended to raga, music concept, music instrument, song. We have analyzed the importance of dependency parse based grammatical role features, its variants a"
R15-1013,C10-1068,0,0.0239488,"in news paper articles, user comments and blog data targeting opinion mining (Hendrickx and Hoste, 2009). Coreference resolution in this domain is restricted to resolve coreferential relations between entities within a discourse of a post. We follow a supervised approach with mention-pair model, learning to identify two mentions are coreferent or not. Mention pairs are constructed from the annotated mentions from the posts. Along with standard set of proven features, grammatical role features and its proposed variants are found to contribute to increase in accuracy. Grammatical role features (Kong et al., 2010; Ng, 2007; Uryupina, 2006) extracted from the dependency parse are intended to capture the characteristics of the human process of coreference resolution, getting the grammatical role of a mention in the corresponding sentence and thus obtaining the relation between the mentions in the pair. Semantic compatibility is a crucial feature in coreference resolution, exploiting named entity (NE) class of mentions. To satisfy the requirements of our domain, NE classes are extended to raga, music concept, music instrument, song. We have analyzed the importance of dependency parse based grammatical ro"
R15-1013,P05-1020,0,0.0294517,"nd statistical approaches. A basic network structure is made use as described in fig 1. We conducted 5-fold cross validation. As the mentions identified through automated mention detection are different from the annotated mentions, the train and test CoNLL content are different in terms of mention boundaries. Still during cross validation the posts considered for training are not included in the testing fold. During 5-fold validation the test mention pairs are classified as coreferent/not coreferent, which are then clustered to form the resultant CoNLL output. We applied best-first clustering(Ng, 2005), where the mention with highest likelihood value is selected as antecedent for an anaphoric mention. Ablation testing is employed to find weakly perTable 2: Details of annotated posts. (#Posts= No. of posts #Sent= No. of sentences in the forum. #M= No. of annotated mentions #P= positive mention pairs formed #N= negative mention pairs formed) The corpus contains coreference annotated forum posts from 2 forums in rasikas.org. Raga & Alapana has discussions about Carnatic ragas and related concepts and Vidwans & Vidushis discusses about Carnatic artistes. Each thread has a title and the posts in"
R15-1013,P07-1068,0,0.0821951,"les, user comments and blog data targeting opinion mining (Hendrickx and Hoste, 2009). Coreference resolution in this domain is restricted to resolve coreferential relations between entities within a discourse of a post. We follow a supervised approach with mention-pair model, learning to identify two mentions are coreferent or not. Mention pairs are constructed from the annotated mentions from the posts. Along with standard set of proven features, grammatical role features and its proposed variants are found to contribute to increase in accuracy. Grammatical role features (Kong et al., 2010; Ng, 2007; Uryupina, 2006) extracted from the dependency parse are intended to capture the characteristics of the human process of coreference resolution, getting the grammatical role of a mention in the corresponding sentence and thus obtaining the relation between the mentions in the pair. Semantic compatibility is a crucial feature in coreference resolution, exploiting named entity (NE) class of mentions. To satisfy the requirements of our domain, NE classes are extended to raga, music concept, music instrument, song. We have analyzed the importance of dependency parse based grammatical role feature"
R15-1013,D08-1067,0,\N,Missing
S10-1028,J93-2003,0,0.0208032,"Missing"
S10-1028,2005.mtsummit-papers.11,0,0.04577,"Missing"
S10-1028,S10-1003,0,0.35227,"Missing"
S10-1028,H05-1052,0,0.0302836,"d in both the cases the word bus appears in the context. Hence, the surface similarity (i.e., word-overlap count) of S1 and S2 would be higher than that of S1 and S3 and S2 and S3 . This highlights the strength of overlap based approaches – frequently co-occurring words can provide strong clues for identifying similar usage patterns of a word. Next, consider the following two occurrences of the word coach: Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD approaches which use Wordnet based semantic si"
S10-1028,C96-1005,0,\N,Missing
S10-1028,W09-2413,0,\N,Missing
S10-1028,J03-1002,0,\N,Missing
S10-1028,P96-1006,0,\N,Missing
S10-1094,H05-1053,0,0.0463367,"thm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based 421 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics sentence. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. The remainder of this paper is organized as follows. In section 2 we describe related work on domain-specific WSD. In section 3 we discuss an Iterative Word Sense Disambiguation algorithm which lies at the"
S10-1094,J07-4005,0,0.373877,"e disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based 421 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics sentence. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. The remainder of this paper is organized as follows. In section 2 we describe related work on domain-specific WSD. In section 3 we discuss an Iterative Word Sense Disambiguation algo"
S10-1094,H93-1061,0,0.793627,"ted using an iterative disambiguation process by considering only those candidate synsets which appear in the top-k largest connected components. Our knowledge based approach performed better than current state of the art knowledge based approach (Agirre et al., 2009). Also, the precision was better than the Wordnet first sense baseline even though the F-score was slightly lower than the baseline. The second approach is a weakly supervised approach which uses a few hand labeled examples for the most frequent words in the target domain in addition to the publicly available mixed-domain SemCor (Miller et al., 1993) corpus. The underlying assumption is that words exhibit “One Sense Per Domain” phenomenon and hence even as few as 5 training examples per word would be sufficient to identify the predominant sense of the most frequent words in the target domain. Further, once the most frequent words have been disambiguated using the predominant sense, they can provide strong clues for disambiguating other words in the We describe two approaches for All-words Word Sense Disambiguation on a Specific Domain. The first approach is a knowledge based approach which extracts domain-specific largest connected compon"
S10-1094,P96-1006,0,0.201004,"Missing"
S10-1094,S10-1013,0,\N,Missing
S10-1094,P07-1007,0,\N,Missing
S13-2082,esuli-sebastiani-2006-sentiwordnet,0,0.0820085,"Missing"
S13-2082,N03-1030,0,0.134108,"Missing"
S13-2082,W06-1317,0,0.0408237,"Missing"
S13-2082,D11-1015,0,0.0171598,"of a sentence. For example, the movie had quite a few memorable moments but I still did not like it. The overall polarity of the sentence is negative even though it has one positive and one negative clause. This is because of the presence of the conjunction but which gives more weightage to the clause following the conjunction. Traditional works in discourse analysis use a discourse parser (Marcu et al., 2003; Polanyi et al., 2004; Wolf et al., 2005; Welner et al., 2006; Narayanan et al., 2009; Prasad et al., 2010). Many of these works and some other works in discourse (Taboada et al., 2008; Zhou et al., 2011) build on the Rhetorical Structure Theory (RTS) proposed by Mann et al. (1988) which tries to identify the relations between the nucleus 1. Micro-blogs like Twitter restricts a post (tweet) to be of only 140 characters. Thus, users do not use formal language to discuss their views. Thus, there are abundant spelling mistakes, abbreviations, slangs, collocations, discontinuities and grammatical errors. These differences cause NLP tools like POStaggers and parsers to fail frequently, as these tools are built for well-structured text. Thus, most of the methods described in the previous works are n"
S13-2082,D11-1100,1,\N,Missing
S13-2082,J05-2005,0,\N,Missing
S13-2082,D09-1019,0,\N,Missing
S13-2082,C12-1113,1,\N,Missing
S13-2082,P11-4022,1,\N,Missing
S17-2009,P05-1045,0,0.0125919,"inding the focus of the question and comment is important in measuring if the comment specifically covers the aspects of the question. We extract keywords from the texts using the RAKE keyword extraction algorithm (Rose et al., 2010), and derive features from the keyword match between question and comment. We also use the relative importance of common keywords as feature values. In case of factoid questions, or especially in subtask B, Named Entity Recognition becomes an important tool for computing the relevance of a text. We extract named entities using the Stanford Named Entity Recognizer (Finkel et al., 2005) and classify words into seven entity categories including PERSON, LOCATION, ORGANIZATION, DATE, MONEY, PERCENT, and TIME. We compute if both question and comment have named entities, and if these belong to the same The algorithm to construct this dynamic graph is given in Algorithm 1. We simultaneously construct two graphs, a user graph and a dialogue graph. Initially, the user graph has the question node and the dialogue graph is empty. We add new users to the graphs according to the timestamp of their occurrence in the thread. For each new comment, we add edges to each previous user and the"
S17-2009,S16-1137,0,0.0215787,"Missing"
S17-2009,P15-2113,0,0.0524853,"Missing"
S17-2009,S15-2048,0,0.0400066,"Missing"
S17-2009,S17-2003,0,0.0764522,"Missing"
S17-2009,S16-1172,0,0.0219561,"Missing"
S17-2009,S15-2036,0,0.134421,"Missing"
S17-2009,S16-1135,0,0.0366377,"Missing"
S17-2087,C00-1044,0,0.485514,"Missing"
S17-2087,P16-2064,0,0.0642479,"(Castillo et al., 2011; Derczynski and Bontcheva, 2014; Qazvinian et al., 2011). Rumours are the statement which cannot be verified for its correctness. These rumours may confuse people with the unverified information and drive them in poor decision making. In many organizations(political, administration etc.), detection and support for rumour invites great interest from the concerned authorities. Recently, researchers across the globe have started addressing the challenges related to rumours. A time sequence classification technique has been proposed for detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in treestructured conversations for classifying stance. A 2 System Overview We adopted a supervised classification approach for both the tasks. We use Decision Tree (DT), Naive Bayes (NB) and Support Vector Machine 497 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 497–501, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Tweet conversation thread Src: Very good on #Putin coup by @CoalsonR: Three Scenarios For A Succession In Russia http://t.co/fotdqxD"
S17-2087,H05-1044,0,0.0810695,"Missing"
S17-2087,C16-1230,0,0.0121576,"Derczynski and Bontcheva, 2014; Qazvinian et al., 2011). Rumours are the statement which cannot be verified for its correctness. These rumours may confuse people with the unverified information and drive them in poor decision making. In many organizations(political, administration etc.), detection and support for rumour invites great interest from the concerned authorities. Recently, researchers across the globe have started addressing the challenges related to rumours. A time sequence classification technique has been proposed for detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in treestructured conversations for classifying stance. A 2 System Overview We adopted a supervised classification approach for both the tasks. We use Decision Tree (DT), Naive Bayes (NB) and Support Vector Machine 497 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 497–501, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Tweet conversation thread Src: Very good on #Putin coup by @CoalsonR: Three Scenarios For A Succession In Russia http://t.co/fotdqxDfEV Rep1: @andersostlun"
S17-2087,D11-1147,0,\N,Missing
S17-2153,baccianella-etal-2010-sentiwordnet,0,0.145786,"Missing"
S17-2153,P13-2005,0,0.0331544,"mic system of a country. Therefore, a reliable and prompt delivery of information plays an important role in the financial market. Up until the last decade printed/television news were the major source of stock marketrelated information. However, with the introduction of micro-blogging websites (e.g. Twitter etc.) the trend has been shifted. The rise of Twitter and StockTwits has given the people and organizations an opportunity to vent out their feelings and views. This information can be used by an individual or an organization to make an informed prediction related to any company or stock (Si et al., 2013). This opens a new avenue for sentiment analysis in the financial domain of microblogs and news. News headlines are a short piece of text describing the nature of an article. Due to space constraints, headlines normally follow a compact writing style, known as headlinese, which limits 894 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 894–898, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Track 1 Message: Score: Span: Cashtag: Track 2 Message: Score: Company: Microblogs Putting on a little $F short, prevail"
S17-2153,S17-2089,0,0.0852026,"Missing"
S17-2153,P97-1023,0,0.632546,"Missing"
S17-2153,J09-3003,0,0.44437,"Missing"
S17-2153,S13-2053,0,0.0274224,"lysis (Khanarian and Alwarez-Melis, 2012). Moreover, each tweet can have reference to multiple company names (or stock symbols) and the expressed sentiment can be different towards different companies. Hence, there is a need to perform fine-grained sentiment analysis wherein, generally, a context is used to decide the relevant portion of a tweet for a particular company. Another inherent challenge with the microblog and news data is the use of short languages, hashtag, emoticons and embedded URL. Special attention should be given to these as they can provide some important hidden information (Mohammad et al., 2013). Example - #bullishMarket and #increasingProfit can reflect positive sentiment. These are some of the major challenges associated with fine-grained sentiment analysis of microblogging and news data. The SemEval-2017 task 5 (Fine-Grained Sentiment Analysis on Financial Microblogs and News) has two tracks (Cortis et al., 2017). For both the tracks, the overall aim was to assign a sentiment score to a cashtag/company over a continuous range of -1 (very negative/bearish) to 1 (very positive/bullish). First track involves finding a sentiment score towards a given ‘cashtag’ (stock symbol preceded b"
S17-2154,H05-1044,0,0.0475796,"c features. We trained a multilayer perceptron on top of the following features. • Character ngrams: tf-idf weighted counts of continuous sequences of 2, 3, and 4 characters; • Word ngrams: tf-idf weighted counts of continuous sequences of 1, 2, 3, and 4 words; • POS-tag: parts of speech tags of each token in the text; • Lexicons: – Following set of features are used for each of the four lexicons: Opinion Lexicon (Liu et al., 2005), Loughran and McDonald Sentiment Word Lists (Loughran and McDonald, 2011), MPQA Lexicon [+1.0 for strong positive, +0.5 for weak positive, similarly for negative] (Wilson et al., 2005) and Harvard’s General Inquirer (Stone et al., 1962): ∗ positive count: number of positive tokens in a tweet/title. Convolutional Neural Network (CNN) Convolutional neural network consists of one or more convolution and pooling layers followed by one or more dense layers. Our system uses 2 convolution layers followed by a max pool layer, 2 dense layers and an output layer. Size of convolution filters dictates the hidden features to be ex1 Multilayer Perceptron (MLP) - Vector Averaging Model Concatenation of word vectors for generating sentence embeddings often face the curse of highdimensional"
S17-2154,S13-2053,0,0.0241879,"Here we discuss second stage of the our proposed system. We merge predicted sentiment scores of all four models (CNN, LSTM, Vector Averaging, Feature Driven) to create a new feature vector, and then fed it into a multilayer perceptron (MLP) network for training. Figure 1 shows, an overall schema of the proposed approach. ∗ negative count: number of negative tokens in a tweet/title. ∗ net count: positive count - negative count in tweet/title. – In addition we use four NRC Lexicons: Hashtag Context, Hashtag Sentiment, Sentiment140, Sentiment140 Context (Svetlana Kiritchenko and Mohammad, 2014; Mohammad et al., 2013) for the microblogs messages. Following set of features are extracted for each of them: ∗ positive count, negative count and net count. ∗ sum of positive scores, negative scores and all scores. ∗ maximum of positive and negative scores. Figure 1: Ensembling Network Structure • Pointwise Mutual Information (PMI): We calculate a sentiment score for each term in our training corpus to get the association of each term with positive as well as negative sentiment. 3 3.1 PMI is calculated as follows:f req(w, pos) ∗ N f req(w) ∗ f req(pos) In the above equation f req(w, pos) is the frequency of word w"
S17-2154,D14-1162,0,0.0818328,"e curse of highdimensionality. In an attempt to get a constant low-dimensional feature vector we employ vector averaging technique for producing sentence vector. We perform an element wise averaging of the word vectors in a tokenized tweet/headline. We then use the sentence embeddings to train a 3layered neural network for the prediction. Word embeddings are generally helpful in many natural processing tasks due to it’s excellence in capturing hidden semantic structures. For word embeddings we used two pre-trained embedding models: GloVe1 and Word2Vec2 . For microblogs messages we used GloVe (Pennington et al., 2014) and Word2Vec (Godin et al., 2015) twitter model trained on 2 billion and 400 million tweets respectively. For news headline we used GloVe common crawl model trained on 802 billion words and Word2Vec Google News model (Mikolov et al., 2013). We experimented with 200, 300 and 400 dimension vectors and observed that 200 & 300 dimension vectors are the near-optimal case for microblogs messages and news headlines respectively. We have used concatenation of word embeddings to form sentence embeddings. 2.2 Long Short Term Memory (LSTM) http://nlp.stanford.edu/projects/glove/ https://code.google.com/"
sankaran-etal-2008-common,I08-7013,1,\N,Missing
U16-1013,W14-2608,0,0.0180467,"example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classe"
U16-1013,W10-2914,0,0.0204562,"hand, sarcasm is a form of verbal irony that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is"
U16-1013,P11-2102,0,0.0231454,"that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we i"
U16-1013,P15-2124,1,0.872736,"new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classes are similar (where"
U16-1013,D16-1104,1,0.796821,"r sarcasm versus irony. 2 Related Work Several approaches have been proposed for sarcasm detection, with context incongruity as the basis of sarcasm detection. Joshi et al. (2015) present features based on explicit and implicit incongruity for sarcasm detection. Maynard and Greenwood (2014) use contrasting sentiment between hashtag and text of a tweet as an indicator of sarcasm. Davidov et al. (2010) rely on Wallace and Do Kook Choe (2014) use properties of reddit comments to add contextual information. Recent work uses deep learning-based techniques for sarcasm detection (Poria et al., 2016; Joshi et al., 2016). The work closest to ours is by Ling and Klinger Aditya Joshi, Vaibhav Tripathi, Pushpak Bhattacharyya, Mark Carman, Meghna Singh, Jaya Saraswati and Rajita Shukla. 2016. How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from English Literature. In Proceedings of Australasian Language Technology Association Workshop, pages 123−127. Original Labels A1 Sarcasm Philosophy Irony Cannot say Original Labels Sarcasm Philosophy Irony 27 5 2 13 18 222 7 24 6 17 12 14 A3 Table 1: Confusion matrix for Annotator 1 A2 Sarcasm Philosophy Irony Cannot say Philosophy Irony 30 9"
U16-1013,W07-0101,0,0.0607824,"result1 . On the other hand, sarcasm is a form of verbal irony that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm ver"
U16-1013,W13-1104,0,0.0130965,"tures for the task. However, our analysis from both human and computational perspectives is novel, along with our observations. Another novelty of this work is the domain of our dataset. Majority of the past works in sarcasm detection use tweets. Riloff et al. (2013a) and Maynard and Greenwood (2014) label these tweets manually whereas Bamman and Smith (2015) and Davidov et al. (2010) rely on hashtags to produce annotations. Some works in the past also explore long text for the task of sarcasm detection. Wallace and Do Kook Choe (2014) download posts from Reddit 2 for irony detection, whereas Lukin and Walker (2013) work with reviews. One past work by Tepperman et al. (2006) performs sarcasm detection on spoken dialogues as well. There are past works using literary quotes corpora for a variety of other NLP problems. Elson and McKeown (2010) extract quotes from popular literary work, for the task of quote attribution3 . Søgaard (2012) perform the task of detecting famous quotes in literary works, gathered from the Gutenberg Corpus. Skabar and Abdalgader (2010) cluster famous quotations by improving sentence similarity measurements. This dataset consists of quotes from a quotes website. In terms of a datas"
U16-1013,maynard-greenwood-2014-cares,0,0.275207,"e disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classes are similar (where sarcasm is hurtful/contemptuous). In case of sarcasm versus phil"
U16-1013,C16-1151,0,0.0193489,"y but not as much for sarcasm versus irony. 2 Related Work Several approaches have been proposed for sarcasm detection, with context incongruity as the basis of sarcasm detection. Joshi et al. (2015) present features based on explicit and implicit incongruity for sarcasm detection. Maynard and Greenwood (2014) use contrasting sentiment between hashtag and text of a tweet as an indicator of sarcasm. Davidov et al. (2010) rely on Wallace and Do Kook Choe (2014) use properties of reddit comments to add contextual information. Recent work uses deep learning-based techniques for sarcasm detection (Poria et al., 2016; Joshi et al., 2016). The work closest to ours is by Ling and Klinger Aditya Joshi, Vaibhav Tripathi, Pushpak Bhattacharyya, Mark Carman, Meghna Singh, Jaya Saraswati and Rajita Shukla. 2016. How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from English Literature. In Proceedings of Australasian Language Technology Association Workshop, pages 123−127. Original Labels A1 Sarcasm Philosophy Irony Cannot say Original Labels Sarcasm Philosophy Irony 27 5 2 13 18 222 7 24 6 17 12 14 A3 Table 1: Confusion matrix for Annotator 1 A2 Sarcasm Philosophy Irony Cannot say P"
U16-1013,D13-1066,0,0.12887,"Missing"
U16-1013,P14-2084,0,0.573573,"Missing"
W03-1201,W96-0213,0,\N,Missing
W09-3518,kang-choi-2000-automatic,0,0.0445592,"Missing"
W09-3518,W98-1005,0,0.118541,"Missing"
W09-3518,P00-1056,0,0.0933892,"Missing"
W09-3518,J93-2003,0,0.00885606,"ied 3000 words from the training set into words of Indic origin and Western origin. These words were used as seed input for the bootstrapping algorithm described below: P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) Z(X) (1) where, X = source word (English) Y = target word (Hindi, Kannada) T = length of source word (English) K = number of f eatures λk = f eature weight Z(X) = normalization constant CRF++2 which is an open source implementation of CRF was used for training and decoding. GIZA++ (Och and Ney, 2000), which is a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for English-Hindi word pairs in the training data. Under this alignment, each character in the English word is aligned to zero or more characters in the corresponding Hindi word. The following features are then generated using this character-aligned data (here ei and hi are the characters at position i of the source word and target word respectively): 1. Build two n-gram language models: one for the already classified names of Indic origin and another for the names of Western origin. Here, by n-gram we mean n-character obtained by splitting the words"
W09-3518,2003.mtsummit-papers.17,0,0.068071,"Missing"
W09-3518,C00-1056,0,0.0674311,"Missing"
W09-3518,I08-6006,0,0.0296835,"Missing"
W09-3518,C02-1099,0,0.0528584,"Missing"
W09-3518,P02-1051,0,\N,Missing
W09-3518,W09-3502,0,\N,Missing
W09-3518,P97-1017,0,\N,Missing
W09-3536,W04-1613,0,0.210166,"Missing"
W09-3536,C08-1068,1,0.795735,"Missing"
W09-3536,W98-1005,0,\N,Missing
W09-3536,J98-4003,0,\N,Missing
W10-2418,M98-1018,0,0.496585,"Missing"
W10-2418,W95-0110,0,0.248333,"Missing"
W10-2418,N01-1004,0,0.0198356,"d word is low, we tag the cluster as NE and add the words in that set as NEs. Also, if most of the words in the cluster have higher rank i.e. lower information content, we remove it from the NE set. S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI - measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new criteria which measures the ratio of the total number of times the wo"
W10-2418,W99-0613,0,0.0298092,"tification Combines the global characteristics derived using NGI with S-MEMM CLGIN Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs where spellings and POS tags are not correct. Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pas¸ca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. Klementiev and Roth (2006) use cooccurrence of sets of terms within documents to boost the certainty (in a cross-lingual setting) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of su"
W10-2418,N09-1032,0,0.0260651,"Missing"
W10-2418,I08-5004,0,0.0163015,"ike person, location, organization, etc. It has many applications in Natural Language Processing (NLP) NER can be divided into two sub-tasks, Named Entity Identification (NEI) and Named Entity Classification (NEC). In this paper, we focus on the first step, i.e., Named Entity Identification. NEI is useful in applications where a list of Named Entities (NEs) is required. Machine Translation needs identification of named entities, so that they can be transliterated. For Indian languages, it is tough to identify named entities because of the lack of capitalization. Many approaches based on MEMM (Saha et al., 2008b), CRFs (Li and McCallum, 2003) and hybrid models have been tried for Hindi Named Entity Recognition. These approaches use only the local context for tagging the text. Many ap• Developing an approach of harnessing the global characteristics of the corpus for Hindi Named Entity Identification using information measures, distributional similarity, lexicon, term co-occurrence and language cues • Demonstrating that combining the global characteristics with the local contexts improves the accuracy; and with a very significant amount when the train and test corpus are not from same domain or simila"
W10-2418,I08-1045,0,0.015392,"ike person, location, organization, etc. It has many applications in Natural Language Processing (NLP) NER can be divided into two sub-tasks, Named Entity Identification (NEI) and Named Entity Classification (NEC). In this paper, we focus on the first step, i.e., Named Entity Identification. NEI is useful in applications where a list of Named Entities (NEs) is required. Machine Translation needs identification of named entities, so that they can be transliterated. For Indian languages, it is tough to identify named entities because of the lack of capitalization. Many approaches based on MEMM (Saha et al., 2008b), CRFs (Li and McCallum, 2003) and hybrid models have been tried for Hindi Named Entity Recognition. These approaches use only the local context for tagging the text. Many ap• Developing an approach of harnessing the global characteristics of the corpus for Hindi Named Entity Identification using information measures, distributional similarity, lexicon, term co-occurrence and language cues • Demonstrating that combining the global characteristics with the local contexts improves the accuracy; and with a very significant amount when the train and test corpus are not from same domain or simila"
W10-2418,J90-1003,0,\N,Missing
W10-2418,N06-1011,0,\N,Missing
W10-2418,I08-5012,0,\N,Missing
W10-3604,C94-1087,0,0.414765,"t and Adalı (2004) propose a suffix stripping approach for Turkish. The rule based and agglutinative nature of Turkish allows the language to be modeled using FSMs and does not need a lexicon. The morphological analyzer does not face the problem of the changes taking place at morpheme boundaries which is not the case with inflectional languages. Hence, although apprehensible this model is not sufficient for handling the morphology of Marathi. Many morphological analyzers have been developed using the two-level morphological model (Koskenniemi, 1983) for morphological analysis. (Oflazer, 1993; Kim et al., 1994) have been developed using PCKimmo (Antworth, 1991), a morphological parser based on the two-level model. Conceptually, the model segments the word in its constituent parts, and accounts for phonological and orthographical changes within a word. While, the model proves to be very useful for developing the morphological analyzers for agglutinative languages or the languages with very less degree of inflection, it fails to explicitly capture the regularities within and between paradigms present in the inflectional languages. Marathi has a well defined paradigm-based system of inflection. Hence,"
W10-3604,E93-1066,0,0.161052,"e study. Eryiğit and Adalı (2004) propose a suffix stripping approach for Turkish. The rule based and agglutinative nature of Turkish allows the language to be modeled using FSMs and does not need a lexicon. The morphological analyzer does not face the problem of the changes taking place at morpheme boundaries which is not the case with inflectional languages. Hence, although apprehensible this model is not sufficient for handling the morphology of Marathi. Many morphological analyzers have been developed using the two-level morphological model (Koskenniemi, 1983) for morphological analysis. (Oflazer, 1993; Kim et al., 1994) have been developed using PCKimmo (Antworth, 1991), a morphological parser based on the two-level model. Conceptually, the model segments the word in its constituent parts, and accounts for phonological and orthographical changes within a word. While, the model proves to be very useful for developing the morphological analyzers for agglutinative languages or the languages with very less degree of inflection, it fails to explicitly capture the regularities within and between paradigms present in the inflectional languages. Marathi has a well defined paradigm-based system of"
W10-3607,J01-2001,0,0.145497,"for a morphologically rich language is an uphill task considering the different inflectional and morphological variations possible. Purely unsupervised approaches on the Background and Related Work The earliest English stemmer was developed by Julie Beth Lovins in 1968. The Porter stemming algorithm (Martin Porter, 1980), which was published later, is perhaps the most widely used algorithm for English stemming. Both of these stemmers are rule based and are best suited for less inflectional languages like English. A lot of work has been done in the field of unsupervised learning of morphology. Goldsmith (2001, 2006) proposed an unsupervised algorithm for learning the morphology of a language based on the minimum description length (MDL) framework which focuses on representing the data in as compact manner as possible. Creutz (2005, 2007) uses probabilistic maximum a posteriori (MAP) formulation for unsupervised morpheme segmentation. Not much work has been reported for stemming for Indian languages compared to English and other European languages. The earliest work reported by Ramanathan and Rao (2003) used a hand crafted suffix list and performed longest match stripping for building a Hindi stemm"
W10-4011,P10-1137,1,0.853465,"ack documents and (b) Lack of Robustness: due to the inherent assumption in PRF, i.e., relevance of top k documents, performance is sensitive to that of the initial retrieval algorithm and as a result is not robust. Typically, larger coverage ensures higher proportion of relevant documents in the top k retrieval (Hawking et al., 1999). However, some resource-constrained languages do not have adequate information coverage in their own language. For example, languages like Hungarian and Finnish have meager online content in their own languages. Multilingual Pseudo-Relevance Feedback (MultiPRF) (Chinnakotla et al., 2010a) is a novel framework for PRF to overcome the above limitations of PRF. It does so by taking the help of a different language called the assisting language. Thus, the performance of a resource-constrained language could be improved by harnessing the good coverage of another language. MulitiPRF showed significant improvements on standard CLEF collections (Braschler and Peters, 2004) over state-of-art PRF system. On the web, each language has its own exclusive topical coverage besides sharing a large number of common topics with other languages. For example, information about Saudi Arabia gove"
W10-4011,J03-1002,0,0.00249935,"n the languages. In case of French, since some function words like l’, d’ etc., occur as prefixes to a word, we strip them off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically λ and α, and choose the values which give the optimal performance on a given collection. We observe that the optimal parameters γ and β are uniform across collections and vary in the range 0"
W10-4011,2005.mtsummit-papers.11,0,0.00417888,"m off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically λ and α, and choose the values which give the optimal performance on a given collection. We observe that the optimal parameters γ and β are uniform across collections and vary in the range 0.4-0.48. We Source Assist. Langs Langs DE-NL DE-FI NL-ES EN ES-FR ES-FI FR-FI EN-FR NL-DE ES-DE FI FR-DE NL-ES NL-FR"
W11-1717,P04-1035,0,0.0451257,"Missing"
W11-1717,W02-1011,0,0.0174595,"ion is a task under Sentiment Analysis (SA) that deals with automatically tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. Thus, a sentiment classifier tags the sentence ‘The movie is entertaining and totally worth your money!’ in a movie review as positive with respect to the movie. On the other hand, a sentence ‘The movie is so boring that I was dozing away through the second half.’ is labeled as negative. Finally, ‘The movie is directed by Nolan’ is labeled as neutral. For the purpose of this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary classification task for output labels as positive and negative. Lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (Pang and Lee, 2008). In addition to this, there also has been work that identifies the roles of different parts-of-speech (POS) like adjectives in sentiment classification (Pang et al., 2002; Whitelaw et 132 al., 2005). Complex features based on parse trees have been explored for modeling high-accuracy polarity classifiers (Matsumoto et al., 2005). Text parsers have also been found to be helpful in"
W11-1717,N04-3012,0,0.163923,"Missing"
W11-1717,R09-1067,0,0.0401426,"Missing"
W11-1717,P02-1053,0,0.00373858,"entiment Analysis (SA) that deals with automatically tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. Thus, a sentiment classifier tags the sentence ‘The movie is entertaining and totally worth your money!’ in a movie review as positive with respect to the movie. On the other hand, a sentence ‘The movie is so boring that I was dozing away through the second half.’ is labeled as negative. Finally, ‘The movie is directed by Nolan’ is labeled as neutral. For the purpose of this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary classification task for output labels as positive and negative. Lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (Pang and Lee, 2008). In addition to this, there also has been work that identifies the roles of different parts-of-speech (POS) like adjectives in sentiment classification (Pang et al., 2002; Whitelaw et 132 al., 2005). Complex features based on parse trees have been explored for modeling high-accuracy polarity classifiers (Matsumoto et al., 2005). Text parsers have also been found to be helpful in modeling valenc"
W11-1717,P06-1134,0,0.0648413,"Missing"
W11-1717,D09-1020,0,\N,Missing
W11-3001,J01-2001,0,0.190286,"lectional stemmer. Similar lists have been used for the derivational stemmer, in the form of orthographic, suffix-stripping and substitution rules. 2. Background and Related Work The earliest English stemmer was developed by Julie Beth Lovins (1968). The Porter stemming algorithm (Martin Porter, 1980), which was published later, is perhaps the most widely used algorithm for stemming in case of English language. Both of these stemmers are rule-based and are best suited for less inflectional languages like English. A lot of work has been done in the field of unsupervised learning of morphology. Goldsmith (2001) proposed an unsupervised approach for learning the morphology of a language based on the Minimum Description Length (MDL) framework which focuses on representing the data in as compact manner as possible. Not much work has been reported for stemming for Indian languages compared to English and other European languages. The earliest work reported by Ramanathan and Rao (2003) used a hand crafted suffix list and performed longest match stripping for building a Hindi stemmer. Majumder et al. (2007) developed YASS: Yet Another Suffix Stripper which uses a clustering-based approach based on string"
W11-3001,W10-3607,1,0.63119,"Missing"
W12-4906,2012.amta-wptp.1,1,0.81987,"Missing"
W12-5020,W10-3604,1,\N,Missing
W12-5209,P99-1008,0,0.10045,"ow expressive ontologies might be, they are all in fact lexical representations of concepts’. The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques. The problem of ontology learning is well studied for English. However, to the best of our knowledge no such efforts have been made so far for Indian languages. Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques. Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns. Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text. She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like ‘NP0 such as NP1, NP2, . . . ,NPn’. Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text. Heur"
W12-5209,bhattacharyya-2010-indowordnet,1,0.68462,"different partitions. This approach not just reduces the amount of computation required for ontology construction but also provides an additional level of term filtering. This early identification of hierarchy creates a better taxonomic structure and avoids false association between the terms. The proposed approach combines evidences from linguistic patterns and WordNet (Fellbaum, 1998) to detect subsumption relation. The patterns used in the system are generic and can be used across languages. Wordnets of Indian languages are linked with each other and English WordNet through a common index (Bhattacharyya, 2010), which makes it possible to share concept definitions across languages. Following are the major features of the proposed system: • Ontology extraction process is completely unsupervised and does not require any human intervention. • The lexical patterns used in the algorithm are generic and can work for any language. • Proposed graph partition based algorithm not only requires less computation than the existing clustering techniques but also reduces false association between terms. • The proposed system does not require sophisticated NLP techniques such as NER or parser and can be used for re"
W12-5209,P99-1016,0,0.0468601,"ndle (1990) performed semantic clustering to find semantically similar nouns. They calculated the co-occurrence weight for each verb-subject and verb-object pair. Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities. Pereira et al. (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia. Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc. Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology. Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering. Hearst-patterns are used to detect hypernymy relation between similar nouns. In a similar approach, Cimiano et al. (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy. Unlike (Caraballo, 1999), the hypernymy sourc"
W12-5209,W99-0609,0,0.0370853,"noted by Fountain and Lapata (2012), ‘Most of the existing approaches construct flat structure rather than a taxonomy. Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007). In order to handle the above mentioned issues, we propose a graph-based ontology learning algorithm. Our approach is based on the information content of the term. ‘Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy’ (Resnik, 1999). Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term. We divide the initial set of terms into different partitions based on the term frequency and then construct k-partite graph by finding subsumption relation between the terms of different partitions. This approach not just reduces the amount of computation required for ontology construction but also provides an additional level of term filtering. This early identification of hierarchy creates a better taxonomic structure and avoids false association between the terms. The proposed approach combines evi"
W12-5209,N12-1051,0,0.0581533,"ructured text. The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy. Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms. Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks. As noted by Fountain and Lapata (2012), ‘Most of the existing approaches construct flat structure rather than a taxonomy. Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007). In order to handle the above mentioned issues, we propose a graph-based ontology learning algorithm. Our approach is based on the information content of the term. ‘Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy’ (Resnik, 1999). Caraballo and Charniak (1999) have"
W12-5209,N03-1011,0,0.0228212,"ht be, they are all in fact lexical representations of concepts’. The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques. The problem of ontology learning is well studied for English. However, to the best of our knowledge no such efforts have been made so far for Indian languages. Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques. Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns. Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text. She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like ‘NP0 such as NP1, NP2, . . . ,NPn’. Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text. Heuristic approaches rely"
W12-5209,C92-2082,0,0.136933,", ‘No matter how expressive ontologies might be, they are all in fact lexical representations of concepts’. The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques. The problem of ontology learning is well studied for English. However, to the best of our knowledge no such efforts have been made so far for Indian languages. Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques. Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns. Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text. She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like ‘NP0 such as NP1, NP2, . . . ,NPn’. Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car"
W12-5209,P90-1034,0,0.334265,"lations in free text by using frequently occurring patterns like ‘NP0 such as NP1, NP2, . . . ,NPn’. Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text. Heuristic approaches rely on language-specific rules which cannot be transferred from one language to another. Statistical approaches model ontology learning as a classification or clustering problem. Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is ‘similar terms appear in the similar context.’ Hindle (1990) performed semantic clustering to find semantically similar nouns. They calculated the co-occurrence weight for each verb-subject and verb-object pair. Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities. Pereira et al. (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia. Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc. Caraballo ("
W12-5209,P93-1024,0,0.0654429,"cannot be transferred from one language to another. Statistical approaches model ontology learning as a classification or clustering problem. Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is ‘similar terms appear in the similar context.’ Hindle (1990) performed semantic clustering to find semantically similar nouns. They calculated the co-occurrence weight for each verb-subject and verb-object pair. Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities. Pereira et al. (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia. Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc. Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology. Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering. Hearst-patterns are used to detect hypernymy relation between si"
W12-5209,W06-0506,0,0.0170239,"9), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy. Domínguez García et al. (2012) used wikipedia to extract ontology for different languages. Like Cimiano et al. (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet. However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem. We use term frequency to determine the position of a concept in the hierarchy. Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy. Other method similar to our work is proposed in Fountain and Lapata (2012). Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step. However, their approach works with a predefined set of seed terms. Our approach is completely unsupervised and does not require any human intervention or predefined seed terms. Term 77 frequency based partition provides early detection o"
W12-5209,L10-1000,0,\N,Missing
W12-5806,C96-2183,0,\N,Missing
W12-5906,I08-1067,1,0.854885,"ke the decoder’s large search space also limit the possible reorderings that can be searched during decoding. Lexical binary reordering model (Koehn, 2008) proposes a limited reordering model conditioned on the phrases. An alternative approach which has been proposed is to reorder the source language sentence to conform to the target language word order before decoding. The search space for the decoder is thus simplified, and thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings"
W12-5906,P05-1066,0,0.112516,"Missing"
W12-5906,D09-1105,0,0.0196911,"manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space. In Section 2, we discuss our word sequence based reordering model, and how it has been partially cast as a sequence labe"
W12-5906,D11-1045,0,0.28083,"thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space. In Section 2, we discuss"
W12-5906,C04-1073,0,0.0358961,"possible reorderings that can be searched during decoding. Lexical binary reordering model (Koehn, 2008) proposes a limited reordering model conditioned on the phrases. An alternative approach which has been proposed is to reorder the source language sentence to conform to the target language word order before decoding. The search space for the decoder is thus simplified, and thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the p"
W13-3611,W13-1703,0,0.206511,"Missing"
W13-3611,D10-1094,0,\N,Missing
W13-3611,P11-1092,0,\N,Missing
W13-3611,N12-1067,0,\N,Missing
W13-3611,W13-3601,0,\N,Missing
W13-4706,W09-3510,0,0.035093,"Missing"
W13-4706,J07-3002,0,0.0220739,"as Urdu computational linguistics (Zia 1999). Like in Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of"
W13-4706,P09-1105,0,0.0159907,"n Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel rep"
W13-4706,W04-1613,0,0.0374663,"on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel representation in Urdu/PersioArabic script is highly complex and contextsensitive (Hussain 2004; Malik, Boitet and Bhattacharyya 2008; Malik et al. 2009). This highly complex and contextual representation leads to wrong character alignments, highlighted in Table 4. In the second row of Table 4, the Hindi vowel इ [ɪ] is aligned with ALEF ( )اand ZER ( ِ◌) is aligned to NULL. The alignment is not completely incorrect, but the vowel इ [ɪ] must be aligned with both ALEF ( )اand ZER (◌ِ ). Similarly, the Hindi vowel उ [ʊ] must be aligned with ALEF ( )اand PESH ( ُ◌) in the third row. In these examples, one character in Hindi must be aligned with a sequence of characters in Urdu. Intere"
W13-4706,P06-1067,0,0.0251764,"2 for Hindi and 4 for Urdu. Another set of 6 target language models are developed by combining the corresponding word and sentence language models. Target Language Models A target language model Ρ is a probabilistic model that scores the well-formedness of different translation solutions produced by the translation model (Koehn, Och and Marcu 2003; Zens and Ney 2003; Och and Ney 2004; AlOnaizan and Papineni 2006). It generates a probability distribution over possible sequences of words and computes the probability of producing a given word given all the words that precede it in the sentence (Al-Onaizan and Papineni 2006). We developed multiple target language models depending on the type of alignments used in the transliteration models and the target language. We broadly categorize them into word language models and sentence language models, discussed below. 3.3.1 Sentence Language Models (SLM) Word Language Models (WLM) A word language model is a 6-gram statistical model that gives a probability distribution over possible sequences of characters and computes the probability of producing a given character or cluster C1, given the 5 characters or clusters that precede it in the word. We developed Hindi – Urdu"
W13-4706,J90-2002,0,0.656839,"Missing"
W13-4706,E09-1050,0,0.0389601,"Missing"
W13-4706,J93-2003,0,0.0897074,"Missing"
W13-4706,W09-3522,0,0.068881,"Missing"
W13-4706,J03-1002,0,0.00591785,"he Urdu words that we have generated from the Roman transcriptions from the DSAL dictionary data contain all required diacritical marks, clearly shown in Table 2. Diacritical marks are the back bone of the Urdu vowel system and they are mandatory for the correct pronunciation of an Urdu word, as well as Urdu computational linguistics (Zia 1999). Like in Arabic, diacritical marks are sparingly used in written Urdu (Zia 1999). To model this unfortunate situation, we developed another From these two types of Urdu – Hindi parallel data, we developed two types of character alignments using GIZA++ (Och and Ney 2003) in both directions:  Hindi and Urdu with diacritics character alignment,  Hindi and Urdu without diacritics character alignment. 45 3.1.2 # Sentence pair (167) source length 9 target length 7 alignment score : 3.20243e‐05 Cluster alignments Alignment plays a critical role in SMT (Fraser and Marcu 2007; Kumar, Och and Macherey 2007; Huang 2009). The quality of parallel data and the word alignment have a significant impact on learning the translation model and consequently on the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment"
W13-4706,D07-1005,0,0.0560221,"Missing"
W13-4706,J04-4002,0,0.0575552,"r and then by applying clustering. Finally, we developed character-level and cluster-level Urdu Sentence Language Models using the SRILM toolkit. Similar to word language model, We have developed total 6 sentence language models, 2 for Hindi and 4 for Urdu. Another set of 6 target language models are developed by combining the corresponding word and sentence language models. Target Language Models A target language model Ρ is a probabilistic model that scores the well-formedness of different translation solutions produced by the translation model (Koehn, Och and Marcu 2003; Zens and Ney 2003; Och and Ney 2004; AlOnaizan and Papineni 2006). It generates a probability distribution over possible sequences of words and computes the probability of producing a given word given all the words that precede it in the sentence (Al-Onaizan and Papineni 2006). We developed multiple target language models depending on the type of alignments used in the transliteration models and the target language. We broadly categorize them into word language models and sentence language models, discussed below. 3.3.1 Sentence Language Models (SLM) Word Language Models (WLM) A word language model is a 6-gram statistical model"
W13-4706,W09-3536,1,0.906635,"n the quality of the SMT system (Fraser and Marcu 2007; Huang 2009). It is always better do an analysis of the alignment and correct the alignment errors to reduce the Alignment Error Rate (AER). We also analyzed the alignments produced by GIZA++. We found that we can improve our alignments to reduce the AER. The incorrect alignments are highlighted in Table 4 (below) that shows Hindi to Urdu with diacritics alignments of our sample words of Table 3. The vowel representation in Urdu/PersioArabic script is highly complex and contextsensitive (Hussain 2004; Malik, Boitet and Bhattacharyya 2008; Malik et al. 2009). This highly complex and contextual representation leads to wrong character alignments, highlighted in Table 4. In the second row of Table 4, the Hindi vowel इ [ɪ] is aligned with ALEF ( )اand ZER ( ِ◌) is aligned to NULL. The alignment is not completely incorrect, but the vowel इ [ɪ] must be aligned with both ALEF ( )اand ZER (◌ِ ). Similarly, the Hindi vowel उ [ʊ] must be aligned with ALEF ( )اand PESH ( ُ◌) in the third row. In these examples, one character in Hindi must be aligned with a sequence of characters in Urdu. Interestingly, we have observed that GIZA++ correctly aligns suc"
W13-4706,C08-1068,1,0.933314,"Missing"
W13-4706,P04-1021,0,\N,Missing
W13-4706,W98-1005,0,\N,Missing
W13-4706,W02-0505,0,\N,Missing
W13-4706,P07-2045,0,\N,Missing
W13-4706,P06-2025,0,\N,Missing
W13-4706,P10-1048,0,\N,Missing
W13-4706,N03-1017,0,\N,Missing
W13-4706,P97-1017,0,\N,Missing
W13-4706,P03-1019,0,\N,Missing
W14-0124,E09-1005,0,0.0339897,"sed on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain a"
W14-0124,W04-2214,0,0.032734,"domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal Classification system (Bentivogli et al., 2004). Beginning with Wordnet 2.0, Domain category pointers were introduced to link domain specific synsets across part of speech. However, the manual determination of a set of domain labels and assigning them to Wordnet synsets is a time consuming task. Also, the senses of words evolves over a period of time and accordingly Wordnet synsets also undergo changes. This makes the static assignment of domain label a costly exercise. With the intention to reduce manual labor of domain categorization and to facilitate use of Wordnet in domain specific applications, there has been efforts to (semi) automa"
W14-0124,J90-1003,0,0.182865,"he figure 1 after preprocessing, the similarity graph is constructed from the corpus. Using a graph based algorithm similarity graph is converted into domain conceptualization and then it is linked with Wordnet synsets to assign domain labels to Wordnet synsets. The detailed description of each component is as follows. 3.1 Preprocessing The text corpus is first POS tagged using Stanford POS tagger 1 and Morph Analyzer 2 . Then term frequency of each term is calculated using weirdness measure (Ahmad et al., 1999). Context vector for each term is constructed using Point Wise Mutual Information (Church and Hanks, 1990) measure. We used a sentence as a boundary to calculate context vector. Output of the preprocessing step is a list of domain specific terms and their context vector. 3.2 Constructing Document Graph Using the term list and context vector generated from the preprocessing step, a graph G(V, E) is constructed in which each vi ∈ V is term and each edge e(vi , vj ) is semantic relatedness between terms vi and vj . Semantic relatedness between two terms vi and vj is calculated by taking cosine of terms vectors of vi and vj , as shown in fig 2. Figure 2: Cosine Similarity 3.3 Constructing Domain Speci"
W14-0124,gonzalez-agirre-etal-2012-proposal,0,0.0151013,"ted methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms using GermaNet synsets. The disambiguation was performed based on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assi"
W14-0124,P10-1155,1,0.932943,"ts for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain la"
W14-0124,Y09-1031,0,0.015977,"et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among these approaches can be summerized as follows, 1. Both Buitelaar and Sacaleanu (2001) and Magnini et al. (2002b) used word frequency to detect domain specificity of a term. They do not consider the label of neighbor terms to determine the label for a term. 2. Fukumoto and Suzuki (2011) have modeled domain labeling"
W14-0124,magnini-cavaglia-2000-integrating,0,0.421849,"ike word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal Classification system (Bentivogli et al., 2004). Beginning with Wordnet 2.0, Domain category pointers were introduced to link domain specific synsets across part of speech. However, the manual determination of a set of domain labels and assigning them to Wordnet synsets is a time consuming task. Also, the senses of words evolves over a period of time and accordingly Wordnet sy"
W14-0124,P11-2097,0,0.0168801,"a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chines"
W14-0124,H92-1045,0,0.200401,"omain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal"
W14-0126,N13-1088,1,0.885924,"Missing"
W14-0126,N03-1032,0,0.0718943,"Missing"
W14-0126,C12-3033,1,\N,Missing
W14-0130,J01-2001,0,\N,Missing
W14-0130,P09-1017,0,\N,Missing
W14-0145,bentivogli-etal-2000-coping,0,0.0858876,"Missing"
W14-0145,C92-2082,0,0.134563,"Missing"
W14-0147,W13-5616,0,\N,Missing
W14-0147,W11-4643,0,\N,Missing
W14-1708,C08-1022,0,0.0268234,"Missing"
W14-1708,P06-1032,0,0.074105,"or category. So, the custom development approach is infeasible for correcting a large number of error categories. Hence, for correction of all the error categories, generic methods have been investigated - generally using language models or statistical machine translation (SMT) systems. The language model based method (Lee and Seneff, 2006; Kao et al., 2013) scores sentences based on a language model or count ratios of n-grams obtained from a large native text corpus. But this method still needs a candidate generation mechanism for each error category. On the other hand, the SMT based method (Brockett et al., 2006) formulates the grammar correction problem as a problem of translation of incorrect sentences to correct sentences. SMT provides a natural unsupervised method for identifying candidate corrections in the form of the translation model, and a method for scoring them with a variety of measures including the language model score. However, the SMT method requires a lot of parallel non-native learner corpora. In addition, the machinery in phrase based SMT is optimized towards solving the language translation problem. Therefore, the community has explored approaches to adapt the In this paper, we pro"
W14-1708,W13-3606,0,0.0218238,"Missing"
W14-1708,N12-1067,0,0.0655488,"Missing"
W14-1708,W13-1703,0,0.053685,"l. (2013). SVA correction is done using a prioritized, conditional rule based system described by Kunchukuttan et al. (2013). 4 Table 2 shows the results on the development set for different experimental configurations generated by varying the tuning metrics, and the method of combining the SMT model and custom correction modules. Table 3 shows the same results on the official CoNLL 2014 dataset without alternative answers. 5.1 We used the NUCLE Corpus v3.1 to build a phrase based SMT system for grammar correction. The NUCLE Corpus contains 28 error categories, whose details are documented in Dahlmeier et al. (2013). We split the corpus into training, tuning and test sets are shown in Table 1. 5.2 Document Count 1330 20 47 Sentence Count 54284 854 2013 The phrase based system was trained using the Moses1 system, with the grow-diag-finaland heuristic for extracting phrases and the msdbidirectional-fe model for lexicalized reordering. We tuned the trained models using Minimum Error Rate Training (MERT) with default parameters (100 best list, max 25 iterations). Instead of BLEU, the tuning metric was the F-0.5 metric. We trained 5-gram language models on all the sentences from NUCLE corpus using the Kneser-"
W14-1708,P03-1021,0,0.0579705,"general, GEC may partly assist in solving natural language processing (NLP) tasks like Machine Translation, Natural Language Generation etc. However, a more evident application of GEC is in building automated grammar checkers thereby non-native speakers of a language. The goal is to have automated tools to help non-native 60 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 60–64, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics al., 2002), tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003). Since BLEU is a form of weighted precision, along with a brevity penalty to factor in recall, it is suitable in the language translation scenario, where fidelity of the translation is an important in evaluation of the translation. Tuning to BLEU ensures that the parameter weights are set such that the fidelity of translations is high. However, ensuring fidelity is not the major challenge in grammar correction since the meaning of most input sentences is clear and most don’t have any grammatical errors. The metric to be tuned must ensure that weights are learnt such that the features most rel"
W14-1708,P02-1040,0,0.0894239,"Missing"
W14-1708,D13-1074,0,0.012303,"used metrics for tuning is the BLEU score (Papineni et • Correct all error categories using the SMT method, followed by correction using the custom modules. • Correct only the error categories not handled by the custom modules using the SMT method, followed by correction using the custom modules. 61 5 The error categories for which we built custom modules are noun number, determiner and subject-verb agreement (SVA) errors. These errors are amongst the most common errors made by non-native speakers. The noun number and determiner errors are corrected using the classification model proposed by Rozovskaya and Roth (2013), where the label space is a cross-product of the label spaces of the possible noun number and determiners. We use the feature-set proposed by Kunchukuttan et al. (2013). SVA correction is done using a prioritized, conditional rule based system described by Kunchukuttan et al. (2013). 4 Table 2 shows the results on the development set for different experimental configurations generated by varying the tuning metrics, and the method of combining the SMT model and custom correction modules. Table 3 shows the same results on the official CoNLL 2014 dataset without alternative answers. 5.1 We used"
W14-1708,W13-3607,0,0.0444389,"relevant to correcting the grammar errors are given due importance and that the tuning focuses on the grammatically incorrect parts of the sentences. The F-β score, as defined for the CoNLL shared task, is the most obvious metric to measure the accuracy of grammar correction on the tuning set. We choose the F-β metric as a score to be optimized using MERT for the SMT based grammar correction model. By choosing an appropriate value of β, it is possible to tune the system to favour increased recall/precision or a balance of both. SMT method for grammar correction (Buys and van der Merwe, 2013; Yuan and Felice, 2013). These include use of factored SMT, syntax based SMT, pruning of the phrase table, disabling or reordering, etc. The generic SMT approach has performed badly as compared to the specific custom made approaches (Yuan and Felice, 2013). Our system also builds upon the SMT methods and tries to address the above mentioned lacunae in two ways: • Tuning the SMT model to a metric suitable for grammar correction (i.e.F-β metric), instead of the BLEU metric. • Combination of custom-engineered methods and SMT based methods, by using classifier based for some error categories. Section 2 describes our met"
W14-1708,W14-1701,0,\N,Missing
W14-1708,W13-3611,1,\N,Missing
W14-1708,W13-3603,0,\N,Missing
W14-2619,P07-1056,0,0.0438468,"sification task. The language used in this case was English. The comparison was done between 5 systems viz. System using words as features, WordNet sense based system as given in Balamurali et al., (2012), Clusters based system as described in Kashyap et al., (2013), Discourse rules based system as given in Mukherjee and Bhattacharyya (2012), UNL rule based system. Two polarity datasets were used to perform the experiments. 1. EN-TD: English Tourism corpus as used in Ye et al., (2009). It consists of 594 positive and 593 negative reviews. 2. EN-PD: English Product (music albums) review corpus Blitzer et al., (2007). It consists of 702 positive and 702 negative reviews. For the WordNet sense, and Clusters based systems, a manually sense tagged version of the (ENPD) has been used. Also, a automatically sense tagged version of (EN-TD) was used on these systems. The tagging in the later case was using an automated WSD engine, trained on a tourism domain Khapra et al., (2013). The results reported for supervised systems are based on 10-fold cross validation. 5 Results The results for monolingual binary sentiment classification task are shown in Table 1. The results reported are the best results obtained in c"
W14-2619,R09-1067,0,\N,Missing
W14-2619,D11-1100,1,\N,Missing
W14-2619,D08-1083,0,\N,Missing
W14-2619,P11-2102,0,\N,Missing
W14-2619,P13-1041,1,\N,Missing
W14-2619,C12-1113,1,\N,Missing
W14-2619,W12-3716,0,\N,Missing
W14-2623,carl-2012-translog,0,0.0543055,"as reported by the participants are shown in Table 1. All participants correctly identified the polarity of document D0. Participant P9 reported that D1 is confusing. 4 out of 12 participants were unable to detect correct opinion in D2. Experiment Setup 3.3 This section describes the framework used for our eye-tracking experiment. A participant is given the task of annotating documents with one out of the following labels: positive, negative and objective. While she reads the document, her eyefixations are recorded. To log eye-fixation data, we use Tobii T120 remote eye-tracker with Translog(Carl, 2012). Translog is a freeware for recording eye movements and keystrokes during translation. We configure Translog for reading with the goal of sentiment. 3.1 Participant description Experiment Description We obtain two kinds of annotation from our annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eyetracker. They are given a set of instructions beforehand and can seek clarifications. This experiment is conducted as follows: 1. A complete document is displayed on the screen. The font size and line separation are set to 17pt and 1.5 cm respectively to e"
W14-2623,N13-1088,1,0.440886,"Missing"
W14-2623,P04-1035,0,\N,Missing
W14-3308,P08-1087,0,0.171182,"Missing"
W14-3308,kunchukuttan-etal-2014-shata,1,0.901107,"Missing"
W14-3308,I11-1013,0,0.0307465,"Missing"
W14-3308,W13-2807,0,0.0300554,"Missing"
W14-3308,C10-1043,0,0.0255003,"rect/oblique case of the parallel nouns in Hindi sentence. We use object of preposition, subject, direct object, tense as our features. These features are extracted using semantic relations provided by Stanford’s typed dependencies (Marneffe, 2008). 4.4 that pre-orders the source sentence to conform to target word order. A substantial volume of work has been done in the field of source-side reordering for machine translation. Most of the experiments are based on applying reordering rules at the nodes of the parse tree of the source sentence. These reordering rules can be automatically learnt (Genzel, 2010). But, many source languages do not have a good robust parser. Hence, instead we can use shallow parsing techniques to get chunks of words and then reorder them. Reordering rules can be learned automatically from chunked data (Zhang, 2007). Hindi does not have a functional constituency or dependency parser available, as of now. But, a shallow parser4 is available for Hindi. Hence, we follow a chunk-based pre-ordering approach, wherein, we develop a set of rules to reorder the chunks in a source sentence. The following are the chunks tags generated by this shallow parser: Noun chunks (NP), Verb"
W14-3308,P07-1037,0,0.0349605,"Missing"
W14-3308,N03-1033,0,0.0119603,"Missing"
W14-3308,P03-1054,0,0.0276253,"ut various experiments. Starting with Phrase Based Statistical Machine Translation (PBSMT)(Koehn et. al., 2003) as baseline system we go ahead with pre-order PBSMT described in Section 4.1. After pre-ordering, we train a Factor Based SMT(Koehn, 2007b) model, where we add factors on the pre-ordered source corpus. In Factor Based SMT we have two variations- (a) using Supertag as factor described in Section 4.2 and (b) using number, case as factors described in Section 4.3. Experimental Setup English Corpus Normalization To begin with, the English data was tokenized using the Stanford tokenizer (Klein and Manning, 2003) and then true-cased using truecase.perl provided in MOSES toolkit. 3.2 100,839 Before splitting the data, we first randomize the parallel corpus. We filter out English sentences longer than 50 words along with their parallel Hindi translations. After filtering, we select 5000 sentences which are 10 to 20 words long as the test data, while remaining 284,832 sentences are used for training. We process the corpus through appropriate filters for normalization and then create a train-test split. 3.1 Hindi 3,092,555 118,285 17,961,357 289,832 182,777 Table 1: en-hi corpora statistics, post normalis"
W14-3308,N03-1017,0,0.0661416,"Missing"
W14-3308,W07-0401,0,0.0666536,"Missing"
W14-3308,P05-1066,0,0.225273,"Missing"
W14-3308,J99-2004,0,\N,Missing
W14-3308,N09-2047,0,\N,Missing
W14-3308,P02-1040,0,\N,Missing
W14-3308,W05-0909,0,\N,Missing
W14-3308,I08-1067,1,\N,Missing
W14-3350,W07-0734,0,0.0600846,"Missing"
W14-3350,W05-0904,0,0.44146,"ce is shorter than the best matching (in length) reference translation. Alternative approaches have been designed to address problems with BLEU. Doddington and George (2003) proposed NIST metric which is derived from the BLEU evaluation criterion but differs in one fundamental aspect: instead of n-gram precision, the information gain from each n-gram is taken into account. TER (Snover, 2006) tries to improve the hypothesis/reference matching process based on the editdistance and METEOR (Alon Lavie, 2007) considered linguistic evidence, mostly lexical similarity, for more intelligent matching. Liu and Gildea (2005), Owczarzak et al. (2007), and Zhang et al. (2004) use syntactic overlap to calculate the similarity between the hypothesis and the reference. Pad´o and Galley (2009) proposed a metric that evaluates MT output based on a rich set of textual entailment features. There are different works that have been done at various NLP layers. Gim´enez tl al. (2010) provided various linguistic measures for MT evaluation at different NLP layers. Ding Liu and Daniel Gildea (2005) focussed the study on the syntactic features that can be helpful while evaluation. 3 3.2 Syntactic layer takes care of the syntax of"
W14-3350,de-marneffe-etal-2006-generating,0,0.129358,"Missing"
W14-3350,2006.amta-papers.25,0,0.238642,"Missing"
W14-3350,P02-1040,0,0.093787,"Missing"
W14-3350,W14-3336,0,0.101571,"Missing"
W14-3350,P09-1034,0,0.0393359,"Missing"
W14-3350,P11-1103,0,\N,Missing
W14-3350,W13-2201,0,\N,Missing
W14-3350,W13-2202,0,\N,Missing
W14-5103,W10-3604,1,0.760804,"corhttp://en.wikipedia.org/wiki/Hindi 3 pora, translation suffers from data pages sparsity. Also, Dhttp://en.wikipedia.org/wiki/Eighth_Schedule_to_the_Constitution S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, 11–19, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) the translation of morphemes does not merely involve independent dictionary substitution but require looking at neighboring morphemes. An important aspect of our work involved handling of participial forms known as Krudantas and Akhyatas (Bhosale et al, 2011; Bapat et al. 2010) which are derivatives of verbs. Consider: “मी धावलयानंतर असलेला वयायाम करत sentence but Marathi can place this either before or after. The sentence “He says that he comes home at 8” is written in Hindi as: “वह कहता है की वह आठ बजे घर आता है ” {vaha kahta hai ki vaha aath baje ghar aata hai} but in Marathi as “तो घरी आठ वाजता येतो असे तो महणतो” {to ghari aath vaj-ta ye-to ase to mhana-to} or “तो आहे ” {mi dhava-lya-nantar asa-le-la vyayam kaमहणतो की तो आठ वाजता घरी येतो” {to mhana-to ra-ta aahe} {I am doing the exercises that come after running} in which “धावलयानंतर” and ki to aath vaj-ta"
W14-5103,D07-1091,0,0.0273067,"Missing"
W14-5103,P02-1040,0,0.106348,"Missing"
W14-5103,W06-3102,0,0.0677239,"Missing"
W14-5105,N09-2047,0,0.0218425,"or an adjunction node marked by ∗ . Substitution and adjunction are those nodes which can be replaced by another supertag. For an adjunction node, it is necessary for its label to match the label of root node of supertag. A supertag can have atmost one adjunction node but can have more than one substitution node. 3.2 Supertagging Supertagging refers to tagging each word of a sentence with a supertag. An example of a supertagged sentence is shown in ﬁgure 2. NP S G♢ NP ↓ 0 I NP VP V♢ A♢ NP1 ↓ dark NP∗ NP N♢ chocolate ate Figure 2: Supertagged sentence “I ate dark chocolate” We use MICA Parser (Bangalore et al., 2009)2 to obtain rich linguistic information like POS tag, supertag, voice, the presence of empty subjects (PRO), wh-movement, deep syntactic argument (deep subject, deep direct object, deepindirect object), whether a verb heads a relative clause and also dependency relation, for each word. From this rich set of features, for each word, we extract word ID (essentially word position in a sentence), supertag, dependency relation and deep syntactic argument. Given a dependency relation, these supertags can be assembled using composition operation of TAG to form a constituent tree. Composition operatio"
W14-5105,P05-1033,0,0.0588163,"ection 3. Section 4 describes our approach for preordering. In Section 5, we provide the methodology for pre-ordering. Experimental setup is explained in Section 6, while corresponding results are shown in Section 7. We conclude our work in Section 8 providing acknowledgement in Section 9. 2 Related Work Word order has direct impact on the ﬂuency of transaltion obtained using SMT systems. There are basically two paradigms for generating correct word order. The ﬁrst paradigm deals with developing a reordering model which is used during decoding. Different solutions such as syntax-based models (Chiang, 2005), lexicalized reordering (Och et al., 2004), and tree-to-string methods (Zhang et al., 2006) have been proposed. Most of these approaches use statistical models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, page"
W14-5105,P05-1066,0,0.147181,"to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 languag"
W14-5105,C94-1024,0,0.153428,"mmar (TAG), a mildly context sensitive formalism as discussed in Section 3. 3 Introduction to TAG/Supertag Tree Adjoining Grammar (TAG) was introduced by Joshi et al. (1975). Tree Adjoining Languages (TALs) generate some strictly context-sensitive languages and fall in the class of the so-called mildly context-sensitive languages. Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1991) is the TAG formalism where each lexical item is associated with atleast one elementary structure. This elementary structure of LTAG is known as Supertag. The concept of Supertag was ﬁrst proposed by Joshi and Srinivas (1994). Supertag localize dependencies, including longdistance dependencies, by requiring that all and only the dependent elements be present within the same structure. They provide syn31 tactic as well as dependency information at the word level by imposing complex constraints in a local context. Supertags provide information like POS tag, subcategorization and other syntactic constraints at the level of agreement feature. Supertag can also be viewed as fragments of parse tree associated with each lexical item. An example of supertag is shown in Figure 1. This supertag is used for transitive verbs."
W14-5105,P07-2045,0,0.0041265,"1.1K. We train a 5 gram language model using SRILM on a 50K monolingual Hindi corpus from the same domain. 4 WMT14 resources:-http://ufallab.ms.mff. cuni.cz/~bojar/hindencorp/ 5 http://www.speech.sri.com/projects/srilm 35 The English data was tokenized using the Stanford tokenizer and then true-cased using truecase.perl provided in MOSES toolkit. We normalize Hindi corpus using NLP Indic Library (Kunchukuttan et. al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Sect"
W14-5105,P02-1040,0,0.102235,"Missing"
W14-5105,W13-2807,0,0.386194,"lkit. We normalize Hindi corpus using NLP Indic Library (Kunchukuttan et. al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Section 5. We provide systematic comparision among these systems in Section 7. As the reordering rules are developed to conform Hindi word order, we were interested to see how does it aﬀect other Indian languages which have the same word order as Hindi. So, we developed various translation systems from English to other Indian languages using ILC"
W14-5105,popovic-ney-2006-pos,0,0.0255179,"on of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 language pairs. They ﬁrst extract a dependency tree and then converts it to a shallow constituent tree. The trees are annotated by both POS tags and by Stanford dependency types, then they learn reordering rules given a set of fea"
W14-5105,I08-1067,1,0.883807,"al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Section 5. We provide systematic comparision among these systems in Section 7. As the reordering rules are developed to conform Hindi word order, we were interested to see how does it aﬀect other Indian languages which have the same word order as Hindi. So, we developed various translation systems from English to other Indian languages using ILCI corpus and compare it with standard pharse based systems. 7 Results In this"
W14-5105,2006.amta-papers.25,0,0.0671824,"Missing"
W14-5105,C04-1073,0,0.0531965,"cal models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007)"
W14-5105,N06-1033,0,0.0259579,"he methodology for pre-ordering. Experimental setup is explained in Section 6, while corresponding results are shown in Section 7. We conclude our work in Section 8 providing acknowledgement in Section 9. 2 Related Work Word order has direct impact on the ﬂuency of transaltion obtained using SMT systems. There are basically two paradigms for generating correct word order. The ﬁrst paradigm deals with developing a reordering model which is used during decoding. Different solutions such as syntax-based models (Chiang, 2005), lexicalized reordering (Och et al., 2004), and tree-to-string methods (Zhang et al., 2006) have been proposed. Most of these approaches use statistical models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source lan"
W14-5105,W07-0401,0,0.022631,"a and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 language pairs. They ﬁrst extract a dependency tree and then converts it to a shallow constituent tree. The trees are annotated by both POS tags and by Stanford dependency types, then they learn reordering rules given a set of features. This paper discusses about creating manual reordering rules with the help of Tree Adjo"
W14-5105,jha-2010-tdil,0,\N,Missing
W14-5105,C10-1043,0,\N,Missing
W14-5105,N04-1021,0,\N,Missing
W14-5113,D07-1091,0,0.0395601,"Missing"
W14-5113,P02-1040,0,0.0937335,"is because French is much closer to Creole than English. 3.5 Testing and Results For the purpose of testing an additional 142 sentence triplets were created, one each for English, French and Creole. These are sentences with more than 10 words per sentence on an average. The number of OOV’s was more in these sentences. Extra 142 simple (short) sentence pairs for English and Creole were created to verify our hypothesis mentioned above. These contained an average of 5 words per sentence with relatively lesser number of OOV’s compared to the earlier sentences. We evaluated the quality using BLEU (Papineni et al. 2002). The scores are given in Table 4 below. In the table “easy” indicates that the simple (short) sentence pairs were used for testing whereas “hard” indicates that the longer ones were used for testing. For the Creole to English, Creole to French and French to Creole systems the pivot language mechanism was not applicable since effective pivot languages were not available. Language Pair En-Cr (direct, hard) En-Cr (direct, easy) En-Cr (bridge (N=1), hard) En-Cr (bridge (N=10), hard) En-Cr (bridge (N=1), easy) En-Cr (bridge (N=10), easy) Cr-En (direct, hard) Cr-En (direct, easy) Fr-Cr (direct, har"
W14-5113,2008.iwslt-papers.1,0,0.019091,"Proc. of the 11th Intl. Conference on Natural Language Processing, pages 82–88, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 1.1 Related Work Sukhoo et al. (2014) had developed a basic English Creole SMT system with a very small amount of parallel corpus (10000-13000 lines including many dictionary words). They manage to get simple sentences translated with reasonable quality but fail when longer sentences (more than five words) are tested. This corpus, after considerable augmentation was used in our experiments. Significant work has been done in using bridge languages (Bertoldi et al. (2008), Utiyama et al. (2007)) to improve translation quality. In many of these works, they develop translation systems from languages A to C using B, where A-B and B-C are resource rich, by either synthesizing new phrase tables or modifying existing corpora. In our case, we had a huge English French corpus but a very small French Creole corpus. Moreover no linguistic processing modules for Creole exist, which would help in reducing data sparsity. Thus we decided to adopt the “transfer method” described by Wu et al. (2009). The remainder of the paper involves a chapter on Linguistic phenomena in Mau"
W14-5113,N07-1061,0,0.0225958,"f RAM. The services for each language pair are hosted using mosesserver. The EnglishFrench system took the maximum training time; a total of 24 hours, including the time for tuning. All the other systems took around 5 minutes of training. Due to lack of corpora we did not perform tuning for these systems. For each system the target side corpus was used for generating the language model. For creole we used the monolingual corpus for language modelling. Tuning was done using MERT. 3.4 Using French as a bridge language We used the “transfer method” or “sentence translation strategy”, proposed by Utiyama and Isahara (2007) and described by Wu and Wang (2009), to translate from English to Creole using French; wherein the first translated from English to French using the En-Fr system and then from French to Creole using the Fr-Cr system. This method is only applicable when either both the En-Fr and Fr-Cr systems or only the En-Fr system is of high quality. The main idea is that French is close to Creole and thus an SMT system built on a small corpus would suffice for good translations. As long as the English-French SMT system gives good translations the resulting Creole translations can be expected to be good. Th"
W14-5113,P09-1018,0,0.0221165,"re hosted using mosesserver. The EnglishFrench system took the maximum training time; a total of 24 hours, including the time for tuning. All the other systems took around 5 minutes of training. Due to lack of corpora we did not perform tuning for these systems. For each system the target side corpus was used for generating the language model. For creole we used the monolingual corpus for language modelling. Tuning was done using MERT. 3.4 Using French as a bridge language We used the “transfer method” or “sentence translation strategy”, proposed by Utiyama and Isahara (2007) and described by Wu and Wang (2009), to translate from English to Creole using French; wherein the first translated from English to French using the En-Fr system and then from French to Creole using the Fr-Cr system. This method is only applicable when either both the En-Fr and Fr-Cr systems or only the En-Fr system is of high quality. The main idea is that French is close to Creole and thus an SMT system built on a small corpus would suffice for good translations. As long as the English-French SMT system gives good translations the resulting Creole translations can be expected to be good. The Moses decoder allows for n-best tr"
W14-5113,J93-2003,0,\N,Missing
W14-5113,P07-2045,0,\N,Missing
W14-5113,N03-1017,0,\N,Missing
W14-5115,W12-5022,0,0.0286164,"y Standard Format (DSF) with extension &apos;.syns&apos;. Some of the features of the existing system are: search by 96 2.2 3 3.1 Advantages and Disadvantages of the Existing System Developed System Synskarta The developed system, ‘Synskarta’ is an online interface for creating synsets by following the expansion approach. This web based tool is developed using PHP and MySQL which uses relational database management system to store and maintain the synset and related data. The IndoWordNet database structure (Prabhu et al., 2012) is used for storing and maintaining the synset data while IndoWordNet APIs (Prabhugaonkar et al., 2012) are used for accessing and manipulating this data. Synskarta overcomes the limitations of the standalone offline tool. The look and feel of the interface is kept similar to that of the existing system for ease of user adaptability. Most of the basic features of the existing system are incorporated in this developed system. Figure 1 shows the Lexicographer’s Interface of the developed system. 3.2 3.2.1 Features of Synskarta Features of Synskarta incorporated from the Existing System The features of the existing system which are implemented with some improvements in Synskarta are as follows – •"
W14-5124,bakliwal-etal-2012-hindi,0,0.552353,"of positive and negative opinion words3 by Liu, but there are not many instances of sentiment lexicons in Hindi that can build an efficient sentiment analysis system for Hindi. The main contributions of this paper are: • A Hindi senti lexicon consisting polar words of four parts of speech: Adjective, Noun, Verb and Adverb, generated from extensive analysis of HindiWordNet4 . • A multi module rule based sentiment analysis system for Hindi that uses words from Hindi senti lexicon as a polarity clue. Our approach that generates Hindi senti lexicon is an improvement over the approach suggested by Bakliwal et al. (2012). They used the same source, that is, HindiWordNet for the polar words extraction, but their approach was not able to handle the instances, where a word has senses of both the orientations: positive and negative. A word can have 2 http://mpqa.cs.pitt.edu/ http://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html 4 Available at: www.cfilt.com D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 150–155, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 positive and negative senses at the same time. This combination is exem"
W14-5124,C04-1200,0,0.0803449,"ites also, for example, we can find Hindi reviews on www.flipkart.com 1 150 hindi.webdunia.com/entertainment/film/review/ or www.homeshop18.com in a big number with English reviews. Therefore, an efficient sentiment analysis system for the Hindi language is the need of current e-commerce organizations and their customers. The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the absence of sufficient amount of corpora, the most efficient way of sentiment analysis is to rely on the words from sentiment lexicons as a key feature. There are many sentiment lexicons for English language, for example, subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu, but there are not many instances of sentiment lexicons in Hindi that can build an efficient sentiment analysis system for Hindi. The main contributions of this paper are: • A Hindi senti lexicon consisting polar words of four parts of speech: Adjective, No"
W14-5124,P04-1035,0,0.0959787,"st their reviews in Hindi on English based e-commerce websites also, for example, we can find Hindi reviews on www.flipkart.com 1 150 hindi.webdunia.com/entertainment/film/review/ or www.homeshop18.com in a big number with English reviews. Therefore, an efficient sentiment analysis system for the Hindi language is the need of current e-commerce organizations and their customers. The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the absence of sufficient amount of corpora, the most efficient way of sentiment analysis is to rely on the words from sentiment lexicons as a key feature. There are many sentiment lexicons for English language, for example, subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu, but there are not many instances of sentiment lexicons in Hindi that can build an efficient sentiment analysis system for Hindi. The main contributions of this paper are: • A Hindi senti lexicon con"
W14-5124,W03-1014,0,0.0782674,"Hindi lovers like to post their reviews in Hindi on English based e-commerce websites also, for example, we can find Hindi reviews on www.flipkart.com 1 150 hindi.webdunia.com/entertainment/film/review/ or www.homeshop18.com in a big number with English reviews. Therefore, an efficient sentiment analysis system for the Hindi language is the need of current e-commerce organizations and their customers. The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the absence of sufficient amount of corpora, the most efficient way of sentiment analysis is to rely on the words from sentiment lexicons as a key feature. There are many sentiment lexicons for English language, for example, subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu, but there are not many instances of sentiment lexicons in Hindi that can build an efficient sentiment analysis system for Hindi. The main contributions of this paper are: • A Hin"
W14-5124,P05-1017,0,0.0239069,"le, we can find Hindi reviews on www.flipkart.com 1 150 hindi.webdunia.com/entertainment/film/review/ or www.homeshop18.com in a big number with English reviews. Therefore, an efficient sentiment analysis system for the Hindi language is the need of current e-commerce organizations and their customers. The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; Danescu-Niculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the absence of sufficient amount of corpora, the most efficient way of sentiment analysis is to rely on the words from sentiment lexicons as a key feature. There are many sentiment lexicons for English language, for example, subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu, but there are not many instances of sentiment lexicons in Hindi that can build an efficient sentiment analysis system for Hindi. The main contributions of this paper are: • A Hindi senti lexicon consisting polar words of four parts of speech: Adjective, Noun, Verb and Adverb, gen"
W14-5126,W04-2215,0,0.0323745,"ously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP tasks such as POS tagging and chunking, but not much work has gone in the direction of corpora generation aid for Machine Translation (MT). The few similar works that we did find are noted below. PolyPhraZ (Hajlaoui and Boitet, 2004) is one such tool which helps in visualizing, editing and evaluating MT systems on parallel corpora. CasualConc (Imao, 2008) is a parallel concordancer which generates keyword in context concordance lines, word clusters, collocation analysis, and word counts. MemoQ (Kilgray, 2006) and Trados (SDL, 2007) are also Computer Aided Translation (CAT) D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 162–166, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen syst"
W14-5126,P07-2045,0,0.00501907,"to ensure time and cost effectiveness of the process. Traditional method of parallel corpus creation 162 involves manual translation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text"
W14-5126,J03-1002,0,0.00655842,"slation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP ta"
W14-5126,E03-1016,0,0.0455033,"Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen systems which are commercially available with features like Translation memory, and Term Extraction. Wordfast is CAT system having just one free version “WordFast Anywhere”. We studied and used the system, but found the interface less intuitive, and hard to use. “WordFast Anywhere” also has an integrated MT system which provides translations via Microsoft Bing and an integrated MT system. Another system we came across is a web based text corpora development system (Yablonsky, 2003) that focuses on the development of UML-specifications, architecture and implementations of DBMS tools. None of the above mentioned systems provide a word alignment visualization, which can be corrected manually, and saved to provide perfect phrase tables later. 3 Parallel Corpora Management System Parallel Corpora Management System (PaCMan) (Figure: 1) is a platform-independent web-based workbench for managing all the processes involved in the generation of good quality parallel corpora. Along with covering the procedural / managerial aspects of the parallel corpora generation process, this t"
W14-5126,E03-1063,0,\N,Missing
W14-5126,kunchukuttan-etal-2014-shata,1,\N,Missing
W14-5136,E09-1015,0,0.0513682,"Missing"
W14-5136,clement-etal-2004-morphology,0,0.018341,"Missing"
W14-5136,C12-3013,1,0.885368,"Missing"
W14-5136,W09-4615,0,0.0727104,"Missing"
W14-5136,2012.freeopmt-1.4,0,0.0583385,"Missing"
W14-5148,P14-2131,0,0.0281978,"(kisI cheez kA dar honA)” [to fear of something] honA (anishta yA hAni kI aAshankA se Akul honA)” Word Embeddings are increasingly being used in variety of NLP tasks. Word Embeddings represent each word with low-dimensional real valued vector. Such models work under the assumption that similar words occur in similar context (Harris, 1968). (Collobert et al., 2011) used word embeddings for POS tagging, Named Entity Recognition and Semantic Role Labeling. Such embeddings have also been used in Sentiment Analysis (Tang et al., 2014), Word Sense Induction (Huang et al., 2012), Dependency Parsing (Bansal et al., 2014) and Constituency Parsing (Socher et al., 2013). Word embeddings have been used for textual similarity computation (Mihalcea et al., 2006). We are using word embeddings for finding gloss similarity between synsets. The fine-grained senses can be merged based on the similarity values. Word embeddings have been trained using word2vec4 tool (Mikolov et al., 2013). word2vec provides two broad techniques for word vectors generation: Continuous SkipGram and Continuous Bag of Words (CBOW). CBOW predicts current word based on surrounding context, whereas Continuous SkipGram model tries to maximize cla"
W14-5148,P13-2096,1,0.881293,"Missing"
W14-5148,W02-0805,0,0.114276,"Missing"
W14-5148,C94-2113,0,0.277965,"a-t hon  tk kA smy (surya nikalne se uske asta hone tak kA samay) [the time after sunrise and before sunset] ek syo dy s  l kr dsr  syo dy tk kA smy яo cObFs GV  kA mAnA яAtA { h (ek suryoday se lekar dusre suryoday tak kA samay jo choubis ghante kA maana jAtA hai) [time between two sunrise which considered as of 24 hours] cObFs GV m  s  vh smy яo son k  bAd kAm krn  m  gяrtA { h (choubis ghante me se vaha samay jo sone ke bad kaam karane se gujaratA hai) [within 24 hours, the time apart from sleeping that is spent working] Table 2: Hindi WordNet Sense Distinction senses. Dolan (1994) first used ontological information for sense clustering. He presented a heuristic based algorithm for clustering senses of Longman’s Dictionary of Contemporary English (LDOCE). Peters (1998) addressed different ways for reducing the fine-grainedness of EuroWordNet. In his approach, senses were grouped depending on the semantic relations like sisters, twins, cousins, autohyponymy etc. Mihalcea and Moldovan (2001) derived a set of semantic and probabilistic rules for reducing average polysemy. This was the first attempt of grouping synsets rather than word senses. The resulting version of WordN"
W14-5148,P12-1092,0,0.12122,"Missing"
W14-5148,W06-2503,0,0.0401519,"Missing"
W14-5148,P08-1028,0,0.117673,"Missing"
W14-5148,P06-1014,0,0.0298312,"with WordNet based similarity measures for sense clustering. Bhagwani et. al., (2013) proposed a semisupervised approach which learns synset similarity by using graph based recursive similarity. Resulting coarse-grained sense inventory boosts performance of noun sense disambiguation. Chugur et. al., (2002) used translational equivalences of word senses for sense merging. Two word senses are expected to be similar, if they lead to the same translation in other languages. Several sense clustering attempts were made by mapping WordNet to other sense inventories either manually or automatically. Navigli (2006) proposed a sense clustering method by mapping WordNet senses to Oxford English Dictionary (OED). Martha Palmer (2007) suggested a semiautomatic technique for verb sense grouping by using Levin class theory. Snow et. al., (2007) proposed a supervised approach using Support Vector Machine in which features were derived from WordNet and other 5 lexical resources. Due to shallow hierarchy of verbs in WordNet, the knowledge based measures which exploit ontology structure are ineffective for sense merging. We therefore make use of gloss to infer finegrained senses. We investigate usage of word embe"
W14-5148,P14-1009,0,0.0344987,"Missing"
W14-5148,N04-3012,0,0.199658,"Missing"
W14-5148,P13-1041,1,0.770258,"semantic relations. Synset is a set of synonyms representing the same concept. Synsets are linked with basic semantic relations viz., hypernymy, hyponymy, meronymy, holonymy, troponymy etc. In comparison with Princeton WordNet, HWN provides extra relations e.g., gradation, causative, 1 compounds, conjunction etc. HWN is widely used Figure 1: IndoWordNet in Natural Language Applications (NLP) viz., Machine Translation (Ananthakrishnan et al., 2008; Kunchukuttan et al., 2012), Word Sense Disambiguation (Khapra et al., 2010; Bhingardive et al., 2013), Sentiment Analysis (Balamurali et al., 2012; Popat et al., 2013) etc. Over-specified sense distinctions in HWN may not be useful for certain applications. Hence, generating a coarse-grained version of HWN is a crucial task in order to get better results for such applications. In this paper, we present a method for merging the fine-grained senses of HWN using gloss similarity. Word embeddings are used for computing this similarity. The presented method performs better as compared to baselines. The paper is organised as follows. Section 2 describes the sense granularity that exists in HWN. Section 3 presents the related work. Section 4 gives details about Wo"
W14-5148,D07-1107,0,0.050354,"Missing"
W14-5148,P13-1045,0,0.0408236,"Missing"
W14-5148,P94-1019,0,0.598456,"ficult to capture programmatically. Sometimes even humans fail in making such distinctions. Hence, for applications which do not need fine-grained senses, a coarse-grained version of HWN is essential. 3 Related Work Recently, a large number of sense clustering techniques have been proposed. These techniques rely on various information resources like ontological structure, external corpora, translation similarities, supervision etc. WordNet ontology structure is very helpful for merging fine-grained word senses. Various synset similarity measures have been proposed viz., Path Based Similarity (Wu and Palmer, 1994), (Leacock and Chodorow, 1998), Information Content Based Measures (Resnik, 1995) (Lin, 1998) (Jiang and Conrath, 1997), Gloss Based Heuristics (Lesk, 1986) (Banerjee and Pedersen, 2003) etc. These measures were used for creating coarse-grained Linguistic Properties Target word Senses S1 Subject property kAVnA (animate/inanimate) (kAtnA) Object property (Knowledge/Experience) rKnA (rakhnA) Compulsion EnkAlnA (nikAlnA) S2 S1 S2 S1 S2 Time period S1 Edn (dina) S2 S3 Gloss/Definition in Hindi WordNet DArdAr f-/ aAEd s  EksF v-t aAEd k  do yA kI KX krnA yA koI BAg alg krnA (dhArdAr shastra Adi"
W14-5148,O97-1002,0,\N,Missing
W14-5148,bojar-etal-2014-hindencorp,0,\N,Missing
W14-5148,kunchukuttan-etal-2012-experiences,1,\N,Missing
W14-5148,I08-1067,1,\N,Missing
W14-5148,W13-5003,0,\N,Missing
W14-5504,W99-0904,0,0.208955,"Missing"
W14-5504,J01-2001,0,0.723516,"duction Learning morphology by a machine is crucial for tasks like stemming, machine translation etc. Rule based affix stripping approach, semi-supervised, unsupervised learning of morphology and finite state approach as some of the well known methods used to learn morphology by a machine. Rule based affix stripping approaches (Lovins, 1968; Porter, 1980; Paice, 1990; Loftsson, 2008; Maung et. al, 2008) depend heavily on linguistic input and require a lot of human effort, especially for morphologically rich languages. Pure unsupervised approaches learn morphology from a corpus (Freitag, 2005; Goldsmith, 2001; Hammarström, 2011). The accuracy of pure unsupervised methods is relatively low. Semi-supervised approaches use minimal linguistic input and unsupervised methods to automate morphology learning process (Forsberg, 2007; Lindén, 2008; Chan, 2008; Dreyer, 2011). Semi-supervised approaches perform better than pure unsupervised approaches. Finite state approaches (Koskenniemi, 1983; Beesley & Kartunnen, 2003) represent morphology using finite state machines. Finite state approaches require linguistic input in the form of paradigm identification. Unsupervised and semi-supervised methods can provid"
W14-5504,W09-4615,0,0.056555,"Missing"
W14-5504,I08-3010,0,0.0544365,"Missing"
W14-5504,W05-0617,0,\N,Missing
W14-5504,J11-2002,0,\N,Missing
W15-2905,P15-1100,0,0.116958,"le for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of improving the feature design, while rule-based sarcasm detection uses rules generated from heuristics. Our paper presents a novel approach to sarcasm detection: ‘looking at historical tweets for sarcasm detection of a target tweet’. It is similar to Wallace et al. (2015) in that it considers text apart from the target text. However, while they look at comments within a thread an"
W15-2905,W14-2609,0,0.00702725,"3) predict a tweet as sarcastic if there is a sentiment contrast between a verb and a noun phrase. 25 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 25–30, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. Figure 1: A motivating example for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of improving the feature d"
W15-2905,W10-2914,0,0.541361,"rk by considering sentiment contrasts beyond the target tweet. Specifically, we look at tweets generated by the same author in the past (we refer to this as ‘historical tweets’). Consider the example in Figure 1. The author USER1 wrote the tweet ‘Nicki Minaj, don’t I hate her?!’. The author’s historical tweets may tell us that he/she has spoken positively about Nicki Minaj in the past. • Implicit Contrast: The tweet contains one word of a polarity, and a phrase of the other polarity. The implicit sentiment phrases are extracted from a set of sarcastic tweets as described in Tsur et al. (2010) Davidov et al. (2010). This is similar to implicit incongruity given by Joshi et al. (2015). For example, the sentence ‘I love being ignored.’ is predicted as sarcastic since it has a positive word 26 ‘love’ and a negative word ‘ignored’. We include rules to discount contrast across conjunctions like ‘but’ 3 . 4.2 2. If the historical tweets contained sarcasm towards the target phrase, and so did the target tweet, the predictor will incorrectly mark the tweet as non-sarcastic. Historical Tweet-based Predictor 3. If an entity mentioned in the target tweet never appeared in the author’s historical tweets, then no in"
W15-2905,P11-2102,0,0.460787,"Similarly, Riloff et al. (2013) predict a tweet as sarcastic if there is a sentiment contrast between a verb and a noun phrase. 25 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 25–30, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. Figure 1: A motivating example for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of"
W15-2905,P15-2124,1,0.593536,"two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past). 1 Introduction Sarcasm1 is defined as ‘the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone’s feelings or to criticize something in a humorous way’2 . An example of sarcasm is ‘Being stranded in traffic is the best way to start my week’(Joshi et al., 2015). There exists a sentiment contrast between the phrases ‘being stranded’ and ‘best way’ which enables an automatic sarcasm detection approach to identify the sarcasm in this sentence. Existing approaches rely on viewing sarcasm as a contrast in sentiment (Riloff et al., 2013; Maynard and Greenwood, 2014). However, consider the sentences ‘Nicki Minaj, don’t I hate her!’ or ‘I love spending four hours cooking on a weekend!’. The sarcasm is ambiguous because of a likely hyperbole in the first sentence, and because 2 1 We use irony and sarcasm interchangeably in this paper, as has been done in pas"
W15-2905,maynard-greenwood-2014-cares,0,0.130196,"entity in the past). 1 Introduction Sarcasm1 is defined as ‘the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone’s feelings or to criticize something in a humorous way’2 . An example of sarcasm is ‘Being stranded in traffic is the best way to start my week’(Joshi et al., 2015). There exists a sentiment contrast between the phrases ‘being stranded’ and ‘best way’ which enables an automatic sarcasm detection approach to identify the sarcasm in this sentence. Existing approaches rely on viewing sarcasm as a contrast in sentiment (Riloff et al., 2013; Maynard and Greenwood, 2014). However, consider the sentences ‘Nicki Minaj, don’t I hate her!’ or ‘I love spending four hours cooking on a weekend!’. The sarcasm is ambiguous because of a likely hyperbole in the first sentence, and because 2 1 We use irony and sarcasm interchangeably in this paper, as has been done in past work. Sarcasm has an element of criticism, while irony may not. 2 http://dictionary.cambridge.org/dictionary/british/sarcasm Related work Sarcasm detection relies mostly on rule-based algorithms. For example, Maynard and Greenwood (2014) predict a tweet as sarcastic if the sentiment embedded in a hasht"
W15-2905,P04-1035,0,0.0304567,"Missing"
W15-2905,D13-1066,0,0.457719,"Missing"
W15-3912,P12-1049,0,0.0552216,"uages. The BrahmiNet corpus contains transliteration corpora for 110 Indian language pairs mined from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). The rest of the paper is organized as follows. Section 2 and Section 3 describes our system and experimental setup respectively. Section 4 discusses the results of various data representation methods and the use of mined corpus respectively. Section 5 concludes the report. 2 2.2 Use of mined transliteration corpus We explore the use of transliteration corpora mined from translation corpora for transliteration. Sajjad et al. (2012) proposed an unsupervised method for mining transliteration pairs from parallel corpus. Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora by setting an appropriate threshold. We compare the quality of the transliteration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-I"
W15-3912,E14-4029,0,0.05518,"Missing"
W15-3912,E12-1015,0,0.0138349,"ze. PBSMT learning of character sequence mappings is agnostic of the position of the character in the word. Hence, we explore to transform the data representation to encode position information. Zhang et al. (2012) did not report any benefit from such a representation for Chinese-English transliteration. We investigated if such encoding useful for alphabetic and consonantal scripts as opposed to logographic scripts like Chinese. It is generally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification approach. We explore this overlapping, segmentation approach for the transliteration task. For many language pairs, parallel transliteration corpora are not publicly available. However, parallel translation corpora like Europarl (Koehn, 2005) and ILCI (Jha, 2012) are available for many language pairs. Transliteration corpora mined from such parallel corpora has been shown to be useful for machine tr"
W15-3912,2010.iwslt-papers.7,0,0.258723,"ng heat maps of confusion matrices. 1 Introduction Machine Transliteration can be viewed as a problem of transforming a sequence of characters in one alphabet to another. Transliteration can be seen as a special case of the general translation problem between two languages. The primary differences from the general translation problem are: (i) limited vocabulary size, and (ii) simpler grammar with no reordering. Phrase based statistical machine translation (PB-SMT) is a robust and well-understood technology and can be easily adopted for application to the transliteration problem (Noeman, 2009; Finch and Sumita, 2010). Our submission to the NEWS 2015 shared task is a PBSMT system. Over a baseline PBSMT system, we address two issues: (i) suitable data representation for training, and (ii) parallel transliteration corpus availability. In many writing systems, the same logical/phonetic symbols can have different charac78 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 78–82, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2015) for transliteration between English and Indian languages. The BrahmiNet corpus contains transliteration"
W15-3912,gupta-etal-2012-mining,0,0.0397785,"Missing"
W15-3912,W12-4407,0,0.0222038,"an scripts have different characters for independent vowels and vowel diacritics. Independent vowels typically occurs at the beginning of the word, while diacritics occur in medial and terminal positions. The pronounciation, and hence the transliteration could also depend on the position of the characters. For instance, the terminal ion in nation would be pronounced differently from initial one in ionize. PBSMT learning of character sequence mappings is agnostic of the position of the character in the word. Hence, we explore to transform the data representation to encode position information. Zhang et al. (2012) did not report any benefit from such a representation for Chinese-English transliteration. We investigated if such encoding useful for alphabetic and consonantal scripts as opposed to logographic scripts like Chinese. It is generally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification"
W15-3912,P07-2045,0,0.0103237,"urce and target training corpus. We use character (P) as well as bigram representations (T). In character based system, the character is the basic unit of transliteration. In bigram-based system, the overlapping bigram is the basic unit of transliteration. We also augmented the word representation with word boundary markers (M) (ˆ for start of word and $ end of word). The various representations we experimented with are illustrated below: character (P) character+boundary marker (M) bigram (T) bigram+boundary marker (M+T) Src Tgt En En En Hi Ba Ta Size 10513 7567 3549 We use the Moses toolkit (Koehn et al., 2007) to train the transliteration system and the language models were estimated using the SRILM toolkit (Stolcke and others, 2002). The transliteration pairs are mined using the transliteration module in Moses (Durrani et al., 2014). 4 Results and Error Analysis 4.1 Effect of Data Representation methods HINDI ˆHINDI$ HI IN ND DI I ˆH HI IN ND DI I$ $ The abbreviations mentioned above are used subsequently to refer to these data representations. 79 Table 1 shows transliteration results for various data representation methods on the development set. We see improvements in transliteration accuracy of"
W15-3912,2005.mtsummit-papers.11,0,0.0117058,"ally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification approach. We explore this overlapping, segmentation approach for the transliteration task. For many language pairs, parallel transliteration corpora are not publicly available. However, parallel translation corpora like Europarl (Koehn, 2005) and ILCI (Jha, 2012) are available for many language pairs. Transliteration corpora mined from such parallel corpora has been shown to be useful for machine translation, cross lingual information retrieval, etc. (Kunchukuttan et al., 2014). In this paper, we make an intrinsic evaluation of the performance of the automatically mined BrahmiNet transliteration corpus (Kunchukuttan et al., Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of wordboundary markers, and (ii) languageindependent, overlapping"
W15-3912,N15-3017,1,0.899131,"Missing"
W15-3912,W09-3525,0,0.17505,"ons by analyzing heat maps of confusion matrices. 1 Introduction Machine Transliteration can be viewed as a problem of transforming a sequence of characters in one alphabet to another. Transliteration can be seen as a special case of the general translation problem between two languages. The primary differences from the general translation problem are: (i) limited vocabulary size, and (ii) simpler grammar with no reordering. Phrase based statistical machine translation (PB-SMT) is a robust and well-understood technology and can be easily adopted for application to the transliteration problem (Noeman, 2009; Finch and Sumita, 2010). Our submission to the NEWS 2015 shared task is a PBSMT system. Over a baseline PBSMT system, we address two issues: (i) suitable data representation for training, and (ii) parallel transliteration corpus availability. In many writing systems, the same logical/phonetic symbols can have different charac78 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 78–82, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2015) for transliteration between English and Indian languages. The BrahmiNet corpus"
W15-3912,P03-1021,0,0.00711804,"teration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-Indian language pairs. System Description We use a standard PB-SMT model for transliteration between the various language pairs. It is a discriminative, log-linear model which uses standard SMT features viz. direct/inverse phrase translation probabilities, direct/inverse lexical translation probabilities, phrase penalty, word penalty and language model score. The feature weights are tuned to optimize BLEU (Papineni et al., 2002) using the Minimum Error Rate Training algorithm (Och, 2003). It would be better to explore optimizing metrics like accuracy or edit distance instead of using BLEU as a proxy for these metrics. We experiment with various transliteration units as discussed in Section 2.1. We use a 5-gram language model over the transliteration units estimated using Witten-Bell smoothing. Since transliteration does not require any reordering, monotone decoding was done. 2.1 3 Experimental Setup For building the transliteration model with the NEWS 2015 shared task corpus as well as the BrahmiNet corpus, we used 500 word pairs for tuning and the rest for SMT training. The"
W15-3912,P02-1040,0,0.0952953,"setting an appropriate threshold. We compare the quality of the transliteration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-Indian language pairs. System Description We use a standard PB-SMT model for transliteration between the various language pairs. It is a discriminative, log-linear model which uses standard SMT features viz. direct/inverse phrase translation probabilities, direct/inverse lexical translation probabilities, phrase penalty, word penalty and language model score. The feature weights are tuned to optimize BLEU (Papineni et al., 2002) using the Minimum Error Rate Training algorithm (Och, 2003). It would be better to explore optimizing metrics like accuracy or edit distance instead of using BLEU as a proxy for these metrics. We experiment with various transliteration units as discussed in Section 2.1. We use a 5-gram language model over the transliteration units estimated using Witten-Bell smoothing. Since transliteration does not require any reordering, monotone decoding was done. 2.1 3 Experimental Setup For building the transliteration model with the NEWS 2015 shared task corpus as well as the BrahmiNet corpus, we used 5"
W15-3912,jha-2010-tdil,0,\N,Missing
W15-5902,C12-1038,0,0.0460358,"Missing"
W15-5902,D12-1052,0,0.0190584,"et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like"
W15-5902,W13-1703,0,0.0223347,"instances. For each sampling method, sampled datasets were created using different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-val"
W15-5902,W11-2838,0,0.0295713,"hod, sampled datasets were created using different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-validation are reported. 7 7.1 Results"
W15-5902,W12-2006,0,0.0171159,"ng different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-validation are reported. 7 7.1 Results and Discussion Limitations of"
W15-5902,D11-1125,0,0.0339617,"les in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification problems. The training loss is described in terms of a loss function (∆) which measures the discrepan"
W15-5902,W14-1703,0,0.0145487,"ible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification pro"
W15-5902,W14-1708,1,0.836405,"nces (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maxim"
W15-5902,P03-1021,0,0.0112546,"e-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification problems. The training loss is described in terms of a loss function (∆)"
W15-5902,D10-1094,0,0.0712138,"Missing"
W15-5902,W12-2032,0,0.0169319,"rity class instances in the4 training set has shown good improvement in many applications (Chawla et al., 2002). For large feature spaces, the generation of synthetic examples can be computationally expensive since it involves a k-nearest neighbourhood search. Optimizing evaluation metrics directly does not incur this computational overhead. 3 Related Work The most common methods to handle imbalanced datasets in GED involve undersampling the correct instances (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the s"
W15-5902,W13-3605,0,0.0128181,"nterpolation of minority class instances in the4 training set has shown good improvement in many applications (Chawla et al., 2002). For large feature spaces, the generation of synthetic examples can be computationally expensive since it involves a k-nearest neighbourhood search. Optimizing evaluation metrics directly does not incur this computational overhead. 3 Related Work The most common methods to handle imbalanced datasets in GED involve undersampling the correct instances (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the w"
W15-5902,W14-1701,0,\N,Missing
W15-5902,W13-3611,1,\N,Missing
W15-5902,W12-2025,0,\N,Missing
W15-5902,W13-3612,0,\N,Missing
W15-5905,P93-1015,0,0.569519,"Missing"
W15-5905,C10-2040,1,0.8939,"Missing"
W15-5905,H05-1124,0,0.106525,"Missing"
W15-5905,P09-1113,0,0.0567678,"abelled data may have some noisy / incorrect labels but it is expected that majority of the automatically obtained labels are correct. Since it is possible to create a large labelled dataset (much larger than manually labelled data), majority correct labels will hopefully reduce the effect of a smaller number of noisy labels in the training set. For any distant supervision based algorithm, there are two essential requirements - i) Large pool of unlabelled data and ii) Heuristic rules to obtain noisy labels. Distant supervision has been successfully used for the problem of Relation Extraction (Mintz et al., 2009). Semantic database like FreeBase (Bollacker et al., 2008) is used to get a list of entity pairs following any particular relation. Also, a large number of unlabelled sentences are used which can be easily obtained by crawling the Web. The labelling heuristic used here is: If two entities participate in a relation, any sentence that contains both of them might express that relation. For example, Freebase contains entity pair &lt;M. Night Shyamalan, The Sixth Sense> for the relation ID /film/director/film, hence both of the following sentences are considered to be positive examples for that relati"
W15-5905,I05-2022,0,0.0447787,"Missing"
W15-5905,C08-1106,0,0.061744,"Missing"
W15-5908,P13-2096,1,0.707832,"p: X v,b P (πL2 (SkL1 )|v, b) · simi(v, b) P (SkL1 |u, a) = X X L Si 1 x,b P (πL2 (SiL1 )|x, b) · simi(x, b) where, SiL1 , SkL1 ∈ synsetsL1 (u) a ∈ context(u) v ∈ crosslinksL2 (u, SkL1 ) b ∈ crosslinksL2 (a) x ∈ crosslinksL2 (u, SiL1 ) Here, u is the target word to be disambiguated, a is the context word, πL2 (SkL1 ) means the linked synset of the sense SkL1 in L2 . simi(x, b) is the 60 WordNet based similarity over all senses of words y,b P (πL1 (SiL2 )|y, a) · simi(y, a) where, SiL2 , SjL2 ∈ synsetsL2 (v) b ∈ context(v) u ∈ crosslinksL1 (v, SjL2 ) Bilingual EM using WordNet-based Similarity Bhingardive et al. (2013) extended the bilingual EM approach (Khapra et al., 2011) and observed that adding contextual information further helps in the disambiguation process. Original bilingual EM approach estimates the sense distribution in one language by using the raw counts of the cross-linked words from the other language using EM algorithm. Bhingardive et al. (2013) modified this approach by replacing the raw counts of the words with the co-occurrence counts of the target word and the context words. They approximated the co-occurrence counts by using WordNet based similarity measures to avoid the data sparsity."
W15-5908,D14-1110,0,0.056098,"Missing"
W15-5908,P02-1033,0,0.27824,"rvised and unsupervised. Supervised WSD approaches (Lee et al., 2004; Ng and Lee, 1996) always perform better because of the availability of the sense-annotated data. However, the cost of creation of the sense-annotated data limits their applicability to only a few resource rich languages. On the other hand, semi-supervised approaches (Yarowsky, 1995; Khapra et al., 2010) provide a fine balance in terms of resource requirements and accuracy, but they still rely on some amount of sense-annotated data. Therefore, despite of the less accuracy, much focus is given for unsupervised WSD algorithms (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011). These algorithms do not need any senseannotated data for the disambiguation. Moreover, they make use of lexical knowledge resources or comparable/parallel corpora for training the algorithm (Kaji and Morimoto, 2002; Diab and Resnik, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Khapra et al. (2011) have shown that how two resource deprived languages can help each other in WSD without using any sense-annotated data in either of the languages. Here, the intuition is that, the sense dis"
W15-5908,C02-1058,0,0.152661,". Supervised WSD approaches (Lee et al., 2004; Ng and Lee, 1996) always perform better because of the availability of the sense-annotated data. However, the cost of creation of the sense-annotated data limits their applicability to only a few resource rich languages. On the other hand, semi-supervised approaches (Yarowsky, 1995; Khapra et al., 2010) provide a fine balance in terms of resource requirements and accuracy, but they still rely on some amount of sense-annotated data. Therefore, despite of the less accuracy, much focus is given for unsupervised WSD algorithms (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011). These algorithms do not need any senseannotated data for the disambiguation. Moreover, they make use of lexical knowledge resources or comparable/parallel corpora for training the algorithm (Kaji and Morimoto, 2002; Diab and Resnik, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Khapra et al. (2011) have shown that how two resource deprived languages can help each other in WSD without using any sense-annotated data in either of the languages. Here, the intuition is that, the sense distribution remains same ac"
W15-5908,I11-1078,1,0.808395,"996) always perform better because of the availability of the sense-annotated data. However, the cost of creation of the sense-annotated data limits their applicability to only a few resource rich languages. On the other hand, semi-supervised approaches (Yarowsky, 1995; Khapra et al., 2010) provide a fine balance in terms of resource requirements and accuracy, but they still rely on some amount of sense-annotated data. Therefore, despite of the less accuracy, much focus is given for unsupervised WSD algorithms (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011). These algorithms do not need any senseannotated data for the disambiguation. Moreover, they make use of lexical knowledge resources or comparable/parallel corpora for training the algorithm (Kaji and Morimoto, 2002; Diab and Resnik, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Khapra et al. (2011) have shown that how two resource deprived languages can help each other in WSD without using any sense-annotated data in either of the languages. Here, the intuition is that, the sense distribution remains same across languages when the comparable corpora is provided. T"
W15-5908,W04-0834,0,0.0869334,"Missing"
W15-5908,S10-1003,0,0.0262902,"Missing"
W15-5908,P04-1036,0,0.116779,"Missing"
W15-5908,C04-1162,0,0.330312,"es (Lee et al., 2004; Ng and Lee, 1996) always perform better because of the availability of the sense-annotated data. However, the cost of creation of the sense-annotated data limits their applicability to only a few resource rich languages. On the other hand, semi-supervised approaches (Yarowsky, 1995; Khapra et al., 2010) provide a fine balance in terms of resource requirements and accuracy, but they still rely on some amount of sense-annotated data. Therefore, despite of the less accuracy, much focus is given for unsupervised WSD algorithms (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011). These algorithms do not need any senseannotated data for the disambiguation. Moreover, they make use of lexical knowledge resources or comparable/parallel corpora for training the algorithm (Kaji and Morimoto, 2002; Diab and Resnik, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Khapra et al. (2011) have shown that how two resource deprived languages can help each other in WSD without using any sense-annotated data in either of the languages. Here, the intuition is that, the sense distribution remains same across languages when the"
W15-5908,P96-1006,0,0.581541,"Missing"
W15-5908,P14-1146,0,0.0453989,"ted by the modified EM formulation as mentioned earlier. The maximum similarity over all senses of the target word is chosen as the sense of the target word. In this way, given the bilingual comparable corpora and the synset aligned dictionary, context aware EM formulation is used to estimate the sense distributions in both the languages. 3 Our approach: Bilingual EM using Distributional Similarity Continuous word embeddings have recently gained popularity in various NLP tasks like POS Tagging, Named Entity Recognition, Semantic Role Labeling, Sentiment Analysis, etc. (Collobert et al., 2011; Tang et al., 2014). Word embeddings have shown to capture the syntactic and semantic information about a word. In our approach, we look forward to use these word embeddings for the bilingual WSD and compare the results with the existing approaches. WSD Algorithm EM-C-DistSimi+WnSimi EM-C-DistSimi EM-C-WnSimi EM WFS RB NOUN 59.32 59.59 59.82 60.68 53.49 32.52 HIN-HEALTH ADV ADJ VERB 68.98 63.18 60.02 69.20 63.87 55.73 67.80 56.66 60.38 67.48 55.54 25.29 73.24 55.16 38.64 45.08 35.42 17.93 Overall 60.94 61.09 59.63 58.16 54.46 33.31 Table 1: Comparison(F-Score) of our approach (EM-C-DistSimi-WnSimi and EM-C-DistS"
W15-5908,P95-1026,0,0.882234,"Missing"
W15-5908,P10-1155,1,\N,Missing
W15-5908,bojar-etal-2014-hindencorp,0,\N,Missing
W15-5910,W12-5022,0,0.0305446,"nt transliteration in all the Indian languages, so that anyone can read other language in their chosen transliteration language. 3.3 Figure 2. IndoWordNet Multilingual Dictionary showing multiple languages with sense based view. Design and Layout of IWN Dictionary The IWN Dictionary is designed keeping in mind, its simplicity and usability; more importantly, the user friendliness of the system. The frontend is designed and developed with PHP, CSS, JavaScript, etc. and at the backend, MySQL database is used. The IWN Dictionary data is retrieved from IndoWordNet database using IndoWordNet APIs (Prabhugaonkar et al., 2012). The basic input parameters of this dictionary are; input language, search string, phonetic transliteration, keyboard and different views selector. Figure 2, figure 3, figure 4 and figure 5 shows sample screen shots of the IWN Multilingual Dictionary Interface with different views. In all these views, initial information is always shown in the source language. This initial information is rendered on the horizontal tab in the card format. User can click on plus or minus button to expand or collapse to see the details in each card respectively. Also, user can check the checkbox next to each Lan"
W15-5910,W14-0130,1,0.834557,". This extracted synset information, i.e., the multilingual senses of an input word are then sent to the IWN Dictionary Display module for rendering information in the dictionary format. 3.2.2 Figure 1. Block diagram of IWN Dictionary. Word Analyzer Word Analyzer analyses and processes the input word. It checks whether it is in its root form. If it is in root form then it is directly passed on to the IWN Database Lookup module, else it will be processed to find the closest possible word in the database. The concept of human mediated lemmatizer is adopted from the work of Bhattacharyya et al., (2014) to find the closest possible words of an input word. Here, the trie data structure is created out of the vocabulary in the input language and this structure is navigated to find the match between the input word and an entry in trie. Accordingly, the closest possible word(s) is populated and given for the next module for the database lookup. 74 IWN Dictionary Display Module The IWN Dictionary Display module takes the extracted multilingual senses of a given word and renders it in the traditional dictionary format. Here, the multilingual IndoWordNet data can be rendered using different views. T"
W15-5910,W12-5021,1,\N,Missing
W15-5911,O97-1002,0,0.167893,"Missing"
W15-5911,J03-1002,0,0.00478677,"following steps. 1. Given a word WEN and WHN and its senses must be present in their respective WordNets i.e. English and Hindi WordNet. 2. Let SEN = {sEN 1 , sEN 2 ...sEN m } & SHN = {sHN 1 , sHN 2 ...sHN n } be the set sense bags. The sense bags are obtained from its synset constituents i.e. content words from concept, examples, synonyms and hypernym (depth=1). We make sure that the words in sense bag must be present in the WordNet. 3. SHN Hindi sense bags must be translated to resourceful language i.e. English. The translations are obtained by making use of bilingual dictionary or GIZA++ (Och and Ney, 2003) aligner that identifies the word alignment considering a parallel corpus. We say that two words are semantically similar if 1. WEN is compared with sEN i and sHN j then their score must be similar. i.e. CLW Sw (WEN , sEN i ) ≈ CLW Sw tr (WEN , sHN j ) this is further explained in section 3.1. 2. If they have similar sense bags. i.e. i j CLW Ss (sEN , sHN ) ≈ 1 & CLW Ss weight (wEN , sEN i , sHN j ) ≈ 1 this is further explained in section 3.2.1 and section 3.2.2. 3 CLWS Similarity Measures Following are the different measures that are used to compute CLWS similarity score. 3.1 Measure 1: Word"
W15-5911,N10-1047,0,0.0169992,"o date, there is no work carried out in computing the CLWS similarity for English and Hindi word pairs. So we define our own baseline system. The synsets in English and Hindi WordNet are organized based on their most frequent sense usage. As a baseline we say that given a word pair their most frequent sense is similar to each other. The baseline system makes an assumption by considering most frequent sense for both the word pairs. So, given a word in English and Hindi their most frequent sense are similar to each other. 5.2 The size of sense bag The IC measures the specificity of the concept (Pedersen, 2010). The general concepts are assigned low value and specific concepts are assigned high value. In this scenario, we measure the specificity of the word instead of concept i.e. IC(w) = −log(P (w)). The IC value is computed for every word present in the sense bag. The size of sense bag depends on the IC threshold i.e. α. The sense bag will contain only those words with IC(w) ≥ α. Figure 2 shows how the size of the sense bag affects the performance of the system. In this, figure the α value is iterated from 0 to 13. When α = 0 the sense bag will contain all the words but as α value increases to t i"
W15-5911,P94-1019,0,\N,Missing
W15-5912,W05-0402,0,0.0946813,"Missing"
W15-5912,S15-2136,0,0.033932,"pression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news documents. In recent times, TER has also been applied to other domains like medical. Approaches for medical domain TER in the past have been either rule-based (Sohn"
W15-5912,S13-2002,0,0.108708,"anner, etc. Early work in TER considers it as a part of named entity recognition (Bikel et al., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news d"
W15-5912,chang-manning-2012-sutime,0,0.130476,"ssed point in time, a duration or a frequency (Wikipedia, 2014). These expressions can be used in information extraction and question-answering to (a) answer time-specific queries, (b) arrange information in a chronological manner, etc. Early work in TER considers it as a part of named entity recognition (Bikel et al., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal wi"
W15-5912,S10-1071,0,0.0942351,"Missing"
W15-5912,S10-1062,0,0.0344268,"., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news documents. In recent times, TER has also been applied to other domains like medical. Approaches for medi"
W15-5914,P08-1087,0,0.0262307,"est of the paper is organized as follows: We present related work in Section 2. Then, we study the basics of factored translation models in Section 3. We also describe a general factored model for handling morphology. Then, we discuss the sparsity problem and the morphology generation, in general in Section 4, and in context of Hindi and Marathi in Section 5. Section 6 draws conclusion and points to future work. 2 Related work Substantial volume of work has been done in the field of translation into morphologically rich languages. The source language can be enriched with grammatical features (Avramidis and Koehn, 2008) or standard translation model can be appended with synthetic phrases (Chahuneau et al., 2013). Also, previous work has been done in order to solve the verb morphology in English to Hindi SMT (Gandhe et al., 2011). Although past work focuses on studying complexity (Tamchyna and Bojar, 2013) and solving morphology using factored translation models (Ramanathan et al., 2009), the problem of data sparsity is not addressed, to the best of our knowledge. 3 Factored translation models Hindi and Marathi are morphologically rich languages Factored translation models can be seen as the compared to Engli"
W15-5914,D13-1174,0,0.0216186,"e basics of factored translation models in Section 3. We also describe a general factored model for handling morphology. Then, we discuss the sparsity problem and the morphology generation, in general in Section 4, and in context of Hindi and Marathi in Section 5. Section 6 draws conclusion and points to future work. 2 Related work Substantial volume of work has been done in the field of translation into morphologically rich languages. The source language can be enriched with grammatical features (Avramidis and Koehn, 2008) or standard translation model can be appended with synthetic phrases (Chahuneau et al., 2013). Also, previous work has been done in order to solve the verb morphology in English to Hindi SMT (Gandhe et al., 2011). Although past work focuses on studying complexity (Tamchyna and Bojar, 2013) and solving morphology using factored translation models (Ramanathan et al., 2009), the problem of data sparsity is not addressed, to the best of our knowledge. 3 Factored translation models Hindi and Marathi are morphologically rich languages Factored translation models can be seen as the compared to English. They are widely spoken in Indian sub95 combination of several components (language contine"
W15-5914,I11-1013,0,0.0153539,"hen, we discuss the sparsity problem and the morphology generation, in general in Section 4, and in context of Hindi and Marathi in Section 5. Section 6 draws conclusion and points to future work. 2 Related work Substantial volume of work has been done in the field of translation into morphologically rich languages. The source language can be enriched with grammatical features (Avramidis and Koehn, 2008) or standard translation model can be appended with synthetic phrases (Chahuneau et al., 2013). Also, previous work has been done in order to solve the verb morphology in English to Hindi SMT (Gandhe et al., 2011). Although past work focuses on studying complexity (Tamchyna and Bojar, 2013) and solving morphology using factored translation models (Ramanathan et al., 2009), the problem of data sparsity is not addressed, to the best of our knowledge. 3 Factored translation models Hindi and Marathi are morphologically rich languages Factored translation models can be seen as the compared to English. They are widely spoken in Indian sub95 combination of several components (language continent. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 95–99,"
W15-5914,P02-1040,0,0.0995611,"Missing"
W15-5914,P09-1090,1,0.829265,"s conclusion and points to future work. 2 Related work Substantial volume of work has been done in the field of translation into morphologically rich languages. The source language can be enriched with grammatical features (Avramidis and Koehn, 2008) or standard translation model can be appended with synthetic phrases (Chahuneau et al., 2013). Also, previous work has been done in order to solve the verb morphology in English to Hindi SMT (Gandhe et al., 2011). Although past work focuses on studying complexity (Tamchyna and Bojar, 2013) and solving morphology using factored translation models (Ramanathan et al., 2009), the problem of data sparsity is not addressed, to the best of our knowledge. 3 Factored translation models Hindi and Marathi are morphologically rich languages Factored translation models can be seen as the compared to English. They are widely spoken in Indian sub95 combination of several components (language continent. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 95–99, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) model, reordering model, translation steps, generation steps). These components define"
W15-5914,Y11-1030,0,0.0176327,"logy Generation A simple and effective solution to the sparsity problem is to generate the unseen morphological forms of words and inject them into original model. Note that, we also need to generate factors that affect the inflections of the newly generated morphological forms. For example, for the factored model described in Section 3.1, we need to generate new Source root|{S} → Target surface 96 word|Target root|suffix pairs. Figure 2: Factored model setup to handle nominal inflections Similarly, verb inflections in Hindi are affected by gender, number, person, tense, aspect, and modality (Singh and Sarma, 2011). As it is difficult to extract gender from English verbs, we do not use it as a factor on English side. We just replicate English verbs for each gender inflection on Hindi side. Hence, set S, as in Section 3.1, consists of number, person, tense, aspect, and modality only. We build similar factored model for Marathi nouns and verbs. But, Marathi is morphologically more complex than Hindi, as multiple suffixes can be attached with Marathi root nouns and root verbs. But, still we can generate one-suffix word forms of Marathi nouns and verbs. 5.2 Building word-form dictionary Word-form dictionary"
W15-5914,N03-1033,0,0.00633839,"icon. It consisted of 182,544 noun forms and 310,392 verb forms of Hindi and 9,869 noun forms and 101,621 verb forms of Marathi. Moses toolkit3 was used for training and decoding. Language model was trained on the target side corpus with IRSTLM 4 . For our experiments, We compared the translation outputs of: Phrase-based (unfactored) model (Phr), basic factored model (Fact) as in Section 5.1, phrase-based model trained on the corpus augmented with word-form dictionary (Phr’), and factored model trained on the corpus augmented with the word-form dictionary (Fact’). We use Stanford POS tagger5 (Toutanova et al., 2003) and Stanford’s typed dependencies (De Marneffe et al., 2008) to extract the factors that affect the inflections (number, person, tense, etc.) from English sentence. 5.3.1 Automatic evaluation The translation systems were evaluated by BLEU score (Papineni et al., 2002). Also, as the reduc2 To use null when particular word can not have that factor http://www.statmt.org/moses/ 4 https://hlt.fbk.eu/technologies/irstlm-irst-languagemodelling-toolkit 5 http://nlp.stanford.edu/software/tagger.shtml 3 Morph. problem Noun Verb Noun & Verb Noun & Verb Model Fact Fact’ Fact Fact’ Fact Fact’ Phr Phr’ BLE"
W15-5914,D07-1091,0,\N,Missing
W15-5914,N03-1017,0,\N,Missing
W15-5916,W14-3348,0,0.0151644,"cy score of the chat output and Adequacy score of the chat output. Fluency indicates the grammatical correctness where as adequacy indicates how appropriate the machine translation is, when it comes to semantic transfer. We then compute the average Inter Annotator Agreement (IAA) between their scores. Table 6 presents the results. We have obtained a substantial IAA via Fliess’ Kappa evaluation. To justify the second point, we input the un-normalized chat messages to three differ110 ent translators viz. Google, Bing and SataAnuvaadak. We then obtain the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) translation scores. We then compute the evaluation scores after applying the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 2007). On the other hand, our METEOR modu"
W15-5916,W14-3308,1,0.899573,"the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 2007). On the other hand, our METEOR module has been modified to support stemming, paraphrasing and lemmatization (Dungarwal et al., 2014) for Indian Languages, tackling such nuances to some extent. This may have accounted for the score differences. 5 Conclusion and Future Work In today’s era of short messaging and chatting, there is a great need to make available a multilingual chat system where users can interact despite the language barrier. We present such an Indian language IM application that facilitates cross lingual text based communication. Our success in developing such an application for English to Indian languages is a small step in providing people with easy access to such a chat system. We plan to make efforts towa"
W15-5916,2000.tc-1.6,0,0.0626886,"Missing"
W15-5916,P07-2045,0,0.0059458,"MS and Chatting) to plain English. Use of such language (often referred as Chatting Language) induces noise which poses additional processing challenges. While dictionary lookup based methods6 are popular for Normalization, they can not make use of context and domain knowledge. For example, yr can have multiple translations like year, your. We tackle this by implementing a normal6 http://www.lingo2word.com 108 ization system7 (Raghunathan and Krawczyk, 2009) as a Phrase Based Machine Translation System, that learns normalization patterns from a large number of training examples. We use Moses (Koehn et al., 2007), a statistical machine translation system that allows training of translation models. Training process requires a Language Model of the target language and a parallel corpora containing aligned un-normalized and normalized word pairs. Our language model consists of 15000 English words taken from the web. Parallel corpora was collected from the following sources : 1. Stanford Normalization Corpora which consists of 9122 pair of un-normalized and normalized words / phrases. 2. The above corpora, however, lacks acronyms and short hand texts like 2mrw, l8r, b4 which are frequently used in chattin"
W15-5916,kunchukuttan-etal-2014-shata,1,0.853522,"t time or a farmer from Punjab trying to get tips from a Tamil speak1 http://www.cfilt.iitb.ac.in/transchat/ 106 ing professor on modern agricultural tools and techniques, communication is often hindered by language barrier. This problem has been recognized and well-studied by computational linguists as a result of which a large number of automatic translation systems have been proposed and modified in the last 30 years. Some of the notable Indian Language translation systems include Anglabharati (Sinha et al., 1995), Anusaraka (Padmanathrao, 2009), Sampark (Anthes, 2010) and Sata-Anuvaadak2 (Kunchukuttan et al., 2014). Popular organizations like Google and Microsoft also provide translation solutions for Indian languages through Google- and BingTranslation systems. Stymne (2011) demonstrate techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation, which can be utilized in a chat scenario. But even after so many years of MT research, one can still claim that these systems have not been able to attract a lot of users. This can be attributed to factors like(a) poor user experience in terms of UI design, (b) systems being highly computationalresource intensive and slow i"
W15-5916,P02-1040,0,0.108661,"the system, Usability score, Fluency score of the chat output and Adequacy score of the chat output. Fluency indicates the grammatical correctness where as adequacy indicates how appropriate the machine translation is, when it comes to semantic transfer. We then compute the average Inter Annotator Agreement (IAA) between their scores. Table 6 presents the results. We have obtained a substantial IAA via Fliess’ Kappa evaluation. To justify the second point, we input the un-normalized chat messages to three differ110 ent translators viz. Google, Bing and SataAnuvaadak. We then obtain the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) translation scores. We then compute the evaluation scores after applying the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 20"
W15-5916,W11-2159,0,0.0191858,"nication is often hindered by language barrier. This problem has been recognized and well-studied by computational linguists as a result of which a large number of automatic translation systems have been proposed and modified in the last 30 years. Some of the notable Indian Language translation systems include Anglabharati (Sinha et al., 1995), Anusaraka (Padmanathrao, 2009), Sampark (Anthes, 2010) and Sata-Anuvaadak2 (Kunchukuttan et al., 2014). Popular organizations like Google and Microsoft also provide translation solutions for Indian languages through Google- and BingTranslation systems. Stymne (2011) demonstrate techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation, which can be utilized in a chat scenario. But even after so many years of MT research, one can still claim that these systems have not been able to attract a lot of users. This can be attributed to factors like(a) poor user experience in terms of UI design, (b) systems being highly computationalresource intensive and slow in terms of response time, and (c) bad quality translation output. Moreover, the current MT interfaces do not provide a natural environment to attract more number of"
W15-5920,D11-1100,1,0.896406,"Missing"
W15-5920,P07-1056,0,0.118002,"lar words. We have performed an extensive evaluation of DDPW participation in sentiment analysis using three domains: movie, product and tourism. 3.1 Domain and Dataset Providing polarity information about movie reviews is a very useful service (Turney, 2002). Its proof is the continuously growing popularity of the several film review websites67 . For movie domain, we use 1000 positive and 1000 negative reviews8 . Product reviews directly affect the business of e-commerce organizations. For product domain, we use 1000 positive and 1000 negative reviews (music instruments) from Amazon, used by Blitzer et al. (2007). The third domain is the tourism domain, a more accurate sentiment analysis in tourism domain can suggest a more accurate place for visit. We use 700 positive and 700 negative tourism reviews, used by Khapra et al. (2010) to train a word sense disambiguation system. In this paper, we report domain-dedicated words for the movie, product and tourism domain and show that these words are better features than universal sentiment lexicons, unigrams and adjectives for sentiment analysis in the respective domain. 3.2 Chi-Square Test The Chi-Square test is a statistical test to identify the class (pos"
W15-5920,C00-1044,0,0.237932,"Missing"
W15-5920,C04-1200,0,0.642089,"itive in the movie domain. Hence, unpredictable assigns negative polarity to the first sentence and positive polarity to the second sentence. Due to the absence of chameleon words like unpredictable, the USL based classifier fails to determine the correct polarity of the sentences that contain chameleon words. Introduction The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; DanescuNiculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the real world, most sentiment analysis applications are domain oriented. All business organizations are interested in sentiment information about the product they 130 deal with. For instance, an automobile organiza• On the other hand, consistency in use of a polar word in a particular domain, makes it a 1 http://sentiwordnetisti.cnr.it/ http://mpqa.cs.pitt.edu/ 3 http://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 130–137, c Trivandrum, India. December 2"
W15-5920,P98-2127,0,0.0663408,"Missing"
W15-5920,P07-1055,0,0.0237651,"very large. As a whole, accuracy figures validate the prominence of identification of domain-dedicated polar words prior to the implementation of classification algorithm. 7 Related Work Several works use the universal sentiment lexicons to decide whether a sentence expresses a sentiment (Riloff and Wiebe, 2003; Whitelaw et al., 2005; Mukherjee et al., 2012). Considering that the USL solely is not sufficient to achieve satisfactory performance, there are some more works that combine additional feature types for sentiment classification exist (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; McDonald et al., 2007; Melville et al., 2009; Ng et al., 2006). 135 Wiebe (2000), for the first time, worked in the area of sentiment lexicon. She focused on the problem of identifying subjective adjectives with the help of the corpus. She proposed an approach to find subjective adjectives based on the distributional similarity from the Lin (1998) thesaurus. Her approach was seeded by manually provided strong subjectivity clues. She used this new set of adjectives to find subjectivity in sentences, just by the presence of an adjective from the new set. However, the approach was unable to predict sentiment orientat"
W15-5920,P06-2079,0,0.164139,"e document d and n be the size of the feature set. Then, each document d is represented as a feature vector as shown here. → − d = (p1 (d), p2 (d), p3 (d), ..........., pn (d)) 6 Results and Discussion The table 1 shows the classification accuracies for the movie domain obtained with various feature sets and techniques separately. Accuracy is calculated as a fraction of total input documents that are correctly classified by the classifiers. The accuracies resulting from using only DDPW as features are shown in row (1) of table 1. In literature, unigrams are considered as stateof-art features (Ng et al., 2006; Pang et al., 2002) for sentiment analysis, we also experimented with unigrams. Domain-dedicated words as features perform better than unigrams with all the three classification algorithms. At the same time, domain-dedicated words speed up the classification process with a small feature set of size 920. Since adjectives have been crucial clues in sentiment prediction (Hatzivassiloglou and Wiebe, 2000), we experimented with all the adjectives and top adjectives. We find that both the feature sets are not as effective as DDPW. We experimented with a universal sentiment lexicon provided by Wiebi"
W15-5920,P04-1035,0,0.120441,"negative polarity in the automobile domain, but it is positive in the movie domain. Hence, unpredictable assigns negative polarity to the first sentence and positive polarity to the second sentence. Due to the absence of chameleon words like unpredictable, the USL based classifier fails to determine the correct polarity of the sentences that contain chameleon words. Introduction The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; DanescuNiculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the real world, most sentiment analysis applications are domain oriented. All business organizations are interested in sentiment information about the product they 130 deal with. For instance, an automobile organiza• On the other hand, consistency in use of a polar word in a particular domain, makes it a 1 http://sentiwordnetisti.cnr.it/ http://mpqa.cs.pitt.edu/ 3 http://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language P"
W15-5920,W02-1011,0,0.033555,"n be the size of the feature set. Then, each document d is represented as a feature vector as shown here. → − d = (p1 (d), p2 (d), p3 (d), ..........., pn (d)) 6 Results and Discussion The table 1 shows the classification accuracies for the movie domain obtained with various feature sets and techniques separately. Accuracy is calculated as a fraction of total input documents that are correctly classified by the classifiers. The accuracies resulting from using only DDPW as features are shown in row (1) of table 1. In literature, unigrams are considered as stateof-art features (Ng et al., 2006; Pang et al., 2002) for sentiment analysis, we also experimented with unigrams. Domain-dedicated words as features perform better than unigrams with all the three classification algorithms. At the same time, domain-dedicated words speed up the classification process with a small feature set of size 920. Since adjectives have been crucial clues in sentiment prediction (Hatzivassiloglou and Wiebe, 2000), we experimented with all the adjectives and top adjectives. We find that both the feature sets are not as effective as DDPW. We experimented with a universal sentiment lexicon provided by Wiebi to capture more con"
W15-5920,W03-1014,0,0.431071,"a concise feature set for sentiment analysis in a domain. Use of domaindedicated polar words as features beats the state of art accuracies achieved independently with unigrams, adjectives or Universal Sentiment Lexicon (USL). 1 tion is concerned only about recognizing the sentiment information received for automobiles only. Therefore, a list of Domain Dedicated Polar Words (DDPW) can be proved as the best lexical resource for domain oriented sentiment analysis. Most sentiment analysis applications rely on the Universal Sentiment Lexicons (USL) as a key feature along with additional features (Riloff and Wiebe, 2003). There are many USL resources like senti-word-net1 , subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu. These lexicons contain only those words that are usual and have the same polarity across all the domains. These universal sentiment lexicons have the following problems: • The words that have fluctuating polarity across domains, but have fixed polarity in a domain are strong candidate for the sentiment analysis in that domain. We call such words chameleon words. Consider the following example of fluctuating polarity phenomenon. 1. The cars steering was"
W15-5920,I13-1076,1,0.711383,"Missing"
W15-5920,P05-1017,0,0.0541039,"omain. Hence, unpredictable assigns negative polarity to the first sentence and positive polarity to the second sentence. Due to the absence of chameleon words like unpredictable, the USL based classifier fails to determine the correct polarity of the sentences that contain chameleon words. Introduction The general approach of Sentiment Analysis (SA) is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientation of the individual words (Riloff and Wiebe, 2003; Pang and Lee, 2004; DanescuNiculescu-Mizil et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). In the real world, most sentiment analysis applications are domain oriented. All business organizations are interested in sentiment information about the product they 130 deal with. For instance, an automobile organiza• On the other hand, consistency in use of a polar word in a particular domain, makes it a 1 http://sentiwordnetisti.cnr.it/ http://mpqa.cs.pitt.edu/ 3 http://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 130–137, c Trivandrum, India. December 2015. 2015 NLP Associatio"
W15-5920,P02-1053,0,0.068801,"ntification of Domain Dedicated Polar Words The orientation of the polarity of a word and its frequency of usage vary from domain to domain. Such domain-dedicated polar words are the important clues for sentiment analysis in that domain. This section illustrates the stage-1 of the two-stage sentiment analysis approach that generates domain-dedicated polar words. We have performed an extensive evaluation of DDPW participation in sentiment analysis using three domains: movie, product and tourism. 3.1 Domain and Dataset Providing polarity information about movie reviews is a very useful service (Turney, 2002). Its proof is the continuously growing popularity of the several film review websites67 . For movie domain, we use 1000 positive and 1000 negative reviews8 . Product reviews directly affect the business of e-commerce organizations. For product domain, we use 1000 positive and 1000 negative reviews (music instruments) from Amazon, used by Blitzer et al. (2007). The third domain is the tourism domain, a more accurate sentiment analysis in tourism domain can suggest a more accurate place for visit. We use 700 positive and 700 negative tourism reviews, used by Khapra et al. (2010) to train a word"
W15-5920,W03-1017,0,0.0984093,"perform the best, although the differences are not very large. As a whole, accuracy figures validate the prominence of identification of domain-dedicated polar words prior to the implementation of classification algorithm. 7 Related Work Several works use the universal sentiment lexicons to decide whether a sentence expresses a sentiment (Riloff and Wiebe, 2003; Whitelaw et al., 2005; Mukherjee et al., 2012). Considering that the USL solely is not sufficient to achieve satisfactory performance, there are some more works that combine additional feature types for sentiment classification exist (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; McDonald et al., 2007; Melville et al., 2009; Ng et al., 2006). 135 Wiebe (2000), for the first time, worked in the area of sentiment lexicon. She focused on the problem of identifying subjective adjectives with the help of the corpus. She proposed an approach to find subjective adjectives based on the distributional similarity from the Lin (1998) thesaurus. Her approach was seeded by manually provided strong subjectivity clues. She used this new set of adjectives to find subjectivity in sentences, just by the presence of an adjective from the new set. However, the approa"
W15-5920,C98-2122,0,\N,Missing
W15-5931,E09-1015,0,0.0529456,"Missing"
W15-5931,clement-etal-2004-morphology,0,0.057294,"Missing"
W15-5931,W09-4615,0,0.0765342,"Missing"
W15-5931,C12-1006,0,0.0197559,"al., 2012; Linden and Tuovila, 2009) to select a single paradigm in cases where more than one paradigm generates the same set of word forms. These systems use POS information or some additional user in1 Citation form of words D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 203–208, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) put from native language speakers to map words to paradigms, instead of a corpus alone. Lexicon acquisition methods (Carlos et al., 2009; Clement et al., 2004; Forsberg et al., 2006; Mohammed et al., 2012) exist for many languages that extract lemmas from a corpus and map them to morphological paradigms. Functional Morphology has been used to deﬁne morphology for languages like Swedish and Finnish, and tools based on Functional Morphology, namely Extract (Forsberg et al., 2006) which suggest new words for a lexicon and map them to paradigms, have been developed. To be able to use a tool like Extract, the morphology of the language has to be ﬁtted into the Functional Morphology deﬁnition. 3 Terminology and Notations Used Deﬁnition (Root, Stem, Base, Preﬁx and Suﬃx): A Root is the basic part of a"
W15-5931,2012.freeopmt-1.4,0,0.070262,"Missing"
W15-5937,W10-3710,0,0.030442,"Rhyme: Either identical ultimate stressed vowels or identical phoneme sequences following the ultimate stressed vowel, but not both Examples cat-cat, বাঁেক-বাঁেক (baanke-baanke) cat-rat, বাঁেক-থােক (baanke-thaake) stick-picket জবা - অবাক (joba-obaak) queen-afternoon কেলাল - েকালাহল (kallol-kolahol) Table 1: Types of Rhyme (anaadore abohelay). To detect alliteration, we check the beginning sound of each word for every pair of consecutive words in a line. Reduplication refers to the repetition of any linguistic unit such as a phoneme, morpheme, word, phrase, clause or the utterance as a whole (Chakraborty and Bandyopadhyay, 2010). It is mainly used for emphasis, generality, intensity or to show continuation of an act. It may be partial (খাওয়া দাওয়া khaawa daawa) or complete (আকােশ আকােশ akaashe akaashe). We check only for complete reduplication. We use a simple algorithm that basically checks if two consecutive words in the poem are identical. 4.2 Rhyme Scheme Detection A rhyme scheme is the pattern of rhymes at the end of each line of a poem or song. The rhyme scheme of the poem can be determined by looking at the end word in each line of a poem. Various rhyme schemes are used. Ex: abab, aabb, ababcc and so on. In th"
W15-5937,W12-2502,0,0.0521765,"n Natural Language Processing, pages 247–253, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) cuss the literature in Section 2, and describe our approach in Section 3. The system architecture and its details have been described in 4. The experimental setup and results are covered in sections 5 and 6, respectively. We delve into analysis of the results in Section 7. We conclude our work and discuss scope for future work in Section 8. 2 Related Work Computational understanding of poetry has been previously studied for languages such as English (Kaplan and Blei, 2007), (Kao and Jurafsky, 2012), Chinese (Voigt and Jurafsky, 2013) and Malay (Jamal et al., 2012). Kaplan and Blei (2007) analyse American poems in terms of style and visualise them as clusters. Kao and Jurafsky (2012) use various stylistic features to categorise poems into ones written by professional and amateur poets, and establish the importance of Imagism in poetry of high-quality. Lou et al. (2015) use of a SVM to classify poems in English into 3 main categories and 9 subcategories by combining tf-idf and Latent Dirichlet Allocation. All this work has been done for English. Voigt and Jurafsky (2013) observed through"
W15-5937,W13-1403,0,\N,Missing
W15-5943,P14-1023,0,0.248687,"co-occurrence estimate of w1 w2 using the corpus alone. 1. Use WordNet to detect synonyms, antonyms. 2. Further refine this estimate by using co-occurrence estimate of w1′ w2′ , where w1′ and w2′ are synonyms or antonyms of w1 and w2 respectively. 2. Use similarity measures either facilitated by WordNet or by the word embeddings. These options lead to the following three concrete heuristics for the detection of noun compounds and noun+verb compounds for word pair w1 w2 . In order to estimate co-occurrence of w1 w2 , one can use word embeddings or word vectors. Such techniques try to predict (Baroni et al., 2014), rather than count the co- 3.1 Approach 1: Using occurrence patterns of different tuples of WordNet-based Features words. The distributional aspect of these 1. Let WNBag = {w′ | w′ representations enables one to estimate the = IsSynOrAnto(w1 )}, where the co-occurrence of, say, cat and sleeps, using function IsSynOrAnto returns either the co-occurrence of dogs and sleep. Such a synonym or an antonym of w1 , by word embeddings are typically trained on looking up the WordNet. raw corpora, and the similarity between a pair of words is computed by calculating 2. If w2 ∈ WNBag, then w1 w2 is a MWE"
W15-5943,J90-1003,0,0.338866,"of above MWEs. Characteristics of MWEs MWE has different characteristics based on their usage, context and formation. They are as followsCompositionality: Compositionality refers to the degree to which the meaning of MWEs can be predicted by combining the meanings of their components. E.g. 2 Background and Related Work तरण ताल (taraNa taala, swimming pool), Most of the proposed approaches for the धन लकषमी (dhana laxmii, wealth), चाय पानी detection of MWEs are statistical in na- (chaaya paanii, snacks), etc. Non-Compositionality: In nonture. Some of these approaches use association measures (Church and Hanks, 1990), compositionality, the meaning of MWEs deep linguistics based methods (Bansal et cannot be completely determined from the meaning of its constituent words. 2 IndoWordNet is available in following Indian lanIt might be completely different from guages: Assamese, Bodo, Bengali, English, Gujarati, its constituents. E.g. गुजर जाना, (gujara Hindi, Kashmiri, Konkani, Kannada, Malayalam, Manipuri, Marathi, Nepali, Punjabi, Sanskrit, Tamil, Teljaanaa, passed away), नजर डालना, (najara ugu and Urdu. These languages cover three different Daalanaa, flip through). There might be language families, Indo Ar"
W15-5943,W13-1010,0,0.0473307,"Missing"
W15-5943,N13-1090,0,0.138025,"an, etc. Similarly, IndoWordNet2 (Bhattacharyya, 2010) covers the major families of languages, viz., Indo-Aryan, Dravidian and Sino-Tibetian which are used in the subcontinent. Building WordNets is a complex task. It takes lots of time and human expertise to build and maintain WordNets. A recent development in computational linguistics is the concept of distributed representations, commonly referred to as Word Vectors or Word Embeddings. The first such model was proposed by Bengio et al. (2003), followed by similar models by other researchers viz., Mnih et al. (2007), Collobert et al. (2008), Mikolov et al. (2013a), Pennington et al. (2014). These models are extremely fast to train, are automated, and rely only on raw corpus. Mikolov et al. (2013c; 2013b) have reported various linguistic regularities captured by such models. For instance, vectors of synonyms and antonyms will be highly similar when evaluated using cosine similarity measure. Thus, these models can be used to replace/supplement WordNets and other such resources in different NLP applications (Collobert et al., 2011). The roadmap of the paper is as follows, Section 2 describes the background and related work. Our approaches are detailed i"
W15-5943,W06-1205,0,0.0199095,"he evaluation is given in section 4. Experiments and results are presented in Section 5. Section 6 concludes the paper and points to the future work. al., 2014), word embeddings based measures (Salehi et al., 2015), etc. The work related to the detection of MWEs has been limited in the context of Indian languages. The reasons are, unavailability of gold data (Reddy, 2011), unstructured classification of MWEs, complicated theory of MWEs, lack of resources, etc. Most of the approaches of Hindi MWEs have used parallel corpus alignment and POS tag projection to extract MWEs (Sriram et al., 2007) (Mukerjee et al., 2006). Venkatapathy et al. (2007) used a classification based approach for extracting noun+verb collocations for Hindi. Gayen and Sarkar et al. (2013) used Random Forest approach wherein features such as verb identity, semantic type, case marker, verbobject similarity, etc. are used for the detection of compound nouns in Bengali using MaxEnt Classifier. However, our focus is on detecting MWEs of the type compound noun and noun+verb compounds while verb based features are not implemented in our case. We have used word embeddings and WordNet based features for the detection of above MWEs. Characteris"
W15-5943,D14-1162,0,0.0845045,"WordNet2 (Bhattacharyya, 2010) covers the major families of languages, viz., Indo-Aryan, Dravidian and Sino-Tibetian which are used in the subcontinent. Building WordNets is a complex task. It takes lots of time and human expertise to build and maintain WordNets. A recent development in computational linguistics is the concept of distributed representations, commonly referred to as Word Vectors or Word Embeddings. The first such model was proposed by Bengio et al. (2003), followed by similar models by other researchers viz., Mnih et al. (2007), Collobert et al. (2008), Mikolov et al. (2013a), Pennington et al. (2014). These models are extremely fast to train, are automated, and rely only on raw corpus. Mikolov et al. (2013c; 2013b) have reported various linguistic regularities captured by such models. For instance, vectors of synonyms and antonyms will be highly similar when evaluated using cosine similarity measure. Thus, these models can be used to replace/supplement WordNets and other such resources in different NLP applications (Collobert et al., 2011). The roadmap of the paper is as follows, Section 2 describes the background and related work. Our approaches are detailed in section 3. The description"
W15-5943,I11-1024,0,0.025576,"rces in different NLP applications (Collobert et al., 2011). The roadmap of the paper is as follows, Section 2 describes the background and related work. Our approaches are detailed in section 3. The description of the datasets used for the evaluation is given in section 4. Experiments and results are presented in Section 5. Section 6 concludes the paper and points to the future work. al., 2014), word embeddings based measures (Salehi et al., 2015), etc. The work related to the detection of MWEs has been limited in the context of Indian languages. The reasons are, unavailability of gold data (Reddy, 2011), unstructured classification of MWEs, complicated theory of MWEs, lack of resources, etc. Most of the approaches of Hindi MWEs have used parallel corpus alignment and POS tag projection to extract MWEs (Sriram et al., 2007) (Mukerjee et al., 2006). Venkatapathy et al. (2007) used a classification based approach for extracting noun+verb collocations for Hindi. Gayen and Sarkar et al. (2013) used Random Forest approach wherein features such as verb identity, semantic type, case marker, verbobject similarity, etc. are used for the detection of compound nouns in Bengali using MaxEnt Classifier. H"
W15-5943,N15-1099,0,0.0223059,"s and antonyms will be highly similar when evaluated using cosine similarity measure. Thus, these models can be used to replace/supplement WordNets and other such resources in different NLP applications (Collobert et al., 2011). The roadmap of the paper is as follows, Section 2 describes the background and related work. Our approaches are detailed in section 3. The description of the datasets used for the evaluation is given in section 4. Experiments and results are presented in Section 5. Section 6 concludes the paper and points to the future work. al., 2014), word embeddings based measures (Salehi et al., 2015), etc. The work related to the detection of MWEs has been limited in the context of Indian languages. The reasons are, unavailability of gold data (Reddy, 2011), unstructured classification of MWEs, complicated theory of MWEs, lack of resources, etc. Most of the approaches of Hindi MWEs have used parallel corpus alignment and POS tag projection to extract MWEs (Sriram et al., 2007) (Mukerjee et al., 2006). Venkatapathy et al. (2007) used a classification based approach for extracting noun+verb collocations for Hindi. Gayen and Sarkar et al. (2013) used Random Forest approach wherein features s"
W15-5943,W01-0513,0,0.0833145,"skrit, Tamil, Teljaanaa, passed away), नजर डालना, (najara ugu and Urdu. These languages cover three different Daalanaa, flip through). There might be language families, Indo Aryan, Sino-Tibetan and Dra296 vidian. http://www.cfilt.iitb.ac.in/indowordnet/ some added elements or inline meaning to MWEs that cannot be predicted from its parts. E.g. नौ दो गयारह होना (nau do gyaaraha honaa, run away). Non-Substitutability: In non substitutability, the components of MWEs cannot be substituted by its synonyms without distorting the meaning of the expression even though they refer to the same concept (Schone and Jurafsky, 2001). E.g. in the expression चाय पानी (chaaya paanii, snacks), the word paanii (water) cannot be replaced by its synonym जल (jala, water) or नीर (niira, water) to form the meaning ’snacks’. Collocation: Collocations are a sequence of words that occur more often than expected by chance. They do not show either statistical or semantical idiosyncrasy. They are fixed expressions and appear very frequently in running text. E.g. कड़क चाय (kaDaka chaaya, strong tea), काला धन (kaalaa dhana, black money), etc. Non-Modifiability: In nonmodifiablility, many collocations cannot be freely modified by grammatica"
W15-5943,W09-2906,0,0.0126871,"hat, for the detection of above mentioned MWEs, word embeddings can be a reasonable alternative to WordNet, especially for those languages whose WordNets does not have a better coverage. 1 Introduction Multiword Expressions or MWEs can be understood as idiosyncratic interpretations or words with spaces wherein concepts cross the word boundaries or spaces (Sag et al., 295 2002). Some examples of MWEs are ad hoc, by and large, New York, kick the bucket, etc. Typically, a multiword is a noun, a verb, an adjective or an adverb followed by a light verb (LV) or a noun that behaves as a single unit (Sinha, 2009). Proper detection and sense disambiguation of MWEs is necessary for many Natural Language Processing (NLP) tasks like machine translation, natural language generation, named entity recognition, sentiment analysis, etc. MWEs are abundantly used in Hindi and other languages of Indo Aryan family. Common part-of-speech (POS) templates of MWEs in Hindi language include the following: noun+noun, noun+LV, adjective+LV, adjective+noun, etc. Some examples of Hindi multiwords are पुणय ित थ (puNya tithi, death anniversary), वादा करना (vaadaa karanaa, to promise), आग लगाना (aaga lagaanaa, to burn), धन द"
W15-5943,J15-4004,0,\N,Missing
W15-5944,H05-1085,0,0.0339948,"similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be considered for selection of good pivot language. Use of sourceside segmentation as pre-processing technique has been demonstrated by (Kunchukuttan et al., 2014). Goldwater and McClosky (2005) investigates several methods for incorporating morphological information to achieve better translation from Czech to English. Most of the pivot strategies mentioned above focus on the situation of resource-poor languages where direct translation is either very poor or not available. Our approach, like Dabre et al. (2014), tries to employ pivot strategy to help improve the performance of existing SMT systems. To the best of our knowledge, our work is the first attempt to integrate word segmentation with pivot-based SMT. 3 Our System We propose a system which integrates word segmentation with t"
W15-5944,D07-1091,0,0.0548085,"orpus Pivot-Target Corpus Morfessor SourceMorphPivotMorph PivotMorphTarget SMT Training SMT Training SourceMorphPivotMorph Phrase table PivotMorphTarget Phrase table Triangulation SourceMorphTarget Phrase table Tune And Test Figure 1: Integration of word segmentation with triangulation 3.4 Multiple Decoding Paths We use the triangulated phrase table to supplement the direct phrase table. In order to integrate these two phrase tables, we use the multiple decoding paths (MDP) feature provided by the 1 https://github.com/anoopkunchukuttan/indic_nlp_library Moses decoder. Multiple decoding paths (Koehn and Hoang, 2007) allows us to lookup multiple translation models for hypothesis at decoding time, and the choice of best hypothesis at decoding time based on available evidence. We use MDP to combine one or more pivot-based MT systems with the direct MT system. This constitutes our final decoding system. We preferred this option over offline linear interpolation of phrase tables since the framework can dynamically consider phrases from multiple phrase tables and wouldn’t need any hyperparameter tuning. 4 Experiments The aim of experiments is to study impact of pivot strategies and word segmentation, separatel"
W15-5944,N03-1017,0,0.0188284,"ively. Using these two models, we induce a source-target model. The two important components to be calculated are - 1) phrase translation probability and 2) lexical weight. The Phrase translation probability is estimated by marginalizing over all possible pivot phrase, along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text dat"
W15-5944,P02-1040,0,0.0925006,"g, 500 sentences are used for tuning and 2000 sentences are used for testing. For the experiments, we use phrase-based SMT training and 5-gram SRILM language model. Tuning is done using the MERT algorithm. The triangulated MT systems use default distance based reordering while direct systems use wbe-msdbidirectional-fe-allff model 4.2 Experimental Setup We trained various phrase based SMT systems by combining the basic systems mentioned in Section 3. We use a threshold of 0.001 for phrase translation probability to manage size of triangulated phrase table. The performance metric used is BLEU (Papineni et al., 2002). The following are the configurations we experimented with: 1. DIR: MT system trained on direct SourceTarget corpus. 2. DIR_Morph: DIR system with source-text word-segmented. 3. PIVOT: MT system based on triangulated phrase table of Source-Target using a single 305 Pivot language. 4. PIVOT_Morph: PIVOT system with both Source and Pivot texts segmented. 5. PIVOT_SourceMorph: PIVOT system with only Source text segmented. 6. DIR+PIVOT: MT system based on integration of DIR and PIVOT phrase tables using MDP. 7. DIR_Morph+PIVOT_Morph: MT system based on integration of DIR_Morph and PIVOT_Morph usi"
W15-5944,N07-1061,0,0.0200742,"t work done in the field. Section 3 explains the design of our system in detail. Section 4 describes the experimental setup. Results of the experiments are discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, Ind"
W15-5944,P07-1108,0,0.453884,"re discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be consid"
W15-5944,P09-1018,0,0.0175912,", along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text data. Morfessor uses probabilistic machine learning methods to do the task of segmentation. The trained models for word segmentation of Indian languages are available to use1 . 3.3 Integrating word segmentation with Triangulation In our system, we use both phrase table"
W15-5945,P98-1004,0,0.0494915,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignment"
W15-5945,J93-2003,0,0.0480692,"for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 20"
W15-5945,P03-1012,0,0.0503326,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in lo"
W15-5945,P08-1112,0,0.0229563,", statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently"
W15-5945,C04-1005,0,0.051151,"nments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may"
W15-5945,I13-2006,1,0.856431,"form word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may belong. This helps in disambiguation and thus selecting word correspondences but can also give rise to increased vocabulary thus making the training data more sparse. Joshi et al. (2013) use in domain parallel data to inject additional alignment mappings for the news headline domain. Finally, Koehn et al. (2007) propose a factored translation model that can incorporate any linguistic factors including POS information in phrase-based SMT. It provides a generalized representation of a translation model, because it can map multiple source and target factors. It may help to effectively handle out-of-vocabulary (OOV) by incorporating many linguistic factors, but it still crucially relies on the initial quality of word alignment that will dominate the translation probabilities. In"
W15-5945,P10-1155,1,0.670319,"y the topic model for the tourism dataset 5 Experiment Setup 2. Cartesian product Approach: In this approach the pseudo parallel data was created using MLTM approach described earlier, and added to the training data before training the MT systems. We added the pseudo parallel data to training data using the approach indicated in Figure 3. Thus, for 50 topics and 11 top words, we add 550 pseudo-parallel sentences, each of length 1. In this section, we describe the dataset, and setup for the experiments conducted. 5.1 Dataset For our experiments, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We use these for both the creation of pseudo parallel data, and training Machine translation systems. We separate 500 sentences each for testing and tuning purposes. We ensure that they are not present in the training corpus. 5.2 3. Sentential Approach: We added the pseudo parallel data created using MLTM approach to the training data using the approach indicated in Figure 4. Thus, for 50 topics and 11 top words, we add 50 pseudo-parallel sentences, each of length 11. Setup We implemented the mul"
W15-5945,P07-2045,0,0.0298051,"dely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 2008; Lee et al., 2006; Koehn et al., 2007). Mimno et al. (2009) present a multilingual topic model called PolyLDA, and apply it for Machine Translation for European and other languages such as Danish, German, Greek, English, Spanish, etc. Since multilingual topic models generate parallel topics: parallel clusters of words that are likely to be about the same theme, these topics provide coarse alignment that a Moses-like translation system can leverage on. The idea is to not rely on any external ontology such as WordNet and to rely purely on a parallel corpus to create such coarse alignments. The focus of our paper is to improve word a"
W15-5945,J00-2004,0,0.0224318,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanc"
W15-5945,D09-1092,0,0.0304376,"Missing"
W15-5945,P00-1056,0,0.750202,"the top terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daum´e III (2010) use a bilingual dictionary, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barb"
W15-5945,J03-1002,0,0.00636946,"ense disambiguation, statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between"
W15-5945,tufis-barbu-2002-lexical,0,0.0496959,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
W15-5945,J97-3002,0,0.384589,"nment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
W15-5945,C98-1004,0,\N,Missing
W15-5946,N15-1125,1,0.903423,"he sourcetarget corpus from source-pivot and pivot-target corpora. It was found through their experiments that the Triangulation Method outperforms the Sythetic method as well. This encouraged us to focus our work on the triangulation approach. Nakov and Ng (2012) showed that merging the phrase tables by giving priority to the original table that is extracted from a direct parallel corpus(direct table) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weight values to the translation probabilities from a direct source-pivot phrase table. Dabre et al. (2015) studied the use of more than one pivots simultaneously. They also mentioned that the use of pivot for reordering would be an interesting problem. The reordering model on which we focus in our work is a lexicalized reordering model discussed by Koehn et al. (2005). Lexicalized reordering was first proposed by Tillmann (2004). Similar approach was suggested by Ohashi et al. (2005). Mirkin (2014) proposed several ways for incrementally updating the lexicalized reordering model when new training data is introduced and described a way to combine new and existing reordering models. To the best of o"
W15-5946,2005.iwslt-1.8,0,0.0249234,"showed that merging the phrase tables by giving priority to the original table that is extracted from a direct parallel corpus(direct table) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weight values to the translation probabilities from a direct source-pivot phrase table. Dabre et al. (2015) studied the use of more than one pivots simultaneously. They also mentioned that the use of pivot for reordering would be an interesting problem. The reordering model on which we focus in our work is a lexicalized reordering model discussed by Koehn et al. (2005). Lexicalized reordering was first proposed by Tillmann (2004). Similar approach was suggested by Ohashi et al. (2005). Mirkin (2014) proposed several ways for incrementally updating the lexicalized reordering model when new training data is introduced and described a way to combine new and existing reordering models. To the best of our knowledge, no work has been done on triangulation of reordering 317 tables in specific. 3 Background for Our Approaches Our aim is to generate a source language to target language reordering table given a source language to pivot language reordering table and a"
W15-5946,P07-2045,0,0.0110349,"e, no work has been done on triangulation of reordering 317 tables in specific. 3 Background for Our Approaches Our aim is to generate a source language to target language reordering table given a source language to pivot language reordering table and a pivot language to target language reordering table. This is called as Triangulation of Reordering tables or Reordering Table Triangulation. In this paper, we propose two approaches for triangulation of reordering tables. Both of them make use of reordering tables, which have a peculiar format in Moses, a statistical machine translation system (Koehn et al., 2007). Before going forward with the description of our approaches, we explain the concepts of reordering orientations and give a short overview of the structure of a reordering table in Moses. 3.1 Reordering Orientations: Monotone, Swap and Discontinuous Moses implements reordering using three kinds of orientations - Monotone, Swap and Discontinuous (Koehn et al., 2005; Koehn, 2009). For each phrase and its corresponding translation, there are three possible orientations. Let S1 , S2 be phrases in source sentence, T1 , T2 be their corresponding translations in the target sentence and T1 be a phras"
W15-5946,P09-5002,0,0.0258199,"Missing"
W15-5946,Y14-1033,0,0.0149961,"e) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weight values to the translation probabilities from a direct source-pivot phrase table. Dabre et al. (2015) studied the use of more than one pivots simultaneously. They also mentioned that the use of pivot for reordering would be an interesting problem. The reordering model on which we focus in our work is a lexicalized reordering model discussed by Koehn et al. (2005). Lexicalized reordering was first proposed by Tillmann (2004). Similar approach was suggested by Ohashi et al. (2005). Mirkin (2014) proposed several ways for incrementally updating the lexicalized reordering model when new training data is introduced and described a way to combine new and existing reordering models. To the best of our knowledge, no work has been done on triangulation of reordering 317 tables in specific. 3 Background for Our Approaches Our aim is to generate a source language to target language reordering table given a source language to pivot language reordering table and a pivot language to target language reordering table. This is called as Triangulation of Reordering tables or Reordering Table Triangu"
W15-5946,2005.iwslt-1.16,0,0.0346406,"lel corpus(direct table) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weight values to the translation probabilities from a direct source-pivot phrase table. Dabre et al. (2015) studied the use of more than one pivots simultaneously. They also mentioned that the use of pivot for reordering would be an interesting problem. The reordering model on which we focus in our work is a lexicalized reordering model discussed by Koehn et al. (2005). Lexicalized reordering was first proposed by Tillmann (2004). Similar approach was suggested by Ohashi et al. (2005). Mirkin (2014) proposed several ways for incrementally updating the lexicalized reordering model when new training data is introduced and described a way to combine new and existing reordering models. To the best of our knowledge, no work has been done on triangulation of reordering 317 tables in specific. 3 Background for Our Approaches Our aim is to generate a source language to target language reordering table given a source language to pivot language reordering table and a pivot language to target language reordering table. This is called as Triangulation of Reordering tables or Reorderin"
W15-5946,P02-1040,0,0.0915296,"ow is a small description of each system. • DIR-DIR : This system uses a Direct Phrase Table and a Direct Reordering Table. • INTER-DIR : This system uses an Interpolated Phrase Table and a Direct Reordering table. • INTER-TRI : This system uses an Interpolated Phrase Table and a Triangulated Reordering table. • INTER-INTER : This system uses an Interpolated Phrase Table and an Interpolated Reordering table. 7 Results and Discussion We performed experiments for both Table Based and Count Based approaches. The improvements were observed from the quantitative point of view - in the BLEU scores (Papineni et al., 2002) as well as from the qualitative point of view - in the reordering of the translation that a system produces and better lexical choice in the translation. 7.1 Improvements in BLEU Scores For the table based approach, we found an improvement of 0.48 in the BLEU score from 17.98 for INTER-DIR system to 18.46 for INTERINTER system, for Marathi to English translation when Gujarati is used as a pivot. Small im321 provements were also observed for other systems, like Hindi-Gujarati-English (0.4), Hindi-PunjabiEnglish (0.33) and Hindi-Gujarati-Marathi (0.16). The improvements in other five systems we"
W15-5946,N04-4026,0,0.0626393,"riginal table that is extracted from a direct parallel corpus(direct table) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weight values to the translation probabilities from a direct source-pivot phrase table. Dabre et al. (2015) studied the use of more than one pivots simultaneously. They also mentioned that the use of pivot for reordering would be an interesting problem. The reordering model on which we focus in our work is a lexicalized reordering model discussed by Koehn et al. (2005). Lexicalized reordering was first proposed by Tillmann (2004). Similar approach was suggested by Ohashi et al. (2005). Mirkin (2014) proposed several ways for incrementally updating the lexicalized reordering model when new training data is introduced and described a way to combine new and existing reordering models. To the best of our knowledge, no work has been done on triangulation of reordering 317 tables in specific. 3 Background for Our Approaches Our aim is to generate a source language to target language reordering table given a source language to pivot language reordering table and a pivot language to target language reordering table. This is c"
W15-5946,N07-1061,0,0.0259401,"nstrated the use of pivot languages for extracting better word alignments when a sourcetarget parallel corpus is either unavailable or is very small in size. Wu and Wang (2007) proposed the entire formulation for a pivot based approach and triangulation of phrase tables and showed that a pivot can be used for inducing new phrase pairs that were not extracted while building a phrase table from the training corpus. This forms a basis for a valid speculation that, pivot may also prove to be useful in extracting reordering information. Several strategies have been proposed for the use of a pivot. Utiyama and Isahara (2007) proposed two strategies, a phrase translation strategy that is similar to the triangulation method and a sentence translation strategy that makes use of two different MT systems - one from sourcepivot and other from pivot-target. They showed through their experiments that the phrase translation strategy performs significantly better than the sentence translation strategy. Wu and Wang (2009) additionally introduced a Synthetic Method for Pivot based SMT, that synthesizes the sourcetarget corpus from source-pivot and pivot-target corpora. It was found through their experiments that the Triangul"
W15-5946,P06-2112,0,0.0478616,"Missing"
W15-5946,P07-1108,0,0.0190419,"with the discussion of those results. We conclude our work in Section 8 and also point out a few directions for future work. 2 Related Work Use of a pivot in machine translation has been extensively studied by many researchers. Wang et al. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 316–324, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) (2006) demonstrated the use of pivot languages for extracting better word alignments when a sourcetarget parallel corpus is either unavailable or is very small in size. Wu and Wang (2007) proposed the entire formulation for a pivot based approach and triangulation of phrase tables and showed that a pivot can be used for inducing new phrase pairs that were not extracted while building a phrase table from the training corpus. This forms a basis for a valid speculation that, pivot may also prove to be useful in extracting reordering information. Several strategies have been proposed for the use of a pivot. Utiyama and Isahara (2007) proposed two strategies, a phrase translation strategy that is similar to the triangulation method and a sentence translation strategy that makes use"
W15-5946,P09-1018,0,0.0161584,"aining corpus. This forms a basis for a valid speculation that, pivot may also prove to be useful in extracting reordering information. Several strategies have been proposed for the use of a pivot. Utiyama and Isahara (2007) proposed two strategies, a phrase translation strategy that is similar to the triangulation method and a sentence translation strategy that makes use of two different MT systems - one from sourcepivot and other from pivot-target. They showed through their experiments that the phrase translation strategy performs significantly better than the sentence translation strategy. Wu and Wang (2009) additionally introduced a Synthetic Method for Pivot based SMT, that synthesizes the sourcetarget corpus from source-pivot and pivot-target corpora. It was found through their experiments that the Triangulation Method outperforms the Sythetic method as well. This encouraged us to focus our work on the triangulation approach. Nakov and Ng (2012) showed that merging the phrase tables by giving priority to the original table that is extracted from a direct parallel corpus(direct table) and using additional features is a good strategy. In case of an interpolation, it refers to giving higher weigh"
W15-5946,jha-2010-tdil,0,\N,Missing
W15-5947,2011.eamt-1.6,0,0.0938611,"Missing"
W15-5947,C12-3032,1,0.846488,"alyzes why PE time, typLike the proponents of the Bologna project, we ically between 15 and 30 mn/page, was considerthink the real need is that foreign students get acably longer for the 3 Indian languages (bn, hi, mr). 326 4 3 Language code ISO639-2: https://www.loc.gov/standards/iso639-2/php/code list.php Test of English for International Communication Test of English as a Foreign Language 6 International English Language Testing System 5 cess not only to courses and tutorials, but also to notes from fellow students, in their respective languages, and possibly in parallel with the original (Kalitvianski et al., 2012). Simultaneously, they should also be invited to participate and improve the translations as time progresses. A subgoal is also that they learn better the language of their host university. 2.2 Obstacles The Bologna project started well, with clearly defined goals, and produced the beginning of a web service. However, very few syllabi were accessible, and the quality was not comparable with that of GT (Google Translate), Systran or Reverso. As no collaborative PE framework was included in the design, it could not be added afterwards and also could not produce a long-lasting service. This is no"
W15-5947,2011.tc-1.2,0,0.0271395,"essary coining, the terms in the target languages. 1 Introduction We are interested by using existing Machine Translation (MT) systems in the situations where their output does not (and often cannot, because, in its general form, MT is an ”AI-complete” problem) provide ”good enough” results. Post-editing MT results has been a professional activity in Japan since about 1985 (Nagao et al., 1985), and professional translators have begun to adopt that approach in other countries. Speaking of ”translation accelerators”, see for example the systems deployed at WIPO1 and UN2 (Pouliquen et al., 2013; Pouliquen and Mazenc, 2011a; Pouliquen and Mazenc, 2011b). There are other situations in which MT outputs could be brought to a quality ”good enough” for goals requiring a high level of precision and reliability. One of them is making pedagogical material accessible to foreign students in their own language. Cooperative post-editing (PE) of free MT pre-translations by the foreign students themselves is a good way to produce the ”target” versions, if the students find it rewarding, and not too timeconsuming., that is, if PE takes them no longer than about 15-20 minutes per standard page (of 325 1400 characters or 250 wo"
W15-5947,2011.mtsummit-plenaries.5,0,0.0560146,"Missing"
W15-5947,2013.mtsummit-user.7,0,0.035243,"Missing"
W15-5947,2013.mtsummit-wptp.12,1,0.684105,"f a segment is deemed to be ”good enough” if its quality score is higher or equal to 12/20. 2.3 Hypotheses and motivations behind the experiment Our first hypothesis is that the objective is reachable only if PE is put at the center of the approach, and if foreign students that can benefit from it, do it themselves in a voluntary fashion. Our second hypothesis is that we need to consider many more target languages than English and Chinese, including languages that are terminologically less equipped, for instance Arabic and South and South-East languages. Having the knowledge of an experiment (Wang and Boitet, 2013) in fr-zh for 1 year (in which the authors did not take part), we wanted to investigate the possiblity to do it for many languages, and to isolate the positive and negative factors, according to the target languages and to the profiles of the contributors. 3 Experiment 3.1 Setting We considered the following constraints: • to translate into as many target languages as possible, some of them being ”distant” from English, • to have as participants mostly PhD students, and all native speakers of the target languages, • to tackle a text of significant length, in a representative document, • to use"
W15-5947,J85-2001,0,\N,Missing
W15-5950,P05-1066,0,0.070739,"using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic information are the primary challenges in tackling the reordering problem. While there is active research in preordering and in-decoder approaches, there has been lit"
W15-5950,W14-3348,0,0.0135241,"g the target text to match the source order, thus necessitating the second reordering stage. 3 Generating Oracle Translations To estimate an upper bound on the improvement in translation accuracy possible with post-ordering, we generate oracle reorderings of the baseline SMT output hypothesis. An oracle reordering is the best possible word order of the hypothesis, in terms of fluency and syntactic correctness. We propose the following algorithm for computing the oracle reordering. 1. Obtain word alignments between the hypothesis and reference using the monolingual aligner algorithm in METEOR (Denkowski and Lavie, 2014). 2. Construct a new sentence by rearranging aligned words from the hypothesis using the 353 word-order from the reference. 3. Use additional heuristics to include as many unaligned words from the hypothesis into the oracle reordering as possible. In our experiments, the words in the hypothesis that were not aligned by METEOR but found a stemmatch in the reference were inserted in the oracle sentence. The resulting oracle hypothesis is a permutation of a subset of words in the original MT decoding step, such that they reflect the word order in the reference. 4 Experimental Setup We studied dif"
W15-5950,W07-0414,0,0.0299061,"estimate the upper bound. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output o"
W15-5950,D08-1089,0,0.0336209,"oring the post-ordering problem. 1 Introduction Word order divergence is a central problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse"
W15-5950,P12-2061,0,0.0225532,"वरदान स ध झाल Ref: these injections have proved to be a boon for heart patients . Output: this injection of heart patients are proved to be a boon for the . 0 Oracle: 1 2 3 5 4 7 6 8 9 10 11 12 13 this injection are proved to be a boon for heart patients . 5 re-ordered using alignments 6 7 8 9 10 11 re-ordered with heuristics 3 4 13 not aligned/used Figure 1: Construction of Oracle reordering post-ordering system in the sense in which we have defined it. Theirs is actually a two stage translation system that decomposes the translation problem into lexical transfer and reordering subproblems. Goto et al. (2012) and Goto et al. (2013) also propose post-ordering systems with the same architecture, but different reordering methods in the second stage. The motivation in these postordering methods is not to improve upon the word order. Rather, lexical mappings are learnt in the first stage after reordering the target text to match the source order, thus necessitating the second reordering stage. 3 Generating Oracle Translations To estimate an upper bound on the improvement in translation accuracy possible with post-ordering, we generate oracle reorderings of the baseline SMT output hypothesis. An oracle"
W15-5950,W06-3114,0,0.0220096,"on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical systems were also trained with Moses using default parameters. For three phrase-based SMT systems with Hindi, Marathi and Malayalam respectively as source and English as target language, qualitative analysis was performed through manual evaluation of output sentences by native speakers of each of the source languages. Given the source and gold reference, the evaluators were asked to rate the adequacy and fluency of a system&apos;s output and oracle sentences on a scale of 1 to 5, as described in (Koehn and Monz, 2006) (see Table 1). The weighted average of the scores over all sentences was then calculated as: average_score = 5 ∑ s.f (s) Lang-pair hin-eng mar-eng ben-eng guj-eng kon-eng pan-eng urd-eng tam-eng tel-eng mal-eng Model PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT Original 22.77 23.87 12.6 14.65 16.24 14.69 17.66 15.96 15.56 13.67 19.98 20.12 17.31 19.05 10.54 10.3 12.63 11.9 8.3 8.46 Pruned 23.12 24.5 11.23 13.95 15.67 14.01 17.03 15.17 14.74 12.77 19.87 20.00 17.17 18.58 8.9 9.0 11.42 10.66 6.03 6.48 Oracle 36.74 37.36"
W15-5950,W09-0437,0,0.0200953,"ollowing is an outline of the paper. In Section 2, we describe related work. In the remainder of the paper, we estimate an upper bound on the potential gains in translation accuracy by postordering. Section 3 describes our method for computing oracle reorderings from the translation output, which is used to estimate the upper bound. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output."
W15-5950,2011.mtsummit-papers.35,0,0.0959409,"Missing"
W15-5950,P07-2045,0,0.00530632,". 4 Experimental Setup We studied different SMT systems from 10 Indian languages to English for quantifying the potential improvement in translation accuracy. The experiments were carried across 10 Indian languages included in the multilingual ILCI corpus (Jha, 2010), which contains nearly 50,000 parallel sentences . For each language pair, the corpus was split into 46,277 sentences for training, 500 sentences for tuning and 2000 sentences for testing. We trained phrase-based and hierarchical phrase-based systems on this data. The phrase-based systems were trained using the Moses SMT toolkit (Koehn et al., 2007) with the grow-diag-final-and heuristic for phrase extraction and the msd-bidirectional-fe model for lexicalized reordering. The trained models were tuned with Minimum Error Rate Training (MERT) (Och, 2003) with default parameters. We trained Score 1 2 3 4 5 Adequacy No meaning Little meaning Much meaning Most meaning All meaning Fluency Incomprehensible Disfluent Non-native fluency Good fluency Flawless fluency Table 1: Description of scores for manual evaluation 5-gram language models on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical syste"
W15-5950,N09-2003,0,0.0237438,"nd. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output of an MT system using anot"
W15-5950,D14-1133,0,0.123827,"nslations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic informa"
W15-5950,P03-1021,0,0.0628413,"included in the multilingual ILCI corpus (Jha, 2010), which contains nearly 50,000 parallel sentences . For each language pair, the corpus was split into 46,277 sentences for training, 500 sentences for tuning and 2000 sentences for testing. We trained phrase-based and hierarchical phrase-based systems on this data. The phrase-based systems were trained using the Moses SMT toolkit (Koehn et al., 2007) with the grow-diag-final-and heuristic for phrase extraction and the msd-bidirectional-fe model for lexicalized reordering. The trained models were tuned with Minimum Error Rate Training (MERT) (Och, 2003) with default parameters. We trained Score 1 2 3 4 5 Adequacy No meaning Little meaning Much meaning Most meaning All meaning Fluency Incomprehensible Disfluent Non-native fluency Good fluency Flawless fluency Table 1: Description of scores for manual evaluation 5-gram language models on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical systems were also trained with Moses using default parameters. For three phrase-based SMT systems with Hindi, Marathi and Malayalam respectively as source and English as target language, qualitative analysis was"
W15-5950,P02-1040,0,0.0914798,"Missing"
W15-5950,I08-1067,1,0.831334,"rdering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic information are the primary challenges in tackling the reordering problem. While there is active research in preordering and in-decoder approaches, there has been little work on the problem of"
W15-5950,W07-0728,0,0.0339413,"s kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output of an MT system using another phrase-based MT system trained on parallel data constructed from previously decoded output (e) and corresponding references (e&apos;). Béchara et al. (2011) improvizes on this approach by appending source words (f) to the output part of the parallel data (e), creating a new language (e&apos;#f) and retaining source context. Marie et al. (2014) use a second-pass decoder to improve translation quality. However, none of these works have focussed on word order and the effect on the word order has not been"
W15-5950,2011.mtsummit-papers.36,0,0.0337307,"of an MT system using another phrase-based MT system trained on parallel data constructed from previously decoded output (e) and corresponding references (e&apos;). Béchara et al. (2011) improvizes on this approach by appending source words (f) to the output part of the parallel data (e), creating a new language (e&apos;#f) and retaining source context. Marie et al. (2014) use a second-pass decoder to improve translation quality. However, none of these works have focussed on word order and the effect on the word order has not been explicitly evaluated. Post-ordering as a problem has been introduced by Sudoh et al. (2011). However, it is not a Source: दया या आहे त . णांसाठ ह इंजे शने एक वरदान स ध झाल Ref: these injections have proved to be a boon for heart patients . Output: this injection of heart patients are proved to be a boon for the . 0 Oracle: 1 2 3 5 4 7 6 8 9 10 11 12 13 this injection are proved to be a boon for heart patients . 5 re-ordered using alignments 6 7 8 9 10 11 re-ordered with heuristics 3 4 13 not aligned/used Figure 1: Construction of Oracle reordering post-ordering system in the sense in which we have defined it. Theirs is actually a two stage translation system that decomposes the tran"
W15-5950,N04-4026,0,0.0279876,"g effort in exploring the post-ordering problem. 1 Introduction Word order divergence is a central problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being so"
W15-5950,P02-1039,0,0.0636105,"tral problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching th"
W15-5950,jha-2010-tdil,0,\N,Missing
W15-5950,J93-2003,0,\N,Missing
W15-5950,P05-1033,0,\N,Missing
W16-0415,P06-4018,0,0.00513565,"We evaluate two key components of PIE model, namely, segregation of words and hierarchy of author-group distributions. Segregation of words: A key component of PIE model is the strategy to decide whether a word is an issue word or position word. We experiment with following alternatives to do this: (a) POSbased segregation as done in Fang et al. (2012) using twitter POS tagger Bontcheva et al. (2013). We experimentally determine the optimal split as: nouns as issue words, and adjectives, verbs and adverbs as position words, (b) POS-based+PMIbased collocation handler to include n-grams, using Bird (2006), (c) Subjectivity-based segregation classifies words present in the subjectivity word list by McAuley and Leskovec (2013) as position words, (d) POS+Subjectivity-based segregation where we first categorize nouns as issue words, and then look for other words in the subjectivity word list. In order to select the best strategy of segregation, we compute topic coherence metric Cv using Palmetto by R¨oder, Both, and Hinneburg Average cosine similarity With Without Within members of the same group Demo.-Demo. 0.261 0.253 Repub.-Repub. 0.014 0.013 Within members of different groups Demo.-Repub. 0.10"
W16-0415,R13-1011,0,0.0327122,"l discover? (Section 5.2) • Once we discovered political issues, positions and group-wise distribution, can the model be used to predict political affiliation? (Section 5.3) 5.1 Impact of Model Components on Performance We evaluate two key components of PIE model, namely, segregation of words and hierarchy of author-group distributions. Segregation of words: A key component of PIE model is the strategy to decide whether a word is an issue word or position word. We experiment with following alternatives to do this: (a) POSbased segregation as done in Fang et al. (2012) using twitter POS tagger Bontcheva et al. (2013). We experimentally determine the optimal split as: nouns as issue words, and adjectives, verbs and adverbs as position words, (b) POS-based+PMIbased collocation handler to include n-grams, using Bird (2006), (c) Subjectivity-based segregation classifies words present in the subjectivity word list by McAuley and Leskovec (2013) as position words, (d) POS+Subjectivity-based segregation where we first categorize nouns as issue words, and then look for other words in the subjectivity word list. In order to select the best strategy of segregation, we compute topic coherence metric Cv using Palmett"
W16-0415,N09-1054,0,0.0743337,"Missing"
W16-0415,D10-1006,0,0.0382301,"r tweet, and divide words into categories based on POS tags. Our model improves upon these two works in the following key ways: • A richer latent variable structure. In our model, the opinion words depend on BOTH topic and opinion latent variables (as opposed to only the latter). This structure allows our model to generate topics corresponding to political positions, which is not achieved in the past work. Related Work Our model is based on Latent Dirichlet Allocation (LDA) given by Blei, Ng, and Jordan (2003). Models based on LDA by Jo and Oh (2011), Mei et al. (2007), Lin and He (2009), and Zhao et al. (2010) present approaches to extract sentiment-coherent topics in datasets. The past work in analytics related to political opinion can be broadly classified in three categories. The first category predicts the outcome of an election. Gayo-Avello, Metaxas, and Mustafaraj (2011) predict the election outcome for a pair of Democrat and Republican candidates, while Metaxas, Mustafaraj, and Gayo-Avello (2011) aggregate sentiment in tweets in order to map it to votes. Conover et al. (2011) use a network graph of authors and their known political orientations, in order to predict the political orientation."
W16-1904,P98-2143,0,0.190674,"n has better role in pronoun interpretation. Arnold et al. (2000) examine the effect of gender information and accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expressions in a discourse referring to the same entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of hu"
W16-1904,W12-4503,0,0.0460366,"Missing"
W16-1904,P02-1014,0,0.175133,"gender information and accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expressions in a discourse referring to the same entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of human process involved in coreference resolution that eventually can help automatic coreference"
W16-1904,P14-1005,0,0.0329205,"Missing"
W16-1904,W12-4501,0,0.0296323,"entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of human process involved in coreference resolution that eventually can help automatic coreference resolution. Our participants are given a set of documents to perform coreference annotation and the eye movements during the exercise are recorded. Eyemovement patterns are characterized by two basic attributes: (1) Fixations, corresponding to a longer stay of gaze on a visual object"
W16-1904,D10-1048,0,0.0307132,"ole in pronoun interpretation. Arnold et al. (2000) examine the effect of gender information and accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expressions in a discourse referring to the same entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of human process involved in cor"
W16-1904,P15-1136,0,0.0463868,"Missing"
W16-1904,I11-1052,0,0.0218432,"nd accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expressions in a discourse referring to the same entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of human process involved in coreference resolution that eventually can help automatic coreference resolution. Our partic"
W16-1904,D13-1203,0,0.0340101,"Missing"
W16-1904,N13-1088,1,0.753406,"of psycholinguistics to study language comprehension (Rayner and Sereno, 1994), lexical (Rayner and Duffy, 1986) and syntactic processing(von der Malsburg and Vasishth, 2011). Recently, eye-tracking studies have been conducted for various language processing tasks like Sentiment Analysis, Translation and Word Sense Disambiguation. Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2013) measure complexity in text to be translated based on gaze input of translators which is used to label training data. Joshi et al. (2013) propose a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-tracking. Introduction Eye-tracking studies have also been conducted for the task of coreference resolution. Cunnings et al. (2014) check for whether the syntax or discourse representation has better role in pronoun interpretation. Arnold et al. (2000) examine the effect of gender information and accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expr"
W16-1904,J01-4004,0,0.690026,"amine the effect of gender information and accessibility to pronoun interpretation. Vonk (1984) studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns. Coreference resolution deals with identifying the expressions in a discourse referring to the same entity. It is crucial to many information retrieval tasks (Elango, 2005). One of its main objectives of is to resolve the noun phrases to the entities they refer to. Though there exist many rule based (Kennedy and Boguraev, 1996; Mitkov, 1998; Raghunathan et al., 2010) and machine learning based (Soon et al., 2001; Ng and Cardie, 2002; Rahman and Ng, 2011) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution. Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task (Pradhan et al., 2012), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bj¨orkelund and Farkas, 2012; DurWe perform yet another eye-tracking study to understand certain facets of human process involved in coreference resolution that eventually can help a"
W16-1904,P14-2007,1,0.785825,". 1 This paper attempts to gain insight into the cognitive aspects of coreference resolution to improve mention-pair model, a well-known supervised coreference resolution paradigm. For this we employ eye-tracking technology that has been quite effective in the field of psycholinguistics to study language comprehension (Rayner and Sereno, 1994), lexical (Rayner and Duffy, 1986) and syntactic processing(von der Malsburg and Vasishth, 2011). Recently, eye-tracking studies have been conducted for various language processing tasks like Sentiment Analysis, Translation and Word Sense Disambiguation. Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2013) measure complexity in text to be translated based on gaze input of translators which is used to label training data. Joshi et al. (2013) propose a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-tracking. Introduction Eye-tracking studies have also been conducted for the task of coreference resolution. Cunnings et al. (2014) check for whether the syntax or discourse representation has better role in pronoun interpretation. Arnold et al."
W16-1904,C96-1021,0,\N,Missing
W16-1904,C98-2138,0,\N,Missing
W16-1904,P15-4011,0,\N,Missing
W16-2111,P15-2124,1,0.865158,"evaluation of the quality of sarcasm annotation, and the impact of this quality on sarcasm classification. This study forms a stepping stone towards systematic evaluation of quality of these datasets annotated by non-native annotators, and can be extended to other culture combinations. 1 Introduction Sarcasm is a linguistic expression where literal sentiment of a text is different from the implied sentiment, with the intention of ridicule (Schwoebel et al., 2000). Several data-driven approaches have been reported for computational detection of sarcasm (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). As is typical of supervised approaches, they rely on datasets labeled with sarcasm. We refer to the process of creating such sarcasm-labeled datasets as sarcasm annotation. Linguistic studies concerning cross-cultural dependencies of sarcasm have been reported (Boers, 2003; Thomas, 1983; Tannen, 1984; Rockwell and Theriot, 2001; Bouton, 1988; Haiman, 1998; Dress et al., 2008). 2 Why is such an evaluation of quality important? To build NLP systems, creation of annotated corpora is common. When annotators are hired, factors such as language competence are considered. However, while tasks like"
W16-2111,D13-1066,0,0.227557,"Missing"
W16-2111,P06-4018,0,0.00947142,"Missing"
W16-2111,P13-1004,0,0.0256494,"es importance. Our work is the first-of-itskind study related to sarcasm annotation. Similar studies have been reported for related tasks. Hupont et al. (2014) deal with result of cultural differences on annotation of images with emotions. Das and Bandyopadhyay (2011) describe multi-cultural observations during creation of an emotion lexicon. For example, they state that the word ‘blue’ may be correlated to sadness in some cultures but to evil in others. Similar studies to understand annotator biases have been performed for subjectivity annotation (Wiebe et al., 1999) and machine translation (Cohn and Specia, 2013). Wiebe et al. (1999) show how some annotators may have individual biases towards a certain subjective label, and devise a method to obtain bias-corrected tags. Cohn and Specia (2013) consider annotator biases for the task of assigning quality scores to machine translation output. 3.3 Experiments The annotation experiment is conducted as follows. Our annotators read a unit of text, and determine whether it is sarcastic or not. The experiment is conducted in sessions of 50 textual units, and the annotators can pause anywhere through a session. This results in datasets where each textual unit ha"
W16-2111,walker-etal-2012-corpus,0,0.0302106,"lity, (b) degradation in annotation quality, (c) impact of quality degradation on sarcasm classification, and (c) prediction of disagreement. 3.1 4.1 3 Our Annotation Experiments Datasets We use two sarcasm-labeled datasets that have been reported in past work. The first dataset is Tweet-A. This dataset, introduced by Riloff et al. (2013), consists of 2278 manually labeled tweets, out of which 506 are sarcastic. We call these annotations American1. An example of a sarcastic tweet in this dataset is ‘Back to the oral surgeon #yay’. The second dataset is DiscussionA: This dataset, introduced by Walker et al. (2012), consists of 5854 discussion forum posts, out of which 742 are sarcastic. This dataset was created using Amazon Mechanical Turk. IP addresses of Turk workers were limited to USA during the experiment1 . We call these annotations American2. An example post here is: ‘A master baiter like you should present your thesis to be taken seriously. You haven’t and you aren’t.’. 3.2 What difficulties do our Indian annotators face? Table 1 shows examples where our Indian annotators face difficulty in annotation. We describe experiences from the experiments in two parts: 1. Situations in which they were u"
W16-2111,P11-4009,0,0.014095,"hree annotators create a sarcasm-labeled dataset. the host for a meal may be perceived as polite in some cultures, but sarcastic in some others. Due to popularity of crowd-sourcing, cultural background of annotators may not be known at all. Keeping these constraints in mind, a study of non-native annotation, and its effect on the corresponding NLP task assumes importance. Our work is the first-of-itskind study related to sarcasm annotation. Similar studies have been reported for related tasks. Hupont et al. (2014) deal with result of cultural differences on annotation of images with emotions. Das and Bandyopadhyay (2011) describe multi-cultural observations during creation of an emotion lexicon. For example, they state that the word ‘blue’ may be correlated to sadness in some cultures but to evil in others. Similar studies to understand annotator biases have been performed for subjectivity annotation (Wiebe et al., 1999) and machine translation (Cohn and Specia, 2013). Wiebe et al. (1999) show how some annotators may have individual biases towards a certain subjective label, and devise a method to obtain bias-corrected tags. Cohn and Specia (2013) consider annotator biases for the task of assigning quality sc"
W16-2111,P99-1032,0,0.378434,"Missing"
W16-2111,W10-2914,0,0.222093,"Missing"
W16-4206,J81-4005,0,0.760879,"Missing"
W16-4206,D14-1181,0,0.00265422,"the rules. This incurs cost and time as the appropriate set of features or rules can be framed only after analyzing the full records. The advent of deep learning algorithms has facilitated to introduce a new framework where we do not require handcrafted features or rules. These models have the abilities to learn automatically the relevant features by performing composition over the words represented in the form of vectors known as word embedding. In recent times, deep neural network architecture has shown promise for solving various NLP tasks such as text classification (Socher et al., 2013; Kim, 2014), language modeling (Mikolov et al., 2010), question answering (Weston et al., 2015), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) etc. In this paper, we propose a novel system (DI-RNN) based on deep learning for patient data de-identification (PDI). We formulate the task as a sequence labeling problem and develop a technique based on Recurrent Neural Network (RNN) (Mikolov et al., 2010). RNN, unlike other techniques, does not require features to be explicitly generated for classifier’s training or testing. Instead it learns features by itsel"
W16-4206,H05-1026,0,0.0901288,". The underlying idea is that similar words appear in close vicinity of each other. The vector corresponding to each input word wi is produced whose dimensionality is set at the time of learning the neural language model from the given unsupervised corpus. This representation provides the continuousspace representation for each word. Usually, training of the word embedding is done in an unsupervised manner using external natural language text like Wikipedia, news article, bio-medical literature etc. The architecture can be varied by adopting various architectures like shallow neural networks (Schwenk and Gauvain, 2005), RNN (Mikolov et al., 2010; Mikolov et al., 2011), SENNA (Collobert et al., 2011), 34 word2vec (Mikolov et al., 2013) etc. We use three different procedures to learn word embeddings like random number initialization, RNN’s word embedding and continuous bag-of-words (CBOW). For random word embedding we initialize the vector of dimension 100 in the range −0.25 to +0.25. In order to evaluate the impact of RNN we use the word embedding as provided by RNNLM 4 of dimension 80 which is trained on Broadcast news corpus. In addition to this we also use word embedding model trained by CBOW technique as"
W16-4206,D13-1170,0,0.0313007,"minent feature set or the rules. This incurs cost and time as the appropriate set of features or rules can be framed only after analyzing the full records. The advent of deep learning algorithms has facilitated to introduce a new framework where we do not require handcrafted features or rules. These models have the abilities to learn automatically the relevant features by performing composition over the words represented in the form of vectors known as word embedding. In recent times, deep neural network architecture has shown promise for solving various NLP tasks such as text classification (Socher et al., 2013; Kim, 2014), language modeling (Mikolov et al., 2010), question answering (Weston et al., 2015), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) etc. In this paper, we propose a novel system (DI-RNN) based on deep learning for patient data de-identification (PDI). We formulate the task as a sequence labeling problem and develop a technique based on Recurrent Neural Network (RNN) (Mikolov et al., 2010). RNN, unlike other techniques, does not require features to be explicitly generated for classifier’s training or testing. Instead it learns featu"
W16-4206,W00-1308,0,0.0526202,"ication task. Here R,P and F denotes Recall, Precision and F-score respectively. 2. Bag-of-word feature: This feature includes uni-grams, bi-grams, tri-grams of the target token. We use window size of [-2, 2] with respect to the target token. Here, n-gram is referred as the continuous sequence of n items. An n-gram generated having sizes of 1, 2, 3 are known as an uni-gram, bi-gram and tri-gram, respectively. 3. Part-of-Speech (PoS) Information: The PoS information of current word, previous two words and the next two words are used as features. We obtain PoS of words from the Stanford tagger (Toutanova and Manning, 2000). 4. Chunk Information: The chunk information is an important feature to identify the PHI termboundary. We use chunk information obtained from openNLP5 . 5. Combined POS-token and Chunk-token Feature: This feature is generated by the combination of other token features like PoS, Chunk within the context window of [-1, 1]. This is represented as [w0 p−1 , w0 p0 , w0 p1 ] where w0 represents the target word, and p−1 , p0 and p1 represent the previous, current and the next POS or Chunk tags, respectively. We build our model by incorporating the above features. We use the CRF implementation6 of CR"
W16-4604,N12-1047,0,0.020397,"ge model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-En"
W16-4604,P14-1129,0,0.162182,"rained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-English translation and English-Indonesian translation by making use of distributed representations. We experimented with various word embedding sizes of 700, 750 and 800 for the first hidden layer in the network to get the optimal parameter while decoding. 3. Use of Bilingual Neural Joint Language Model : Devlin et al. (2014) have shown that including source side context information in the neural language model can lead to substantial improvement in translation quality. We experimented with Devlin&apos;s method which uses NPLM3 in the back-end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequenc"
W16-4604,P11-1105,0,0.066572,"Missing"
W16-4604,N13-1001,0,0.0204767,"end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequence model was trained as it integrates Ngram-based reordering and translation in a single generative process which can result in relatively improved translation over phrase based system. OSM approach as suggested in Durrani et al. (2013) considers both source and target information for generating a translation. It deals with minimum translation units i.e. words, along with context information of source and target sentence which spans across phrasal boundries. A 5-gram OSM was used for the experimentation here. 4 http://nlg.isi.edu/software/nplm/ 69 These 4 systems were trained for both directions of language pair and the test data was decoded and evaluated with BLEU points, RIBES scores, AMFM scores, Pairwise crowdsourcing scores and Adequacy scores for comparative performance evaluation. 3 Experimental Setup The data provide"
W16-4604,P13-2071,0,0.0164479,"end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequence model was trained as it integrates Ngram-based reordering and translation in a single generative process which can result in relatively improved translation over phrase based system. OSM approach as suggested in Durrani et al. (2013) considers both source and target information for generating a translation. It deals with minimum translation units i.e. words, along with context information of source and target sentence which spans across phrasal boundries. A 5-gram OSM was used for the experimentation here. 4 http://nlg.isi.edu/software/nplm/ 69 These 4 systems were trained for both directions of language pair and the test data was decoded and evaluated with BLEU points, RIBES scores, AMFM scores, Pairwise crowdsourcing scores and Adequacy scores for comparative performance evaluation. 3 Experimental Setup The data provide"
W16-4604,W08-0509,0,0.0423785,"both the systems show that RNN model system outperforms SMT system with n-gram LM. The results of Hermanto et al.(2015) and various other research outcomes on different language pair using neural language model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the"
W16-4604,P07-2045,0,0.00519506,"d on same data. The perplexity analysis of both the systems show that RNN model system outperforms SMT system with n-gram LM. The results of Hermanto et al.(2015) and various other research outcomes on different language pair using neural language model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the p"
W16-4604,W11-2124,0,0.0628648,"Missing"
W16-4604,C12-2104,0,0.0475729,"Missing"
W16-4604,D13-1140,0,0.0461239,"on heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-English translation and English-Indonesian translation by making use of distributed representations. We experimented with various word embedding sizes of 700, 750 and 800 for the first hidden layer in the network to get the optimal parameter while decoding. 3. Use of Bilingual Neural Joint Language Model : Devlin et al. (2014) have shown that including source side context information in the neural language model can lead to substantial improvement in translation quality. We experimented with Dev"
W16-4622,P05-1033,0,0.66211,"language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2005; Chiang, 2007) manage to perform efficiently as long as sufficient parallel text are available. Our submitted system is based on hierarchical SMT, performance of which is improved by performing reordering in the source side and augmenting English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two systems for English to Hindi (En-Hi) translation, viz"
W16-4622,P05-1066,0,0.0672619,"ght of a rule tells the decoder how probable the rule is. Decoder implements a parsing algorithm which is inspired by monolingual syntactic chart parsing along with a beam search to find the best target sentence. 2.3 Reordering One of the major difficulties of machine translation lies in handling the structural differences between the language pair. Translation from one language to another becomes more challenging when the language pair follows different word order. For example, English language follows subject-verb-object (SVO) whereas Hindi follows subject-object-verb (SOV) order. Research (Collins et al., 2005; Ramanathan et al., 2008) has shown that syntactic reordering of the sourceside to conform the syntax of the target-side alleviates the structural divergence and improves the translation quality significantly. Though the PBSMT has an independent reordering model which reorders the phrases but it has limited potential to model the word-order differences between different languages (Collins et al., 2005). We perform syntactic reordering of the source sentences in the preprocessing phase in which every English sentence is modified in such a way that its word order is almost similar to the word o"
W16-4622,W11-2123,0,0.0213498,"365 22164816 49,394 57,037 10,656 10174 844,925,569 Table 1: Statistics of data set 2.6 Preprocessing We begin with a preprocessing of raw data, which includes tokenization, true-casting, removing long sentences as well as sentences with a length mismatch exceeding certain ratio. Training and development sets were already tokenized. For tokenizing English sentences we use tokenizer.perl5 script and for Hindi sentences we use indic_NLP_Library6 . 2.7 Training For all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierar"
W16-4622,N03-1017,0,0.394895,"or English-Hindi language pair. This year English-Hindi language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2005; Chiang, 2007) manage to perform efficiently as long as sufficient parallel text are available. Our submitted system is based on hierarchical SMT, performance of which is improved by performing reordering in the source side and augmenting English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two syste"
W16-4622,P07-2045,0,0.0927874,"English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two systems for English to Hindi (En-Hi) translation, viz. one without adding any external data to the training corpus and the other by augmenting bilingual dictionary in training. Both systems are reordered in the source side. As a baseline model we develop a phrase-based SMT model using Moses (Koehn et al., 2007). We perform several experiments with the hierarchical SMT in order to study the effectiveness of reordering and bilingual dictionary augmentation. These were done to improve syntactic order and alignment with linguistic knowledge. 2.1 Phrase-based Machine Translation Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) is the most popular approach among all other approaches to machine translation and it has became benchmark for machine translation systems in academia as well as in industry. A phrase-based SMT consists 216 Proceedings of the 3rd Workshop on Asian Translati"
W16-4622,W16-4601,0,0.0230641,"ical phrase-based SMT for English to Hindi language pair. We perform reordering and augment bilingual dictionary to improve the performance. As a baseline we use a phrase-based SMT model. The MT models are fine-tuned on the development set, and the best configurations are used to report the evaluation on the test set. Experiments show the BLEU of 13.71 on the benchmark test data. This is better compared to the official baseline BLEU score of 10.79. 1 Introduction In this paper, we describe the system that we develop as part of our participation in the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016) for English-Hindi language pair. This year English-Hindi language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT"
W16-4622,J03-1002,0,0.00927902,"or all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the trained system was tuned with Minimum Error Rate Training (MERT) (Och, 2003) to learn the weights of different parameters of the model. 3 Results and Analysis We bui"
W16-4622,P03-1021,0,0.0455331,"learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the trained system was tuned with Minimum Error Rate Training (MERT) (Och, 2003) to learn the weights of different parameters of the model. 3 Results and Analysis We build the following systems using Moses7 . 1. Phrase-based model (Phr) 2. Phrase-based model after reordering the source side (PhrRe) 3. Hierarchical phrase-based model (Hie) 4. Hierarchical phrase-based model after reordering the source side. We build two variations of this model: one (HieRe) without adding any external resources to the train set and another (HieReDict) with adding bilingual dictionary to the train set. 5 https://github.com/moses-smt/mosesdecoder/blob/RELEASE-3.0/scripts/tokenizer/tokenizer."
W16-4622,P02-1040,0,0.0974368,"velopment sets were already tokenized. For tokenizing English sentences we use tokenizer.perl5 script and for Hindi sentences we use indic_NLP_Library6 . 2.7 Training For all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the t"
W16-4622,W13-2807,0,0.0298254,"y English sentence is modified in such a way that its word order is almost similar to the word order of the Hindi sentence. For example, English: The president of America visited India in June. Reordered: America of the president June in India visited. Hindi: अमे रका के रा पित ने जून म भारत क याऽा क । (amerikA ke rAShTrapati ne jUna meM bhArata kI yAtrA kI .) For source-side reordering we use the rule-based preordering tool1 , which takes parsed English sentence as input and generates sentence whose word order is similar to that of Hindi. This reordering is based on the approach developed by (Patel et al., 2013) which is an extension of an earlier work reported in (Ramanathan et al., 2008). For parsing source side English sentences, we use Stanford parser2 . 2.4 Augmenting Bilingual Dictionary Bilingual dictionaries are always useful in SMT as it improves the word-alignment which is the heart of every SMT. In addition to reordering the source corpus, we add a English-Hindi bilingual dictionary to improve our MT system. We show our proposed model in Figure 1. We use Moses (Koehn et al., 2007), an open source toolkit for training different systems. We start training with Phrase-based SMT as a baseline"
W16-4622,I08-1067,1,0.777796,"e decoder how probable the rule is. Decoder implements a parsing algorithm which is inspired by monolingual syntactic chart parsing along with a beam search to find the best target sentence. 2.3 Reordering One of the major difficulties of machine translation lies in handling the structural differences between the language pair. Translation from one language to another becomes more challenging when the language pair follows different word order. For example, English language follows subject-verb-object (SVO) whereas Hindi follows subject-object-verb (SOV) order. Research (Collins et al., 2005; Ramanathan et al., 2008) has shown that syntactic reordering of the sourceside to conform the syntax of the target-side alleviates the structural divergence and improves the translation quality significantly. Though the PBSMT has an independent reordering model which reorders the phrases but it has limited potential to model the word-order differences between different languages (Collins et al., 2005). We perform syntactic reordering of the source sentences in the preprocessing phase in which every English sentence is modified in such a way that its word order is almost similar to the word order of the Hindi sentence"
W16-4622,vilar-etal-2006-error,0,0.0944589,"Missing"
W16-4811,N16-4006,1,0.498783,"increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy. 1 Introduction Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Examples of languages related by common ancestry are Slavic and Indo-Aryan languages. Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas (Thomason, 2000). Examples of such linguistic areas are the Indian subcontinent (Emeneau, 1956), Balkan (Trubetzkoy, 1928) and Standard Average European (Haspelmath, 2001) linguistic areas. Both forms of language relatedness lead to related languages sharing vocabulary and structural features. There is substantial government, commercial and cul"
W16-4811,N12-1047,0,0.0848712,"in our experiments. The OS is a linguistically motivated, variable length unit of translation, which consists of one or more consonants followed by a vowel (a C+ V unit). But our methodology is not specific to any subword unit. Hence, the results and observations should hold for other subword units also. We used the Indic NLP Library1 for orthographic syllabification. Phrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using subword units (Vilar et al., 2007), we trained 10-gram language models. The language model was trained on the training split of the target language corpus. 1 2 http://anoopkunchukuttan.github.io/indic_nlp_library https://github.com/moses-smt/mgiza 84 Translation Accuracy Relative Decoding Time ben-hin hin-mal mal-hin tel-mal 33.10 11.68 19.86 9.39 Stack tl=10 tl=5 32.84 32.54 11.24 11.01 19.21 18.39 ss=50 ss=10 +tuning 33.10 33.04 32.83 11.69 11"
W16-4811,P14-2022,0,0.0138374,"le design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014). 7 Conclusion and Future Work We systematically study the choice of data format for representing subword units in sentences and various decoder parameters which affect decoding time in a phrase-based SMT setting. Our studies (using OS and character as basic units) show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality. Two data formats, the space marker and the boundary marker, perform roughly equivalently in terms of translation accuracy as well as d"
W16-4811,W11-2123,0,0.0223512,"he phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited"
W16-4811,2016.amta-researchers.4,0,0.011073,"l have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the tra"
W16-4811,P07-1019,0,0.0479251,"d Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014). 7 Conclusion and Future Work We systematically study the choice of data format for representing subword units in sentences and various decoder parameters which affect decoding time in a phrase-based SMT setting. Our studies (using OS and character as basic units) show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality. Two data formats, the space marker and the boundary marker, perform roughly equivalently in terms of translati"
W16-4811,P07-2045,0,0.0108953,"As an example of subword level representation unit, we have studied the orthographic syllable (OS) (Kunchukuttan and Bhattacharyya, 2016) in our experiments. The OS is a linguistically motivated, variable length unit of translation, which consists of one or more consonants followed by a vowel (a C+ V unit). But our methodology is not specific to any subword unit. Hence, the results and observations should hold for other subword units also. We used the Indic NLP Library1 for orthographic syllabification. Phrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using subword units (Vilar et al., 2007), we trained 10-gram language models. The language model was trained on the training split of the target language corpus. 1 2 http://anoopkunchukuttan.github.io/indic_nlp_library https://github.com/moses-smt/mgiza 84 Translation Accuracy Relative Decoding Time ben-hin hin-"
W16-4811,D16-1196,1,0.862751,"s share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segmentation thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n thi s i s a n e xa m p le o f da ta"
W16-4811,P12-2059,0,0.183394,"MT systems based on encoder decoder architectures, particularly without attention mechanism, are more sensitive to sentence length, so we presume that the boundary marker format may be more appropriate. 86 6 Related Work It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007). Aligning long sentences is computationally expensive, hence most work has concentrated on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can"
W16-4811,P02-1040,0,0.094949,"ed as a multiple of wordlevel decoding time. The following methods & parameters in Moses have been experimented with: (i) normal stack decoding - vary ss: stack-size, tt: table-limit; (ii) cube pruning: vary pl:cube-pruningpop-limit. +tuning indicates that the decoder settings mentioned on previous row were used for tuning too. Translation accuracy and decode time per sentence for word-level decoding (in milliseconds) is shown on the last line for comparison. The PBSMT systems were trained and decoded on a server with Intel Xeon processors (2.5 GHz) and 256 GB RAM. 3.3 Evaluation We use BLEU (Papineni et al., 2002) for evaluating translation accuracy. We use the sum of user and system time minus the time for loading the phrase table (all reported by Moses) to determine the time taken for decoding the test set. 4 Effect of decoder parameters We observed that the decoding time for OS-level models is approximately 70 times of the word-level model. This explosion in the decoding time makes translation highly compute intensive and difficult to perform in real-time. It also makes tuning MT systems very slow since tuning typically requires multiple decoding runs over the tuning set. Hence, we experimented with"
W16-4811,P16-1162,0,0.0192064,"which are illustrated in Table 1: • Boundary Marker: The subword at the boundary of a word is augmented with a marker character. There is one boundary subword, either the first or the last chosen as per convention. Such a representation has been used in previous work, mostly related to morpheme level representation. • Internal Marker: Every subword internal to the word is augmented with a marker character. This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh’s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016). • Space Marker: The subword units are not altered, but inter-word boundary is represented by a space marker. Most work on translation between related languages has used this format. 83 For boundary and internal markers, the addition of the marker character does not change the sentence length, but can create two representations for some subwords (corresponding to internal and boundary positions), thus introducing some data sparsity. On the other hand, space marker doubles the sentence length (in terms in words), but each subword has a unique representation. 2.3 Decoder Parameters Given the ba"
W16-4811,R13-1088,0,0.697482,"l corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segmentation thi s i s a n e xa m p le o f da ta fo r"
W16-4811,2009.eamt-1.3,0,0.647772,"lity SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segme"
W16-4811,E12-1015,0,0.349268,"based on encoder decoder architectures, particularly without attention mechanism, are more sensitive to sentence length, so we presume that the boundary marker format may be more appropriate. 86 6 Related Work It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007). Aligning long sentences is computationally expensive, hence most work has concentrated on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can"
W16-4811,W07-0705,0,0.703327,"to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data"
W16-4811,W16-2327,0,0.0150808,"ts for representation, which are illustrated in Table 1: • Boundary Marker: The subword at the boundary of a word is augmented with a marker character. There is one boundary subword, either the first or the last chosen as per convention. Such a representation has been used in previous work, mostly related to morpheme level representation. • Internal Marker: Every subword internal to the word is augmented with a marker character. This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh’s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016). • Space Marker: The subword units are not altered, but inter-word boundary is represented by a space marker. Most work on translation between related languages has used this format. 83 For boundary and internal markers, the addition of the marker character does not change the sentence length, but can create two representations for some subwords (corresponding to internal and boundary positions), thus introducing some data sparsity. On the other hand, space marker doubles the sentence length (in terms in words), but each subword has a unique representation. 2.3 Decoder"
W16-4811,N07-1062,0,0.0127205,"ch actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Ch"
W16-4811,jha-2010-tdil,0,\N,Missing
W16-4811,J07-2003,0,\N,Missing
W16-5001,baccianella-etal-2010-sentiwordnet,0,0.027947,"bels are determined based on hashtags, as stated above. The total number of distinct labels (L) is 3, and the total number of distinct sentiment (S) is 2. The total number of distinct topics (Z) is experimentally determined as 50. We use block-based Gibbs sampling to estimate the distributions. The block-based sampler samples all latent variables together based on their joint distributions. We set asymmetric priors based on sentiment word-list from McAuley and Leskovec (2013b). A key parameter of the model is ηw since it drives the split of a word as a topic or a sentiment word. SentiWordNet (Baccianella et al., 2010) is used to learn the distribution ηw prior to estimating the model. We average across multiple senses of a word. Based on the SentiWordNet scores to all senses of a word, we determine this probability. 6 Results Work day morning night today work Women Women Wife Compliment(s) Fashion Love Party Jokes Weather Quote Jokes Humor Comedy Satire Snow Today Rain Weather Day School Love Politics tomorrow school work morning night love feeling break-up day/night sleep Ukraine Russia again deeply raiders life friends night drunk parties Table 2: Topics estimated when the topic model is learned on only"
W16-5001,W14-2608,0,0.0162925,"reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is based on the idea of context incongruity. In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers using features such as unigrams, laughter expressions, hyperbolic expressions, etc. Liebrecht et al. (2013) experiment with unigrams, bigrams and trigrams as features. To the best of our knowledge, past approaches for sarcasm detection do not use topic modeling, which we do. 3 Motivation Topic models enable discovery of thematic structures in a large-sized corpus. The motivation behind using topic models for sarcasm detection arises from two reasons: (a) presence of sarcasm-prevalent topics, and (b) differences in s"
W16-5001,P15-2124,1,0.942754,"ast work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is based on the idea of context incongruity. In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers usi"
W16-5001,W16-0415,1,0.853443,"i-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is ba"
W16-5001,K16-1015,1,0.87285,"i-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is ba"
W16-5001,W15-2905,1,0.896339,"Missing"
W16-5001,W13-1605,0,0.0775844,"Missing"
W16-5001,P12-1036,0,0.439376,"he dataset and the experiment setup. Section 6 discusses the results in three steps: qualitative results, quantitative results and application of our topic model to sarcasm detection. Section 7 concludes the paper and points to future work. 2 Related Work Topic models are popular for sentiment aspect extraction. Jo and Oh (2011) present an aspect-sentiment unification model that learns different aspects of a product, and the words that are used to express sentiment towards the aspects. In terms of using two latent variables: one for aspect and one for sentiment, they are related to our model. Mukherjee and Liu (2012a) use a semi-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model"
W16-5001,P12-1034,0,0.112964,"he dataset and the experiment setup. Section 6 discusses the results in three steps: qualitative results, quantitative results and application of our topic model to sarcasm detection. Section 7 concludes the paper and points to future work. 2 Related Work Topic models are popular for sentiment aspect extraction. Jo and Oh (2011) present an aspect-sentiment unification model that learns different aspects of a product, and the words that are used to express sentiment towards the aspects. In terms of using two latent variables: one for aspect and one for sentiment, they are related to our model. Mukherjee and Liu (2012a) use a semi-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model"
W16-5001,D09-1026,0,0.0309919,"topic z and switch =0 (topic word), with prior γ χs Distribution over words given sentiment s and switch=1 (sentiment word), with prior δ1 χsz Distribution over words given a sentiment s and topic z and switch=1 (sentiment word), with prior δ2 ψl Distribution over sentiment given a label l and switch =1 (sentiment word), with prior β1 ψzl Distribution over sentiment given a label l and topic z and switch =1 (sentiment word), with prior β2 Table 1: Glossary of Variables/Distributions used 4 Sarcasm Topic Model 4.1 Design Rationale Our topic model requires sentiment labels of tweets, as used in Ramage et al. (2009). This sentiment can be positive or negative. However, in order to incorporate sarcasm, we re-organize the two sentiment values into three: literal positive, literal negative and sarcastic. The observed variable l in our model indicates this sentiment label. For sake of simplicity, we refer to the three values of l as positive, negative and sarcastic, in rest of the paper. Every word w in a tweet is either a topic word or a sentiment word. A topic word arises due to a topic, whereas a sentiment word arises due to combination of topic and sentiment. This notion is common to several sentiment-ba"
W16-5001,D13-1066,0,0.0698258,"Missing"
W16-5001,K16-1017,0,0.148106,"Missing"
W16-5001,P14-2084,0,0.0522059,"Missing"
W16-6303,2010.amta-papers.6,0,0.0388479,"d combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for"
W16-6303,W05-0808,0,0.0333302,"an be developed using the strengths of both SMT and RBMT. In this paper, we develop a hybrid model to exploit the benefits of disambiguation, linguistic rules, and structural issues. Knowledge of coupling is very useful to build hybrid model of machine translation. There are different types of coupling, viz. serial coupling and Parallel coupling. In serial coupling, SMT and RBMT are processed one after another in sequence. In parallel coupling, models are processed in parallel to build a hybrid model. In Indian languages, few hybrid models have been proposed as in(Dwivedi and Sukhadeve, 2010; Aswani and Gaizauskas, 2005). The rest of the paper is structured as follows. We present a brief review of the existing works in Section 2. Motivations and various characteristic features have been discussed in Section 3. We describe our proposed method in Section 4. Experiential setup and results are discussed in Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in transl"
W16-6303,J90-2002,0,0.76531,"have been proposed as in(Dwivedi and Sukhadeve, 2010; Aswani and Gaizauskas, 2005). The rest of the paper is structured as follows. We present a brief review of the existing works in Section 2. Motivations and various characteristic features have been discussed in Section 3. We describe our proposed method in Section 4. Experiential setup and results are discussed in Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for"
W16-6303,D07-1007,0,0.0500122,"and endi−1 are the starting positions of the translation of ith phrase and end position of the (i − 1)th phrase of e in f. In the above equation, it is well defined that most probable phrases present in training corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system gr"
W16-6303,P16-2007,0,0.0234208,"bability P (f |e) is modeled as, P (f1−I |e−I 1 )= I ∏ i=1 ϕ(f¯i |e¯i )d(starti −endi−1 −1) ϕ is phrase translation probability and d(.) is distortion probability. starti −endi–1 −1, which is the argument of d(.) is a function of i, whereas starti and endi−1 are the starting positions of the translation of ith phrase and end position of the (i − 1)th phrase of e in f. In the above equation, it is well defined that most probable phrases present in training corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target"
W16-6303,2008.tc-1.2,0,0.632026,"ss in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for Combining SMT and RBMT In this work we propose a hybrid architecture for translating English documents into Hindi. Both of these languages are very popular. English is an internation"
W16-6303,N03-1017,0,0.186143,"main. The data is often mixed, comprising of very short sentences (even the phrases) and the long sentences. To the best of our knowledge, for such a domain, there is no work involving Indian languages.Below we describe SMT and RBMT very briefly. 1.1 Statistical Machine Translation (SMT) Statistical machine translation (SMT) systems are considered to be good at capturing knowledge of the domain from a large amount of parallel data. This has robustness in resolving ambiguities and other related issues. SMT provides good translation output based on statistics and maximum likelihood expectation (Koehn et al., 2003a): ebest = argmaxe P (e|f ) = argmaxe [P (f |e)PLM (e)] where f and e are the source and target languages, respectively. PLM (e) and P (f |e) are the language and translation model, respectively. The best output translation is denoted D S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural Language Processing, pages 10–19, c Varanasi, India. December 2016. 2016 NLP Association of India (NLPAI) by ebest . Language model corresponds to the n-gram probability. The translation probability P (f |e) is modeled as, P (f1−I |e−I 1 )= I ∏ i=1 ϕ(f¯i |e¯i )d(starti −endi−1 −1)"
W16-6303,W10-3707,0,0.023742,"tput. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pre-processing, morp"
W16-6303,P02-1040,0,0.105342,"Missing"
W16-6303,W09-3528,0,0.0243521,"as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pr"
W16-6303,I08-1067,1,0.908514,"ng corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysi"
W16-6303,P09-1090,1,0.83307,"e useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pre-processing, morphological analysis, chunki"
W16-6303,W07-0728,0,0.0325171,"-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for Combining SMT and RBMT In this work we propose a hybrid architecture for translating English doc"
W16-6303,2003.mtsummit-systems.15,0,0.10308,"ally, generation step consists of genderization, vibhakti computation, TAM computation, agreement computing, word generator and sentence generator. The agreement computing can be accomplished with three sub steps: intra-chunk, inter-chunk and default agreement computing. In (Dave et al., 2001) authors have proposed an inter-lingua based English–Hindi machine translation system. In (Poornima et al., 2011), authors have described how to simplify English to Hindi translation using a rule-based approach. AnglaHindi is one of the very popular English-Hindi rule-based translation tools proposed in (Sinha and Jain, 2003). Multilingual machine aided translation for English to Indian languages has been developed in (Sinha et al., 1995). Apertium is an open source rule-based machine translation tool proposed in (Forcada et al., 2011). Rule-based approach for machine translation has been proposed with respect to Indian language (Dwivedi and Sukhadeve, 2010). 1.3 Hybrid Machine Translation A hybrid model of machine translation can be developed using the strengths of both SMT and RBMT. In this paper, we develop a hybrid model to exploit the benefits of disambiguation, linguistic rules, and structural issues. Knowle"
W16-6303,2009.mtsummit-posters.21,0,0.0275789,"Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In"
W16-6311,P98-1041,0,0.081381,"pta et al., 2012). For example, the word khusboo (”fragrance”) can be written in Roman script using different variations such as kushboo, khusbu, khushbu and so on. This type of problem is termed as a non-trivial term matching problem for search engines with the aim to match the native-script or Roman-transliterated query with the documents in multiple scripts after considering the spelling variations. Many single (native) script queries and documents with spelling variations have been studied (French et al., 1997; Zobel and Dart, 1996) as well as transliteration of named entities (NE) in IR (Collier et al., 1998; Wang et al., 2009). It is important for every IR engine to present users with information that are most relevant to the users’ needs. While searching, user has an idea of what s/he wants, but in many instances, due to the variations in query formulations, retrieved results could greatly vary. As a result, understanding the nature of information need behind the queries issued by Web users has become an imD S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural Language Processing, pages 81–89, c Varanasi, India. December 2016. 2016 NLP Association of India (NLPAI) por"
W16-6311,gupta-etal-2012-mining,0,0.03166,"as well as Roman scripts, and these should also be matched to the documents written in both the scripts. Transliteration (Lopez, 2008) is the process of phonetically describing the words of a given language using a non-native script. For both the web documents and intended search queries to retrieve those documents, transliteration, especially into Roman script, is generally used. Since no standard ways of spelling any word into a non-native script exist, transliterated contents offer extensive spelling variations; typically, we can transliterate a native term into Roman script in many ways (Gupta et al., 2012). For example, the word khusboo (”fragrance”) can be written in Roman script using different variations such as kushboo, khusbu, khushbu and so on. This type of problem is termed as a non-trivial term matching problem for search engines with the aim to match the native-script or Roman-transliterated query with the documents in multiple scripts after considering the spelling variations. Many single (native) script queries and documents with spelling variations have been studied (French et al., 1997; Zobel and Dart, 1996) as well as transliteration of named entities (NE) in IR (Collier et al., 1"
W16-6315,D11-1100,1,0.851474,"oks (B). Results show that the sense-based classifier in the target domain is more accurate than wordbased classifier for 12 pairs of the source and target domains. 2 Related Work Sentiment analysis within a domain has been widely studied in literature, where the train and test datasets are assumed to be from the same domain (Pang et al., 2002; Turney, 2002; Ng et al., 2006; Kanayama and Nasukawa, 2006; Breck et al., 2007; Pang and Lee, 2008; Li et al., 2009; Taboada et al., 2011). This kind of sentiment analysis where the train and test data are from the same domain is known as in-domain SA. Balamurali et al., (2011) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs due to the differences across domains. Hence, getting a high accuracy classifier in the unlabeled target domain from a labeled source domain is a challenging task. Domain adaptation for cross-domain sentiment classification has been explored by many researchers (Jia"
W16-6315,K15-1006,0,0.0162677,"e of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs due to the differences across domains. Hence, getting a high accuracy classifier in the unlabeled target domain from a labeled source domain is a challenging task. Domain adaptation for cross-domain sentiment classification has been explored by many researchers (Jiang and Zhai, 2007; Ji et al., 2011; Saha et al., 2011; Xia et al., 2013; Bhatt et al., 2015; Zhou et al., 2014; Glorot et al., 2011). Most of the works have focused on learning a shared low dimensional representation of features that can be generalized across different domains. Glorot et al., (2011) proposed a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Zhou et al., (2014) also proposed a deep learning approach to learn a feature mapping between cross-domain heterogeneous features as well as a better feature representation for mapped data to reduce the bias issue caused by the cross-domain correspondences. Al"
W16-6315,P07-1056,0,0.0770128,"Missing"
W16-6315,P06-2079,0,0.0159126,"target domain to an extent, which in turn results into a more accurate classifier in the target domain. In this paper, we show the effectiveness of senses over words across four domains, viz., DVD (D), Electronics (E), Kitchen (K), and Books (B). Results show that the sense-based classifier in the target domain is more accurate than wordbased classifier for 12 pairs of the source and target domains. 2 Related Work Sentiment analysis within a domain has been widely studied in literature, where the train and test datasets are assumed to be from the same domain (Pang et al., 2002; Turney, 2002; Ng et al., 2006; Kanayama and Nasukawa, 2006; Breck et al., 2007; Pang and Lee, 2008; Li et al., 2009; Taboada et al., 2011). This kind of sentiment analysis where the train and test data are from the same domain is known as in-domain SA. Balamurali et al., (2011) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs due to the differ"
W16-6315,W02-1011,0,0.0196012,"abeled source domain to unlabeled target domain to an extent, which in turn results into a more accurate classifier in the target domain. In this paper, we show the effectiveness of senses over words across four domains, viz., DVD (D), Electronics (E), Kitchen (K), and Books (B). Results show that the sense-based classifier in the target domain is more accurate than wordbased classifier for 12 pairs of the source and target domains. 2 Related Work Sentiment analysis within a domain has been widely studied in literature, where the train and test datasets are assumed to be from the same domain (Pang et al., 2002; Turney, 2002; Ng et al., 2006; Kanayama and Nasukawa, 2006; Breck et al., 2007; Pang and Lee, 2008; Li et al., 2009; Taboada et al., 2011). This kind of sentiment analysis where the train and test data are from the same domain is known as in-domain SA. Balamurali et al., (2011) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in perfo"
W16-6315,S14-2009,0,0.0253916,"e the differences across domains. Results show that senses of words provide a better sentiment classifier in the unlabeled target domain in comparison to words for 12 pairs of source and target domains. 1 Introduction Generally users do not explicitly indicate sentiment orientation (positive or negative) of the reviews posted by them on the Web, it needs to be predicted from the text, which has led to plethora of work in the field of Sentiment Analysis (SA) (Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). Most of the proposed techniques for sentiment analysis are based on the availability of labeled train data considering that train data and 115 test data belong to the same domain. Easy access of internet has made the users to post their experiences with any product, service or application very frequently. Consequently, there is a high increase in number of domains in which sentimental data is available. Getting sentiment (positive or negative) annotated data manually in each domain is not feasible due to the cost incurred in annotation process. Cross-domain SA provides a solution to build a"
W16-6315,J11-2001,0,0.0651485,"s of words in place of words help to overcome the differences across domains. Results show that senses of words provide a better sentiment classifier in the unlabeled target domain in comparison to words for 12 pairs of source and target domains. 1 Introduction Generally users do not explicitly indicate sentiment orientation (positive or negative) of the reviews posted by them on the Web, it needs to be predicted from the text, which has led to plethora of work in the field of Sentiment Analysis (SA) (Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). Most of the proposed techniques for sentiment analysis are based on the availability of labeled train data considering that train data and 115 test data belong to the same domain. Easy access of internet has made the users to post their experiences with any product, service or application very frequently. Consequently, there is a high increase in number of domains in which sentimental data is available. Getting sentiment (positive or negative) annotated data manually in each domain is not feasible due to the cost incurred in annotation process."
W16-6315,P07-1034,0,0.0341781,"11) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs due to the differences across domains. Hence, getting a high accuracy classifier in the unlabeled target domain from a labeled source domain is a challenging task. Domain adaptation for cross-domain sentiment classification has been explored by many researchers (Jiang and Zhai, 2007; Ji et al., 2011; Saha et al., 2011; Xia et al., 2013; Bhatt et al., 2015; Zhou et al., 2014; Glorot et al., 2011). Most of the works have focused on learning a shared low dimensional representation of features that can be generalized across different domains. Glorot et al., (2011) proposed a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Zhou et al., (2014) also proposed a deep learning approach to learn a feature mapping between cross-domain heterogeneous features as well as a better feature representation for mapped da"
W16-6315,W06-1642,0,0.0276113,"an extent, which in turn results into a more accurate classifier in the target domain. In this paper, we show the effectiveness of senses over words across four domains, viz., DVD (D), Electronics (E), Kitchen (K), and Books (B). Results show that the sense-based classifier in the target domain is more accurate than wordbased classifier for 12 pairs of the source and target domains. 2 Related Work Sentiment analysis within a domain has been widely studied in literature, where the train and test datasets are assumed to be from the same domain (Pang et al., 2002; Turney, 2002; Ng et al., 2006; Kanayama and Nasukawa, 2006; Breck et al., 2007; Pang and Lee, 2008; Li et al., 2009; Taboada et al., 2011). This kind of sentiment analysis where the train and test data are from the same domain is known as in-domain SA. Balamurali et al., (2011) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs due to the differences across domains. Hence,"
W16-6315,P09-1028,0,0.105219,"ransfer. In this paper, we propose that senses of words in place of words help to overcome the differences across domains. Results show that senses of words provide a better sentiment classifier in the unlabeled target domain in comparison to words for 12 pairs of source and target domains. 1 Introduction Generally users do not explicitly indicate sentiment orientation (positive or negative) of the reviews posted by them on the Web, it needs to be predicted from the text, which has led to plethora of work in the field of Sentiment Analysis (SA) (Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). Most of the proposed techniques for sentiment analysis are based on the availability of labeled train data considering that train data and 115 test data belong to the same domain. Easy access of internet has made the users to post their experiences with any product, service or application very frequently. Consequently, there is a high increase in number of domains in which sentimental data is available. Getting sentiment (positive or negative) annotated data manually in each domain is not feasibl"
W16-6315,P02-1053,0,0.0153186,"n to unlabeled target domain to an extent, which in turn results into a more accurate classifier in the target domain. In this paper, we show the effectiveness of senses over words across four domains, viz., DVD (D), Electronics (E), Kitchen (K), and Books (B). Results show that the sense-based classifier in the target domain is more accurate than wordbased classifier for 12 pairs of the source and target domains. 2 Related Work Sentiment analysis within a domain has been widely studied in literature, where the train and test datasets are assumed to be from the same domain (Pang et al., 2002; Turney, 2002; Ng et al., 2006; Kanayama and Nasukawa, 2006; Breck et al., 2007; Pang and Lee, 2008; Li et al., 2009; Taboada et al., 2011). This kind of sentiment analysis where the train and test data are from the same domain is known as in-domain SA. Balamurali et al., (2011) have shown that use of senses in places of words improves the performance of indomain SA significantly. Though they did not explore senses as features for cross-domain SA. Performance of the sentiment analysis systems drops severely when the test data is from some other domain than the train domain. This drop in performance occurs"
W16-6325,D14-1181,0,0.00470024,"Missing"
W16-6325,H05-1026,0,0.0222709,"and syntactic variations of words (Mikolov et al., 2013). The vector initially can be generated randomly or can be pre-trained from the large unlabeled corpus in an unsupervised fashion using external resources such as Wikipedia, news article, bio-medical literature etc. Word embedding is learned through sampling word cooccurrence distribution. These techniques are useful to identify similar words which appear in close vicinity in vector space. There are several ways of generating the word-vectors using different architectures such as word2vec (Mikolov et al., 2013), shallow neural networks (Schwenk and Gauvain, 2005), RNN (Mikolov et al., 2010; Mikolov et al., 2011) etc. We learn our word embedding through three different ways such as random number initialization, RNN’s word embedding and continuous bag-of-words (CBOW) based models. In case Sentence Named Entity De-identified Sentence Discussed O Discussed this O this case O case with O with Dr. O Dr. John Doe B-DOCTOR I-DOCTOR XYZ DOCTOR for O for Mr. O Mr. Ness B-PATIENT XYZ PATIENT Table 1: Sample sentence (sequence of words with the corresponding labels using BIO notation) and its corresponding de-identified sentence of random number initialization, w"
W16-6325,W16-4206,1,0.679779,"al., 2013) as well as named entity recognition (Collobert et al., 2011; Lample et al., 2016). Motivated by the success of deep learning techniques, in this paper, we have adopted in particular Recurrent Neural Network (RNN) (Mikolov et al., 2010) architecture to capture PHI terms. RNN has shown advantages over other machine learning 189 and rule based techniques. RNN unlike other techniques does not require features explicitly developed for the classifier learning. The virtue of system learning by itself makes the system adaptable and scalable. This work is an extension of our previous work (Shweta et al., 2016) where we identified only 7 PHI category (Patient, Doctor, Hospital, Location, Date, Age, ID) irrespective of subcategories using only i2b2-2014 training dataset. The current work provide comprehensive experimentation on i2b2-2014 challenge dataset to deidentify 7 categories and 25 subcategories. We have formulated this task as the sequence labeling problem and developed the baseline model using a supervised machine learning technique. Conditional random field (CRF) (Lafferty et al., 2001) along with a set of handcrafted features are used to build the base classifier. In the current study, we"
W16-6325,N16-1030,0,0.0290698,"rned by other hyperparameters which are initialized randomly or can be pre-trained on large unlabeled corpus. Pretraining is much beneficial in improving performance as it effectively captures the linguistic variations and patterns. Recently, there has been significant success of deep learning techniques in solving various natural language processing tasks such as text classification (Kim, 2014), language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) as well as named entity recognition (Collobert et al., 2011; Lample et al., 2016). Motivated by the success of deep learning techniques, in this paper, we have adopted in particular Recurrent Neural Network (RNN) (Mikolov et al., 2010) architecture to capture PHI terms. RNN has shown advantages over other machine learning 189 and rule based techniques. RNN unlike other techniques does not require features explicitly developed for the classifier learning. The virtue of system learning by itself makes the system adaptable and scalable. This work is an extension of our previous work (Shweta et al., 2016) where we identified only 7 PHI category (Patient, Doctor, Hospital, Loca"
W16-6325,W00-1308,0,0.0482235,"Missing"
W16-6331,W13-3520,0,0.0297224,"news data. A comment is finally represented by a vector composed of the word vectors of the individual tokens. The vector is generated as follows: P ti ∈Comment(T ) Reps(ti ) Reps(comment) = number Of Lookups (5) Here, Reps(t) is the token representation obtained by Google news word embedding and number of lookups is equal to the number of tokens from the comments present in the word embedding model. Representation of Hindi comments: For Hindi we build our own word embedding model. For training we use the data sets obtained from the Hindi Wikipedia and some other sources (Joshi. et al., 2010; Al-Rfou et al., 2013) including all the comments that we crawled. We use skip-gram representation (Mikolov et al., 2013) for the training of word2vec tool. Further, we use Eq-5 to obtain the representations of Hindi comments. We set dimension to 200 and window size as 5. After we represent the comments in terms of vectors, we develop two baselines to compare with our proposed approach. 7 253 https://code.google.com/archive/p/word2vec/ 6.2 Baseline-1 The hypothesis behind this baseline being the fact that, if two comments have the same sentiments (positive or negative) then the similarity between them would be high"
W16-6331,dey-fung-2014-hindi,0,0.0283545,"ics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong Kong (Li, 2000) indicated that linguistic constructions predominantly trigger code mixing. The work reported in (Hidayat, 2012) showed that Facebook users tend to mainly use inter-sentential switching over intra-sentential. A code-mixed speech corpus of English-Hindi on student interviews is presented in (Dey and Fung, 2014). It shows analysis and motivations of code mixing, and discusses in what grammatical contexts code mixing occurs (Dey and Fung, 2014) . To the best of our knowledge we do not see the use of any deep learning that addresses the problem of sentiment analysis in a code-mixed environment. In our current work we discuss the scope for text analysis based on deep learning architecture on government data / citizen views which can very well frame a new concept of better e-governance. 3 Resource Creation We design a web-crawler to crawl user comments from mygov.in portal. We consider the comments writt"
W16-6331,C14-1008,0,0.0128698,"1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixed is very common that demands special attention. Significant characteristics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong Kong (Li, 2000) indicated tha"
W16-6331,W15-3904,0,0.0636274,"Missing"
W16-6331,P06-2025,1,0.421909,"on the classes assigned at the token-level we classify the sentence based on the majority voting. The sentence is classified to belong to that particular class which appears most in the sentence. Mathematically, it can be defined as follows: S = {x|x ∈ lang(t), ∀t ∈ T } Lang(comments) = argmax(f (s)) s∈S (1) abundant of data sources from Hindi Wikipedia. We back-transliterate the roman script into Devanagari script. A transliteration system takes as input a character string in the source language and generates a character string in the target language as output. The transliteration algorithm (Ekbal et al., 2006) that we used here can be conceptualized as two-levels of decoding: (a) segmenting source and target language strings into transliteration units (TUs); and (b). defining appropriate mapping between the source and target TUs by resolving different combinations of alignments and unit mappings. The TU is defined based on a regular expression. For a given token belonging to ‘non-native’ script X6 written in English Y as the observed channel output, we have to find out the most likely English transliteration Y that maximizes P (Y |X). The Indic word(Hindi) is divided into TUs that have the pattern"
W16-6331,P14-1062,0,0.0604684,"ent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social medi"
W16-6331,D14-1181,0,0.0152011,"Missing"
W16-6331,D15-1280,0,0.0181688,"alysis are discussed in Section-9. Finally, we conclude in Section-10. 2 Related Works Nowadays deep learning models are being used to solve various natural language processing (NLP) problems. Usually, the input to any deep learning based model is the word representation. Some of the commonly used word representation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic"
W16-6331,P11-1015,0,0.0531074,"3) for the training of word2vec tool. Further, we use Eq-5 to obtain the representations of Hindi comments. We set dimension to 200 and window size as 5. After we represent the comments in terms of vectors, we develop two baselines to compare with our proposed approach. 7 253 https://code.google.com/archive/p/word2vec/ 6.2 Baseline-1 The hypothesis behind this baseline being the fact that, if two comments have the same sentiments (positive or negative) then the similarity between them would be higher than any other comments having different sentiments. We use the IMDB movie reviews data sets (Maas et al., 2011) containing 2K positive and 2K negative reviews for English and Hindi movie reviews, and tourism data (Joshi. et al., 2010) to compare the opinions represented in our Hindi comments. This algorithm takes as input a comment and the source documents (i.e. datasets that we collected). Each comment and all the documents belonging to a particular set (positive set: containing all the positive comments and negative set: containing all the negative comments) are represented as vectors following the representation techniques that we discussed in Subsection-6.1. We compute the cosine similarity of a gi"
W16-6331,D14-1162,0,0.0767908,"ails of our proposed convolutional network model to identify opinion from comments. The experimental setup along with the details of external data are described in Section-8. The obtained results, key observations and error analysis are discussed in Section-9. Finally, we conclude in Section-10. 2 Related Works Nowadays deep learning models are being used to solve various natural language processing (NLP) problems. Usually, the input to any deep learning based model is the word representation. Some of the commonly used word representation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convoluti"
W16-6331,P13-1045,0,0.0118334,"presentation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learnin"
W16-6331,D13-1170,0,0.00292371,"presentation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learnin"
W16-6331,S14-2033,0,0.0163851,"rk(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixed is very common that demands special attention. Significant characteristics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong"
W16-6331,P10-1040,0,0.0171598,"3), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment cla"
W16-6331,P14-2105,0,0.0313506,"ence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixe"
W16-6336,W04-0404,0,0.503402,"Missing"
W16-6336,P98-1015,0,0.074586,"t bases rule learning and SVM WordNet Similarity (k-NN) SVM and String Kernels MaxEnt / SVM Deep neural network Table 1: Performance of past approaches for noun compound interpretation. Note that (almost) all methods use different relation inventory and different dataset making it difficult to compare performance. Levi (1978)’s study is purely based on linguistics. On the other hand, Warren (1978) analyzed the Brown corpus and proposed a four-level hierarchy of semantic relations. Nastase and Szpakowicz (2003) extended Warren (1978)’s approach. Their proposed set of relations is also based on Barker and Szpakowicz (1998)’s semantic relations. Vanderwende (1994) proposed a set of 13 relations based on the syntactical category and types of questions. Girju et al. (2005) provided another inventory of semantic relation based on Moldovan et al. (2004) for semantic relation in noun phrases. Finally, Tratz and Hovy (2010) compared and consolidated most of these theories and proposed a two level hierarchy of 43 semantic relations, which are grouped in 9 coarse relations. This inventory of relations was iteratively refined to improve inter-annotator agreement. Tratz and Hovy (2010) used crowd sourcing for the iterativ"
W16-6336,W15-0122,0,0.127639,"ovan et al. (2004) for semantic relation in noun phrases. Finally, Tratz and Hovy (2010) compared and consolidated most of these theories and proposed a two level hierarchy of 43 semantic relations, which are grouped in 9 coarse relations. This inventory of relations was iteratively refined to improve inter-annotator agreement. Tratz and Hovy (2010) used crowd sourcing for the iterative process. We have used this relation repository for our experiments and analysis. 2.2 Automatic Interpretation Researchers have proposed various methods for automatic interpretation (Wermter, 1989; Nakov, 2013; Dima and Hinrichs, 2015). Unfortunately, these methods have been tested on different relation inventories and datasets, which makes it hard to compare their performance. Table 1 summarizes various methods for automatic interpretation. All these methods can be categorized in two categories based on how it models the relation: (1) model the relation using only component features (Kim and Baldwin, 2005, 2013), or (2) directly modeling the relation based on how components of a compound can interact in real world (Nakov and Hearst, 2008). A system based on the latter approach should ideally perform better. But, generaliza"
W16-6336,I05-1082,0,0.34124,"terative process. We have used this relation repository for our experiments and analysis. 2.2 Automatic Interpretation Researchers have proposed various methods for automatic interpretation (Wermter, 1989; Nakov, 2013; Dima and Hinrichs, 2015). Unfortunately, these methods have been tested on different relation inventories and datasets, which makes it hard to compare their performance. Table 1 summarizes various methods for automatic interpretation. All these methods can be categorized in two categories based on how it models the relation: (1) model the relation using only component features (Kim and Baldwin, 2005, 2013), or (2) directly modeling the relation based on how components of a compound can interact in real world (Nakov and Hearst, 2008). A system based on the latter approach should ideally perform better. But, generalization of such information needs large annotated data and a Web scale resource for paraphrase searching. 2.3 Word Embeddings For classification task, we need to represent a given noun compound as a feature vector. One way is to concatenate the word embeddings of its constituent 294 words. Mathematically, a word embedding is a function V : D → Rn , where D is a dictionary of the"
W16-6336,P95-1007,0,0.387624,"Missing"
W16-6336,N13-1090,0,0.00684169,"space. Word embeddings are based on the distributional hypothesis. So, the words which occur in similar context have similar vectors. As word vector captures the context, the embedding technique approximates the interaction of a word with other words. This intuition can help us in modeling semantic relation using vectors of the components only. Dima and Hinrichs (2015) has shown that the noun compound interpretation using only word embeddings, without any feature engineering, gives results comparable to the state-of-the-art. For our experiments, we used Google’s pre-trained word embeddings1 (Mikolov et al., 2013a,b). 3 Approach Our aim is to classify a given noun compound into one of fine classes. The classes are arranged in two level hierarchy. We want to exploit the hierarchy to improve the system. Noun Compound Coarse Classifier coarse class Fine Classifier Fine Class Figure 1: Pipeline architecture If the coarse class of a noun compound is known, then inter coarse class confusion can be avoided. We try to leverage this fact by proposing a pipeline architecture (refer Figure 1) for the classification task. We 1 https://code.google.com/archive/p/word2vec/ System Type Input Output Type-1 vectors of"
W16-6336,W04-2609,0,0.288705,"ion inventory and different dataset making it difficult to compare performance. Levi (1978)’s study is purely based on linguistics. On the other hand, Warren (1978) analyzed the Brown corpus and proposed a four-level hierarchy of semantic relations. Nastase and Szpakowicz (2003) extended Warren (1978)’s approach. Their proposed set of relations is also based on Barker and Szpakowicz (1998)’s semantic relations. Vanderwende (1994) proposed a set of 13 relations based on the syntactical category and types of questions. Girju et al. (2005) provided another inventory of semantic relation based on Moldovan et al. (2004) for semantic relation in noun phrases. Finally, Tratz and Hovy (2010) compared and consolidated most of these theories and proposed a two level hierarchy of 43 semantic relations, which are grouped in 9 coarse relations. This inventory of relations was iteratively refined to improve inter-annotator agreement. Tratz and Hovy (2010) used crowd sourcing for the iterative process. We have used this relation repository for our experiments and analysis. 2.2 Automatic Interpretation Researchers have proposed various methods for automatic interpretation (Wermter, 1989; Nakov, 2013; Dima and Hinrichs,"
W16-6336,P08-1052,0,0.0156195,"e proposed various methods for automatic interpretation (Wermter, 1989; Nakov, 2013; Dima and Hinrichs, 2015). Unfortunately, these methods have been tested on different relation inventories and datasets, which makes it hard to compare their performance. Table 1 summarizes various methods for automatic interpretation. All these methods can be categorized in two categories based on how it models the relation: (1) model the relation using only component features (Kim and Baldwin, 2005, 2013), or (2) directly modeling the relation based on how components of a compound can interact in real world (Nakov and Hearst, 2008). A system based on the latter approach should ideally perform better. But, generalization of such information needs large annotated data and a Web scale resource for paraphrase searching. 2.3 Word Embeddings For classification task, we need to represent a given noun compound as a feature vector. One way is to concatenate the word embeddings of its constituent 294 words. Mathematically, a word embedding is a function V : D → Rn , where D is a dictionary of the words in a language and Rn is an n-dimensional real space. Word embeddings are based on the distributional hypothesis. So, the words wh"
W16-6336,P07-3013,0,0.0747295,"Missing"
W16-6336,P02-1032,0,0.69507,"Missing"
W16-6336,P10-1070,0,0.442058,"a Girish K. Palshikar Dept. of CSE, Dept. of CSE, TCS Research, IIT Bombay, India IIT Bombay, India Tata Consultancy Services, India girishp@cse.iitb.ac.in pb@cse.iitb.ac.in gk.palshikar@tcs.com Abstract Sequences of long nouns, i.e., noun compounds, occur frequently and are productive. Their interpretation is important for a variety of tasks located at various layers of NLP. Major reasons behind the poor performance of automatic noun compound interpretation are: (a) lack of a well defined inventory of semantic relations and (b) non-availability of sufficient, annotated, high-quality dataset. Tratz and Hovy (2010) presented an inventory of semantic relations. They compared existing inventories with their two-level hierarchy, and created a large annotated dataset. We performed both theoretical as well as datadriven analysis of this inventory. Theoretical analysis reveal ambiguities in the coarse relations. Data-driven analysis report similar performance for coarse as well as fine relations prediction. Our experiments show that improving the coarse classification accuracy can improve the performance of fine class predictor by 13 to 30 points in F-score. 1 Introduction An important characteristic of a lan"
W16-6336,C94-2125,0,0.628245,"-NN) SVM and String Kernels MaxEnt / SVM Deep neural network Table 1: Performance of past approaches for noun compound interpretation. Note that (almost) all methods use different relation inventory and different dataset making it difficult to compare performance. Levi (1978)’s study is purely based on linguistics. On the other hand, Warren (1978) analyzed the Brown corpus and proposed a four-level hierarchy of semantic relations. Nastase and Szpakowicz (2003) extended Warren (1978)’s approach. Their proposed set of relations is also based on Barker and Szpakowicz (1998)’s semantic relations. Vanderwende (1994) proposed a set of 13 relations based on the syntactical category and types of questions. Girju et al. (2005) provided another inventory of semantic relation based on Moldovan et al. (2004) for semantic relation in noun phrases. Finally, Tratz and Hovy (2010) compared and consolidated most of these theories and proposed a two level hierarchy of 43 semantic relations, which are grouped in 9 coarse relations. This inventory of relations was iteratively refined to improve inter-annotator agreement. Tratz and Hovy (2010) used crowd sourcing for the iterative process. We have used this relation rep"
W16-6337,begum-etal-2008-developing,0,0.52206,"Natural Language Processing, pages 299–304, c Varanasi, India. December 2016. 2016 NLP Association of India (NLPAI) 2 Related Work Verb POS easily represents the activity, state or action in a text which can lead to its semantic interpretation. This property of verb is useful in creating the necessary knowledge base required for NLP applications. Though, for English language, there is a substantial amount of work which has been done regarding the development and analysis using verb frames, but only limited work can be seen for Indian languages. This can be seen in the following research work. Begum et al. (2008) discussed their experience with the creation of verb frames for Hindi language. These frames are further classified based on Paninian grammar framework using 6 karaka relations. The method followed by the authors considers the morphology, syntactic variations and semantics of the verb to divide it into different classes. This resource focuses on NLP related activities for Hindi language. Based on similar approach, Ghosh (2014) created a resource for verb frames for compound verbs in Bengali language. The objective of the paper was to investigate if the vector verb from the compound verb is ab"
W16-6337,W14-5143,0,0.164648,"egarding the development and analysis using verb frames, but only limited work can be seen for Indian languages. This can be seen in the following research work. Begum et al. (2008) discussed their experience with the creation of verb frames for Hindi language. These frames are further classified based on Paninian grammar framework using 6 karaka relations. The method followed by the authors considers the morphology, syntactic variations and semantics of the verb to divide it into different classes. This resource focuses on NLP related activities for Hindi language. Based on similar approach, Ghosh (2014) created a resource for verb frames for compound verbs in Bengali language. The objective of the paper was to investigate if the vector verb from the compound verb is able to retain its case marking properties and argument structure or not. And also the knowledge and syntax associated with verb frames can be utilized for categorizing and analyzing the verb words for various NLP applications. Soni et al. (2013) explored the application of verb frames and the conjuncts in sentence simplification for Hindi language. The method proposed by the authors includes usage of conjuncts as a first level o"
W16-6337,I13-1151,0,0.360023,"e morphology, syntactic variations and semantics of the verb to divide it into different classes. This resource focuses on NLP related activities for Hindi language. Based on similar approach, Ghosh (2014) created a resource for verb frames for compound verbs in Bengali language. The objective of the paper was to investigate if the vector verb from the compound verb is able to retain its case marking properties and argument structure or not. And also the knowledge and syntax associated with verb frames can be utilized for categorizing and analyzing the verb words for various NLP applications. Soni et al. (2013) explored the application of verb frames and the conjuncts in sentence simplification for Hindi language. The method proposed by the authors includes usage of conjuncts as a first level of sentence simplification. This is followed by using verb frames enhanced with tense, aspect and modality features. It is a rule based system and its output was evaluated manually and automatically using BLEU score for the ease of readability and simplification. Other related work by Schulte (2009), Theakston et al. (2015) has also explored verb frames for English language. After reviewing these papers, the ne"
W17-2338,W16-2915,0,0.0377241,"Missing"
W17-2338,P12-1092,0,0.0796168,"background on different notions used later in the paper. Section 3 motivates our approach through examples. Section 4 explains our approach in detail. Section 5 enlists the experimental setup. Section 6 details the results and analysis, followed by conclusion and future work. 2 2.1 Background Word Embeddings Word embeddings are a crucial component of modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013; Levy and Goldberg, 2014). The central idea behind word embeddings is the distributional hypothesis, which states that words which are similar in meaning occur in similar contexts (Rubenstein and Goodenough, 1965). Consider the Continuous Bag of Words model by (Mikolov et al., 2013), where the following problem is poised to a neural network: given the context, predict the word that comes in between. The weights of the network are the word embeddings. Training the model over running text brings embeddings of words with similar meaning closer. 2.2 • Synonymy: Multiple word"
W17-2338,D16-1104,1,0.879546,"Missing"
W17-2338,D14-1181,0,0.00660222,"s Table 2 shows the results of 5-fold evaluation on automated review of medical coding. Note that the modified embeddings consistently outperform the original ones for all pre-trained embeddings that we used. The reason behind this improvement is evident from the analysis table 1 where we show how the constraints are better modeled by the modified embeddings (Mod) as compared to the original embeddings (Orig). 7 Related Work Word embeddings have proved to be useful for various tasks, such as Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016). Medical domain specific pre-trained word embeddings were released by different groups, such as Pyysalo et al. (2013), Brokos et al. (2016), etc. Wu et al. (2015) apply word embeddings for clinical abbreviation disambiguation. 8 Conclusion and Future Work In this paper, we proposed a modification of the CBOW algorithm to add task and domain specific information to pre-trained word embeddings. We added information from a medical claims dataset and the ICD-10 code hierarchy to improve the utility of the pre-trained w"
W17-2338,D15-1168,0,0.0719719,"Missing"
W17-2338,W15-3822,0,0.0593097,"Missing"
W17-2605,W12-4501,0,0.0316958,"(MemN2N). All the results reported with MemN2N are averaged across 10 different executions with different seeds used for training data shuffling. This is done to make the results independent of data-shuffling during training. The hyper-parameters are fixed as embedding size=20, hops=3 under the training configuration as optimizer=Adam, #epochs=100, batch size=32, learning rate=0.01. To make the results of Cort comparable with the answer prediction accuracy of memory networks, accuracy of Cort is computed based on the number of correctly identified coreferent mentions, instead of CoNLL score (Pradhan et al., 2012). This evaluation is valid since there is only one coreferent chain comprising 2 mentions in each synthetic dataset instance. We experiment Cort with the available pre-trained coreference model and with the model trained on training data from the corresponding synthetic dataset. We also check for the effectiveness of attention mechanism in memory networks to aid coreference resolution, through attention mechanism accuracy. Attention mechanism accuracy indicates, given an anaphoric mention, how capable the memory networks approach approach is in identifying the probable sentences to find the an"
W17-2605,C88-1021,0,0.274248,"ts. In the subsequent step, the answer module generates the answer based on the information got from the memory module (Sukhbaatar et al., 2015; Kumar et al., 2015). We utilize memory networks for coreference resolution, modeling it as a question answering task. The context of the mentions and its relative salience in a discourse are beneficial to resolve coreference. In practice, there are 2 ways in which coreference resolution can be asIntroduction Coreference resolution resolves anaphoric mentions against the co-referring entities by integrating syntactic, semantic and pragmatic knowledge (Carbonell and Brown, 1988). Even when syntactic knowledge has a crucial role in resolving many coreferential mentions, semantic knowledge is a much more challenging aspect of coreference (Durrett and Klein, 2013). This makes the attempts to bring significant improvement to the state-of-the-art results difficult. There has been quite a few research in coreference resolution to bring in semantic knowledge through identification of semantic class of the entities (Ng, 2007a,b) and incorporating world knowledge with the help of sources like Wikipedia (Ponzetto and Strube, 2006; Rahman and Ng, 2011). The semantic analysis ap"
W17-2605,P11-1082,0,0.0234134,"ragmatic knowledge (Carbonell and Brown, 1988). Even when syntactic knowledge has a crucial role in resolving many coreferential mentions, semantic knowledge is a much more challenging aspect of coreference (Durrett and Klein, 2013). This makes the attempts to bring significant improvement to the state-of-the-art results difficult. There has been quite a few research in coreference resolution to bring in semantic knowledge through identification of semantic class of the entities (Ng, 2007a,b) and incorporating world knowledge with the help of sources like Wikipedia (Ponzetto and Strube, 2006; Rahman and Ng, 2011). The semantic analysis approach for coreference resolution discussed by Hobbs (1978) takes semantics into consideration. Vincent Ng 37 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 37–42, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics sisted by memory networks, viz. (i) for end-to-end coreference resolution, identifying the antecedents for the anaphoric mentions (ii) for identifying the relevant sentences for resolving anaphoric mentions using attention mechanism. End-to-end memory networks proposed by Sukhbaatar et al.(2015) fo"
W17-2605,D13-1203,0,0.0213976,"orks for coreference resolution, modeling it as a question answering task. The context of the mentions and its relative salience in a discourse are beneficial to resolve coreference. In practice, there are 2 ways in which coreference resolution can be asIntroduction Coreference resolution resolves anaphoric mentions against the co-referring entities by integrating syntactic, semantic and pragmatic knowledge (Carbonell and Brown, 1988). Even when syntactic knowledge has a crucial role in resolving many coreferential mentions, semantic knowledge is a much more challenging aspect of coreference (Durrett and Klein, 2013). This makes the attempts to bring significant improvement to the state-of-the-art results difficult. There has been quite a few research in coreference resolution to bring in semantic knowledge through identification of semantic class of the entities (Ng, 2007a,b) and incorporating world knowledge with the help of sources like Wikipedia (Ponzetto and Strube, 2006; Rahman and Ng, 2011). The semantic analysis approach for coreference resolution discussed by Hobbs (1978) takes semantics into consideration. Vincent Ng 37 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 37"
W17-2605,N04-1037,0,0.121144,"Missing"
W17-2605,J81-4005,0,0.730392,"Missing"
W17-2605,P15-4011,0,0.0165677,"on: As mentioned in Section 2, the probability associated with a memory vector is computed by softmax over the dot product between query representation and each memory vector. This modification applies tanh activation before the softmax is computed. The clipping of higher values by the tanh activation helps to avoid getting skewed attention weights. While the first modification is specific to coreference resolution, the latter 2 are task independent. 4 4.2 All the results are reported on the test data from 4 synthetic datasets. One of the state-of-the-art coreference resolution systems, Cort (Martschat et al., 2015) is chosen to compare with end-toend memory networks (MemN2N). All the results reported with MemN2N are averaged across 10 different executions with different seeds used for training data shuffling. This is done to make the results independent of data-shuffling during training. The hyper-parameters are fixed as embedding size=20, hops=3 under the training configuration as optimizer=Adam, #epochs=100, batch size=32, learning rate=0.01. To make the results of Cort comparable with the answer prediction accuracy of memory networks, accuracy of Cort is computed based on the number of correctly iden"
W17-2605,P07-1068,0,0.0245328,"e resolution resolves anaphoric mentions against the co-referring entities by integrating syntactic, semantic and pragmatic knowledge (Carbonell and Brown, 1988). Even when syntactic knowledge has a crucial role in resolving many coreferential mentions, semantic knowledge is a much more challenging aspect of coreference (Durrett and Klein, 2013). This makes the attempts to bring significant improvement to the state-of-the-art results difficult. There has been quite a few research in coreference resolution to bring in semantic knowledge through identification of semantic class of the entities (Ng, 2007a,b) and incorporating world knowledge with the help of sources like Wikipedia (Ponzetto and Strube, 2006; Rahman and Ng, 2011). The semantic analysis approach for coreference resolution discussed by Hobbs (1978) takes semantics into consideration. Vincent Ng 37 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 37–42, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics sisted by memory networks, viz. (i) for end-to-end coreference resolution, identifying the antecedents for the anaphoric mentions (ii) for identifying the relevant sentence"
W17-2605,N06-1025,0,0.0583,"g syntactic, semantic and pragmatic knowledge (Carbonell and Brown, 1988). Even when syntactic knowledge has a crucial role in resolving many coreferential mentions, semantic knowledge is a much more challenging aspect of coreference (Durrett and Klein, 2013). This makes the attempts to bring significant improvement to the state-of-the-art results difficult. There has been quite a few research in coreference resolution to bring in semantic knowledge through identification of semantic class of the entities (Ng, 2007a,b) and incorporating world knowledge with the help of sources like Wikipedia (Ponzetto and Strube, 2006; Rahman and Ng, 2011). The semantic analysis approach for coreference resolution discussed by Hobbs (1978) takes semantics into consideration. Vincent Ng 37 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 37–42, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics sisted by memory networks, viz. (i) for end-to-end coreference resolution, identifying the antecedents for the anaphoric mentions (ii) for identifying the relevant sentences for resolving anaphoric mentions using attention mechanism. End-to-end memory networks proposed by Sukh"
W17-4102,N16-4006,1,0.865643,"Missing"
W17-4102,N12-1047,0,0.0409206,"ize of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification for Urdu, Korean and Japanese, we selected the merge operations as follows: For Urdu, number of merge operations were selected based on Hindi OS vocabulary since Hindi and Urdu are registers of the same language. For Korean and Japanese, the number of BPE merge operations was set to 3000, discovered by tuning on a separate validation set. We trained phrase-based SMT systems using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. Subword level representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000). This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We used unsupervised morphologicalsegmenters for generating morpheme representations (trained using Morfessor (Smit et al., 2014)). For Indian languages, we used th"
W17-4102,W16-4811,1,0.943419,"n with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliteration post-editing approaches mentioned earlier. They also showed orthographic syllables can outperform other units even when: (i) the lexical distance between related languages is reasonably large, (ii) the languages do not have a genetic relation, but only a contact relation. Recently, subword level models have also genThe paper is organized as follows. Section 2 discusses related w"
W17-4102,D15-1249,0,0.0725485,"Section 5 reports the results of our experiments and analyses the results. Based on experimental results, we analyse why BPE units out15 erated interest for neural machine translation (NMT) systems. The motivation is the need to limit the vocabulary of neural MT systems in encoder-decoder architectures (Sutskever et al., 2014). It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn subword units for NMT (Sennrich et al., 2016). Other subword units for NMT have also been proposed: character (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), wordpieces (Schuster and Nakajima, 2012; Wu et al., 2016). Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units. lable is a sequence of one or more consonants followed by a vowel, i.e. a C+ V unit, which approximates a linguistic syllable (e.g. spacious would be segmented as spa ciou s). Orthographic syllabification is rule based and applies to writing systems which represent vowels (alphabets and abugidas). Both OS and BPE units are variable"
W17-4102,D16-1196,1,0.71549,"n with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliteration post-editing approaches mentioned earlier. They also showed orthographic syllables can outperform other units even when: (i) the lexical distance between related languages is reasonably large, (ii) the languages do not have a genetic relation, but only a contact relation. Recently, subword level models have also genThe paper is organized as follows. Section 2 discusses related w"
W17-4102,P16-1160,0,0.0314708,"dels. Section 4 describes our experimental set-up. Section 5 reports the results of our experiments and analyses the results. Based on experimental results, we analyse why BPE units out15 erated interest for neural machine translation (NMT) systems. The motivation is the need to limit the vocabulary of neural MT systems in encoder-decoder architectures (Sutskever et al., 2014). It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn subword units for NMT (Sennrich et al., 2016). Other subword units for NMT have also been proposed: character (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), wordpieces (Schuster and Nakajima, 2012; Wu et al., 2016). Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units. lable is a sequence of one or more consonants followed by a vowel, i.e. a C+ V unit, which approximates a linguistic syllable (e.g. spacious would be segmented as spa ciou s). Orthographic syllabification is rule based and applies to writing systems which represent vowels (alp"
W17-4102,N15-3017,1,0.828649,"m,tel-mal, hin-mal,mal-hin urd-hin,ben-urd urd-mal,mal-urd bul-mac dan-swe may-ind kor-jpn,jpn-kor train tune test 44,777 1000 2000 38,162 843 1707 150k 150k 137k 1000 1000 1000 2000 2000 2000 69,809 1000 2000 Library1 (Kunchukuttan et al., 2014). We used orthographic syllabification rules from the Indic NLP Library for Indian languages, and custom rules for Latin and Slavic scripts. For training BPE models, we used the subword-nmt2 library. We used Juman3 and Mecab4 for Japanese and Korean tokenization respectively. For mapping characters across Indic scripts, we used the method described by Kunchukuttan et al. (2015) and implemented in the Indic NLP Library. (a) Parallel Corpora Size (no. of sentences) Language hin (Bojar et al., 2014) tam (Ramasamy et al., 2012) mal (Quasthoff et al., 2006) mac (Tiedemann, 2009b) Size Language 4.4 10M urd (Jawaid et al., 2014) 5M 1M mar (news websites) 1.8M 200K swe (OpenSubtitles2016) 2.4M 680K ind (Tiedemann, 2009b) 640K (b) Details of additional monolingual corpora for training word-level language models (source and size in number of sentences) Table 2: Training Corpus Statistics 5 for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus col"
W17-4102,N10-1077,0,0.0157459,"re dialects/registers of the same language and exhibit high lexical similarity. At the other end, pairs like Hindi-Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012). The chosen languages cover 5 types of writing systems. Of these, alphabetic and abugida writing systems represent vowels, logographic writing systems do not have vowels. The use of vowels is optional in abjad writing systems and depends on various factors and conventions. For instance, Urdu word segmentation can be very inconsistent (Durrani and Hussain, 2010) and generally short vowels are not denoted. The Korean Hangul writing system is syllabic, so the vowels are implicitly represented in the characters. (a) List of languages used in experiments along with ISO 6393 codes. These codes are used in the paper. Language Family Dravidian Type of writing system mal,tam,tel hin,urd,ben Indo-Aryan kok,mar,pan Slavic bul,mac Germanic dan,swe Polynesian may,ind Altaic jpn,kor dan1 ,swe1 ,may1 ind1 ,buc2 ,mac2 mal,tam,tel,hin Abugida ben,kok,mar,pan Syllabic kor Logographic jpn Abjad urd Alphabet (b) Classification of the languages and writing systems. (i)"
W17-4102,P10-1048,0,0.16581,"ranslation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context"
W17-4102,W95-0115,0,0.011889,"cation of Japanese and Korean into the Altaic family is debated, but various lexical and grammatical similarities are indisputable, either due to genetic or cognate relationship (Robbeets, 2005; Vovin, 2010). However, the source of lexical similarity is immaterial to the current work. For want of a better classification, we use the name Altaic to indicate relatedness between Japanese and Korean. The chosen language pairs also exhibit varying levels of lexical similarity. Table 3 shows an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995). The LCSR has been computed over the parallel training sentences at character level (shown only for language pairs where the writing systems are the same or can be easily mapped in order to do the LCSR computation). At one end of the spectrum, MalayalamIndia, Urdu-Hindi, Macedonian-Bulgarian are dialects/registers of the same language and exhibit high lexical similarity. At the other end, pairs like Hindi-Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012). The chosen languages cover 5 types of wri"
W17-4102,P12-2059,0,0.275814,"e, this is the largest experiment for translation over related languages and the broad coverage strongly supports our results. • We also show BPE units outperform other translation units in a cross-domain translation task. 2 Related Work There are two broad set of approaches that have been explored in the literature for translation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks"
W17-4102,P02-1040,0,0.0981059,"ual corpora for training word-level language models (source and size in number of sentences) Table 2: Training Corpus Statistics 5 for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection (Tiedemann, 2009b). Language models for wordlevel systems were trained on the target side of training corpora plus additional monolingual corpora from various sources (See Table 2b for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs. 4.3 Evaluation The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores as an alternative evaluation metric. LeBLEU is a variant of BLEU that does an edit-distance based, soft-matching of words and has been shown to be better for morphologically rich languages. We used bootstrap resampling for testing statistical significance (Koehn, 2004). Size Results and Analysis This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size a"
W17-4102,quasthoff-etal-2006-corpus,0,0.110024,"Missing"
W17-4102,W12-5611,0,0.116087,"Missing"
W17-4102,W04-3250,0,0.0474643,"additional monolingual corpora from various sources (See Table 2b for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs. 4.3 Evaluation The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores as an alternative evaluation metric. LeBLEU is a variant of BLEU that does an edit-distance based, soft-matching of words and has been shown to be better for morphologically rich languages. We used bootstrap resampling for testing statistical significance (Koehn, 2004). Size Results and Analysis This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size are studied. We also report initial results with a joint bilingual BPE model. 5.1 System details Comparison of BPE with other units Table 3 shows translation accuracies of all the language pairs under experimentation for different translation units, in terms of BLEU as well as LeBLEU scores. The number of BPE merge operations was"
W17-4102,P07-2045,0,0.00790685,"erations was chosen such that the resultant vocabulary size would be equivalent to the vocabulary size of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification for Urdu, Korean and Japanese, we selected the merge operations as follows: For Urdu, number of merge operations were selected based on Hindi OS vocabulary since Hindi and Urdu are registers of the same language. For Korean and Japanese, the number of BPE merge operations was set to 3000, discovered by tuning on a separate validation set. We trained phrase-based SMT systems using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. Subword level representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000). This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We used unsupervised morphologicalsegmenters for generating"
W17-4102,P16-1162,0,0.66759,"stems given the lack of parallel corpora. Modelling lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies related languages share many words with similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Subword level transformations are an effective way for translation of such shared words. In this work, we propose use of Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016), a encoding method inspired from text compression literature, to learn basic translation units for translation between related languages. In previous work, the basic units of translation are either linguistically motivated (word, morpheme, syllable, etc.) or ad-hoc choices (character n-gram). In contrast, BPE is motivated by statistical properties of text. We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best"
W17-4102,E14-2006,0,0.16106,"Missing"
W17-4102,2009.eamt-1.3,0,0.241829,"can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level mo"
W17-4102,E12-1015,0,0.599155,"the largest experiment for translation over related languages and the broad coverage strongly supports our results. • We also show BPE units outperform other translation units in a cross-domain translation task. 2 Related Work There are two broad set of approaches that have been explored in the literature for translation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks"
W17-4102,R13-1088,0,0.0162884,"y integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliterati"
W17-4102,W07-0705,0,0.481373,"tem. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and"
W17-4102,W15-3052,0,0.222921,"Missing"
W17-4102,D12-1027,0,0.0501317,"Missing"
W17-5229,W15-4319,0,0.158286,"Missing"
W17-5229,C16-1047,1,0.842663,"ion. Upon completion of training, the output of the top most hidden layer is used as sentence embedding. The trained sentence embeddings represent the relevant semantic and syntactic features of the tweets. Next, optimized feature set, as obtained by PSO, is concatenated with sentence embeddings for training a SVR model. The idea of cascading SVR with LSTM was motivated by the • Lexicon based Features: For each tweet we extract the following lexicon based features: – Polar word count: Count of positive and negative words using the MPQA subjectivity lexicon (Wiebe and Mihal214 recent works of (Akhtar et al., 2016; Wang et al., 2016). utilizing GloVe common crawl embeddings. The resultant network produces average Pearson score of merely 0.1877. We observe that a good percentage of tokens (mostly noisy) were missing in the embeddings - thus poses challenge to the network during the learning phase. Subsequently, we try to minimize the effect of noisy tokens by utilizing GloVe Twitter embeddings. Though, the network obtains improved average Pearson score at 0.1921, improvement is not significant. On analysis we find similar issues with Twitter embeddings. To address the problem of data sparsity we employ"
W17-5229,E17-1109,1,0.80842,"ormance. However, finding the relevant set of features is cumbersome and time-consuming task. Motivated by this we employ a Particle Swarm Optimization (PSO) based feature selection technique for selecting a subset of features from a feature pool. By utilizing the reduced and pruned feature set for training and evaluation, resultant system often performs considerably well. At the same time complexity of the system also reduces as it requires fewer parameters to learn. Literature survey shows successful application of PSO for various tasks and/or domains (Lin et al., 2008; Akhtar et al., 2017; Yadav et al., 2017). 212 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 212–218 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 System Description • Frequent noisy term: We compile a dictionary of frequently used slang terms and abbreviations along with its normal form that are commonly in practice in the Twitter domain. Every token in a tweet is searched in this dictionary. If a match is found then it is replaced with the normal form. The list was compiled utilizing the datasets of WNUT2015 sha"
W17-5229,S13-2053,0,0.0333875,"tion intensity. All these features are fed to the PSO to generate the optimized feature set. – • VADER Sentiment: VADER (Gilbert, 2014) stands for Valence Aware Dictionary and Sentiment Reasoner. It is a rule-based sentiment analysis technique designed to work with contents on social media. For every input tweet, it provides positive, negative, neutral and compound sentiment score. We use these four values as features. 2.4 cea, 2006) and Bing Liu lexicon (Ding et al., 2008). Aggregate polarity scores: Positive and negative scores are obtained from each of the following lexicons: Sentiment140 (Mohammad et al., 2013), AFINN (Nielsen, 2011) and Sentiwordnet (Baccianella et al., 2010). It is calculated by aggregating the positive and negative word scores provided by each lexicon. Aggregate polarity scores (Hashtags): Aggregate of positive and negative scores of the hashtags in a tweet is calculated from NRC Hashtag Sentiment lexicon (Mohammad et al., 2013). Emotion word count: Count of the number of words matching each emotion from NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2013). Aggregate emotion score: Sum of emotion associations of the words present in NRC-10 Expanded lexicon (Bravo-Marq"
W17-5229,W17-5205,0,0.0590887,"Missing"
W17-5229,D14-1162,0,0.0855275,"o dense layers. Hidden layer of the LSTMs consists of 100 neurons whereas the dense layers contain 100 and 50 neurons, respectively. • Elongation: User tends to express their state of emotion by elongating a valid word e.g. ‘jooooy’, ‘goooodd’ etc. In this step, all such elongated words are identified and converted into valid words by removing the consecutive characters. For example ‘jooyyyy’ and ‘jooooy’ are converted to ‘joy’. 2.2.1 Word Embeddings Word embedding (or word vector) is a distributed representation of words that contains syntactic and semantic information (Mikolov et al., 2013; Pennington et al., 2014). For this task, we use GloVe (Pennington et al., 2014) pre-trained word embedding trained on common crawl corpus. Each token in the tweet is represented by 300 dimension word vector. The choice of common crawl word embeddings for Twitter datasets is because of the normalization steps (Section 2.1). We observe that the application of normalization has a positive effect on the overall performance of the system. • Verb present participle: In Twitter domain, it is observed that user tends to omit the character ‘i’ or ‘g’ in words ending with ‘ing’. For example, ‘going’ is written as ‘goin’ or ‘go"
W17-5229,D16-1059,0,0.029088,"of training, the output of the top most hidden layer is used as sentence embedding. The trained sentence embeddings represent the relevant semantic and syntactic features of the tweets. Next, optimized feature set, as obtained by PSO, is concatenated with sentence embeddings for training a SVR model. The idea of cascading SVR with LSTM was motivated by the • Lexicon based Features: For each tweet we extract the following lexicon based features: – Polar word count: Count of positive and negative words using the MPQA subjectivity lexicon (Wiebe and Mihal214 recent works of (Akhtar et al., 2016; Wang et al., 2016). utilizing GloVe common crawl embeddings. The resultant network produces average Pearson score of merely 0.1877. We observe that a good percentage of tokens (mostly noisy) were missing in the embeddings - thus poses challenge to the network during the learning phase. Subsequently, we try to minimize the effect of noisy tokens by utilizing GloVe Twitter embeddings. Though, the network obtains improved average Pearson score at 0.1921, improvement is not significant. On analysis we find similar issues with Twitter embeddings. To address the problem of data sparsity we employ a series of heuristi"
W17-5717,P02-1040,0,0.132838,"Missing"
W17-5717,P15-2116,0,0.0486369,"Missing"
W17-7517,W17-1322,0,0.027019,"1 indicates exactly opposite, 1 indicates exactly same, and 0 indicates the independence. It is to be assumed that higher the similarity score obtained more is the chance that the pair of text snippets become textually entailed, so it could be a good predictor of TE. 2. Jaccard Similarity: Jaccard similarity (Jaccard, 1901) is a set based similarity metric. It is defined as follows: Jaccard(A, B) = |A ∩ B|/|A ∪ B| 1 http://www.cs.waikato.ac.nz/ml/weka/ (2) 134 where A and B represent two sets of documents. A similar pair is expected to share more words and hence the entailment relation holds (Almarwani and Diab, 2017). Following this intuition we make use of set based similarity metric in our work. This is very well established similarity metric and measure the similarity between the two finite sets. 3. Dice Similarity: Dice Similarity (Dice., 1945) is also a vector based similarity metric. It’s value lies within the range of 0 to 1. It can be calculated using the following formula. Dice(A, B) = 2|A ∩ B|/(|A |+ |B|) (3) Here, A and B represent the first and second set of documents, respectively. The mathematical derivation of this measure is same as the derivation of F-measure, where precision and recall b"
W17-7517,N10-3003,0,0.0774551,"Missing"
W17-7517,H05-1090,0,0.0865571,"Missing"
W17-7518,P15-2124,1,0.83101,"admissions process. A well written SOP is a must for an applicant to ensure their admission in any university, and more so for elite universities. Their thoughts and ideas should be organized in their statement. University guidelines1,2 , Alumni blogs3 , and Admission consultancy blogs4 recommend spending ample time on each SOP and tailoring it to perfection. They also recommend stylometry for writing an essay i.e. word limit, active voice, coherence, and continuity. Various NLP applications like Essay grading (Larkey, 1998), Text Summarization (Gupta and Lehal, 2010) and Sentiment Analysis (Joshi et al., 2015) utilize these features. Hence, we believe that an application that evaluates their statement is crucial. The key question that this paper attempts to answer is: ‘ Can information gained from an SOP be used to predict the outcome of a candidate application for graduate school admissions? ’ 3 Related Work Ward (2006) discuss a qualitative model for Graduate Admissions to Computer Science programs but do not use any Machine Learning or Deep Learning based techniques for estimating a likelihood. According to them, other factors which affect the decision of the committee reviewing the applications"
W17-7518,W02-0109,0,0.241608,"Missing"
W17-7518,D14-1162,0,0.0807167,"olysemy - Average number of WordNet (Fellbaum, 2010) senses per word. 4.5.3 Document Similarity Score and Error based Features 1. Cosine Similarity - Cosine Similarity Score of an SOP with the corpus of accepted essays dataset, where we ensure that the SOP being compared is not a part of the accepted essay corpus. 8 http://www.cfilt.iitb.ac.in/ cognitive-nlp/ 2. Similarity-based features using GloVe The similarity between every pair of content words in adjacent sentences. The similarity is computed as the cosine similarity between their word vectors from the pre-trained GloVe word embeddings (Pennington et al., 2014). We calculate the mean and maximum similarity values. 3. Spell Check Errors - We use PyEnchant9 to embed a spell checker and count the number of errors in each document. The count is then used as another feature for training classifier. 4. Out of Vocabulary Words - We use the pretrained Google news word embeddings and find out word vectors for every token in the document. The tokens which do not return any vector are either rare words or in all probability out of vocabulary words. We use the count of such tokens as another feature set. 5 Results We perform the experiments detailed in section"
W17-7531,W17-7531,1,0.103677,"ct Vocabulary building is fundamental to any language learning and effective communication relies on the mastery of vocabulary. Hindi is one of the widely spoken languages in the world. However, there is a scarcity of quality e-learning resources for Hindi. Also, there is lack of e-learning content which is in-sync with Hindi curriculum. This was the motivation in building educational application, Hindi Shabdamitra, for language teaching and learning. Hindi Shabhadamitra is an e-learning tool developed using Hindi wordnet for Hindi language learning. This is an insight of the work reported by Redkar et al. (2017) in which Hindi Shabdamitra enhances the teaching-learning process has been presented. The paper presents the teacher and user benefits of this e-learning tool. Further, the user evaluation information has been reported. 1 Introduction Hindi language is a member of the Indo-Aryan group of the Indo-European language family1 . Hindi is the official language of India and is 4th among the most spoken languages in the world2 . Devanagari script is recommended as the official script for Hindi3 . Hindi in Devanagari script is a part of most of the school curriculum and covers a large spectrum of lear"
W18-0522,S16-1160,0,0.196726,". The training datasets were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word"
W18-0522,S16-1154,0,0.130387,"sh speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative fe"
W18-0522,S16-1150,0,0.146995,"instances in which the context was as long as 3 - 4 sentences. The training datasets were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the"
W18-0522,S16-1153,0,0.181128,"ne amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative features. One of them was the M"
W18-0522,S16-1156,0,0.170441,"single word. However, there were a few target words that were over a word long. Similarly, in most cases, the context was only one sentence, except for a few instances in which the context was as long as 3 - 4 sentences. The training datasets were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as l"
W18-0522,S16-1147,0,0.17163,"Missing"
W18-0522,S16-1151,0,0.163402,"larly, in most cases, the context was only one sentence, except for a few instances in which the context was as long as 3 - 4 sentences. The training datasets were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence"
W18-0522,S16-1152,0,0.180489,"ontext was only one sentence, except for a few instances in which the context was as long as 3 - 4 sentences. The training datasets were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Si"
W18-0522,S16-1148,0,0.208749,"ts were annotated by 10 native and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sente"
W18-0522,W17-7518,1,0.89696,"get word as either complex or simple, we assign the majority label to that word. In case of a 4-4 tie, (where 4 classifiers say the target word is complex and 4 say that it is simple), we use a word-embedding based classifier to act as a tie-breaker. 6 Discussion As it is evident from Tables 2 and 4, we see that individual classifiers do not work as well as ensembling them together, which agrees with the expression “The whole is greater than the sum of its parts”. Classifier Ensembling would further prove to be an efficacy for contextual documents similarity-based binary classification tasks (Kanojia et al., 2017) which rely heavily on lexical features, as well as it should also potentially crosspollinate to benefit probabilistic touch classification problems (Wani et al., 2017) where spatial and contextual information has been proven to be pivotal. Figure 3: Feature significance observed by ranking them from highest to lowest using Attribute Evaluation based on Information Gain. The length of the bar corresponds to the actual Information Gain value. 7 Conclusion and Future Work In this paper, we describe our participation to NLP-BEA’ S CWI 2018 Shared Task at NAACL concerning Complex Word Identificati"
W18-0522,S16-1164,0,0.104044,"xt column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative features. One of them was the MRC Psycholinguistic database (Wilson, 1988) used by Davoodi and Kosseim (2016). Another system by Konkol (2016) used a single feature namely do"
W18-0522,S16-1149,0,0.125897,"(Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative features. One of them was the MRC Psycholinguistic database (Wilson, 1988) used by Davoodi and Kosseim (2016). Another system by Konkol (2016) used a single feature namely document frequency of the word in Wikipedia, for classifying using a maximum entropy classifier. 3 4 Methodology In this section, we describe the experiment setup, such as the features used and provide analysis for their selection. This is followed by a detailed system overview which explains the system’s architecture. Datasets Figure 2: CWI System Architecture For this shared task (Yimam et al., 2018), we used only the English monolingual dataset, which made use of data from a number of sources, such as News articles, WikiNews and"
W18-0522,S16-1162,0,0.129801,"ences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative features. One of them was the MRC Psycholinguistic database (Wilson, 1988) used by Davoodi and Kosseim (2016). Another system by Konkol (2016) used a single feature namely document frequency of the word in Wikipe"
W18-0522,S16-1158,0,0.103897,"otated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems used some innovative features. One of them was the MRC Psycholinguistic database (Wilson, 1988) used by Davoodi"
W18-0522,S16-1155,0,0.192236,"Missing"
W18-0522,D14-1162,0,0.089942,"to lowest using Attribute Evaluation based on Information Gain. The length of the bar corresponds to the actual Information Gain value. 7 Conclusion and Future Work In this paper, we describe our participation to NLP-BEA’ S CWI 2018 Shared Task at NAACL concerning Complex Word Identification. We presented and evaluated our system across three datasets and showed that Ensemble Classifiers with hard and GloVe Voting are effective by means of lexical, size and vocabulary features for identifying complex words. For the word-embedding based classifier, we use the GloVe pre-trained word embeddings (Pennington et al., 2014). We first split the target into its constituent words (in most cases, it is a single word, but in a few cases, it is a phrase). We find the most similar word to each of the constituent 203 As part of our future work, we plan to incorporate Parts of Speech (POS) tags, Named Entity Recognition (NER) tag and word position features to improve our existing effective system. pages 1038–1041, San Diego, California. Association for Computational Linguistics. Onur Kuru. 2016. Ai-ku at semeval-2016 task 11: Word embeddings and substring features for complex word identification. In Proceedings of the 10"
W18-0522,S16-1161,0,0.181691,"ve and 10 non-native English speakers. Even if one amongst them found the word to be difficult, it was annotated as complex. Table 1: Description of the Dataset. The first column gives the dataset. The next column gives the total number of sentences. The last column gives the number of unique sentences. task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; Zampieri et al., 2016), Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Zampieri et al., 2016; Brooke et al., 2016; Ronzano et al., 2016), Neural Networks (Bingel et al., 2016; Nat, 2016), Decision Trees (Quijada and Medero, 2016; Malmasi et al., 2016; Malmasi and Zampieri, 2016), Nearest Centroid classifier (Palakurthi and Mamidi, 2016), Naive Bayes (Mukherjee et al., 2016), threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Mart´ınez Mart´ınez and Tan, 2016). The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc. However, a few of the systems u"
W18-0522,S16-1157,0,0.157597,"Missing"
W18-0522,S16-1159,0,0.113699,"Missing"
W18-0522,S16-1146,0,0.155084,"Missing"
W18-1207,D14-1179,0,0.0371266,"Missing"
W18-1207,P16-1160,0,0.0219814,"ata may be absent in system vocabulary. NMT model cannot interpret the semantics of these OOV words. So, translation quality deteriorates as the number of unseen (rare) words increases (Sutskever et al., 2014). OOV words are mainly of three types described in Table 1. The first type of OOV words needs transliteration. But for translating the second type of OOV words, we need to look deeper. A word based NMT system treats ‘house’ and ‘houses’ as two com2 Related work A word can be segmented as BPE (Sennrich et al., 2016), orthographic syllable (Kunchukuttan and Bhattacharyya, 2016), character (Chung et al., 2016; Costa-jussà and Fonollosa, 2016), Huffman encoding (Chitnis and DeNero, 2015). In our experiment we show that, for translation between linguistically close language-pair BPE subword segmentation is suitable, whereas for transla55 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 55–60 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics tion between linguistically distant languagepair Morfessor-based segmentation is suitable. Our proposed subword segmentation approach utilizes benefit of both BPE and Morfessor (Creutz and Lagus, 200"
W18-1207,P16-2058,0,0.0197173,"system vocabulary. NMT model cannot interpret the semantics of these OOV words. So, translation quality deteriorates as the number of unseen (rare) words increases (Sutskever et al., 2014). OOV words are mainly of three types described in Table 1. The first type of OOV words needs transliteration. But for translating the second type of OOV words, we need to look deeper. A word based NMT system treats ‘house’ and ‘houses’ as two com2 Related work A word can be segmented as BPE (Sennrich et al., 2016), orthographic syllable (Kunchukuttan and Bhattacharyya, 2016), character (Chung et al., 2016; Costa-jussà and Fonollosa, 2016), Huffman encoding (Chitnis and DeNero, 2015). In our experiment we show that, for translation between linguistically close language-pair BPE subword segmentation is suitable, whereas for transla55 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 55–60 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics tion between linguistically distant languagepair Morfessor-based segmentation is suitable. Our proposed subword segmentation approach utilizes benefit of both BPE and Morfessor (Creutz and Lagus, 2006; Smit et al., 2014; Grönroos et"
W18-1207,W17-4102,1,0.831731,"d code for the occurrences of the pair.” (Shibata et al., 1999) 3.1 The goal of morphological analyzers such as Morfessor is to segment a word into its morphs, the surface forms of morphemes. Comparison between BPE subword segmentation and Morfessor is described below. • BPE is a greedy approach. Morfessor takes highest probable segmentation of words and deals with local optima by removing and adding word tokens. So, Morfessor produces more acceptable morphological segmentation than BPE. BPE as subword unit BPE works as subword segmentation method for both NMT (Sennrich et al., 2016) and SMT (Kunchukuttan and Bhattacharyya, 2017). In this method, two vocabularies are used: training vocabulary and symbol vocabulary. Words in training vocabulary are charactersequences followed by an end-of-word symbol. At first, all characters are added to symbol vocabulary. This step is followed by adding the most frequent symbol bigram to the vocabulary, and all its occurrences are replaced by a new symbol (merged symbol bigram). This step is repeated for a number of times, which is a hyperparameter. Starting from character level as the number of merge operations is increased, primarily frequent character-sequences and then full words"
W18-1207,D15-1249,0,0.0558704,"emantics of these OOV words. So, translation quality deteriorates as the number of unseen (rare) words increases (Sutskever et al., 2014). OOV words are mainly of three types described in Table 1. The first type of OOV words needs transliteration. But for translating the second type of OOV words, we need to look deeper. A word based NMT system treats ‘house’ and ‘houses’ as two com2 Related work A word can be segmented as BPE (Sennrich et al., 2016), orthographic syllable (Kunchukuttan and Bhattacharyya, 2016), character (Chung et al., 2016; Costa-jussà and Fonollosa, 2016), Huffman encoding (Chitnis and DeNero, 2015). In our experiment we show that, for translation between linguistically close language-pair BPE subword segmentation is suitable, whereas for transla55 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 55–60 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics tion between linguistically distant languagepair Morfessor-based segmentation is suitable. Our proposed subword segmentation approach utilizes benefit of both BPE and Morfessor (Creutz and Lagus, 2006; Smit et al., 2014; Grönroos et al., 2014) and performs well for both linguis"
W18-1207,kunchukuttan-etal-2014-shata,1,0.858062,"entation of English and Indian words. 5.1 Datasets Step 1: Use Morfessor on the set of all words from the dataset. We have used data from English-Hindi (EnHi), English-Bengali (En-Bn) and BengaliHindi (Bn-Hi) language-pairs from health and tourism domain multilingual parallel Indian Language Corpora Intitiative (ILCI) corpus (Jha, 2010). We clean and tokenize the training corpus. English data was tokenized using the Stanford tokenizer (Klein and Manning, 2003) and then true-cased using truecase.perl provided in MOSES toolkit3 . For Hindi and Bengali data, we tokenized using NLP Indic Library (Kunchukuttan et al., 2014). Then parallel sentences were divided into three parts for training, testing and tuning/validation. For each language-pair, we have 44,777 sentence-pairs in training data, 1,000 sentence-pairs in tuning data and 2,000 sentence-pairs in test data. Step 2: Find and replace all occurrences of these words with their segmented form (symbol ‘**’ is used to keep information of segmenting positions). For example‘googling’ will be segmented into two morphs ‘googl**’ and ‘ing’. Step 3: Learn and apply BPE on that morphsegmented data. Use symbol ‘@@’ for these segmentations. For example, this may segmen"
W18-1207,P02-1040,0,0.102361,"here we have kept the number of merge operations of M-BPE higher than that of BPE. 5 6 Results and discussion The example given below shows the difference among three segmentations: Example: Word level: focusing your mind BPE level: foc@@ us@@ sing your mind Morfessor level: focus@@ ing your mind M-BPE level: foc@@ us@@ ing your mind Experimental setup 1 https://github.com/rsennrich/subword-nmt anoopkunchukuttan.github.io/indic_nlp_library/ 3 http://www.statmt.org/moses/ There are three systems of subword segmentation in our experiment, namely- BPE, Mor2 57 Fig 1 shows changes in BLEU scores (Papineni et al., 2002) when we train NMT models using sentences with increasing average number of elements (by tuning hyperparameters). Here, two paths indicate two different approaches of segmentation: i) from word level to BPE level, ii) from word level to M-BPE level via Morfessor level. and 15k respectively. Translation between lexically close language-pairs like Bn-Hi has more character-to-character mappings than En-Hi. For that reason, Bn-Hi language-pair needs a lower value of hyperparameter than English-Hindi. Pair Bn-Hi En-Hi En-Bn W 30.71 26.22 14.44 M 32.74 27.87 15.18 BPE 33.09 27.16 14.89 M-BPE 34.21 2"
W18-1207,E17-3017,0,0.0338875,"ented subwords followed by symbol‘**’, because it’ll treat already segmented subwords as different elements. 5.2 System details Step 4: Replace symbol ‘**’ with the symbol ‘@@’. Finally, the word ‘googling’ will become ‘go@@ og@@ l@@ ing@@’. 4.1 After tokenization, words of source sentences are broken into subwords using a segmentation algorithm. NMT system receives a sequence subwords of a sentence as input and produces the output of a subword-sequence in target language. Then, subwords are combined to produce words in order to get an actual sentence in target language. We have used NEMATUS (Sennrich et al., 2017) as an attention-based encoder-decoder NMT system in our experiment. Hyperparameter selection of M-BPE With increasing average number of elements per sentence, performance of an NMT model degrades (Bahdanau et al., 2015). Using the same number of merge operations for both BPE and M-BPE produces a higher number of elements per sentence in case of M-BPE than BPE because the Morfessor part of M-BPE increases the number of elements of a sentence before applying the BPE part on it. In order to get a fair comparison between BPE and M-BPE, we have adjusted their hyperparameter in such a way that aver"
W18-1207,P16-1162,0,0.540959,"d be constant. But in reality, the vocabulary of a natural language is open. Some words in test data may be absent in system vocabulary. NMT model cannot interpret the semantics of these OOV words. So, translation quality deteriorates as the number of unseen (rare) words increases (Sutskever et al., 2014). OOV words are mainly of three types described in Table 1. The first type of OOV words needs transliteration. But for translating the second type of OOV words, we need to look deeper. A word based NMT system treats ‘house’ and ‘houses’ as two com2 Related work A word can be segmented as BPE (Sennrich et al., 2016), orthographic syllable (Kunchukuttan and Bhattacharyya, 2016), character (Chung et al., 2016; Costa-jussà and Fonollosa, 2016), Huffman encoding (Chitnis and DeNero, 2015). In our experiment we show that, for translation between linguistically close language-pair BPE subword segmentation is suitable, whereas for transla55 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 55–60 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics tion between linguistically distant languagepair Morfessor-based segmentation is suitable. Our proposed su"
W18-1207,E14-2006,0,0.0264074,"osta-jussà and Fonollosa, 2016), Huffman encoding (Chitnis and DeNero, 2015). In our experiment we show that, for translation between linguistically close language-pair BPE subword segmentation is suitable, whereas for transla55 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 55–60 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics tion between linguistically distant languagepair Morfessor-based segmentation is suitable. Our proposed subword segmentation approach utilizes benefit of both BPE and Morfessor (Creutz and Lagus, 2006; Smit et al., 2014; Grönroos et al., 2014) and performs well for both linguistically close and distant languagepairs. learn well from them (Bahdanau et al., 2015). So, proper tuning of this hyperparameter is needed. Higher number of merge operations makes the elements more word-like. Lower number of merge operations makes the elements more character-like, where sometimes character-to-character mappings add transliterated words in the translation output. 3 3.3 Comparison of BPE segmentation with Morfessor BPE algorithm BPE (Gage, 1994) is originally a data compression technique. The main idea behind BPE is- “Fin"
W18-3705,D16-1115,0,0.149232,"Missing"
W18-3705,N18-1024,0,0.0135543,"h. A key question to ask here is: Can a cunning student easily con the entire system into giving a good grade by submitting rubbish? The answer is probably no. At least not easily. While it is possible for the writer to write an essay using only good words, this may not necessarily translate to a higher score than what he would have scored had he written the essay sincerely. There are many ways to generate adversarial essays. Taghipour (2017) suggests using contextfree grammars, and language modeling to create spurious essays, before trying to detect whether an input essay is spurious or not. Farag et al. (2018) construct adversarial essays by permuting the sentences of good scoring essays. We created our own version of adversarial essays, by constructing essays that were long, but contained only “good” words (i.e. words with a high goodness score). In order to see if such a thing would be possible we generated a set of 100 essays (50 from each prompt). These essays were generated from a vocabulary of good words, having above average length sentences and a reasonably large word count. We then graded these essays, using the original ASAP data for training. Table 4 shows how much is the average score,"
W18-3705,D16-1193,0,0.457991,"entral contribution of our paper is the definition of goodness and its use in predicting the style, word choice and sentence fluency scores of student essays. We define the goodness of a word (or phrase) as the weighted average of the count of the word (or phrase), weighted by score of the essay (either style, or word choice, or sentence fluency). In this way, words or phrases that occur more often in essays with a better score, get scored higher. Using this property, we show a significant improvement over our baseline measures, as well as a state-of-the-art deep learning system, developed by Taghipour and Ng (2016). The rest of the paper is organized as follows. Section 2 defines the problem statement of our paper - in particular the terms style, word choice and sentence fluency. Section 3 describes our approach to predict the goodness scores of essays. Section 4 describes other features that we use, as well as a state-of-the-art system. Section 5 describes the dataset used. Section 6 describes the experiments that we performed. Section 7 gives our results and provides an analysis on the goodness of words and other features, and how they impact the sentence fluency score of essays. We also use ablation"
W18-3705,goldhahn-etal-2012-building,0,0.0174874,"parse tree features, like the average parse tree depth and the number of subordinate clauses (SBAR) in the text. 4.4 4.6 Language modeling features Deep learning networks, like LSTMs are quite good in predicting the score of the essays. We perform the experiments done by Taghipour and Ng (2016)1 . We ran multiple configurations of their system. We used the default hyperparameters as described in Section 5.1 of Taghipour and Ng (2016). For pre-trained word embeddings, we ran experiments using These are language modeling features of the essay using the English Wikipedia from the Leipzig corpus (Goldhahn et al., 2012). These features are the output from the SRILM toolkit (Stolcke et al., 2002). We use the following features: 1. Number of sentences per essay. 2. Number of words per sentence. 1. No pre-trained word embeddings 3. Number of OOVs in the sentence. 2. The same word embeddings that Taghipour and Ng (2016) used; and 4. Language model score. 5. Perplexity of the text. 3. GloVe word embeddings (Pennington et al., 2014) 6. Average perplexity per words of the text. 4.5 LSTMs - The State-of-the-Art The word-embeddings dimension for the lookup table layer was 50 for the first 2 experiments, and 300 for t"
W18-3705,D14-1162,0,0.080708,"y” (“Sally Yates said that she was concerned about Michael Flynn’s ties with Russia.”). Sentence fluency is the quality of an essay that measures the writer’s command of the language that they are writing in. A writer who is proficient in writing, will be able to form good quality phrases, construct sentences quite easily, and show a flow between the sentences that they write. We model each of these as an ordinal classification problem, where each score point corresponds to a class. 3 1. In case it is an unknown word, we find the most similar word to the unknown word using GloVe word vectors (Pennington et al., 2014) that is also present in the training data. 2. In case it is a spelling mistake. In case an unknown word does not exist in our set of word embeddings, we tag such a word as a spelling mistake, and assign a goodness score of 0. 3. In case it is an unknown phrase. In case there is a phrase that is not present in the training data, then it is marked as an unknown phrase. The score given to it is the mean score of its corresponding words. We calculate the overall goodness score of the essay as the mean of the goodness scores of all the relevant words and phrases in the essay. Goodness 4 We hypothe"
W18-3705,D10-1023,0,0.509223,"Missing"
W18-3705,P10-1056,0,0.026382,"d; and 4. Language model score. 5. Perplexity of the text. 3. GloVe word embeddings (Pennington et al., 2014) 6. Average perplexity per words of the text. 4.5 LSTMs - The State-of-the-Art The word-embeddings dimension for the lookup table layer was 50 for the first 2 experiments, and 300 for the experiment using GloVe. Coherence-based Features We define sentence flow as the content word similarity between two adjacent sentences. For every pair of adjacent sentences, we find out M axSim and M eanSim, which are the maximum and mean similarity values between the content words of the 2 sentences (Pitler et al., 2010). We use the GloVe pre-trained word embeddings (Pennington et al., 2014) for the vectors of the content words. In addition to the above, we also construct PoStag and lemma vectors of each of the sentences, and calculate the average similarity between adjacent sentences (Pitler et al., 2010). We also look at entity grid features (Barzilay and Lapata, 2005). An entity grid is a 1-0 grid of sentences × entities. A cell (E[i][j]) in the grid is a 1 if the entity i is present in the sentence j, and 0 otherwise. We count the number of sequences of length between 2 to 4, that have at least one 1 and"
W18-3705,D15-1300,1,0.897307,"Missing"
W18-3705,C14-1090,0,0.153658,"dian Language Technology Department of Computer Science and Engineering Indian Institute of Technology, Bombay {sam,pb}@cse.iitb.ac.in Abstract mark of a writer being able to express his / her thoughts in the language of their writing. Style is necessary for providing a rich and diverse structure to the writing of the essay. Proficient and crisp vocabulary, as well as good sentence fluency is a mark of a writer being able to articulate his / her thoughts well in the language of their writing. There has been a fair bit of recent work in predicting other aspects of the essay, such as coherence (Somasundaran et al., 2014), organization (Persing et al., 2010), etc. However, not much work has been done for grading either style, sentence fluency, or word choice in student essays. The central contribution of our paper is the definition of goodness and its use in predicting the style, word choice and sentence fluency scores of student essays. We define the goodness of a word (or phrase) as the weighted average of the count of the word (or phrase), weighted by score of the essay (either style, or word choice, or sentence fluency). In this way, words or phrases that occur more often in essays with a better score, get"
W19-0413,L16-1429,1,0.942898,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,C16-1047,1,0.934723,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,S16-1049,0,0.04135,"Missing"
W19-0413,baccianella-etal-2010-sentiwordnet,0,0.0144947,"* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et"
W19-0413,C12-2008,1,0.824775,"n detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA"
W19-0413,W13-5006,1,0.768299,"positive sentiment of the sentence. “It was used to be a horrible place to eat but not any more.” In contrast to A4, architecture A3 does not rely on the sequence information of the extracted features and allows the network to learn on its own. We use 300 dimension Word2Vec (Mikolov et al., 2013) word embeddings for the experiments. Each Bi-LSTM layer contains 100 neurons while two dense layers contain 100 and 50 neurons, respectively. Features As additional features, we extract the following information for each token in an instance. – Aspect term extraction: Distributional thesaurus (DT)1 (Biemann and Riedl, 2013) defines the lexicon expansion of a token based on a similar context. It is usually very effective for the handling of unseen text. If a token in the test set never appears in the training set, it becomes a non-trivial task for the classifier to make a correct prediction. By employing DT feature, the classifier can additionally utilize lexical expansion of the current token for mapping with the training set, thus minimize the chance of unseen text. For each token, we use its top 3 DT expansions as features. Language English Spanish French Dutch German Hindi #sent. 2,000 2,070 1,733 1,711 19,43"
W19-0413,S16-1044,0,0.270428,"m identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network (Graves et al"
W19-0413,S16-1054,0,0.0389318,"Missing"
W19-0413,P14-2063,0,0.0690449,"Missing"
W19-0413,D18-1382,1,0.797108,"Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achi"
W19-0413,S15-2080,0,0.0298934,"ral language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & D"
W19-0413,W06-0301,0,0.0146414,"evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Condition"
W19-0413,S14-2076,0,0.167189,"2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags"
W19-0413,S16-1174,1,0.942977,"n a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) ne"
W19-0413,S13-2053,0,0.0243541,"ing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & Dutch. System XRCE (Brun"
W19-0413,P05-1015,0,0.48177,"Missing"
W19-0413,W02-1011,0,0.029084,"or aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez"
W19-0413,S16-1002,0,0.0636458,"Missing"
W19-0413,S14-2004,0,0.525133,"timent towards food and service are positive and negative, respectively. Such analysis offers finegrained information to a user or an organization who seeks users opinion towards any specific entity. For example, based on the users’ feedback, an individual can draw a general perception about the specific attribute or aspect of a product or service, and he/she can make an informed decision about the product or service under observation. Similarly, an organization can utilize the feedback to refine its product/service or to take a decision in the business model. Aspect-based sentiment analysis (Pontiki et al., 2014, 2016) has two subproblems at its core, i.e., aspect term identification (or opinion target extraction) and aspect sentiment classification. Given a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and re"
W19-0413,S16-1045,0,0.0591637,"Missing"
W19-0413,P02-1053,0,0.0202289,"es (one each for aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA,"
W19-0413,waltinger-2010-germanpolarityclues,0,0.0321484,"9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et al., 2016) (Task 5), which contain user reviews across multiple languages. The datasets of English, Spanish, French and Dutch are related to the reviews of consumer electronics a"
W19-0413,H05-1044,0,0.101206,"65.6 65.7 64.2 23.1 22.0 22.4 24.0* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspe"
W19-0413,P18-1208,0,0.0149591,"n 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for"
W19-1309,K16-1017,0,0.0606252,"e impact of different features for sarcasm/irony classification. Bouazizi and Ohtsuki (2016) propose a patternbased approach to detect sarcasm on Twitter. As deep learning techniques gain popularity, Ghosh and Veale (2016) propose a neural network semantic model for sarcasm detection. They use Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh and Veale (2017) propose a neural architecture that considers the speaker’s mood on the basis of most recent prior tweets for sarcasm detection. Far´ıas et al. (2016) propose a 1."
W19-1309,P11-2102,0,0.596948,"stic text poses to sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels: sarcastic due to number, non-sarcastic. † 72 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 72–80 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics 3 putational sarcasm. While several approaches to detect sarcasm have been reported (Gonz´alezIb´an˜ ez et al., 2011; Joshi et al., 2015), they may fall short in case of sarcasm expressed via numbers. Consider the following three sentences: Related Work Sarcasm and irony detection has been extensively studied in linguistics, psychology, and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years (Joshi et al., 2017). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic"
W19-1309,baccianella-etal-2010-sentiwordnet,0,0.091032,"Missing"
W19-1309,C18-1156,0,0.510074,"s in Section 4. Then, in Section 5, we present two deep learning-based approaches. In Section 6, we outline the experimental setup and present the results of our experiments in Section 7. We present both qualitative as well as quantitative error analysis in Section 8. Finally, we conclude the paper and discuss future work in Section 9. Introduction Sarcasm is a challenge to sentiment analysis because it uses verbal irony to express contempt or ridicule, thereby, potentially confusing typical sentiment classifiers. Several approaches for sarcasm detection have been reported in the recent past (Hazarika et al., 2018; Joshi et al., 2017; Ghosh and Veale, 2017; Buschmeier et al., 2014; Riloff et al., 2013). In this paper, we focus on a peculiar form of sarcasm: sarcasm expressed through numbers. In other words, the goal of this paper is the classification task where a tweet containing one or more numbers is classified as sar2 Motivation The challenge that sarcastic text poses to sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels:"
W19-1309,D13-1066,0,0.21924,"Missing"
W19-1309,P15-2124,1,0.959436,"o sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels: sarcastic due to number, non-sarcastic. † 72 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 72–80 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics 3 putational sarcasm. While several approaches to detect sarcasm have been reported (Gonz´alezIb´an˜ ez et al., 2011; Joshi et al., 2015), they may fall short in case of sarcasm expressed via numbers. Consider the following three sentences: Related Work Sarcasm and irony detection has been extensively studied in linguistics, psychology, and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years (Joshi et al., 2017). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic features like an int"
W19-1309,S18-1005,0,0.0372944,"Missing"
W19-1309,W13-1605,0,0.292471,"Missing"
W19-1309,N16-1174,0,0.109314,"Missing"
W19-1309,P17-1155,0,0.0146795,"tector. Figure 1 shows the interfacing of our module with the overall sarcasm detection system. Figure 1: Interfacing of our module with the overall sarcasm detection system 3 Ivanko and Pexman (2003) describe the relationship between incongruity and sarcasm. 73 novel model using affective features based on a wide range of lexical resources available for English for detecting irony in tweets. Sulis et al. (2016) study the difference between sarcasm and irony in tweets. They propose a novel set of sentiment, structural and psycholinguistic features for distinguishing between irony and sarcasm. Peled and Reichart (2017) and Dubey et al. (2019) model sarcasm interpretation as a monolingual machine translation task. They use Moses4 , attention networks, and pointer generator networks for the task of sarcasm interpretation. Van Hee et al. (2018) present the first shared task in irony detection in tweets. Recently, Hazarika et al. (2018) propose a hybrid approach for sarcasm detection in online social media discussions. They extract contextual information from the discourse of a discussion thread. They also use user embeddings that encode stylometric and personality features of users and content-based feature ex"
W19-1309,C16-1231,0,0.138871,"Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh and Veale (2017) propose a neural architecture that considers the speaker’s mood on the basis of most recent prior tweets for sarcasm detection. Far´ıas et al. (2016) propose a 1. This phone has an awesome battery backup of 38 hours 2. This phone has a terrible battery backup of 2 hours 3. This phone has an awesome battery backup of 2 hours At the time of writing this paper, a battery backup of 38 hours is good for phones while a battery backup of 2 hours is bad. Therefore, sentences"
W19-1309,D14-1162,0,0.0838694,"e user embeddings that encode stylometric and personality features of users and content-based feature extractors such as CNN and show a significant improvement in classification performance on a large Reddit corpus. 4 Figure 2: Rule-Based Approach that the 14th instance in the dataset is the sarcastic tweet ‘This phone has an awesome battery backup of 2 hours’. This tweet contains two noun phrases: ‘phone’ and ‘awesome battery backup’. The words in these two noun phrases are ‘phone’, ‘awesome’, ‘battery’, ‘backup’. We first convert these words into 200-D word vectors (initialized using GloVe (Pennington et al., 2014) and finetuned on our dataset). Then we sum up word vectors of words in the noun phrase list and normalize them by the length of the noun phrase list. We call this the noun phrase vector. Given these entities, the tweet representation is: (14, Noun Phrase Vector, 2, ‘hours’). Since the tweet is sarcastic, it is stored in the sarcastic repository. In addition to tweet entries, both sarcastic and non-sarcastic repositories also maintain two dictionaries: (a) Dictionary of mean values where each entry is a key-value pair where key is the unit of measurement and value is the average of all the num"
W19-1309,C16-1151,0,0.0852134,"sentiment word and a negative situation. Joshi et al. (2015) show how sarcasm arises because of implicit or explicit incongruity in the sentence. Buschmeier et al. (2014) analyze the impact of different features for sarcasm/irony classification. Bouazizi and Ohtsuki (2016) propose a patternbased approach to detect sarcasm on Twitter. As deep learning techniques gain popularity, Ghosh and Veale (2016) propose a neural network semantic model for sarcasm detection. They use Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh"
W19-2404,P14-5010,0,0.0052004,"h empty, we ignore that interaction. If only one of them is empty, we add a special actor Environment (ENV) to that set. Once such sets are identified, a message is created for each unique combination of a sender and a receiver for a particular interaction verb. Dependency parsing-based Approaches: We developed two approaches for message creation based on dependency parsing output: i) Baseline B1 which directly maps the dependencies output to messages and ii) Approach M1 (Algorithm 1) which builds on the dependencies output by applying additional linguistic knowledge. We use Stanford CoreNLP (Manning et al., 2014) for dependency parsing. Baseline B1 simply maps each interaction verb in the dependency tree to a set of messages. Actors directly connected to an interaction verb with certain dependency relations (nsubj, nmod:agent) are identified as senders whereas actors directly connected to the verb with certain other dependency relations (dobj, nsubjpass, xcomp, iobj, advcl:to, nmod:∗) are identified as receivers. Interaction Identification Typically the input text mentions many different interactions, and identifying each verbal interaction is required, omitting non-interactions as discussed in Sectio"
W19-2404,W17-5912,1,0.76547,"sages M 1 (Algorithm 1), M2: create messages M 2 (Algorithm 2), M3: Combined approach using SRL and dependencies. Setting S1 corresponds to using gold actors and interaction verbs, Setting S2 uses predicted actors and interaction verbs Jurafsky (2009) on modelling of narrative schemas and their participants. They need a corpus of narratives to identify prototypical schemas which try to capture common sequence of events. We address a different problem of extracting MSC from a single narrative and do not need a corpus. MSC has been proposed as a knowledge representation for a narrative text in (Bedi et al., 2017). We extend their work to automatically construct MSC. Open Information Extraction (OpenIE) systems aim to extract tuples consisting of relation phrases and their multiple associated argument phrases from an input sentence (Mausam et al., 2012). The predicate-argument structures in OpenIE seem similar to SRL and dependency parsing. However, in dependency parsing the relations are fixed, while SRL systems require deeper semantic analysis of a sentence and hence they depend on lexical resources like PropBank and FrameNet. On the other hand, the predicate-argument structures in OpenIE are not res"
W19-2404,D12-1048,0,0.262806,"ii) Setting S2 : using predicted actors and interaction verbs. We use the approach proposed by Patil et al. (2018) for predicting actor mentions and identifying canonical mentions; and a simple algorithm for predicting interaction verbs. For evaluating our temporal ordering approach, we use Kendall’s τ rank correlation coefficient (Kendall, 1938) to compare predicted and gold time-lines of a key actor in each dataset (e.g., Mao Zedong in the Mao dataset). As goal of Kof’s work (Kof, 2007) is same as our work on message extraction, we use it as one of the baselines (B-Kof). We also use OpenIE (Mausam et al., 2012) as another baseline (BOIE). To avoid unnecessarily penalizing B-OIE, we consider only those extractions where relations fit our definition of interaction verbs and arguments fit our definition of actors. We compare our temporal ordering approach with the default text order based baseline (Text-Order). Table 2 shows comparative performance of the proposed approaches for message extraction and temporal ordering. 4.3 5 Related Work Though there has been some work in applying MSC for Software Engineering domain, less attention is given to the automatic construction of MSC using NLP. Feijs (2000)"
W19-2404,P09-1068,0,0.0492721,"that actor has participated. Message Sequence Chart (MSC) is an intuitive visual notation with rigorous mathematical semantics that can help to precisely represent and analyze (Alur et al., 1996) such scenarios. Feijs (2000), and Li (2000) propose techniques to convert software requirements to MSC. Event timeline construction is a related task about inferring the temporal ordering among events, but where events are not necessarily interactions among actors (Do et al., 2012). Another related line of research is storyline or plot generation from narrative texts such as news stories or fiction (Chambers and Jurafsky, 2009; Vossen 2 Problem Definition The input is a document D containing narrative text, and the desired output is an MSC depicting the interactions among the actors. No information about the actors or interactions is given as input; they need to be identified. For history narratives, we define an actor as an entity of type Person, Organization (ORG) or Location (LOC), which actively participates in various interactions 28 Proceedings of the First Workshop on Narrative Understanding, pages 28–36 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics Figure 1: MSC for"
W19-2404,P18-2011,1,0.923163,"ugh the text and identify all the actors who are involved in one or more interactions. We group all co-referring mentions of an actor into a set, and choose one canonical mention as a representative on the MSC. One complication can occur due to complex actors, which is an actor that contains multiple actors, one of which is independent and the others are dependent and serve to elaborate on the independent actor; e.g., his parents, military school, the army of the new government. We need to identify a complex actor as a whole, and not its constituent actors separately. We use the algorithm in (Patil et al., 2018) to identify an actor and all its coreferents. 3.2 3.3 We need to map each identified interaction to one or more messages in the output MSC. We also need to identify the sender (initiator of the interaction) and receiver (other actors involved in the interaction) for each message. We have developed several approaches for identifying a set of senders (SX) and a set of receivers (RX) for each valid interaction verb. If SX and RX are both empty, we ignore that interaction. If only one of them is empty, we add a special actor Environment (ENV) to that set. Once such sets are identified, a message"
W19-2404,D12-1062,0,0.0342475,"set of inter-related timelines, one for each actor, where the timeline of an actor specifies the temporal order of interactions in which that actor has participated. Message Sequence Chart (MSC) is an intuitive visual notation with rigorous mathematical semantics that can help to precisely represent and analyze (Alur et al., 1996) such scenarios. Feijs (2000), and Li (2000) propose techniques to convert software requirements to MSC. Event timeline construction is a related task about inferring the temporal ordering among events, but where events are not necessarily interactions among actors (Do et al., 2012). Another related line of research is storyline or plot generation from narrative texts such as news stories or fiction (Chambers and Jurafsky, 2009; Vossen 2 Problem Definition The input is a document D containing narrative text, and the desired output is an MSC depicting the interactions among the actors. No information about the actors or interactions is given as input; they need to be identified. For history narratives, we define an actor as an entity of type Person, Organization (ORG) or Location (LOC), which actively participates in various interactions 28 Proceedings of the First Worksh"
W19-2404,Q15-1032,0,0.0279814,"er. But as S2 indicates that the actor (Mao) is willingly performing the action of moving, we expect Mao to be a sender. Hence, for an ergative verb, even if the SRL assigns A1 role to an actor, we consider such an actor for being sender if no A0 role is assigned for the ergative verb by the SRL (lines 9-13 in Algorithm 2). SRL-based Approaches: We developed two approaches for message creation based on SRL: i) Baseline B2 which directly maps the SRL output to messages and ii) Approach M2 (Algorithm 2) which builds on the SRL output by applying additional linguistic knowledge. We use MatePlus (Roth and Lapata, 2015) for SRL which produces predicate-argument structures as per PropBank (Kingsbury and Palmer, 2002). The baseline B2 simply maps each verbal predicate corresponding to an interaction verb to a set of messages. Actors corresponding to A0 arguments of a verbal predicate are identified as senders whereas actors corresponding to other arguments are identified as receivers. Combined SRL and Dependency parsing based Approach (M3): SRL tools are useful to identify senders and receivers of a message, but they do have a few important limitations. E.g. (i) SRL tool may fail to identify any A0 even when i"
W19-2404,D15-1063,0,0.0626977,"Missing"
W19-2404,D10-1008,0,0.0822706,"Missing"
W19-2404,W15-4507,0,0.0509534,"Missing"
W19-2404,J95-2003,0,0.516851,"Missing"
W19-2404,W16-5702,0,0.0438654,"Missing"
W19-2404,kingsbury-palmer-2002-treebank,0,0.271159,"expect Mao to be a sender. Hence, for an ergative verb, even if the SRL assigns A1 role to an actor, we consider such an actor for being sender if no A0 role is assigned for the ergative verb by the SRL (lines 9-13 in Algorithm 2). SRL-based Approaches: We developed two approaches for message creation based on SRL: i) Baseline B2 which directly maps the SRL output to messages and ii) Approach M2 (Algorithm 2) which builds on the SRL output by applying additional linguistic knowledge. We use MatePlus (Roth and Lapata, 2015) for SRL which produces predicate-argument structures as per PropBank (Kingsbury and Palmer, 2002). The baseline B2 simply maps each verbal predicate corresponding to an interaction verb to a set of messages. Actors corresponding to A0 arguments of a verbal predicate are identified as senders whereas actors corresponding to other arguments are identified as receivers. Combined SRL and Dependency parsing based Approach (M3): SRL tools are useful to identify senders and receivers of a message, but they do have a few important limitations. E.g. (i) SRL tool may fail to identify any A0 even when it is present or when it assumes the verb does not require A0 in the sentence; (ii) the identified"
W19-2404,P15-2059,0,0.15776,"n in Figure 1, which can be used to answer questions like ""Whom did Napoleon defend the National Convention from?"". To the best of our knowledge, this is the first work that uses MSC to represent knowledge about actors and their interactions in narrative history text. Our approach is general, and can represent interactions among actors in any narrative text (e.g., news, fiction and screenplays). We propose unsupervised approaches using dependency parsing and Semantic Role Labelling for extracting interactions and corresponding senders/receivers. We use a state-ofthe-art tense based technique (Laparra et al., 2015) to temporally order the interactions to create the MSC. In this paper, we advocate the use of Message Sequence Chart (MSC) as a knowledge representation to capture and visualize multiactor interactions and their temporal ordering. We propose algorithms to automatically extract an MSC from a history narrative. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extrac"
W19-5346,W16-2342,0,0.0118923,"ata is tokenized using moses tokenizer, and truecased. For tokenizing Gujarati data, we use indic nlp library8 . After tokeninzation and truecasing, we subword (Sennrich et al., 2015) all original data. We apply 10,000 BPE merge operations over English and Gujarati data independently. For back-translation of monolingual data, two PBSMT models English→Gujarati and Gujarati→English are trained over original available parallel subworded corpora. 4-gram lan8 5 Results The official automatic evaluation uses the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), CharactTER (Wang et al., 2016). The official scores are shown in the Table 2. Phrase-base SMT (PBSMT) obtains BLEU scores of 5.2 and 7.3 for English→Gujarati and Gujarati→Englsih, respectively. Whereas, baseline NMT (Transformer) obtains lower BLEU scores of 4.0 and 5.5 for the same directions. Though, SMT systems outperforms baseline NMT systems trained using small amount of original parallel data only. We observe from the Table 2 that Transformer with synthetic (Transformer + https://github.com/anoopkunchukuttan/indic nlp library 409 82.7 80.3 82.4 76.3 43.3 −0.119 −0.129 −0.132 −0.400 −1.769 Ave. 64.8 61.7 59.4 60.8 59."
W19-5346,W17-3204,0,0.0428257,"rtment of Computer Science and Engineering Indian Institute of Technology Patna {sukanta.pcs15,kamal.pcs17,asif,pb}@iitp.ac.in Abstract NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (Koehn and Knowles, 2017). However, back-translation (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard ph"
W19-5346,N03-1017,0,0.417658,"ion (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NMT systems are based on Transformer (Vaswani et al., 2017) architecture. Experiments show that NMT systems achieve BLEU (Papineni et al., 2002) scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data. Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Guj"
W19-5346,P03-1021,0,0.0514261,"different SMT and NMT based systems. Synth: Synthetic data Apart from these parallel data, we use monolingual English (news crawl) and Gujarati (Wikipedia dumps) sentences for synthetic parallel data creation. After training two models i.e. English→Gujarati and Gujarati→English using the parallel data mentioned in Table 1, English and Gujarati monolingual sentences are back translated respectively. 4.2 guage model is trained using KenLM (Heafield, 2011). For word alignment, we use GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. Model is tuned with Minimum Error Rate Training (Och, 2003). After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English→Gujarati and Gujarati→English PBSMT model, respectively. We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati→English and English→Gujarati translation directions. Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction. We use the following hyperparameters values of Sockeye toolkit: 6 layers in both encoder and decoder, word embedding size of 512, hidden size of 5"
W19-5346,J03-1002,0,0.0435229,"haractTER 0.987 1.005 0.919 0.782 0.884 0.763 0.883 0.905 0.828 0.817 0.859 0.774 Table 2: BLEU scores of different SMT and NMT based systems. Synth: Synthetic data Apart from these parallel data, we use monolingual English (news crawl) and Gujarati (Wikipedia dumps) sentences for synthetic parallel data creation. After training two models i.e. English→Gujarati and Gujarati→English using the parallel data mentioned in Table 1, English and Gujarati monolingual sentences are back translated respectively. 4.2 guage model is trained using KenLM (Heafield, 2011). For word alignment, we use GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. Model is tuned with Minimum Error Rate Training (Och, 2003). After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English→Gujarati and Gujarati→English PBSMT model, respectively. We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati→English and English→Gujarati translation directions. Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction. We use the following hyperparameters values of So"
W19-5346,P02-1040,0,0.105073,"th English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NMT systems are based on Transformer (Vaswani et al., 2017) architecture. Experiments show that NMT systems achieve BLEU (Papineni et al., 2002) scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data. Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Gujarati translation pair, and finally, Section 6 concludes the work. We describe our submission to WMT 2019 News translation shared task for GujaratiEng"
W19-5346,P16-1009,0,0.0331945,"titute of Technology Patna {sukanta.pcs15,kamal.pcs17,asif,pb}@iitp.ac.in Abstract NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (Koehn and Knowles, 2017). However, back-translation (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NM"
W19-5346,2006.amta-papers.25,0,0.0274195,"using it for experiment. English data is tokenized using moses tokenizer, and truecased. For tokenizing Gujarati data, we use indic nlp library8 . After tokeninzation and truecasing, we subword (Sennrich et al., 2015) all original data. We apply 10,000 BPE merge operations over English and Gujarati data independently. For back-translation of monolingual data, two PBSMT models English→Gujarati and Gujarati→English are trained over original available parallel subworded corpora. 4-gram lan8 5 Results The official automatic evaluation uses the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), CharactTER (Wang et al., 2016). The official scores are shown in the Table 2. Phrase-base SMT (PBSMT) obtains BLEU scores of 5.2 and 7.3 for English→Gujarati and Gujarati→Englsih, respectively. Whereas, baseline NMT (Transformer) obtains lower BLEU scores of 4.0 and 5.5 for the same directions. Though, SMT systems outperforms baseline NMT systems trained using small amount of original parallel data only. We observe from the Table 2 that Transformer with synthetic (Transformer + https://github.com/anoopkunchukuttan/indic nlp library 409 82.7 80.3 82.4 76.3 43.3 −0.119 −0.129 −0.132 −0.400 −1."
W19-5426,W18-6315,0,0.0117086,"ated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016), source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017), there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018), it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in (Artetxe et al., 2017) follows an architecture where encoder is shared and decoder is separate for each language. Encoder tries to map sentences from both languages in the same space, which is supported by cross-lingual word embeddings. They fix cross-lingual word embeddings in the encoder while training, which helps in generating cross-lingual sentence representations in the sam"
W19-5426,W17-4715,0,0.0151261,"tations, this leads to adversarial training. was introduced by (Sennrich et al., 2016b), to utilize monolingual data of target language. This requires a translation system in opposite direction. In (Sennrich et al., 2016b), a method where empty sentences are provided in the input for target side monolingual data is also evaluated, backtranslation performs better than this. In iterative Back-Translation, systems in both directions improve each other (Hoang et al., 2018), it is done in an incremental fashion. To generate backtranslated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016), source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017), there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018), it is claimed that quality of back-translated sentences is im"
W19-5426,D17-1158,0,0.0154845,"o evaluated, backtranslation performs better than this. In iterative Back-Translation, systems in both directions improve each other (Hoang et al., 2018), it is done in an incremental fashion. To generate backtranslated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016), source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017), there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018), it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in (Artetxe et al., 2017) follows an architecture where encoder is shared and decoder is separate for each language. Encoder tries to map sentences from both languages in t"
W19-5426,P18-1073,0,0.0819171,"d embedding spaces of different languages in the same space. The basic assumption for generating the cross-lingual word embeddings in most papers is that both the embedding spaces must be isometric. Cross-lingual word embeddings is generated by learning a linear transformation which minimizes the distances between words given in a dictionary. There are many methods proposed for training cross-lingual word embeddings in an unsupervised way. While training cross-lingual word embeddings in an unsupervised manner there is no dictionary available, only the monolingual embeddings are available. In (Artetxe et al., 2018), cross lingual word embeddings are generated following a series of steps which involves: normalization of the embeddings so they can be used together to utilize for a distance metric, unsupervised initialization using normalized embeddings, self-learning framework using adversarial training where it iterates between creating the dictionary and finding the optimal mapping, and some weighting refinement over this. Through these steps a transformation of these spaces to a common space is learnt. In (Conneau et al., 2017) an adversarial training process is followed where discriminator tries to co"
W19-5426,W18-2703,0,0.0124744,"eddings trained using MUSE(Conneau et al., 2017). They also utilize a discriminator which tries to identify the language from the encoder representations, this leads to adversarial training. was introduced by (Sennrich et al., 2016b), to utilize monolingual data of target language. This requires a translation system in opposite direction. In (Sennrich et al., 2016b), a method where empty sentences are provided in the input for target side monolingual data is also evaluated, backtranslation performs better than this. In iterative Back-Translation, systems in both directions improve each other (Hoang et al., 2018), it is done in an incremental fashion. To generate backtranslated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016), source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017), there is a separate layer for target side language model in training, decoder utilize both source dependent and source independ"
W19-5426,L18-1548,1,0.851065,"Missing"
W19-5426,D18-1549,0,0.105364,"em with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a), utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016), complete unsupervised architectures (Artetxe et al., 2017) (Lample et al., 2018) and many others have been proposed. Introduction In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data in addition to parallel data because the provided parallel data is very small in amount. The detail of task is provided in (Barrault et al., 2019). We participated for Hindi-Nepali language pair and submi"
W19-5426,D15-1166,0,0.0607058,"re by (Artetxe et al., 2017) and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data. 1 2 Neural Machine Translation Many architectures have been proposed for neural machine translation. Most famous one is RNN based encoder-decoder proposed in (Cho et al.), where encoder and decoder are both recursive neural networks, encoder can be bi-directional. After this attention based sequence to sequence models where attention is utilized to improve performance in NMT are proposed in (Bahdanau et al., 2014), (Luong et al., 2015). Attention basically instructs the system about which words to focus more, while generating a particular target word. Transformer based encoder-decoder architecture for NMT is proposed in (Vaswani et al., 2017), which is completely based on self-attention and positional encoding. This does not follow recurrent architecture. Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for w"
W19-5426,P16-1009,0,0.402119,"cture for NMT is proposed in (Vaswani et al., 2017), which is completely based on self-attention and positional encoding. This does not follow recurrent architecture. Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a), utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016), complete unsupervised architectures (Artetxe et al., 2017) (Lample et al., 2018) and many others have been proposed. Introduction In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data i"
W19-5426,P16-1162,0,0.597671,"cture for NMT is proposed in (Vaswani et al., 2017), which is completely based on self-attention and positional encoding. This does not follow recurrent architecture. Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a), utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016), complete unsupervised architectures (Artetxe et al., 2017) (Lample et al., 2018) and many others have been proposed. Introduction In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data i"
W19-5426,D16-1160,0,0.0241074,"ection. In (Sennrich et al., 2016b), a method where empty sentences are provided in the input for target side monolingual data is also evaluated, backtranslation performs better than this. In iterative Back-Translation, systems in both directions improve each other (Hoang et al., 2018), it is done in an incremental fashion. To generate backtranslated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016), source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017), there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018), it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in (Artetxe et al., 2017) follows an"
W19-5426,D16-1163,0,0.0268802,"is does not follow recurrent architecture. Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a), utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016), complete unsupervised architectures (Artetxe et al., 2017) (Lample et al., 2018) and many others have been proposed. Introduction In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data in addition to parallel data because the provided parallel data is very small in amount. The detail of task is provided in"
W19-5440,P07-2045,0,0.0209917,"Missing"
W19-5440,D18-1549,0,0.0129773,"extracted from each of those two parallel corpora. The quality of the scoring method is judged based on the quality of the neural machine translation (NMT) and statistical machine translation (SMT) systems trained on these smaller corpora. We participated in both language pair: Nepali-English and Sinhala-English. Building machine translation (MT) systems, specifically NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) systems, require supervision of huge amount of high-quality parallel training data. Though recently emerged unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it takes time, money and expert translators. Though we can have parallel data compiled from online sources, it is not reliable as it is often very noisy and poor in quality. It has been found that MT systems are sens"
W19-5440,N03-1017,0,0.0635676,"Missing"
W19-5440,W14-4012,0,0.0715786,"Missing"
W19-5440,D19-1632,0,0.032813,"Missing"
W19-5440,L18-1275,0,0.0522698,"Missing"
W19-5440,W11-2123,0,0.0385153,"Missing"
W19-5440,P02-1040,0,0.109937,"Missing"
W19-5440,D13-1176,0,0.0153023,"are asked to submit score for each sentence in each of these two parallel corpora (Nepali-English and SinhalaEnglish). Based on the scores, two smaller sets of parallel sentences that amount to 1 million and 5 millions are extracted from each of those two parallel corpora. The quality of the scoring method is judged based on the quality of the neural machine translation (NMT) and statistical machine translation (SMT) systems trained on these smaller corpora. We participated in both language pair: Nepali-English and Sinhala-English. Building machine translation (MT) systems, specifically NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) systems, require supervision of huge amount of high-quality parallel training data. Though recently emerged unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it"
W19-5440,W18-6319,0,0.121023,"ool (Koehn et al., 2007). For training the SMT system we keep the following settings: growdiag-final-and heuristics for word alignment, msdbidirectional-fe for reordering model, and 5gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). The BLEU6 (Papineni et al., 2002) scores for these SMT systems are 3.7 and 4.6 for Nepali-English and Sinhala-English, respectively. 5 Results 4 https://bitbucket.org/anoopk/indic_ nlp_library 5 https://github.com/moses-smt/ mosesdecoder/blob/RELEASE-3.0/scripts/ tokenizer/tokenizer.perl 6 We use sacreBLEU (Post, 2018). 291 Scoring Scheme Arithmetic Mean Geometric Mean Arithmetic Mean Geometric Mean 1 million SMT NMT test devtest test devtest Nepali-English 3.84 3.64 5.48 5.94 3.89 3.57 5.28 5.57 Sinhala-English 3.07 3.63 3.16 3.70 3.03 3.52 3.01 3.36 5 million SMT NMT test devtest test devtest 4.34 4.27 4.03 4.01 1.29 1.32 1.25 1.25 4.44 4.42 5.12 5.17 3.87 4.28 4.54 5.08 Table 3: Official BLEU scores for 1-million and 5-million sub-sampled sets. Scoring Scheme 1 million Nepali-English Arithmetic Mean 56,868 Geometric Mean 53,821 Sinhala-English Arithmetic Mean 70,114 Geometric Mean 67,888 we observe that"
W19-5440,W18-2709,0,0.066798,"2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it takes time, money and expert translators. Though we can have parallel data compiled from online sources, it is not reliable as it is often very noisy and poor in quality. It has been found that MT systems are sensitive to noise (Khayrallah and Koehn, 2018). This necessitates to filter out noisy sentences from a large pool of parallel parallel sentences. Parallel corpus filtering task of WMT 2019 focuses on two new low-resource languages pairs: In this paper, we describe the IIT Patna’s submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original"
W19-5440,W19-5404,0,0.0400032,"Missing"
W19-7509,bhattacharyya-2010-indowordnet,1,0.6862,"e showed that there is a lack of quality resources which can cover all aspects of language learning such as grammar, concepts, usage, and pronunciations in an effective manner. This motivated us to develop a digital aid, viz., Sanskrit Shabdamitra, that would fill this gap for Sanskrit language teaching and learning in both formal and informal learning environment. 4 Sanskrit Shabdamitra: an educational application using Sanskrit Wordnet 4.1 Shabdamitra Shabdamitra is an umbrella of multilingual digital aid of language teaching and learning for Indian languages. It is built using IndoWordNet (Bhattacharyya, 2010) as a resource and is related to Hindi Shabdamitra (Redkar et al., 2017b), which is an initiative of IIT Bombay, India29 , exploring the applications of wordnet in education domain. The term Shabdamitra and its meanings were originally conceived by Malhar Kulkarni. The term Shabdamitra, शिमऽ is coined from two words ‘shabda’, श, i.e., ‘a word’ and ‘mitra’, िमऽ, i.e., ‘a friend’; also means ‘the Sun’. Therefore, Shabdamitra means a friend which helps in understanding a given word/concept. Using the second meaning of the word ‘mitra’ mentioned above, the word Shabdamitra would mean an illumina"
W19-7509,W14-0145,1,0.891191,"Missing"
W19-7509,J04-2002,0,0.0743907,"ic and semantic information. However, this tool does not facilitate Sanskrit language learning. Other online resources for Sanskrit are bilingual dictionaries and thesauri which provide only the meanings of the words, such as Monier-Williams Dictionary21 , Apte’s Dictionary22 , Spoken Sanskrit Dictionary23 , etc. Apart from these, there are some online dictionaries and thesauri in Sanskrit viz., Amarakosha24 , Sabda-kalpadruma25 , Vacaspatyam26 , etc. These online resources have domain-specific ontology, i.e., mythological ontology. Whereas, Wordnet does has been considered an upper ontology (Navigli and Velardi, 2004). Most of these tools and platforms are in the form of text material, presentations, videos, lesson plan, etc. However, they do not provide relational semantics. Majority of them are not interactive and curriculum specific vocabulary learning is not available. It should be noted that one common thing among all the above resources is that they are more focused on individual learning and do not provide the active classroom learning. This is the desideratum as the knowledge of words or concepts in Sanskrit is not available as per the school curriculum. On the other hand, Sanskrit Shabdamitra, int"
W19-7509,W14-5115,1,0.635367,"work. 1 http://www.cfilt.iitb.ac.in/wordnet/webswn/wn.php https://wordnet.princeton.edu/ 3 http://www.cfilt.iitb.ac.in/indowordnet/ 2 2 Literature Survey Sanskrit, belonging to the Indo-Aryan family of languages, is one of the ancient languages in the world. There is a rich tradition of developing a vast vocabulary in Sanskrit literature (Kulkarni et al., 2010a). Most of the languages in the Indo-European language family can be traced back to Sanskrit (Kulkarni et al., 2010b). There are various grammatical features and properties of Sanskrit which may not be present in other Indian languages (Redkar et al., 2014). With the increase in the digital presence across the globe, content digitization and digital language learning have been growing enormously. Vocabulary is a crucial part of language learning. Learning Sanskrit vocabulary is one of the challenging tasks for any language learner. There are several applications and platforms available for curriculum based education, but very few are meant for language learning and active classroom. The Indian government is now supporting digital education and has taken several steps in digital language education. Following are government-driven platforms in dig"
W19-7509,2016.gwc-1.46,1,0.851745,"Missing"
W19-7509,W17-7531,1,0.834836,"/ 23 http://spokensanskrit.org/ 24 https://sanskritdocuments.org/sanskrit/amarakosha/ 25 http://www.sanskrit-lexicon.uni-koeln.de/scans/SKDScan/2013/web/webtc2/index.php 26 https://archive.org/details/vacaspatyam02tarkuoft 27 https://wordnet.princeton.edu/ 16 Figure 1: Shabdamitra as a friend of a word by providing word-meaning, example usage, pronunciation, picture, synonyms, other grammatical features, etc. Hindi Wordnet28 (HWN) has been used to build a teaching and learning digital aid, Hindi Shabdamitra, for Hindi language education in formal (schools) and informal (self-learning) setups (Redkar et al., 2017a). Additionally, the development of Marathi Shabdamitra, using Marathi Wordnet as a resource, is also under process. A study of current digital resources used by the various educational institutions was also done as part of the background study. The outcome showed that there is a lack of quality resources which can cover all aspects of language learning such as grammar, concepts, usage, and pronunciations in an effective manner. This motivated us to develop a digital aid, viz., Sanskrit Shabdamitra, that would fill this gap for Sanskrit language teaching and learning in both formal and inform"
W19-7511,Q17-1010,0,0.0950375,"Missing"
W19-7511,D18-2029,0,0.0476738,"Missing"
W19-7511,P06-1035,0,0.0485466,"Missing"
W19-7511,W97-1102,0,0.0664099,"Missing"
W19-7511,N18-1202,0,0.0519215,"Missing"
W19-7511,R09-1064,0,0.0640674,"Missing"
W19-7511,N18-2063,0,0.0206677,"Missing"
W19-7511,C16-1097,0,0.0337873,"Missing"
W19-7511,W07-1306,0,0.0722626,"Missing"
W19-7511,W06-1109,0,0.131246,"Missing"
W19-7512,D18-2029,0,0.0214687,"Missing"
W19-7512,W14-0147,1,0.862478,"Missing"
W19-7512,P10-1023,0,0.0177927,"Missing"
W19-7512,W97-1102,0,0.156064,"Missing"
W19-7512,W13-5616,0,0.0179109,"Missing"
Y18-3012,W14-4012,0,0.137348,"Missing"
Y18-3012,P15-1166,0,0.18267,"rs. Sharing of parameters between low-resource and high-resource language pairs helps low-resource pairs to learn better model compared to model trained separately. However, it has been seen that training multiple languages together sometimes degrades the performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1"
Y18-3012,N16-1101,0,0.110114,"Missing"
Y18-3012,E17-3017,0,0.0284114,"1,500 2,000 5,248 4,150 5,273 7,417 2,114 1,633 2,126 Table 2: Original vocabulary size, number of BPE merge and final vocabulary size after applying BPE for each training data pair. We decided the BPE merge values without any rigorous exploration. 3.3 Experimental Setup We train 2 multilingual models namely XX→EN (Indic languages to English) and EN→XX (Englsih to Indic languages) and 14 bilingual models (7 for Indic languages to English, and 7 for English to Indic languages). All of these models are based on Transformer (Vaswani et al., 2017) network. For training the models, we use Sockeye (Hieber et al., 2017), a toolkit for NMT. Each token in training, development and test sets are split in subword units in preprocessing stage. Along with that an additional token5 indicating which Indic language a sentence pair belong to is added at the beginning of every source6 sentence. Then parallel data of all pairs are appended in one parallel corpus with Indic languages in one side and English on other side, for training a single multilingual model for each of EN→XX and XX→EN directions. These tokens are added with development and test sets too and likewise, development sets are also appended in a single de"
Y18-3012,D13-1176,0,0.0249306,"l translation model by means of sharing parameters with high-resource languages is a common practice to improve the performance of low-resource language pairs. Sharing of parameters between low-resource and high-resource language pairs helps low-resource pairs to learn better model compared to model trained separately. However, it has been seen that training multiple languages together sometimes degrades the performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train s"
Y18-3012,P02-1040,0,0.102539,"l., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 model as a baseline for each translation direction involving English. We evaluate the multilingual models against the bilingual models using BLEU (Papineni et al., 2002) metric. We found that multilingual NMT is better than bilingual models for all XX→EN directions, however for EN→XX directions, multilingual NMT performs better than bilingual NMT for low-resource language pairs only. In the next section, we briefly mention some notable multilingual NMT works. We describe our submitted systems in section 3 which includes description on datasets, preprocessing, experimental setup. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Dong et al. (2015) implemented a system with oneto-many mapping of languages. They tra"
Y18-3012,P16-1162,0,0.410155,"performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 model as a baseline for each translation direction involving English. We evaluate the multilingual models against the bilingual models using BLEU (Papineni et al., 2002) metric. We found that m"
