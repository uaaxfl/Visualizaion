2020.acl-main.26,W18-6518,0,0.0159447,"Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the questions are only generated from answers instead of reviews. Transfer learning (Pan and Yang, 2009; Tan et al., 2017; Li et al., 2020) refers to a broad scope of methods that exploit knowledge across domains for handling tasks in the target domai"
2020.acl-main.26,D13-1172,0,0.0287804,"on. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consu"
2020.acl-main.26,P17-1055,0,0.0299294,"pans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the"
2020.acl-main.26,D17-1219,0,0.102854,"arning is categorized into four groups (Pan and Yang, 2009), namely instance transfer, feature representation transfer, parameter transfer, and relational knowledge transfer. Our learning framework can be regarded as a case of instance transfer with iterative instance adaptation and augmentation. Related Work Question generation (QG) is an emerging research topic due to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various m"
2020.acl-main.26,P18-1177,0,0.112982,"Missing"
2020.acl-main.26,P17-1123,0,0.468063,"s or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to utilize question generation (QG) (Du et al., 2017) as a new means to overcome this problem. Specifically, given a review sentence, the generated question is expected to ask about the concerned aspect of this product, from the perspective of the review writer. Such question can be regarded as a reading anchor of the review sentence, and it is easier to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find o"
2020.acl-main.26,D19-1317,1,0.843323,"to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a du"
2020.acl-main.26,P18-1087,1,0.843326,"acted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long rev"
2020.acl-main.26,D14-1162,0,0.0844023,"Missing"
2020.acl-main.26,D17-1090,0,0.079253,"r to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second o"
2020.acl-main.26,D18-1427,0,0.230602,"ive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second one is that differen"
2020.acl-main.26,D17-1087,0,0.0166934,"have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not"
2020.acl-main.26,P17-1036,0,0.153488,"spect Extraction. Product aspects usually play a major role in all of product questions, answers and reviews, since they are the discussion focus of such text content. Thus, such aspects can act as connections in modeling input pairs of qa and r via the partially shared structure. To help the semantic vector hα in Eqn 3 capture salient aspects of reviews, an autoencoder module is connected to the encoding layer for reconstructing hα . Together with the matrix M, the autoencoder can be used to extract salient aspects from reviews. Note that this combined structure is similar to the ABAE model (He et al., 2017), which has been shown effective for unsupervised aspect extraction. Compared with supervised aspect detection methods, such a unsupervised module avoid the burden of aspect annotation for different product categories, and our experiments demonstrate that regularization based on this module is effective. Specifically, hα is mapped to an aspect distribution pα and then reconstructed: pα = softmax(Wp · hα + bp ) α0 h α =p ·A We adapt the Seq2Seq model for the aspect-focused generation model, which is updated gradually via the transferred and augmented instances. With the help of aspect-based var"
2020.acl-main.26,N18-2072,0,0.0266661,"questions. The second challenge, namely the issue that some verbose answers contain irrelevant content especially for subjective questions. To handle this challenge, we propose a learning framework with adaptive instance transfer and augmentation. Firstly, a pre-trained generation model based on user-posed answer-question pairs is utilized as an initial question generator. A ranker is designed to work together with the generator to improve the training instance set by distilling it via removing unsuitable answer-question pairs to avoid “negative transfer” (Pan and Yang, 2009), and augmenting (Kobayashi, 2018) it by adding suitable reviewquestion pairs. For selecting suitable reviews for question generation, the ranker considers two factors: the major aspects in a review and the review’s suitability for question generation. The two factors are captured via a reconstruction objective and a reinforcement objective with reward given by the generator. Thus, the ranker and the generator are iteratively enhanced, and the adaptively transferred answer-question pairs and the augmented reviewquestion pairs gradually relieve the data lacking problem. In accordance with the second characteristic of our task,"
2020.acl-main.26,N18-1141,0,0.0350535,"Missing"
2020.acl-main.26,P15-1060,0,0.0440336,"Missing"
2020.acl-main.26,P17-1096,0,0.0171941,"fter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such gener"
2020.acl-main.26,D18-1424,0,0.396601,"ervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to"
2020.acl-main.338,P19-1140,0,0.0276113,"present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zhang et al. (2019a) build GCN over the dependency tree of a sentence to exploit syntactical information and word dependencies for learning better aspect-related sentence representation for ASC. Different from all the above studies, this paper proposes a novel Cooperative Graph Attention Netw"
2020.acl-main.338,D17-1047,0,0.0234547,"sting studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-le"
2020.acl-main.338,P19-1052,0,0.0183307,"information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-level sentiment classiﬁcation to improve the performance of ASC. Ma et al. (2018) propose an extension of LSTM to integrate the commonsense knowledge into the recurrent encoder for improving the performance of ASC. In addition, it is worthwhile to note that Hazarika et al. (2018) also investigate the inter-aspect sentiment dependency for ASC, but is limited to capture this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the"
2020.acl-main.338,N19-1423,0,0.00741175,"es each new vertex vector h tex vi by considering neighboring vertices’ vectors {hj }Ij=1 with the following formulas: ˆ i = tanh( h I  αij W hj + b) j=1 exp(f (w [W hi ; W hj ])) αij = I t=1 exp(f (w  [W h (1) i ; W ht ])) where αij is the attention weight (i.e., the edge weight) between vertex vi and vertex vj . f (·) is a LeakyReLU activation function. [; ] denotes vector concatenation. W ∈ Rd×d and w ∈ R2d are the trainable parameters. In the following, we will illustrate the ﬁve main components of our CoGAN approach respectively. 3.2 Encoding Block As a text encoding mechanism, BERT (Devlin et al., 2019) can be ﬁne-tuned to create state-of-the-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-base2 (uncased) model to encode both the aspect and the sentence as follows. • Aspect Encoding. Since an aspect ak consists of an entity eentity and an attribute eattribute (Pontiki et al., 2015), we process the entity-attribute pair (eentity , eattribute ) into the input pair format of BERT as: [CLS] eentity [SEP] eattribute [SEP] Then, we feed the entity-attribute pair into BERT and regard the mark “[CLS]” representation as the as"
2020.acl-main.338,C18-1096,0,0.0832994,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P18-2092,0,0.0685403,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P19-1048,0,0.0578973,"n task to a sentence pair classiﬁcation task. In our implementation, we regard the pair of sentence and its aspect as the input pair of BERT-base model (Devlin et al., 2018) for performing ASC. 8) CADMN. This approach employs attention model to attend on relevant aspects for enhancing the aspect representation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system i"
2020.acl-main.338,S16-1174,0,0.0691356,"Missing"
2020.acl-main.338,P10-1043,1,0.697296,"ITY, polarity = positive - Category = FOOD#PRICES, polarity = positive S3: The lava cake dessert was incredible and I recommend it. - Category = FOOD#QUALITY, polarity = positive Figure 1: Two documents from SemEval 2016 (Pontiki et al. (2016)) datasets, where aspect category is deﬁned as the entity E and attribute A pair (i.e., E#A). Red lines denote the intra-aspect sentiment consistency and blue lines denote the inter-aspect sentiment tendency. Introduction Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang"
2020.acl-main.338,S16-1002,0,0.132215,"Missing"
2020.acl-main.338,S15-2082,0,0.248872,"Missing"
2020.acl-main.338,P19-1423,0,0.043128,"e document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zh"
2020.acl-main.338,S15-2130,0,0.15077,"entation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), while achieving the best accuracy score in the laptop16 domain. 15) CoGAN w/o Intra-Aspect Consistency. Our approach only modeling Inter-Aspect Tendency. 16) CoGAN w/o Inter-Aspect Tendency. Our approach only modeling Intra-Aspect Consistency. 17) CoGAN"
2020.acl-main.338,N19-1035,0,0.360347,"S] Excellent food … [SEP] What do you … Encoding Block  ൌ ܹݎ  ܾ ݒො Layer L ܧ sentence vector ݒ ܵ ݒොଶ ݒොଵ Layer 2 … ܧሾௌாሿ … ܽ [SEP] … Shared BERT ܧଵ ୀଵ Layer 1 ܧሾௌሿ quality … ܧሾௌாሿ … ܧଵ ܵାଵ ௧௧௨௧ [SEP] ൌ ሺሺݒ  ߙ ሺ ߙ ܹ ݒ ሻሻ  ܾሻ ܵ Aspect Input: FOOD#QUALITY ௧௧௬ ூᇲ ሺ௧ሻ ݒො ூ ܵଷ ሺ௧ሻ ݒො ܵଶ ൌ ሺ ߙ ܹఈ ݒ  ܾఈ ሻ ୀଵ Inter-Aspect Tendency Modeling Block Interaction Block Figure 2: The overall framework of our proposed Cooperative Graph Attention Networks (CoGAN). • Sentence Encoding. We borrow the approach proposed by Sun et al. (2019) to generate the aspectrelated sentence representation, which has achieved promising performance for the ASC task. Following Sun et al. (2019), we ﬁrst process the sentence si and its corresponding aspect ak into the input pair format of BERT as: [CLS] si [SEP] question(ak ) [SEP] where question(·) denotes the construction of auxiliary question sentence for aspect ak proposed by Sun et al. (2019). For example, the auxiliary sentence for aspect FOOD#PRICE is constructed as “what do you think of the food and price?”. Then, we similarly feed the above pair into BERT (shared with aspect encoding)"
2020.acl-main.338,C16-1311,0,0.576477,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,D16-1021,0,0.477432,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,P19-1345,1,0.819933,"010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment preference information is crucial to 3667 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3667–3677 c"
2020.acl-main.338,D19-1560,1,0.883168,"Missing"
2020.acl-main.338,D16-1058,0,0.0694742,"hree top-performed systems from SemEval-2015 Task 12 and SemEval2016 Task 5 (Pontiki et al., 2015, 2016). 2 Related Work In this section, we ﬁrst review the Aspect Sentiment Classiﬁcation (ASC) task, and then introduce the related studies on graph-based neural networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information"
2020.acl-main.338,D19-1464,0,0.0482997,"e this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies"
2020.emnlp-main.488,N19-4010,0,0.02402,"Missing"
2020.emnlp-main.488,C18-1139,0,0.0275194,"Missing"
2020.emnlp-main.488,P18-1246,0,0.0265305,"Missing"
2020.emnlp-main.488,Q17-1010,0,0.0180151,"4) with initial learning rate 1e-3 and batch size 32. Learning rate is decayed by 0.5 if the performance on dev set does not improve in 3 consecutive epochs. We stop training when the learning rate drops below 1e-5 or number of epochs reaches 100. We use the pre3 Condition tag c is also in w<t , since it is a special token added to the beginning of each sentence. We write it explicitly to emphasize the conditional effect. 4 The baseline model provided in the original paper is used for evaluating end to end target based sentiment analysis task. trained 300-dimensional fastText word embeddings (Bojanowski et al., 2017) for all languages. We employ relatively simple basic models because: 1) They help to avoid the possible overfitting problems due to the small data size under the low resource setting; 2) They allow more faithful understanding on the effects of the proposed data augmentation method. 4.2 Supervised Experiments To verify the effectiveness of our data augmentation method in the supervised settings, we evaluate it on three different tagging tasks, including NER, POS and E2E-TBSA. Most of the prior works rely on additional information, so we use random deletion (rd) (Wei and Zou, 2019) as our basel"
2020.emnlp-main.488,D17-1047,1,0.833037,"Missing"
2020.emnlp-main.488,D17-1091,0,0.0158196,"t is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Emp"
2020.emnlp-main.488,P17-2090,0,0.0976786,"lexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 202"
2020.emnlp-main.488,P19-1553,1,0.679347,"wt |w<t ) in Eq. 2 becomes pθ (wt |w<t , c).3 A similar approach is used in CTRL (Keskar et al., 2019) to control style, task-specific behavior, etc., during text generation. 4 Experiments In this section, we present our experiments in both supervised and semi-supervised settings. In the supervised settings, only gold data are used for augmentation. In the semi-supervised settings, we also leverage unlabeled data and knowledge bases. 4.1 Basic Models Language Model We use the language model described in Section 3.2 for synthetic data generation. We modified the decoder of the LSTM-LM model in Kruengkrai (2019) to implement this language model. We set LSTM hidden state size to 512 and embedding size to 300. We use dropout rate 0.5 for the two dropout layers. All language model are trained using Stochastic gradient descent (SGD) with initial learning rate 1 and batch size 32. Learning rate will be decayed by 0.5 in the next epoch if the perplexity on dev set does not improve. We set the maximum number of epochs to 30 and stop training early if the perplexity on dev set does not improve in 3 consecutive epochs. During synthetic data generation, we use the average length of gold sentences in the traini"
2020.emnlp-main.488,2020.lifelongnlp-1.3,0,0.13528,"2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et al., 2019). Weakly labeled data will inevitably introduce more nois"
2020.emnlp-main.488,N16-1030,0,0.308802,"ings, our method demonstrates strong ability to exploit useful information from unlabeled data and knowledge base. 2 Background Named Entity Recognition (NER) Named entities refer to phrases that are names of persons, organizations and locations, etc. in text. For example, “[ORG U.N.] official [PER Ekeus] heads for [LOC Baghdad] ”. Named entity recognition is an important task of information extraction and it aims to locate and classify named entities in text into the predefined types (Mikheev et al., 1999; Sang and De Meulder, 2003; Li et al., 2020). It is a challenging task for two reasons (Lample et al., 2016): 1) in most languages and domains, the amount of manually labeled training data for NER is limited; 2) it is difficult to generalize from this small sample of training data due to the constraints on the kinds of words that can be names. Part-of-Speech (POS) Tagging Part-of-speech tagging consists of assigning a tag that represents a grammatical class to each word in a given sentence. It is a critical component of most NLP systems and is fundamental to facilitate downstream tasks such as syntactic parsing (Sch¨utze, 1993) and opinion analysis (Liu et al., 2015). The current state-ofthe-art POS"
2020.emnlp-main.488,D19-5505,1,0.898289,"sunaga et al., 2018). Target Based Sentiment Analysis The target based sentiment analysis is a fundamental task of sentiment analysis and it aims to detect the opinion targets in sentences and predict the sentiment polarities over the targets (Liu et al., 2015; Chen et al., 2017; Li et al., 2018, 2019a). For example, “USB3 Peripherals are noticeably less expensive than the ThunderBolt ones”. In this sentence, two opinion targets were mentioned, namely “USB3 Peripherals” and “ThunderBolt ones” and the user expresses a positive sentiment over the first, and a negative sentiment over the second. Li et al. (2019a,b) propose an end-to-end solution (E2ETBSA) of TBSA, which converts TBSA to a tagging task, and aims to solve the two subtasks (i.e. target detection and sentiment classification) in a unified manning by predicting unified tags. For example, the tag “B-POS” indicates the beginning of a target with positive sentiment. So after annotation, the above example becomes “[B-POS USB3] [E-POS Peripherals] are noticeably less expensive than the [B-NEG ThunderBolt] [E-NEG ones]”. 3 Proposed Method We propose a novel data augmentation method for sequence tagging tasks. We first linearize labeled sentenc"
2020.emnlp-main.488,D15-1168,1,0.889428,"Missing"
2020.emnlp-main.488,P14-5010,0,0.00283339,"gold data). Our method. Generate synthetic data with LM, where LM is trained on gold data and unlabeled data. Baseline method. Annotate unlabeled data with knowledge base. Our method. Generate synthetic data with LM, where LM is trained on gold data and knowledge base annotated data. Table 5: Data sources for the semi-supervised setting. 4.3.1 Only Using Unlabeled Data Dataset We use CoNLL2003 English NER data (Tjong Kim Sang and De Meulder, 2003) for evaluation. In addition to the gold NER training data, we utilize unlabeled data for semi-supervised training. The Stanford CoreNLP tokenizer (Manning et al., 2014) is used to tokenize Wikipedia sentences. Experimental Settings Similar to the above experiments, we use 1k, 2k, 4k, 6k and 8k sentences randomly sampled from NER gold data as well as the full dataset to evaluate our method. For fair comparison, we only use the same set of 10k sentences randomly sampled from Wikipedia dump in both of our and baseline methods. Let Dgold and Dunlabeled be the sampled gold NER data and the Wikipedia data, respectively. In our method, Dgold and Dunlabeled are concatenated to train language models, following the steps 6051 Method 1k 2k 4k 6k 8k all gold 58.06 67.85"
2020.emnlp-main.488,E99-1001,0,0.544131,"Missing"
2020.emnlp-main.488,P16-2067,0,0.0505893,"Missing"
2020.emnlp-main.488,S15-2082,0,0.0602815,"Missing"
2020.emnlp-main.488,S14-2004,0,0.149978,"Missing"
2020.emnlp-main.488,W03-0419,0,0.698327,"Missing"
2020.emnlp-main.488,P93-1034,0,0.295575,"Missing"
2020.emnlp-main.488,P16-1009,0,0.0477906,"owever, due to the complexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 P"
2020.emnlp-main.488,P16-1056,0,0.021012,"a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compa"
2020.emnlp-main.488,W02-2024,0,0.554171,"ted. See Table 1 for the notations of the methods used in our supervised experiments. Method Description gold gen rd rd* Only use the gold data. Our method. Generate synthetic data with the language models, and oversample gold data. Baseline method. Generate synthetic data by random deletion, and oversample gold data with the same ratio as gen. Baseline method. Similar to rd, except that gold and synthetic data are equally sampled. Table 1: Data sources for the supervised setting. 4.2.1 Named Entity Recognition Dataset We evaluate our proposed methods on the CoNLL2002/2003 NER data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), with four languages: English, German, Dutch and Spanish. Besides, we evaluate our methods on Thai and Vietnamese NER data, which are product titles obtained from major e-commerce websites in Southeast Asian countries and annotated with 11 product attribute NER tags, including PRODUCT, BRAND, CONSUMER GROUP, MATERIAL, PATTERN, COLOR, FABRIC, OCCASION, ORIGIN, SEASON and STYLE. See Appendix for the statistics of the Thai and Vietnamese NER data used in our experiments. Experimental Settings In addition to evaluating our method on the full training data, we"
2020.emnlp-main.488,Q16-1035,0,0.0522738,"does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar"
2020.emnlp-main.488,D19-1670,0,0.046969,"o generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et"
2020.emnlp-main.569,D19-1291,0,0.110337,"Missing"
2020.emnlp-main.569,Q16-1026,0,0.0668326,"Missing"
2020.emnlp-main.569,N19-1423,0,0.0240504,"Missing"
2020.emnlp-main.569,N19-1129,0,0.0324957,"Missing"
2020.emnlp-main.569,W15-4631,0,0.143997,"Missing"
2020.emnlp-main.569,D19-1564,0,0.0469096,"Missing"
2020.emnlp-main.569,P17-2039,0,0.0350856,"Missing"
2020.emnlp-main.569,P11-2088,0,0.0470329,"Missing"
2020.emnlp-main.700,D19-1255,1,0.855705,"nell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer deco"
2020.emnlp-main.700,W11-0609,0,0.0284332,"ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 2018); b (Zhao et al., 2018); c (Dong et al., 2019); d (Xiao et al., 2020). 3.6 Fine-tuning on Response Generation Conversational response generation aims to produce a flexible response to a conversation (Vinyals and Le, 2015). Following MASS, we conduct experiments on the Cornell Movie Dialog corpus5 (Danescu-Niculescu-Mizil and Lee, 2011) that contains 140K conversation pairs, and use the training/test splits provided by the dataset. The same training hyperparameters from generative QA fine-tuning are adopted on the response generation task. We report the results in perplexity following (Vinyals and Le, 2015) (lower is better). We compare PALM with the competing methods including the baseline trained on the data pairs available and the pre-trained BERT+LM and MASS. Following MASS, we train every model on 10K pairs randomly sampled and all 110K training pairs. As shown in Table 5, PALM significantly performs better than all the"
2020.emnlp-main.700,P18-1177,0,0.0595509,"Missing"
2020.emnlp-main.700,N19-1409,0,0.0206538,"gradation resulted from ablating pre-training clearly demonstrates the power of PALM in leveraging an unlabeled corpus for downstream generation. 4 Related Work ELMo (Peters et al., 2018) is an early prominent pre-training method based on bidirectional LSTMs. It concatenates left-only and right-only representations, but does not pre-train interactions between these features. GPT (Radford, 2018), GPT2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) are proposed to base language modeling on the Transformer architecture, and use only the Transformer decoder for pre-training. Edunov et al. (Edunov et al., 2019) examine different strategies (e.g., ELMo) to add contextualized embeddings to sequence-to-sequence models, and observe the most improvement by adding the learned embeddings to the encoder. BERT (Devlin et al., 2018) introduces Masked Language Modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). However, BERT does not make predict"
2020.emnlp-main.700,D18-1443,0,0.0254376,"d, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 20"
2020.emnlp-main.700,P18-1031,0,0.0189712,"fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to com"
2020.emnlp-main.700,D18-1054,0,0.0464293,"Missing"
2020.emnlp-main.700,W04-1013,0,0.0214703,"user queries issued to the Bing search engine and the contextual passages are from real web documents. The data has been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers). To evaluate the generative capability, we focus on the Q&A + Natural Language Generation task, the goal of which is to provide the best answer available in natural language that could be used by a smart device / digital assistant. The answers are human-generated and not necessarily sub-spans of the contextual passages, so we use the ROUGE-L (Lin, 2004) metric for our evaluation to measure the quality of generated answers against the ground truth. We fine-tune the pre-trained PALM on the MARCO training set for 10 epochs. We set the batch size to 64, the learning rate to 1e-5, and the maximum input length to 512. The other hyperparameters are kept the same as pre-training. In fine-tuning PALM, the encoder takes as input x a contextual passage concatenated with a question at the end, and the decoder takes an answer as input y. During decoding, we use beam search with a beam of size 5. Table 2 presents the answer generation results on the test"
2020.emnlp-main.700,D19-1387,0,0.0578447,"Missing"
2020.emnlp-main.700,2021.ccl-1.108,0,0.129719,"Missing"
2020.emnlp-main.700,P19-1220,0,0.19461,"ing in the two stages, respectively. 2.3 Copying Tokens from Context In a human-written document, subsequent text often refers back to entities and tokens present earlier in the preceding text. Therefore, it would increase coherence of text generated in downstream to incorporate the copy mechanism into pre-training on an unlabeled corpus. This allows the model to learn from pre-training when and how to copy tokens in generating text, and the knowledge is transferred to downstream fine-tuning. PALM incorporates the copy mechanism by plugging in the pointer-generator network (See et al., 2017b; Nishida et al., 2019) on top of the decoder in Transformer. Figure 2 illustrates the pointer-generator network, which allows every token to be either generated from a vocabulary or copied from context in generating text. Extended vocabulary distribution. Let the extended vocabulary, V , be the union of words in the vocabulary and all tokens present in context. P v (yt ) then denotes the probability distribution of the t-th word token, yt , over the extended vocabulary, defined as: P v (yt ) = softmax(W e (W v st + bv )), (2) where st denotes the output representation of t-th token from the decoder. The output embe"
2020.emnlp-main.700,N18-1202,0,0.137717,"han reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on w"
2020.emnlp-main.700,D15-1044,0,0.0675287,"on on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been propose"
2020.emnlp-main.700,P17-1099,0,0.640208,"as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language gen"
2020.emnlp-main.700,P18-1178,0,0.0422221,"Missing"
2020.emnlp-main.700,D16-1264,0,0.0618804,"Dong et al., 2019), T5 (Raffel et al., 2019), BART (Lewis et al., 2019), PEGASUS (Zhang et al., 2019) and ERNIE-GEN (Xiao et al., 2020). By consistently outperforming the pre-training methods, PALM confirms its effectiveness in leveraging unsupervision signals for language generation. 3.5 Fine-tuning on Question Generation We conduct experiments for the answer-aware question generation task. Given an input passage and an answer span, question generation aims to generate a question that leads to the answer. Following the practice in (Zhao et al., 2018; Dong et al., 2019), we use the SQuAD 1.1 (Rajpurkar et al., 2016) dataset, and the BLEU-4, METEOR and ROUGEL metrics for evaluation. As shown in Table 4, PALM outperforms all previous question generation systems and achieves a new state-of-the-art result on BLEU-4 and ROUGE-L for question generation on the SQuAD 1.1 dataset. Method CorefNQGa MP-GSNb UNILMc ERNIE d ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 20"
2020.emnlp-main.700,D18-1424,0,0.0990213,"lf-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer decoder to generate a text sequence token-by-"
2020.emnlp-main.700,P17-4012,0,\N,Missing
2020.emnlp-main.700,E17-2047,0,\N,Missing
2020.emnlp-main.700,P18-1015,0,\N,Missing
2020.emnlp-main.700,N19-1423,0,\N,Missing
2020.emnlp-main.700,D19-1304,0,\N,Missing
2020.emnlp-main.90,D17-1209,0,0.0151228,"sk is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, such graph-to-sequence models without graph transformatio"
2020.emnlp-main.90,P18-1026,0,0.30088,"specially when using fewer GCN layers. Our main contributions include: Our dataset is not only more practical but also more challenging due to lack of explicit alignment between the input and the output. Therefore, some knowledge is useful for generation, while others might be noise. In such a case that many different relations from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transfo"
2020.emnlp-main.90,P11-2031,0,0.0173309,". 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples following the occurrence of entities in"
2020.emnlp-main.90,W11-2107,0,0.0226219,"7.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) base"
2020.emnlp-main.90,P19-1483,0,0.0182176,"rom entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples"
2020.emnlp-main.90,2020.emnlp-main.738,1,0.746045,"from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the ar"
2020.emnlp-main.90,W17-3518,0,0.530368,"nowledge into comprehensive natural language, is an important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and"
2020.emnlp-main.90,Q19-1019,1,0.911671,"amount of labels for edges in KG, such graph-to-sequence models without graph transformation will incur information loss and parameter explosion. Beck et al. (2018) proposed to transform the graph into Levi graph in order to work towards the aforementioned deficiencies, together with gated graph neural network (GGNN) to build graph representation for AMR-to-text problem. However, they face some new limitations brought in by Levi graph transformation: the entityto-entity information is being ignored in Levi transformation, as also mentioned in their paper. Afterwards, deeper GCNs were stacked (Guo et al., 2019) to capture such ignored information implicitly. In contrast, we intend to use fewer GCN layers to capture more global contextualization by explicitly stating all types of graph information with different transformations. 3 Task Description WebNLG AGENDA E2E ENT-DESC # instances Input vocab Output vocab # distinct entities # distinct relations Avg. # triples per input Avg. # words per output 41K 54K 78K 297K 7 4.4 141.3 51K 120 5.2K 77 8 5.6 20.3 110K 420K 248K 691K 957 27.4 31.0 Table 1: Dataset statistics of WebNLG, AGENDA and our prepared ENT-DESC. tice, it is difficult to describe an entit"
2020.emnlp-main.90,N19-1238,0,0.299874,"ive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the article’s infobox. Novikova et al. (2"
2020.emnlp-main.90,P17-1014,0,0.023481,"ons associated with Levi graphs. • Experiments and analysis on our new dataset show that our proposed MGCN model incorporated with aggregation methods outperforms strong baselines by effectively capturing and aggregating multi-graph information. 2 1188 Related Work for a single domain, while ours focuses on multiple domains of over 100 categories, including people, event, location, organization, etc. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-"
2020.emnlp-main.90,D16-1128,0,0.439154,"important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and expensive annotation process. More importantly, in"
2020.emnlp-main.90,W18-6501,0,0.36384,"s from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transformation incorporated with an aggregation layer. Multi-graph transformation is able to represent the original graph information more accurately, while the aggregation layer learns to extract useful information from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attent"
2020.emnlp-main.90,D17-1159,0,0.201683,"re complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, su"
2020.emnlp-main.90,W17-5525,0,0.223746,"Missing"
2020.emnlp-main.90,P02-1040,0,0.106802,"31.9 31.5 58.2 59.2 60.0 59.3 27.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well"
2020.emnlp-main.90,E17-2025,0,0.0284364,"described above, we can capture the information of higher-degree neighbors by stacking multiple MGCN layers. Inspired by Xu et al. (2018), we employ a concatenation operation over h(1) , · · · , h(n) to aggregate the graph representations from all MGCN layers (Figure 3 right) to form the final layer h(f inal) , which can be written as follows:   h(f inal) = h(1) , · · · h(n) . Such a mechanism allows weight sharing across graph nodes, which helps to reduce overfitting problems. To further reduce the number of parameters and overfitting problems, we apply the softmax weight tying technique (Press and Wolf, 2017) by tying source embeddings and target embeddings with a target softmax weight matrix. 5.2 Attention-based LSTM Decoder We adopt the commonly-used standard attentionbased LSTM as our decoder, where each next word yt is generated by conditioning on the final graph representation h(f inal) and all words that have been predicted y1 , ..., yt−1 . The training objective is to minimize the negative conditional log-likelihood. Thus, the objective function can be written as: T P L=− log pθ (yt |y1 , ..., yt−1 , h(f inal) ), t=1 where T represents the length of the output sequence, and p is the probabi"
2020.emnlp-main.90,2006.amta-papers.25,0,0.0222088,"x E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN,"
2020.findings-emnlp.179,K18-1048,0,0.0258478,"echanism to locate the most relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and sp"
2020.findings-emnlp.179,D17-1230,0,0.0279764,"ering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et"
2020.findings-emnlp.179,P02-1040,0,0.114129,"Missing"
2020.findings-emnlp.179,P19-1362,0,0.0962756,"et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et al., 2020). Zhao et al. (2019) proposed a review response generation model in the E-commerce platform, which used the reinforcement learning and copy mechanism to fuse external product information, thereby generating informative and diverse responses. Zheng et al. (2019) proposed a dialogue generation model considering personality traits such as age, name, and gender. Meng et al. (2019) proposed RefNet, which used background descriptions about the target dialogue and used a copy mechanism to copy tokens or semantic units. However, all these models are difficult to generalize in re"
2020.findings-emnlp.179,P18-1103,0,0.0115415,"ost relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent stud"
2021.acl-long.172,2020.acl-main.740,0,0.0400597,"Missing"
2021.acl-long.172,D19-1424,0,0.021136,"tperform those of fine-tuning. The differences between the mean and the best values are also smaller with adapter-based tuning. The results suggest that the performance of adapters is more stable over fine-tuning along the training process. Training neural networks can be viewed as searching for a good minima in the non-convex landscape defined by the loss function. Prior work (Hochreiter and Schmidhuber, 1997; Li et al., 2018) shows that the flatness of a local minima correlates with the generalization capability. Thus, we further show the loss landscapes of the two tuning methods. Following Hao et al. (2019), we plot the loss curve by linear interpolation between θ0 and θ1 with function f (α) = L(θ0 + α · (θ1 − θ0 )), where θ0 and θ1 denote the model weights before and after tuning. L(θ) is the loss function and α is a scalar parameter. In our experiments, we set the range of α to [−2, 2] and uniformly sample 20 points. Figure 6 shows the loss landscape curves on CoLA and SST based on BERT-base. It shows that the minimas of adapter-based tuning are more wide and flat, which indicates that adapter-based tuning tends to generalize better. Compare to Mixout The focus of this paper is to answer the q"
2021.acl-long.172,P18-1031,0,0.163589,"for evaluation. Note that the same set of tokens is used for all models. Finally, we compare the representations obtained from Madapt or Mf t to those from Morg using RSA. Figure 2 plots the results on STS-2, results of other tasks demonstrate a similar trend and can be found in Appendix A.3. For both fine-tuning and adapter-based tuning, we observe that the repre1 2 Cosine similarity is used We skip [PAD], [CLS], [SEP] for token selection. sentation change generally arises in the top layers of the network, which is consistent with previous findings that higher layers are more task relevant (Howard and Ruder, 2018). It can be clearly observed that compared to fine-tuning, adapterbased tuning yields representations with less deviation from those of BERT-base at each layer, which verifies our claim that adapter-based tuning can better regularize the tuning process by mitigating the forgetting problem. Apparently, this property of adapter tuning comes from that it freezes all the parameters of PrLMs. And because of the skipconnection in the adapter, the hidden representation out of the adapter can mimic the input representation, in this way, some of the original knowledge of PrLMs (before injecting adapter"
2021.acl-long.172,2020.acl-main.197,0,0.0289874,"s a new set of weights for each task, which is parameter inefficient. Adapterbased tuning is proposed to deal with this problem (Houlsby et al., 2019). Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; R¨uckl´e et al., 2020; Wang et al., 2020; Guo et al., 2020). However, existing work mostly focuses on the parameter-efficient aspect while overlooks the effectiveness. 2215 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021). Previous work points out that with large-scale parameters, fine-tuning on a few samples can lead to overfitting and bad generalization, which causes the results unstable. Phang et al. (2018) find that pretraining on an intermediate task can improve fine-tuning outcomes. Jiang et al. (2020) improve the robustness of fine-tuning by controlling the model complexity and preventing aggressive updating. On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where t"
2021.acl-long.172,2020.blackboxnlp-1.4,0,0.0238567,"the initial weights. Since adapter-based tuning does not update the weights of PrLMs at all, we suspect that it has a similar effect of alleviating the issue of catastrophic forgetting. Since the weights of the PrLM are the same before and after adapter-based tuning, to verify this, we use Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000) to assess the similarity of tuned representations to those without tuning at each transformer layer. RSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models. For each set of representations, a n × n pairwise similarity1 matrix is calculated. The final RSA similarity score between the two representation space is computed as the Pearson correlation between the flattened upper triangulars of the two similarity matrices. We use a subset of GLUE tasks (Wang et al., 2018) for our analysis. Given a task, we first perform adapter-based tuning and fine-tuning to adapt a BERT-base model (Morg ) to the target task, which yields models Madap"
2021.acl-long.172,2020.emnlp-main.617,0,0.0602973,"Missing"
2021.acl-long.172,W18-5446,0,0.0540876,"Missing"
2021.acl-long.308,P17-1178,0,0.0223577,"ision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 201"
2021.acl-long.308,W18-6319,0,0.012074,"ize Baseline 42.9 Liu et al. (2020a) 43.8 WMT14 En-De BLEU SacreBLEU 30 29 40.4 41.8 28.7 30.1 27.8 29.5 Randomly Initialize + More Bilingual Data* Baseline* 30.6 29.5 Cross-lingual Model Initialize mBART 43.2 mRASP 44.3 XLM-R 43.8 VECO 44.5 29.1 29.9 30.6 41.0 41.7 41.2 42.0 30.0 30.3 30.9 31.7 sacreBLEU Model 28 27 26 VECO Init. XLM-R Init. Random Init. 25 10 15 20 25 Epochs 30 35 Table 3: (left) Results on machine translation. (right) Learning curves of different initialization methods. tokenized SacreBLEU 7 to avoid the influence of different tokenization and normalization between models (Post, 2018). Fine-tuning Setting We fine-tune our model using fairseq 8 toolkit and adopt comparable training settings with baselines. We run WMT 14 EnDe and En-Fr MT experiments on 16 and 32 V100 GPUs, respectively. The batch size is 64k for EnDe and 256k for En-Fr. The total training updates are set to 100k. The learning rate is 1e-4/2e-4, with linear warm-up over the first 16k steps and linear decay. We average the last 10 checkpoints and use beam search with a beam size of 5. Baselines We consider two types of Transformer baselines: randomly initialized and cross-lingual models initialized. For rando"
2021.acl-long.308,D19-1071,0,0.0281093,"lable in the downstream task. Specifically, we concatenated the two repreL sentations [HL x : Sx ] to predict the label of x, L L [Hy : Sy ] to predict the label of y. 4 . 3.2 For pre-trained encoders like XLM, it is not a trivial problem to incorporate them into the sequenceto-sequence architecture – the mainstream backbone model of generation tasks (Zhu et al., 2020). One of the drawbacks or challenges could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted along with the encoder in the following fine-tuning process (Ren et al., 2019). However, under the framework of V E C O , the cross-attention is jointly pre-trained along with the whole network, making it easy to provide full initialization for sequence-to-sequence models. Specifically, the self-attention module is used to initialize both the corresponding modules in the encoder and decoder for contextual modeling, while the cross-attention module is used to initialize the encoder-to-decoder attention. It’s okay whether you continue to tie the self-attention parameters during fine-tuning. Directly pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b) c"
2021.acl-long.308,D19-1382,0,0.0282293,"rgence. Then, we jointly train the whole model. We pre-train our model with mixed-precision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details"
2021.acl-long.308,W17-2512,0,0.015424,"s Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and dire"
2021.acl-long.453,2020.repl4nlp-1.1,0,0.131258,"Missing"
2021.acl-long.453,2021.acl-long.154,1,0.628573,"Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER pe"
2021.acl-long.453,R19-2004,0,0.0242052,"resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection"
2021.acl-long.453,D18-1366,0,0.0226682,"projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual da"
2021.acl-long.453,2020.acl-main.747,0,0.173144,"Missing"
2021.acl-long.453,N19-1423,0,0.0260841,"ine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang e"
2021.acl-long.453,2020.emnlp-main.488,1,0.922928,"e than 100 languages. Alternatively, there are also many pretrained MT models conveniently accessible, e.g., more than 1,000 MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models have been released on the Hugging Face model hub.3 Note that the instance-based transfer methods add limited semantic variety to the training set, since they only translate entities and the corresponding contexts to a different language. In contrast, data augmentation has been proven to be a successful method for tackling the data scarcity problem. Inspired by a recent monolingual data augmentation method (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to increase the diversity, where LMs are trained on multilingual labeled data and then used to generate more synthetic training data. We conduct extensive experiments and analysis to verify the effectiveness of our methods. Our main contributions can be summarized as follows: augmentation method for NER, which leverages the multilingual language models to add more diversity to the training data. • Through empirical experiments, we observe that when fine-tuning pretrained multilingual LMs for low-resource cross-lingual NER, t"
2021.acl-long.453,2020.acl-main.413,0,0.0189525,"tic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.1 1 Introduction Named entity recognition (NER) aims to identify and classify entities in a text into predefined types, which is an essential tool for information extraction. It has also been proven to be useful in various downstream natural language processing (NLP) tasks, including information retrieval (Banerjee et al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-l"
2021.acl-long.453,D19-1100,0,0.150896,"ip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot"
2021.acl-long.453,P18-4020,0,0.024355,"Missing"
2021.acl-long.453,D19-1138,0,0.0154432,"ntities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech"
2021.acl-long.453,N18-2072,0,0.0234329,"n Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effect"
2021.acl-long.453,P19-1553,0,0.0173464,"English NER performance. Particularly, En + Multi-Tran achieves the best performance. Therefore, we can also use multilingual translated data to improve low-resource monolingual NER performance. 3.3 Generation-based Multilingual Data Augmentation In this section, we run experiments to verify whether applying generation-based data augmentation methods to the multilingual translated data can further improve cross-lingual performance in the low resource scenarios. Experimental settings We follow the steps described in §2.2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al., 2020) sep5839 500 Method 1k 2k de es nl avg de es nl avg de es nl avg En + Multi-Tran MulDA-LSTM MulDA-mBART 70.40 70.04 72.37 65.70 67.38 68.19 72.20 72.81 74.59 69.43 70.08 71.72 73.42 74.80 75.04 72.71 74.27 74.56 76.74 77.21 77.78 74.29 75.42 75.79 75.91 76.05 77.54 76.04 76.05 76.32 77.85 78.46 78.21 76.60 76.85 77.36 En + Tgt-Tran BiDA-LSTM 69.16 72.51 64.57 68.77 71.40 72.65 68.38 71.31 73.63 74.97 69.81 73.69 75.83 77.51 73.09 75.39 74.45 76.59 75.88 76.47 78.40 78.97 76.24 77.34 Table 4: Cross-lingual NER results of models trained on multilingual augmented data"
2021.acl-long.453,2020.acl-main.523,1,0.729419,"t al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et"
2021.acl-long.453,2020.lifelongnlp-1.3,0,0.0280795,"ining data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging place"
2021.acl-long.453,2020.tacl-1.47,0,0.298836,"based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection. For model-based transfer, although the large-scale pretrained multilingual language models (LM) (Conneau et al., 2020; Liu et al., 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020). To address the above problems under the setting of low-resource cross-lingual NER, we propose a multilingual data augmentation (MulDA) framework to make better use of the cross-lingual 5834 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5834–5846 August 1–6, 2021. ©20"
2021.acl-long.453,D17-1269,0,0.046708,"Missing"
2021.acl-long.453,2021.findings-acl.267,1,0.434557,"n. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging placeholders during MT. Our generation-based multilingual data augmentation method generates high quality synthetic training data to add more diversity. The proposed framework has demonstrated encouraging performance improvement in various low-res"
2021.acl-long.453,K16-1028,0,0.0200591,"Missing"
2021.acl-long.453,P17-1135,0,0.0321719,"Missing"
2021.acl-long.453,P17-1178,0,0.0256521,"g, while BiDA-LSTM trains one model for each target language in each setting. Therefore, we compare BiDA-LSTM with 8 https://github.com/pytorch/fairseq/blob/master/ examples/mbart/README.md En + Tgt-Tran only. As we can see, the proposed multilingual data augmentation methods further improve cross-lingual NER performance consistently. For the 1k and 2k setting, MulDA-LSTM achieves comparable average performance as BiDA-LSTM. 3.4 Evaluation on More Distant Languages We evaluate the proposed method on a wider range of target languages in this section. Experimental settings The Wikiann NER data (Pan et al., 2017) processed by Hu et al. (2020) is used in these experiments. 1k English sentences S ) are sampled from the gold train data to sim(D1k ulate the low resource scenarios. We also assume MT models are not available for all of the target languages, so we only translate the sampled English sentences to 6 target languages: ar, fr, it, ja, T tr and zh. Dtrans is used to denote the translated target-language sentences by following steps described in §2.1. The low quality translated sentences are filtered out in the same way as §3.2. To evaluate our method in the semi-supervised setting, we also sample"
2021.acl-long.453,2020.coling-main.305,0,0.0361386,"t combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al"
2021.acl-long.453,N12-1052,0,0.0914152,"Missing"
2021.acl-long.453,2021.naacl-main.282,1,0.721511,".1 Example 2 Gold EN: . . . (LOC U.S. Midwest) . . . Jain et al. (2019): . . . (LOC Mittlerer Westen) der (LOC USA) ... Li et al. (2020): . . . Mittlerer (LOC Westen) der (LOC USA) ... Ours: . . . (LOC Mittlerer Westen der USA) . . . Figure 6: Two examples that the previous methods fail to find the correct entity boundaries. Figure 7: Examples of multilingual sentences. the NER tags can be viewed as a shared vocabulary between different languages. As a result, we find that some generated sentences contain tokens from multiple languages, which are useful to help improve cross-lingual transfer (Tan and Joty, 2021). Two examples are shown in Figure 7. 4 Case Study Effectiveness in Label Projection The label projection step of the previous methods needs to locate the entities and determine their boundaries, which is vulnerable to many problems, such as word order change, long entities, etc. Our method effectively avoids these problems with placeholders. In the two examples shown in Figure 6, Jain et al. (2019) either labeled only part of the whole entity or incorrectly split the entity into two, Li et al. (2020) incorrectly split the entities into two in both examples, while our method can correctly map"
2021.acl-long.453,W14-1614,0,0.0595736,"Missing"
2021.acl-long.453,W02-2024,0,0.263791,"forward layer to the Transformer final layer for label classification. Specifically, to demonstrate that our framework can help achieve additional performance gain even on the top of the state-of-the-art multilingual LMs, the checkpoint of the pretrained XLM-R large (Conneau et al., 2020) model is used to initialize our NER models. 3.1 Labeled Sequence Translation We finetune the NER model on the translated targetlanguage data to compare our labeled sequence translation method (§2.1) with the existing instancebased transfer methods. Experimental settings The CoNLL02/03 NER dataset (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) is used for evaluation, which contains data in four different languages: English, German, Dutch and Spanish. All of the data are annotated with the same set of NER tags. We follow the steps described in §2.1 to translate En7 Similar to the token classification https://github.com/huggingface/transformers. model in glish train data to the other three languages. Following Jain et al. (2019) and Li et al. (2020), Google translation system is used in the experiments. Since our NER model is more powerful than those used by Jain et al. (2019) and Li et al. (2020"
2021.acl-long.453,K16-1022,0,0.0163074,"t language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representation"
2021.acl-long.453,Q16-1035,0,0.0256703,"lso some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al"
2021.acl-long.453,D18-1034,0,0.473471,"anslation method to translate the source ping with alignment models or algorithms. Howtraining data to a desired language. Compared with exiting methods, our labeled sentence trans- ever, these methods suffer from a few label projeclation approach leverages placeholders for la- tion problems, such as word order change, wordspan determination (Li et al., 2020), and so on. An bel projection, which effectively avoids many alternative to avoid the label projection problems issues faced during word alignment, such as word order change, entity span determination, noise- is word-by-word translation (Xie et al., 2018), but often at the sacrifice of the translation quality. sensitive similarity metrics and so on. We address the problems identified above by • We propose a generation-based multilingual data first replacing named entities with contextual place2 holders before sentence translation, and then after https://cloud.google.com/translate 3 https://huggingface.co/transformers/model doc/marian.html translation, we replace placeholders in translated 5835 B-PER E-PER O O O S-LOC O Jamie Valentine was born in London . Labeled sentence in the source language: [PER Jamie Valentine] was born in [LOC London]."
2021.acl-long.453,D19-1092,0,0.0452794,"Missing"
2021.acl-long.453,2020.emnlp-main.410,0,0.0245173,"Missing"
2021.acl-long.453,H01-1035,0,0.13327,"k into the data generated by our multilingual data augmentation method. During LM training, Related Work Cross-lingual NER There has been growing interest in cross-lingual NER. Prior approaches can be grouped into two main categories, instancebased transfer and model-based transfer. Instancebased transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word o"
2021.acl-long.493,N19-1423,0,0.522311,"oduction Document understanding is an essential problem in NLP, which aims to read and analyze textual documents. In addition to plain text, many realworld applications require to understand scanned documents with rich text. As shown in Figure 1, such scanned documents contain various structured information, like tables, digital forms, receipts, and invoices. The information of a document image is usually presented in natural language, but the format can be organized in many ways from multicolumn layout to various tables/forms. Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) have pushed the limits of a variety of document image understanding tasks, which learn the interaction between text and layout information across scanned document images. Xu et al. (2019) propose LayoutLM, which is a pre-training method of text and layout for document image understanding tasks. It uses 2Dposition embeddings to model the word-level layout information. However, it is not enough to model the word-level layout information, and the model sh"
2021.acl-long.493,D18-1476,0,0.0575342,"Missing"
2021.acl-long.493,2021.ccl-1.108,0,0.0536789,"Missing"
2021.acl-long.493,D16-1264,0,0.0509289,"tasks, each of which contains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Ther"
2021.acl-long.493,D19-1348,0,0.0269467,"6315 Figure 5: Examples of the output of LayoutLM and StructuralLM on the FUNSD dataset. The division of |means that the two phrases are independent labels. et al., 2005; Wei et al., 2013), they are usually time-consuming to design manually features and difficult to obtain a high-level abstract semantic context. In addition, these methods usually relied on visual cues but ignored textual information. 4.2 Deep Learning Approaches Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019). (Yang et al., 2017) propose a pixel-by-pixel classification to solve the document semantic structure extraction problem. Specifically, they propose a multimodal neural network that considers visual and textual information, while this work is an end-toend approach. (Katti et al., 2018) first propose a fully convolutional encoder-decoder network to predict a segmentation mask and bounding boxes. In this way, the model significantly outperforms approaches based on sequential text or document images. In addition, (Soto and Yoo, 2019) incorporate contextual information into the Faster R-CNN model"
2021.acl-long.493,P18-1158,1,0.847945,"tains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Therefore, we need to re"
2021.acl-short.111,N19-1423,0,0.049222,"w a slot is roughly operated in the current dialog context and connected with all possible tokens regarding its values in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped d"
2021.acl-short.111,2020.lrec-1.53,0,0.516868,"mation, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformerbased and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a str"
2021.acl-short.111,W19-5932,0,0.0172906,"and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et"
2021.acl-short.111,2020.sigdial-1.4,0,0.0310204,"Missing"
2021.acl-short.111,C18-1105,0,0.0278682,"al., 2020), reading comprehension (Tay et al., 2019) and open-domain chatbots (Bao et al., 2020; Cai et al., 2020; Su et al., 2020). Yet, the research on using CL in task-oriented dialog systems is limited. There has been some work (Saito, 2018; Zhao et al., 2021) on using CL in dialog policy learning, but applying CL to DST has not been investigated. Learning a structural inductive bias during pretraining has been shown beneficial in downstream tasks that require parsing semantics, such as textto-SQL (Yu et al., 2021) and table cell recognition (Wang et al., 2020). There are also many works (Hou et al., 2018; Yoo et al., 2020; Yin et al., 2020) on dialog augmentation. We aim to integrate these methods to build a general CL framework for DST. 6 Conclusion In this paper, we propose a model-agnostic framework named as schema-aware curriculum learning for DST, which exploits both the curriculum 883 References Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. Plato-2: Towards building an open-domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779. Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curri"
2021.acl-short.111,2020.acl-main.567,0,0.0308017,"state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and ha"
2021.acl-short.111,2020.acl-main.53,0,0.0359253,"users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As shown in Figure 1,"
2021.acl-short.111,P19-1546,0,0.0447439,"Missing"
2021.acl-short.111,2020.acl-main.41,0,0.0724363,"Missing"
2021.acl-short.111,N18-1202,0,0.017011,"in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped during CL, and utilize a schema-based data augmenter to produce similar cases from the examples. Specifically, the DST m"
2021.acl-short.111,W18-5707,0,0.0427423,"Missing"
2021.acl-short.111,2020.acl-main.563,0,0.0626527,"Missing"
2021.acl-short.111,N10-1116,0,0.0129957,"into our curriculum design: 1) current dialog turn number t; 2) the total token number of (Rt , Ut ); 3) the number of mentioned name entities like ‘hotel names’ in Zt ; 4) the number of newly added or changed slots in Yt . We set the maximum values of above factors as 7/50/4/6 respectively, and normalize all factors into rtrul,i ∈ [0, 1], where i indicates the i-th factor. Finally, the hybrid difficulty calculated P4score isrul,i jointly as rthyb = P α0 rmod + α r , where t i=1 i t 4 hyb r ∈ [0, 1] and i=0 αi = 1. 3.1.2 The Training Scheduler We adopt a widely used strategy called baby step (Spitkovsky et al., 2010) to organize the scored data for CL. Specifically, we divide the score uniformly into N intervals and distribute the sorted data into N buckets accordingly. The optimization starts from the easiest bucket as the initial training stage. After reaching a fixed number of maximum epochs or convergence, the next bucket is merged 880 Figure 2: An overview of the SaCLog training procedures. into the current training subset and shuffled for the next training stage. In our experiment, we set the maximum number of epochs as 3, and treat as the convergence if the training loss ceases to decrease and the"
2021.acl-short.111,P19-1486,0,0.0621828,"Missing"
2021.acl-short.111,E17-1042,0,0.0497929,"Missing"
2021.acl-short.111,P19-1078,0,0.138292,"ng (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As"
2021.acl-short.111,2020.findings-emnlp.95,0,0.0267103,"icitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the schema, enabling the"
2021.acl-short.111,2020.acl-main.542,0,0.0871591,"Missing"
2021.acl-short.111,2020.acl-main.620,0,0.0747792,"Missing"
2021.acl-short.111,2020.findings-emnlp.68,0,0.0237234,"her intention implicitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the sch"
2021.emnlp-main.185,D15-1075,0,0.25344,",shaohan.ljh,jian.sun,f.huang,luo.si}@alibaba-inc.com Abstract different learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields"
2021.emnlp-main.185,D18-2029,0,0.0906045,"mance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presents two limitations: (1) It yields a large gap between training and evaluating, since the semantic textual similarity is commonly measured by th"
2021.emnlp-main.185,L18-1269,0,0.0142213,"MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in this area. Wu et al. (2020); Giorgi et al. (2020); Meng et al. (2021) incorporate the data augmentation methods including the word-level deletion, reordering, substitution, and the sentencelevel corruption into the pre-training of deep Transformer models to improve the sentence representation ability, achieving significantly better performance than BERT especially on the sentence-level tasks (Wang et al., 2018; Cer et al., 2017; Conneau and Kiela, 2018). Gao et al. (2021) apply a twice independent dropout to obtain two same-source embeddings from a single sentence as input. Through 3 Problem Formulation optimizing their cosine distance, SimCSE achieves Suppose that we have a dialogue remarkable gains over the previous baselines. Yan K = {Si }i=1 , where Si = et al. (2021) empirically study more data augmen- dataset D tation strategies in learning sentence embeddings, {u1 , · · · , uk−1 , r, uk+1 , · · · , ut } is the i-th dialogue session in D with t turn utterances. r is the and it also achieves remarkable performance as SimCSE. In this wor"
2021.emnlp-main.185,D17-1070,0,0.0178248,"sponse matching relationships. Our work is closely related to their works. We propose a novel dialogue-based contrastive learning approach, which directly models the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI data"
2021.emnlp-main.185,N19-1423,0,0.476909,"on measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presen"
2021.emnlp-main.185,2021.emnlp-main.552,0,0.0500556,"Missing"
2021.emnlp-main.185,2020.acl-main.740,0,0.0309319,"Missing"
2021.emnlp-main.185,2020.emnlp-main.733,0,0.076929,"are inherently limited and hard to achieve further improvement. Recently, the pre-trained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al.) yield strong performances across many downstream tasks (Wang et al., 2018). However, BERT’s embeddings show poor performance without fine-tuning and many efforts have been devoted to alleviating this issue. Zhang et al. (2020) propose a self-supervised learning approach that derives meaningful BERT sentence embeddings by maximizing the mutual information between the global sentence embedding and all its local context embeddings. Li et al. (2020) argue that BERT induces a non-smooth anisotropic semantic space. They propose to use a flow-based generative module to transform BERT’s embeddings into isotropic semantic space. Similar to this work, Su et al. (2021) replace the flow-based generative module with a simple but efficient linear mapping layer, achieving competitive results with reported experiments in BERT-flow. For dialogue, Yang et al. (2018) train a siamese transformer network with single-turn inputresponse pairs extracted from Reddit. Such architecture is further extended in (Reimers and Gurevych, 2019) by replacing the trans"
2021.emnlp-main.185,marelli-etal-2014-sick,0,0.0249141,"ls the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in t"
2021.emnlp-main.185,D14-1162,0,0.0885511,"ce embedding to the isotropic semantic space. For BERT, we use the [CLS] token embedding (denoted as BERT-CLS) and the average of the sequence output embeddings (denoted as BERT-avg) as the sentence embedding, and the same is true for domain-adaptive BERT. It should be noted that in related sentence embedding researches, domainadaptive BERT is rarely considered since the training datasets are relatively small. Fortunately, the large-scale dialogue datasets allow us to explore whether the domain-adaptive pre-training is helpful for our tasks. We also adopt the average of GloVe word embeddings (Pennington et al., 2014) (denoted as Avg. GloVe) as the sentence embedding to compare with our results. 5.2.2 Dialogue-based self-supervised learning methods In this line, we mainly consider the siamesenetworks commonly applied in dialogue-based researches. Considering none of the previous works (Yang et al., 2018; Henderson et al., 2020) employs the pre-trained language model as encoder, we re2401 Model Microsoft Corpus Corr. MAP MRR Jing Dong Corpus Corr. MAP MRR E-commerce Corpus Corr. MAP MRR Self-supervised models Avg. GloVe embeddings BERT-CLS BERT-avg BERT-flow BERT-whitening 36.64 22.34 40.95 45.56 26.70 31.5"
2021.emnlp-main.185,2020.findings-emnlp.196,0,0.0767416,"Missing"
2021.emnlp-main.185,N16-1162,0,0.0261775,"Missing"
2021.emnlp-main.185,D19-1410,0,0.130655,"in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic rele"
2021.emnlp-main.185,W18-5446,0,0.0419368,"Missing"
2021.emnlp-main.185,N18-1101,0,0.11829,"learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields a large gap between training and evaluatdialogues has begun to attract inc"
2021.emnlp-main.185,2021.acl-long.393,0,0.026849,"closer, resulting in a decrease in Spearman’s correlation. However, as all positive samples in the candidates have identical labels, such degradation may not be fully reflected through the ranking metric (e.g. MAP) or even be covered as the number of retrieved positive samples changes. Impact of negative samples. We vary the number of negative samples for each positive sample within {1, 4, 9, 19}. Table 4 shows the experimental results, from which we find that both metrics improve slightly when the number of negative samples increases. Considering the similar observation in (Gao et al., 2021; Yan et al., 2021), we conclude this phenomenon may be related to the discrete nature of language. Specifically, as the generation of the sentence embeddings in our approach is guided and constrained by the token-level interaction mechanism, our model is more robust than the other contrastive learning approaches and is even effective when only one negative sample is provided. quality of the dialogue-based sentence embeddings. Evaluation results show that DialogueCSE achieves the best result over the baselines while adding no additional parameters. In the next step, we will study how to introduce more interactio"
2021.emnlp-main.185,W18-3022,0,0.356865,"esponse learning methods promising to achieve competembedding (i.e. the context-free embedding) itive or even superior performance against the according to the guidance of the multi-turn supervised-learning methods, especially under the context-response matching matrices. Then it low-resource conditions. pairs each context-aware embedding with its corresponding context-free embedding and fiWhile promising, the issue of how to effectively nally minimizes the contrastive loss across exploit the dialogues for this task has not been sufall pairs. We evaluate our model on three ficiently explored. Yang et al. (2018) propose to multi-turn dialogue datasets: the Microsoft train an input-response prediction model on Reddit Dialogue Corpus, the Jing Dong Dialogue dataset (Al-Rfou et al., 2016). Since they build their Corpus, and the E-commerce Dialogue Corarchitecture based on the single-turn dialogue, the pus. Evaluation results show that our apmulti-turn dialogue history is not fully exploited. proach significantly outperforms the baselines across all three datasets in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue co"
2021.emnlp-main.185,2020.emnlp-main.124,0,0.0212252,"nces of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better performance compared to the previous token-level modthe sentence embedding performance varies greatly eling approaches. The datasets used in these works as the domain of the training data changes. As a 1 dialogue dataset is always about several certain doAll the datasets will be publicly available at mains, evaluating on the STS benchmark may mis- https://github.com/wangruicn/DialogueCSE 2397 are typically built upon the corpus of web pages and books (Zhu et al., 2015). As the semantic connections are relatively weak in these corpora, the model pe"
2021.emnlp-main.185,C18-1317,0,0.0983426,"ches. • Extensive experiments show that DialogueCSE significantly outperforms the baselines, establishing the state-of-the-art results. 2 Related Work 2.1 Self-supervised Learning Approaches We train our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) Early works on sentence embeddings mainly focus on the self-supervised learning approaches. Kiros (Li et al., 2018), the Jing Dong Dialogue Corpus et al. (2015) train a seq2seq network by decod(JDDC) (Chen et al., 2020), and the E-commerce ing the token-level sequences of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better per"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
C18-1215,D15-1075,0,0.185054,"c resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals wi"
C18-1215,N16-1108,0,0.0177049,"studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals with the matching measurement between one sentence (or one piece of text) and another sentence (or another piece of text). In contrast, our approach is a One vs. Many Matching model which deals with the matching measurement between one sentence (or one piece of text) and multiple sentences (or multiple pieces of text). 3 Data Collection and Annotation We collect 4,060 question-answer pairs from “Asking All” in Taobao,"
C18-1215,D16-1244,0,0.0724571,"Missing"
C18-1215,D13-1044,0,0.0493654,"Missing"
C18-1215,P16-1044,0,0.189583,"mine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charl"
C18-1215,P16-1041,0,0.0278233,"ce, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charleston. A2: The film was released on May 25, 2010 on DVD. A: The respon"
C18-1215,C10-1131,0,0.083737,"Missing"
C18-1215,P16-1122,0,0.0269575,"Missing"
C18-1215,D15-1237,0,0.283339,"is a task to determine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filme"
C18-1215,N13-1106,0,0.0393979,"Missing"
C18-1215,P13-1171,0,0.02644,"erent from the above corpora, the question-answer pairs in our corpus are informal text. 2.2 Matching methods Generally speaking, QA matching methods could be split into two categories: shallow learning methods and deep learning methods. In shallow learning methods, some shallow learning algorithms, such as CRF, SVM and MaxEnt, are employed to train the learning models (Wang et al., 2010). Besides the learning algorithms, the related studies on shallow learning methods mainly focus on feature engineering, using linguistic tools and using external resources, such as lexical semantic resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In atten"
D18-1262,D13-1160,0,0.0320649,"ational Society Science Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent. into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons f"
D18-1262,C10-3009,0,0.0789533,"Missing"
D18-1262,W09-1206,0,0.277259,"Missing"
D18-1262,P16-1039,1,0.904877,"Missing"
D18-1262,C18-1233,1,0.856068,"s work (Marcheggiani and Titov, 2017). Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli , a randomly initialized POS tag embedding epos i , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate. To further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018). ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power. Eventually, the resulting word representation is concatenated as ei = f [eri , epi , eli , epos i , ei , ELMoi ]. BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs. Given an input sequence (e1 , . . . , en ), the BiLSTM processes these embedding vectors sequentially from both directions to"
D18-1262,D15-1112,0,0.705882,"units. Specifically, the main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units. A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit. This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit. Given a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai 2404 System Local model Lei et al. (2015) FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani et al. (2017) Marcheggiani and Titov (2017) He et al. (2018) Cai et al. (2018) Ours (Syn-GCN) Ours (SA-LSTM) Ours (Tree-LSTM) Global model Bj¨orkelund et al. (2010) FitzGerald et al. (2015) Roth and Lapata (2016) Ensemble model FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani and Titov (2017) et al., 2015): ˜k = h X hk , (1) j∈C(k) ˜ k + b(i) ), ig = σ(W (i) xk + U (i) h fgk,j = σ(W (f ) xk + U (f ) hj + b(f ) ), ˜ k + b(o) ), og = σ(W (o) xk + U (o) h (2) ˜ k + b(u) ), u = tanh(W (u) xk + U (u) h X fgk,j cj , ck = ig u + j∈C(k) hk = og tanh(ck"
D18-1262,S15-1033,0,0.038785,"Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (2017) also proposed a syntax-agnostic model for dependency SRL and obtained favorable results. Despite the success"
D18-1262,J02-3001,0,0.76716,"Missing"
D18-1262,P02-1031,0,0.0984173,"achine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marche"
D18-1262,P17-1044,0,0.2754,"ification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erron"
D18-1262,D17-1159,0,0.0668081,"2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marcheggiani and Titov (2017) 2401 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics proposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input. Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL. In this paper, we present a general framework1 for SRL, wh"
D18-1262,P18-1192,1,0.422579,"Missing"
D18-1262,P82-1020,0,0.843118,"Missing"
D18-1262,D14-1162,0,0.0824892,"= hk . 4 Experiments We evaluate our models performance of syntactic GCN (henceforth Syn-GCN), SA-LSTM and Tree-LSTM on CoNLL-2009 datasets both for English and Chinese with standard training, development and test splits. For predicate disambiguation, we follow previous work (Marcheggiani and Titov, 2017), using the off-the-shelf disambiguator from Roth and Lapata (2016). For syntactic dependency tree, we parse the corpus with Biaffine Parser (Dozat and Manning, 2017). 4.1 Experimental Settings In our experiments, the pre-trained word embeddings for English are 100-dimensional GloVe vectors (Pennington et al., 2014). For Chinese, we P R F1 − − 88.1 88.7 89.1 89.7 89.9 90.3 90.8 90.0 − − 85.3 86.8 86.8 89.3 89.2 89.3 88.6 88.8 86.6 86.7 86.7 87.7 88.0 89.5 89.6 89.8 89.7 89.4 88.6 85.2 86.9 − − 87.3 90.0 85.5 87.7 − − 87.7 90.3 85.7 87.9 90.5 87.7 89.1 Table 1: Results on the English in-domain test set. exploit Wikipedia documents to train the same dimensional Word2Vec embeddings (Mikolov et al., 2013). All other vectors are randomly initialized, the dimension of lemma embeddings is 100, and the dimension of POS tag embedding is 32. In addition, we use 300-dimensional ELMo embedding for English2 . During"
D18-1262,P18-4024,1,0.765756,"y indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (2017) also proposed a sy"
D18-1262,Q16-1023,0,0.0295115,"89.2 89.8 90.0 R 87.9 89.3 88.6 88.8 88.0 88.8 87.8 F1 88.7 89.8 89.7 89.4 88.6 89.3 88.9 Table 6: Comparison of models with deep encoder and M&T encoder (Marcheggiani and Titov, 2017) on the English test set. Syntactic Input Four types of syntactic inputs are used to explore the role of syntax in our unified framework, (1) the automatically predicted parse provided by CoNLL-2009 shared task, (2) the parsing results of the CoNLL-2009 data by state-of-theart syntactic parser, the Biaffine Parser (used in our previous experiments), (3) corresponding results from another parser, the BIST Parser (Kiperwasser and Goldberg, 2016), which is also adopted by Marcheggiani and Titov (2017), (4) the gold syntax available from the official data set. Evaluation Metric It is worth noting that for SRL task, the standard evaluation metric is the semantic labeled F1 score (Sem-F1 ), and we use the labeled attachment score (LAS) to quantify the quality of syntactic input. In addition, the ratio between labeled F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1 /LAS) proposed by CoNLL2008 shared task3 (Surdeanu et al., 2008), are also given for reference. To a certain extent, the ratio Sem-F1 /LAS cou"
D18-1262,N15-1121,0,0.274848,"Missing"
D18-1262,D09-1133,1,0.800247,"Missing"
D18-1262,N18-1202,0,0.0430818,"lowing previous work (Marcheggiani and Titov, 2017). Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli , a randomly initialized POS tag embedding epos i , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate. To further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018). ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power. Eventually, the resulting word representation is concatenated as ei = f [eri , epi , eli , epos i , ei , ELMoi ]. BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs. Given an input sequence (e1 , . . . , en ), the BiLSTM processes these embedding vectors sequentially from both directions to"
D18-1262,P05-1072,0,0.513969,"Missing"
D18-1262,J08-2005,0,0.142582,"-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marcheggiani and Titov (2017) 2401 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics proposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input. Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL. In this paper, we"
D18-1262,W17-4305,0,0.475622,"ncorporating syntax into neural SRL. In this paper, we present a general framework1 for SRL, which enables us to integrate syntax into SRL in diverse ways. Following Marcheggiani and Titov (2017), we focus on argument labeling and formulate SRL as sequence labeling problem. However, we differ by (1) leveraging enhanced word representation, (2) applying recent advances in recurrent neural networks (RNNs), such as highway connections (Srivastava et al., 2015), (3) using deep encoder with residual connections (He et al., 2016), (4) further extending Syntax Aware Long Short-Term Memory (SA-LSTM) (Qian et al., 2017) for SRL, and (5) introducing the Tree-Structured Long Short-Term Memory (Tree-LSTM) (Tai et al., 2015) to model syntactic information for SRL. In addition, as pointed out by He et al. (2017) for span SRL, the worse syntactic input will hurt performance if the syntactically-driven SRL model trusts syntactic information too much, and high-quality syntax can still make a large impact on SRL, which motivates us to investigate the effect of syntactic quality on dependency SRL. In summary, our major contributions are as follows: • We propose a unified neural framework for dependency SRL to more eff"
D18-1262,C16-1180,1,0.848487,"taking gold syntax as input. It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2"
D18-1262,C18-1271,1,0.638043,"yntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (201"
D18-1262,P16-1113,0,0.272085,"main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units. A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit. This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit. Given a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai 2404 System Local model Lei et al. (2015) FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani et al. (2017) Marcheggiani and Titov (2017) He et al. (2018) Cai et al. (2018) Ours (Syn-GCN) Ours (SA-LSTM) Ours (Tree-LSTM) Global model Bj¨orkelund et al. (2010) FitzGerald et al. (2015) Roth and Lapata (2016) Ensemble model FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani and Titov (2017) et al., 2015): ˜k = h X hk , (1) j∈C(k) ˜ k + b(i) ), ig = σ(W (i) xk + U (i) h fgk,j = σ(W (f ) xk + U (f ) hj + b(f ) ), ˜ k + b(o) ), og = σ(W (o) xk + U (o) h (2) ˜ k + b(u) ), u = tanh(W (u) xk + U (u) h X fgk,j cj , ck = ig u + j∈C(k) hk = og tanh(ck ). where j ∈ C(k), hj i"
D18-1262,K17-1041,0,0.585082,", predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which"
D18-1262,P16-1212,0,0.0372414,"Missing"
D18-1262,W08-2121,0,0.294179,"Missing"
D18-1262,P15-1150,0,0.179857,"Missing"
D18-1262,C16-1295,1,0.90072,"Missing"
D18-1262,P16-2033,0,0.0414918,"ce Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent. into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax a"
D18-1262,W09-1209,1,0.83556,"Missing"
D18-1262,D09-1004,1,0.943973,"Missing"
D18-1262,W09-1208,1,0.896196,"Missing"
D18-1262,W08-2127,1,0.879914,"Missing"
D18-1262,P15-1109,0,0.230713,"of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of"
D18-1262,P15-1112,0,0.0293036,"into a neural SRL model. Qian et al. (2017) proposed SALSTM to model the whole tree structure of dependency relation in an architecture engineering way. Besides, syntax encoding has also successfully promoted other NLP tasks. Tree-LSTM (Tai et al., 2015) is a variant of the standard LSTM that can encode a dependency tree with arbitrary branching factors, which has shown effectiveness on semantic relatedness and the sentiment classification tasks. In this work, we extend the Tree-LSTM with a relation specific gate and employ it to recursively encode the syntactic dependency tree for SRL. RCNN (Zhu et al., 2015) is an extension of the recursive neural network (Socher et al., 2010) which has been popularly used to encode trees with fixed branching factors. The RCNN is able to encode a tree structure with arbitrary number of factors and is useful in a re-ranking model for dependency parsing (Zhu et al., 2015). 2408 In our experiments, we simplify and reformulate the RCNN model. However, the simplified model performs poorly on the development and the test sets. The reason might be that the RCNN model with a single global composition parameter is too simple to cover all types of syntactic relation in a d"
D18-1262,P16-1131,1,0.914273,"Missing"
D18-1262,C18-1153,1,0.786404,"Missing"
D18-1401,P07-1056,0,0.0552003,"g by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a"
D18-1401,D15-1007,0,0.0135061,"darker. Is the sun cream really effective? Answer 2: No, just depending on my own experience. Figure 1: Two examples of QA text pairs from “customer questions & answers” section in Amazon. Introduction Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With th"
D18-1401,P17-1055,0,0.0244919,"ed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. After computing the An"
D18-1401,P11-2104,0,0.019958,"ional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities o"
D18-1401,P10-1041,0,0.015063,"eral, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporati"
D18-1401,P11-1013,0,0.0608791,"Missing"
D18-1401,C04-1200,0,0.274096,"is, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics c"
D18-1401,D15-1180,0,0.0466625,"Missing"
D18-1401,P10-1043,1,0.831516,"Missing"
D18-1401,P15-2005,1,0.840444,"e annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classificatio"
D18-1401,D17-1048,0,0.011926,"et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al."
D18-1401,P14-5010,0,0.00460503,"Missing"
D18-1401,D15-1298,0,0.0617943,"Missing"
D18-1401,W02-1011,0,0.0273117,"[Q-sentence, A-sentence] unit for exploring sentiment information. Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair. Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification. 2 Related Work Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002). In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the searc"
D18-1401,P13-4009,0,0.17484,"Missing"
D18-1401,W06-1652,0,0.0451964,"l 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel"
D18-1401,P15-1098,0,0.01816,"iment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an a"
D18-1401,C14-1053,0,0.0241938,"6). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-s"
D18-1401,D16-1058,0,0.308112,"assification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classificatio"
D18-1401,P15-1102,0,0.0469393,"Missing"
D18-1401,D16-1172,0,0.173147,"ch aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wa"
D18-1401,N16-1174,0,0.313624,"f each word is formed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. Aft"
D18-1401,D16-1021,0,0.0277727,"Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification. To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level. 3 Data Collection and Annotation We collect QA text pairs from “Asking"
D18-1401,P14-1146,0,0.0370401,"their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On o"
D18-1401,P15-1042,0,0.129521,"or Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-style and classical"
D19-1057,W13-2322,0,0.0123005,"the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semant"
D19-1057,W07-1402,0,0.0496309,"d models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, Novemb"
D19-1057,W05-0620,0,0.570923,"Missing"
D19-1057,P81-1022,0,0.476074,"Missing"
D19-1057,D15-1112,0,0.249955,"Missing"
D19-1057,W11-1608,0,0.0274094,"ndencies. For the main architecture of the SRL model, many neural-network-based models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conferen"
D19-1057,W04-2705,0,0.255775,"Missing"
D19-1057,J02-3001,0,0.484379,"onal semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given, either being a constituent tree or a dependency tree, like in the CoNLL shared tasks (Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). 617 Positional Embedding encodes the order of the input word sequence. We follow Vaswani et al. (2017) to use time positional embedding, which is formulated as follows: P E(t, 2i) = sin(t/100002i/d ) P E(t, 2i + 1) = cos(t/100002i/d ) (1) where t is the position, i means the dimension, and d is the dimension of the model input embedding. 3.1.2 Encoder Layer The self-attent"
D19-1057,S15-2153,0,0.0398758,"2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given"
D19-1057,J05-1004,0,0.520095,"ive multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention). We follow their approach of replacing one attention head with the dependency head information, but use a softer way to capture the pairwise relationship between input elements (Shaw et al., 2018). • We present detailed experiments on different aspects of incorporating syntactic information into the SRL model, in what quality, in which representation and how to integrate. For the datasets and annotations of the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and s"
D19-1057,P18-2058,0,0.176608,"Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (20"
D19-1057,D14-1162,0,0.0991902,"onfigurations, and report the labeled precision (P), labeled recall (R) and labeled f-score (F1) for the semantic dependencies. Word Representations Most of our experiments are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the Co"
D19-1057,P17-1044,0,0.649356,"ructural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help to boost the SRL performance further, which is not entirely overlapping with the syntactic information. In summary, the contributions of our work are: Roth and Lapata (2016) utilize an LSTM model to obtain embeddings from the syntactic dependency paths; while Marcheggiani and Titov (2017) construct Graph Convolutional Networks to encode the dependency structure. Although He et al. (2017)’s approach is a pure end-to-end learning, they have included an analysis of adding syntactic dependency information into English SRL in the discussion section. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and Xia et al. (2019) have compared different ways to represent and encode the syntactic knowledge. In another line of research, Tan et al. (2017) utilize the Transformer network for the encoder instead of the BiLSTM. Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-"
D19-1057,N18-1202,0,0.0608331,"are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the CoNLL-09 Chinese dataset. B I AFFINE means the trees are generated by BiaffineParser with pre-trained word embedding on the Gigaword corpus while B IAFFINE B ERT is the sa"
D19-1057,C08-1050,0,0.128118,"al Results on the Chinese Test Data Based on the above experiments and analyses, we present the overall results of our model in this subsection. We train the three models (I NPUT, LISA, and R EL AWE) with their best settings without any external knowledge as C LOSED, and we take the same models with B ERT as O PEN. The D EP PATH &R EL PATH from G OLD without external knowledge serves as the G OLD for reference. Since we have been focusing on the task of argument identification and labeling, for both C LOSED and O PEN, we follow Roth and Lapata (2016) to use existing systems’ predicate senses (Johansson and Nugues, 2008) to exclude them from comparison. Table 7 shows that our O PEN model achieves more than 3 points of f1-score than the stateof-the-art result, and R EL AWE with D EP PATH &R EL PATH achieves the best in both C LOSED and O PEN settings. Notice that our best C LOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pretrained word embeddings. Besides, performance gap between three models under O PEN setting is very small. It indicates that the representation ability of BERT is so powerful and may contains rich syntactic information. At last, the G OLD result"
D19-1057,W17-4305,0,0.0916819,"of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018)"
D19-1057,P16-1113,0,0.641761,"ong with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Str"
D19-1057,D18-1262,1,0.84272,"Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed"
D19-1057,N18-2074,0,0.114208,"e et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (2018), we introduce the Relation-Aware method to incorporate syntactic dependencies, which is a softer way to encode richer structural information. Various experiments for the Chinese SRL on the CoNLL-2009 dataset are conducted to evaluate our hypotheses. From the empirical results, we observe that: 1) The quality of the syntactic information is essential when we incorporate structural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help"
D19-1057,K17-1041,0,0.201985,"For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural mo"
D19-1057,D07-1002,0,0.127289,"icate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly"
D19-1057,D16-1177,0,0.0617731,"Missing"
D19-1057,P14-2120,0,0.015133,"oduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Assoc"
D19-1057,N19-1075,0,0.449868,"rformance for the Chinese SRL task on the CoNLL-2009 dataset. 1 $ $ 中国 China SBJ ROOT 鼓励 encourage 外商 foreign merchant 投资 invest 农业 agriculture COMP COMP COMP Figure 1: An example of one sentence with its syntactic dependency tree and semantic roles. Arcs above the sentence are semantic role annotations for the predicate “鼓励 (encourage)” and below the sentence are syntactic dependency annotations of the whole sentence. The meaning of this sentence is “China encourages foreign merchants to invest in agriculture”. et al. (2017) have observed that only good syntax helps with the SRL performance. Xia et al. (2019) have explored what kind of syntactic information or structure is better suited for the SRL model. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and claim that the syntax-agnostic model surpasses the syntax-aware ones. In this paper, we focus on analyzing the relationship between the syntactic dependency information and the SRL performance. In particular, we investigate the following four aspects: 1) Quality of the syntactic information: whether the performance of the syntactic parser output affects the SRL performance; 2) Representation of the syntactic informati"
D19-1057,D18-1548,0,0.193596,"Missing"
D19-1057,P16-2033,0,0.0392647,"nd assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into tw"
D19-1057,P03-1002,0,0.334907,") is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neur"
D19-1057,P15-1109,0,0.0304126,"the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inje"
D19-1057,W08-2121,0,0.366947,"Missing"
D19-1237,P15-1017,0,0.0203469,"tion model for classification on “age of harasser” put some attention on phrases other than the harasser, and hence aggregated noise. This could explain why the regular BiLSTM model got lower performance than the CNN model. However, when training with key element extractions, it put almost all attention on the harasser “young man” (S2), which helped the model make correct prediction of “young harasser”. When predicting the “type of location” (S3), the joint learning model directed its attention to “micro bus”. CNN Based Models: Since CNN is efficient for capturing the most useful information (Chen et al., 2015), it is quite suitable for the classificaAccuracy Age Macro F1 5.2 Accuracy Single/ Type of Loc Time Multiple Harasser BiLSTM ABiLSTM CNN J-CNN* 90.7 90.0 91.6 91.5 91.1 91.8 92.8 92.7 91.0 91.4 93.2 92.3 80.3 81.6 83.6 82.6 97.0 97.0 97.4 97.2 J-BiLSTM J-ABiLSTM J-SABiLSTM J-CNN J-ACNN J-SACNN 90.8 92.4 92.4 92.5 92.8 92.5 91.8 94.0 93.7 93.8 93.3 93.8 91.3 92.3 92.1 93.3 93.1 92.7 78.3 85.1 84.8 84.2 84.2 83.1 94.8 97.7 97.4 98.0 97.9 98.0 BiLSTM ABiLSTM CNN J-CNN* 90.4 89.7 91.5 91.4 90.3 91.0 92.1 92.0 38.6 33.5 46.7 46.8 44.3 45.5 48.1 45.1 93.8 93.6 94.4 94.5 J-BiLSTM J-ABiLSTM J-SABiLST"
D19-1237,P82-1020,0,0.691108,"Missing"
D19-1237,D18-1303,0,0.172032,"#MeToo and #TimesUp movements, further demonstrate how reporting personal stories on social media can raise awareness and empower women. Millions of people around the world have come forward and shared their stories. Instead of being bystanders, more and more people become up-standers, who take action to protest against sexual harassment online. The stories of people who experienced harassment can be studied to identify different patterns of sexual harassment, which can enable solutions to be developed to make streets safer and to keep women and girls more secure when navigating city spaces (Karlekar and Bansal, 2018). In this paper, we demonstrated the application of natural language processing (NLP) technologies to uncover harassment patterns from social media data. We made three key contributions: 2328 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2328–2337, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 1. Safecity1 is the largest publicly-available online forum for reporting sexual harassment (Karlekar and Bansal, 2018). We annotated about 10"
D19-1237,P15-2060,0,0.0264322,"[(w−l , w−l+1 , ...w0 , ...wl−1 , wl )], where wi (i ∈ [−l, l]) stands for the ith word from w0 . 2330 Figure 2: CNN based Joint learning Model. WL and WR are the left and right context around each word. Figure 3: BiLSM based Joint Learning Model. Here we use an input of five words as an example. Because the context of the two consecutive words in the original text are only off by one position, it will be difficult for the CNN model to detect the difference. Therefore, the position of each word in this context sequence is crucial information for the CNN model to make the correct predictions (Nguyen and Grishman, 2015). That position was embedded as a p dimensional vector, where p is a hyperparameter. The position embeddings were learned at the training stage. Each word in the original text was then converted into a sequence of the concatenation of word and position embeddings. Such sequence was fed into the CNN modules in the first layer of the model, which output the high level word representation (hi , i ∈ [0, n − 1], where n is the number of input words). The high level word representation was then passed into a fully connected layer, to predict the key element type for the word. The CNN modules in this"
D19-1237,P16-2011,0,0.012898,"BiLSTM layer (Figure 3). The aggregation of the outputs was used as document level representation. J-SABiLSTM: Similarly, we experimented with the supervised attention. In all the models, softmax function was used to calculate the probabilities at the prediction step, and the cross entropy losses from extraction and classification tasks were added together. In case of supervised attention, the loss defined in Eq. 5 was added to the total loss as well. We applied the stochastic gradient descent algorithm with mini-batches and the AdaDelta update Rule (rho=0.95 and epsilon=1e-6) (Zeiler, 2012; Feng et al., 2016). The gradients were computed using back-propagation. During training, we also optimized the word and position embeddings. 5 5.1 Experiments and Results Experimental Settings Data Splits: We used the same splits of train, develop, and test sets used by Karlekar and Bansal (Karlekar and Bansal, 2018), with 7201, 990 and 1701 stories, respectively. In this study, we only considered single label classifications. Baseline Models: CNN and BiLSTM models that perform classification and extraction separately were used as baseline models. In classification, we also experimented with BiLSTM with the att"
D19-1237,D15-1309,0,0.0261651,"line stories. Related Work Conventional surveys and reports are often used to study sexual harassment, but harassment on these is usually under-reported (Goldstein, 2018; Griffith, 2018). The high volume of social media data available online can provide us a much larger collection of firsthand stories of sexual harassment. Social media data has already been used to analyze and predict distinct societal and health issues, in order to improve the understanding of wide-reaching societal concerns, including mental health, detecting domestic abuse, and cyberbullying (Balani and De Choudhury, 2015; Schrading et al., 2015; Ziegele et al., 2018; Agrawal and Awekar, 2018). There are a very limited number of studies on 1 https://safecity.in https://github.com/alievent/harassment-analysis Please follow Safecity guidelines for usage. 2 3 Data Collection and Annotation We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal (2018). In addition to the forms of harassment, we manually annotated each sto"
D19-1237,P18-2066,0,0.0134046,"feature vectors weighted by αi . Wω , bω and uw were learned during training. The final representation (v) was passed into one fully connected layer for each classification task. We also applied different attention layers for different classifications, because the classification modules categorize the incident in different dimensions, their focuses vary. For example, to classify “time of day”, one needs to focus on the time phrases, but pays more attention to harassers when classifying “age of harasser”. J-SACNN: To further exploit the information of the key elements, we applied supervision (Zhao et al., 2018) to the attentive pooling layer, with the annotated key element types of the words as ground truth. For instance, in classification of “age of harasser”, the ground truth attention labels for words with key element types of “harasser” are 1 and others are 0. To conform to the CNN structure, we applied convolution to the sequence of ground truth attention labels, with the same window size (w) that was applied to the word sequence (Eq. 4). α∗ (t) = W ◦ [et : et+w−1 ] (4) where ◦ is element-wise multiplication, et is the ground truth attention label, and the W ∈ Rw×1 is a constant matrix with all"
D19-1560,D16-1011,0,0.0517837,"ground-truth sentiment rating for aspect xaspect . δ The model will assign a reward score to each seis a L2 regularization. quence according to the designed scores function, and then estimates b(τ h ) as the average of those 3 Experimentation rewards. Similarly, the policy gradient w.r.t. θl of 3.1 Experimental Settings low-level policy is given by, ∇θl J(θ ) = Eτ l ∼πl [ l ki  Rl ∇θl log π l (ai,j |sli,j ; θl )] j=1 (8) Data. We conduct our experiments on three public datasets on DASC, i.e., TripUser (Li et al., 2018), TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016). In the experiment, we adopt Discourse Segmentation 5585 TripUser Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ SVM 46.35† 1.025† LSTM 53.23 0.787 52.74 0.794 MAMC 55.49† 0.583† HARN 58.15† 0.528† HUARN 60.70† 0.514† C-HAN 58.49 0.602 57.38 0.543 HS-LSTM 59.75 0.566 59.01 0.524 RL-Word-Selection 60.15 0.475 59.55 0.519 RL-Clause-Selection 61.32 0.433 60.54 0.461 HRL 62.97 0.336 62.84 0.351 Approaches TripAdvisor Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ 34.30‡ 1.982‡ 35.26‡ 1.963‡ 43.85‡ 1.525‡ 44.02‡ 1.470‡ 46.21‡ 1.091‡ 46.56‡ 1.083‡ 48.21‡ 0.923‡ 47.61 0.914 47.08 0.955 48.45 0.947 46.84 1.013 48.55"
D19-1560,D15-1167,0,0.042799,".8; λ1 , λ2 and λ3 are 0.25, 0.25 and 0.5 respectively. λ1 , λ2 are 0.6 and 0.4. Additionally, the batch size is set to be 64, regularization weight is set to be 10−5 and the dropout rate is 0.2. Evaluation Metrics. The performance is evaluated using Accuracy (Acc.) and MSE as Yin et al. (2017). Moreover, t-test is used to evaluate the signiﬁcance of the performance difference between two approaches (Yang and Liu, 1999). Baselines. We compare HRL with the following baselines: 1) SVM (Yin et al., 2017). This approach only adopts unigram, bigram as features to train an SVM classiﬁer. 2) LSTM (Tang et al., 2015). This is a neural network approach to document-level sentiment classiﬁcation which employs gated LSTM to learn text representation. 3) MAMC (Yin et al., 2017). This approach employs hierarchical iterative attention to learn aspect-speciﬁc representation. This is a state-of3 http://alt.qcri.org/tools/discourse-parser/ the-art approach to DASC. 4) HARN (Li et al., 2018). This approach adopts hierarchical attention to incorporate overall rating and aspect information so as to learn aspect-speciﬁc representation. This is another state-of-the-art approach to DASC. 5) HUARN (Li et al., 2018). This"
D19-1560,C18-1079,0,0.214585,"t classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating for a speciﬁc asp"
D19-1560,D16-1021,0,0.142468,"Missing"
D19-1560,D16-1127,0,0.103263,"Missing"
D19-1560,P10-1043,1,0.876096,"oach to DASC over the state-of-the-art baselines. 1 is a little uncomfortable .]Clause3 [I’m often nitpicking for room decoration.]]Clause4 [ Besides, the price is very expensive ] Clause5 [ although the staff service is professional .]]Clause6 Rating of Each Aspect - location: # # # # # (5) - value: # $ $ $ $ (1) Figure 1: An example of a review document, where clauses and words with different colors refer to different aspects. Introduction Document-level Aspect Sentiment Classiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - ser"
D19-1560,P08-1036,0,0.0746941,"assiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate th"
D19-1560,D16-1058,0,0.252027,"Missing"
D19-1560,D17-1217,0,0.61734,"e-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating"
D19-1560,C18-1074,0,0.0316687,"ect sentiment-relevant words and discard those irrelevant and noisy words. For instance, for aspect location, words “this”, “is” in Clause1 are noisy words and should be discarded since they make no contribution to implying the sentiment rating. One possible way to alleviate this problem is to also leverage the soft-attention mechanism as proposed in Li et al. (2018). However, this soft-attention mechanism may induce additional noise and lack interpretability because it tends to assign higher weights to some domain-speciﬁc words rather than real sentiment-relevant words (Mudinas et al., 2012; Zou et al., 2018). For instance, this soft-attention mechanism tends to regard the name of a hotel “Hilton” with a good reputation in Clause3 as a positive word which could mislead the model into assigning a higher rating to aspect room. Therefore, a well-behaved approach should highlight sentiment-relevant words and discard noisy words for a speciﬁc aspect during model training. In this paper, we propose a Hierarchical Reinforcement Learning (HRL) approach with a highlevel policy and a low-level policy to address the above two challenges in DASC. First, a highlevel policy is leveraged to select aspect-relevan"
D19-1640,P00-1031,0,0.249953,"il. 3.1 Chinese Character Variation Graph A Chinese character variation graph3 can be denoted as G = (C, R). C denotes the Chinese character set, and each character is represented as a vertex in G. R denotes the variation relation (edge) set, and edge weight is the similarity of two characters given the target relation (variation) type. To accurately characterize both phonetic and glyph information of Chinese character, we utilize three different encoding methods: Pinyin system provides phonetic-based information, which is widely used for representing the pronunciations of Chinese characters (Chen and Lee, 2000). In this system, each Chinese character has one syllable which consists of three components: an initial (consonant), a final (vowel), and a tone. There are four types of tones in Modern Standard Mandarin Chinese. Different tones with the same syllable can have different meanings. For instance, the pinyin code of “裸 (naked)” is “luo3” and “锣 (gong)” is ‘luo2”. The pinyin-based variation similarity is calculated based on their pinyin syllables with tones4 . Stroke is a basic glyph pattern for writing Chinese character (Cao et al., 2018). All Chinese characters are written in a certain stroke or"
D19-1640,D14-1162,0,0.100746,"ation graph is constructed for encapsulating the glyph and phonetic relationships among Chinese characters. Since the graph can be potentially useful for other NLP tasks, we share the graph/embeddings to motivate further investigation. 4. Through the extensive experiments on both SMS and review datasets2 , we demonstrate the efficacy of the proposed method for Chinese spam detection. The proposed method outperforms the state-of-the-art models. 2 Related Work Neural Word Embeddings. Unlike traditional word representations, low-dimensional distributed word representations (Mikolov et al., 2013; Pennington et al., 2014) are able to capture indepth semantics of text content. More recently, ELMo (Peters et al., 2018) employed learning functions of the internal states of a deep bidirectional language model to generate the character embeddings. BERT (Devlin et al., 2018) utilized bidirectional encoder representations from transformers (Vaswani et al., 2017) and achieved improvements for multiple NLP tasks. However, all the prior models only focused on learning the context, whereas the text variation was ignored. Moreover, CSVD problem can be different from other NLP tasks: the intentional character mutations and"
D19-1640,N18-1202,0,0.647153,"cters. Since the graph can be potentially useful for other NLP tasks, we share the graph/embeddings to motivate further investigation. 4. Through the extensive experiments on both SMS and review datasets2 , we demonstrate the efficacy of the proposed method for Chinese spam detection. The proposed method outperforms the state-of-the-art models. 2 Related Work Neural Word Embeddings. Unlike traditional word representations, low-dimensional distributed word representations (Mikolov et al., 2013; Pennington et al., 2014) are able to capture indepth semantics of text content. More recently, ELMo (Peters et al., 2018) employed learning functions of the internal states of a deep bidirectional language model to generate the character embeddings. BERT (Devlin et al., 2018) utilized bidirectional encoder representations from transformers (Vaswani et al., 2017) and achieved improvements for multiple NLP tasks. However, all the prior models only focused on learning the context, whereas the text variation was ignored. Moreover, CSVD problem can be different from other NLP tasks: the intentional character mutations and unseen variations (zero-shot learning (Socher et al., 2013)) can threaten prior models’ performa"
D19-1640,D17-1025,0,0.294144,"l., 2017) and achieved improvements for multiple NLP tasks. However, all the prior models only focused on learning the context, whereas the text variation was ignored. Moreover, CSVD problem can be different from other NLP tasks: the intentional character mutations and unseen variations (zero-shot learning (Socher et al., 2013)) can threaten prior models’ performances. Chinese Word and Sub-word Embeddings. A number of studies explored Chinese representation learning methodologies. CWE (Chen et al., 2015) learned the character and word embeddings to improve the representation performance. GWE (Su and Lee, 2017) introduced the features extracted from the images of traditional Chinese characters. JWE (Yu et al., 2017) used deep learning to generate character embedding based on an extended radical collection. Cw2vec (Cao et al., 2018) in2 1. We propose an innovative CSVD problem, in the context of text spam detection, to address the In order to help other scholars reproduce the experiment outcome, we will release the datasets via GitHub (https://github.com/Giruvegan/stoneskipping) 6188 vestigated Chinese character as a sequence of ngram stroke order to generate its embedding. Although these models had"
D19-1640,W14-6822,0,0.0306223,"graph embedding with human defined metapath rules. HEER (Shi et al., 2018) is a recent state-of-the-art heterogeneous graph embedding model. Though the techniques utilized in these models are different, most existing graph embedding models focus more on local graph structure representation, e.g., modelling of a fixed-size graph neighbourhood. CSVD problem requires graph embedding conducted from a more global perspective, to characterize comprehensive variation patterns. Spelling Correction. Spelling correction may serve as an alternative to address CSVD problem, e.g., using dictionary-based (Yeh et al., 2014) or language model-based method (Yu and Li, 2014) to restore the content variations to their regular format. However, because spammers intentionally mutate the spam text to escape from the detection model, training data sparseness and dynamics may challenge this approach. 3 StoneSkipping Model Figure 2 depicts the proposed SS model. There are three core modules in SS: a Chinese character variation graph to host the heterogeneous variation information; a variation family-enhanced graph embedding for Chinese character variation knowledge extraction and graph representation learning; an enhanced"
D19-1640,D17-1027,0,0.43686,"arning the context, whereas the text variation was ignored. Moreover, CSVD problem can be different from other NLP tasks: the intentional character mutations and unseen variations (zero-shot learning (Socher et al., 2013)) can threaten prior models’ performances. Chinese Word and Sub-word Embeddings. A number of studies explored Chinese representation learning methodologies. CWE (Chen et al., 2015) learned the character and word embeddings to improve the representation performance. GWE (Su and Lee, 2017) introduced the features extracted from the images of traditional Chinese characters. JWE (Yu et al., 2017) used deep learning to generate character embedding based on an extended radical collection. Cw2vec (Cao et al., 2018) in2 1. We propose an innovative CSVD problem, in the context of text spam detection, to address the In order to help other scholars reproduce the experiment outcome, we will release the datasets via GitHub (https://github.com/Giruvegan/stoneskipping) 6188 vestigated Chinese character as a sequence of ngram stroke order to generate its embedding. Although these models had considered the nature of Chinese characters, they only utilized glyph features while the phonetic informati"
D19-1640,W14-6835,0,0.405918,". HEER (Shi et al., 2018) is a recent state-of-the-art heterogeneous graph embedding model. Though the techniques utilized in these models are different, most existing graph embedding models focus more on local graph structure representation, e.g., modelling of a fixed-size graph neighbourhood. CSVD problem requires graph embedding conducted from a more global perspective, to characterize comprehensive variation patterns. Spelling Correction. Spelling correction may serve as an alternative to address CSVD problem, e.g., using dictionary-based (Yeh et al., 2014) or language model-based method (Yu and Li, 2014) to restore the content variations to their regular format. However, because spammers intentionally mutate the spam text to escape from the detection model, training data sparseness and dynamics may challenge this approach. 3 StoneSkipping Model Figure 2 depicts the proposed SS model. There are three core modules in SS: a Chinese character variation graph to host the heterogeneous variation information; a variation family-enhanced graph embedding for Chinese character variation knowledge extraction and graph representation learning; an enhanced bidirectional language model for joint representa"
D19-5008,S17-2006,0,0.0135569,"text, we hypothesize that the KB-based approach would be especially helpful for the rumor early detection. As a starting point, the initial research effort can focus on the topic areas of popular rumors, and the approaches that are already effective in fake news detection can be explored first. We think how effective KBs can help in rumor detection and how we can integrate it with other social context information will be an interesting research topic. Rumor Detection Contests There are two contests for rumor detection: 1. SemEval-2017 Task 8: Determining rumor veracity and support for rumors (Derczynski et al., 2017). The approach from (Enayet and El-Beltagy, 2017) was ranked No. 1 for the rumor detection task. 2. SemEval-2019 Task 7: Determining rumor veracity and support for rumors (Gorrell et al., 2019). The approach from (Li et al., 2019a) was ranked No. 1 for the rumor detection task. The datasets used in these two tasks are listed in Table 1. Both (Enayet and El-Beltagy, 2017) and (Li et al., 2019a) exploited content, user and propagation information. They also utilized user stance directly in their models. The main difference between them are that Li et al. (2019a) used neural networks, while Enaye"
D19-5008,S17-2082,0,0.122067,"Missing"
D19-5008,P19-1113,1,0.890337,"Missing"
D19-5008,P17-1066,0,0.143587,"2nd Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda, pages 66–75 c Hong Kong, China, November 4, 2019 2019 Association for Computational Linguistics Dataset PHEME-R PHEME Ma-Twitter Ma-Weibo Total rumors Text (claims) 330 y 6425 y 992 y 4,664 y User info Time Propagati stamp on info y y y y y y y y y y Platform Description Twitter Twitter Twitter Weibo Tweets from [Zubiaga et al., 2016] Tweets from [Kochkina et al., 2018] Tweets from [Ma et al., 2016] Weibo data from [Ma et al., 2016] Tweets from [Liu et al., 2015; Ma et Twitter al.,2016] Twitter Tweets from [Ma et al., 2017b] Facebook data from [Silverman et al., Facebook 2016] Twitter, Reddit SemEval 2019 Task 7 data set. Twitter15 1,490 y y y y Twitter16 818 y y y y BuzzFeedNews 2,282 y SemEval19 Kaggle Emergent Kaggle Snopes 325 y y y y 2145 y Twitter, Facebook Kaggle rumors based on Emergent.info 16.9K y Facebook Hoax 15.5K y y y Twitter, Facebook Kaggle rumors based on Snopes.com Facebook data from [Tacchini et al., Facebook 2017] 2923 y y y y Twitter Kaggle rumors based on PolitiFact 23,196 y y y y Twitter Dataset from [Shu et al., 2019], enhanced from PolitiFact and GossipCop Kaggle PolitiFact FakeNewsNet"
D19-5008,D11-1147,0,0.146551,"do not have 69 clarity about the veracity of a rumor, they usually just spread it without adding their opinions; credible users are less likely to support rumors, while low credibility accounts provide the most support; in terms of supporting or debunking a rumor, credible users are much more stringent, and hence a more trustworthy source than their corresponding counterparts. Hand-crafted user features like registration age of users, number of followers, the number of posts the user had authored, and the like, are leveraged along with other textual and propagation features in Castillo et al. (2011) and other studies (Liu et al., 2015; Enayet and El-Beltagy, 2017; Li et al., 2019a; Li et al., 2019b). Liu and Wu (2018) construct user representations using network embedding approaches on the social network graph. There has been evidence that lots of rumors come from either fake news websites or hyperpartisan websites (Silverman, 2016; Li et al., 2016; Liu et al., 2015). 3.3 the feature-based algorithms. But we should keep in mind that we usually do not have much propagation information at the early stage of a rumor spread, and early detection is especially critical for a real-time rumor de"
I17-4006,I17-4006,1,0.0512975,"Missing"
I17-4006,J90-1003,0,0.335624,"Missing"
I17-4006,W04-1213,0,0.705905,"Missing"
I17-4006,W16-4906,0,0.606715,"Missing"
I17-4006,W15-4401,0,0.307126,"Missing"
I17-4006,W13-3601,0,0.190158,"Missing"
I17-4006,W14-1701,0,0.133707,"Missing"
I17-4006,C12-1184,0,0.22298,"Missing"
I17-4006,W16-4907,0,0.287363,"Missing"
I17-4016,D14-1162,0,0.0939384,"ckage. Skip-gram (Mikolov et al., 2013) methods are explored. • CLU: Cluster feature of word. We use Kmeans to obtain 300 clusters with the word embedding trained with cw2vec as in section 3 and then represent the word cluster by one hot vector of 300 dimensions. These CWE models are trained with window size of 5, 15 iterations, 5 negative examples, minimum word count of 10, Skip-Gram (Mikolov et al., 2013) with starting learning rate of 0.025 , the output word vectors are of 300 dimensions. • POS: Part-of-speech-tagging (POST) of words containing verb, adverb, adjective, noun. • GloVe GloVe (Pennington et al., 2014) is an unsupervised learning algorithm for obtaining vector representations for words. • VA: Words valence value used in arousal model training, and vice versa. The feature is represented by a one-dimension vector normalized to 1. The GloVe model is trained with window size of 8, minimum word count of 5 and maximum iteration of 20, the output word vectors are of 300 dimensions. • POL: The polarity of word in NTUSD sentiment lexicon dictionary. The polarity of word is either positive of negative. The NTUSD sentiment dictionary is available at http://academiasinicanlplab. github.io/ • Character-"
I17-4016,N16-1066,0,0.026759,"e sharing. 1. Chinese Wikipedia dump with time stamp of 2017-07-20. There are over 1.3 million articles. Download link is at https://dumps.wikimedia.org/ zhwiki/20170720/ 2. Several forums dump (hot, boy-girl, movie, etc) from Taiwan online discussion board https://www.ptt.cc/bbs. There are around 70,000 articles. In addition to the main body, there are also 10 to 20 user comments in each article. Introduction The task is to predict the affective states of a given (traditional) Chinese word in a continuous numerical value (score from 1 to 9) in the two-dimensional valence-arousal (V-A) space (Yu et al., 2016), indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal, which is the same as 2016s task. And in addition, predict the affective states of a given (traditional) Chinese phrase in the same V-A space. A human-tagged training data set containing 2802 words and 2250 phrases 3. Liberty News Times articles from 2016-0101 to 2017-08-15. We have used several subboards include Focus, Politics, Society, Local, Movie and Sports. There are around 100,000 articles. All corpus is normalized to simplified Chinese characters before input into wor"
N07-1066,ko-etal-2006-exploiting,1,0.892666,"input variables. Logistic P (correct(Ai )|Q, A1 , ..., An ) (1) ≈ P (correct(Ai )|val1 (Ai ), ..., valK1 (Ai ), sim1 (Ai ), ..., simK2 (Ai )) K2 K1 P P λk simk (Ai )) βk valk (Ai ) + exp(α0 + k=1 k=1 = K2 K1 P P λk simk (Ai )) βk valk (Ai ) + 1 + exp(α0 + k=1 k=1 where, simk (Ai ) = N X sim0k (Ai , Aj ). j=1(j6=i) ~ ~λ = argmax α ~ , β, ~ ~λ α ~ ,β, Nj R X X logP (correct(Ai )|val1 (Ai ), ..., valK1 (Ai ), sim1 (Ai ), ..., simK2 (Ai )) (2) j=1 i=1 regression has been successfully employed in many applications including multilingual document merging (Si and Callan, 2005). In our previous work (Ko et al., 2006), we showed that logistic regression performed well in merging three resources to validate answers to location and proper name questions. We extended this approach to combine multiple similarity features with multiple answer validation features. The extended framework estimates the probability that an answer candidate is correct given the degree of answer correctness and the amount of supporting evidence provided in a set of answer candidates (Equation 1). In Equation 1, each valk (Ai ) is a feature function used to produce an answer validity score for an answer candidate Ai . Each sim0k (Ai ,"
N07-1066,N03-1022,0,0.0446566,"there are redundant answers (“Shanghai”, as above) or several answers which represent a single instance (e.g. “Clinton, Bill” and “William Jefferson Clinton”) in the candidate list, how much should we boost the answer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies"
N07-1066,buscaldi-rosso-2006-mining,0,\N,Missing
N18-2056,P17-1178,0,0.0262044,"with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The gazetteer is derived from the word-entity statistics from (Pan et al., 2017). More specifically, each coordinate of the entity embedding is the probability distribution of a word occurring as the corresponding entity type. 3.1.2 Domain Adaption To explore external datasets, we apply MT BiLSTM-CRF with domain adaptions, as illustrated in Figure 1(b). The fully connection layer are adapted to different datasets. The CRF features are computed separately, i.e. φT (x) = GT · h, φS (x) = GS · h for target and source dataset respectively. The loss function p(y|x; θT ) and p(y|x; θS ) are optimized in alternating order. 3 Approach This section describes the baseline model use"
N18-2056,D14-1162,0,0.0813453,"n general. Table 3 presents the performance impact of knowledge based constrained decoding. It is worth noting that the performance gain in the Chinese language is more limited in comparison with English. The primary reason behind this is that the English Wikipedia site is more comprehensive than its Chinese counterpart. Constrained decoding does not change the NOM performance because only name mentions are included in the knowledge base. Baseline The baseline is a BiLSTM-CRF model with word and character embeddings which simply combines source and target data as training data. GloVe vectors (Pennington et al., 2014) are used as word embeddings. NAM and NOM models are trained separately with individually tuned parameters. 4.3 NOM 0.587 0.626 0.634 0.305 0.351 0.364 Table 2: Effectiveness of Multi-Task Data Selection (MTDS). (Song et al., 2015) entity annotations as source datasets. It is worth noting that annotation guidelines are different from one dataset to another, especially for nominal entity annotations. 4.2 NAM 0.842 0.842 0.842 0.851 0.851 0.851 Results First, we examine the performance impact of entity embedding. As shown in Table 1, entity embedding is very useful for both NAM and NOM predictio"
N18-2056,D15-1058,0,0.259132,"al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The gazetteer is derived from the word-entity statistics from (Pan et al., 2017). More specifically, each coordinate of the entity embedding is the proba"
N18-2056,W15-0812,0,0.0237704,"s that the English Wikipedia site is more comprehensive than its Chinese counterpart. Constrained decoding does not change the NOM performance because only name mentions are included in the knowledge base. Baseline The baseline is a BiLSTM-CRF model with word and character embeddings which simply combines source and target data as training data. GloVe vectors (Pennington et al., 2014) are used as word embeddings. NAM and NOM models are trained separately with individually tuned parameters. 4.3 NOM 0.587 0.626 0.634 0.305 0.351 0.364 Table 2: Effectiveness of Multi-Task Data Selection (MTDS). (Song et al., 2015) entity annotations as source datasets. It is worth noting that annotation guidelines are different from one dataset to another, especially for nominal entity annotations. 4.2 NAM 0.842 0.842 0.842 0.851 0.851 0.851 Results First, we examine the performance impact of entity embedding. As shown in Table 1, entity embedding is very useful for both NAM and NOM prediction tasks, and for both languages. It provides an overall performance improvement of 2.2 F1 points. Since the entity embeddings are derived from soft gazetteer features, this experiment confirms again the usefulness of gazetteer even"
N18-2056,P07-1034,0,0.00993192,"sk model with domain adaptions. apple is more likely to be a ORG when it occurs in the same discussion forum with Apple Inc. 2 Related Works p(y|x; θ) = There are many works in literature applying neural networks to ER problems (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Peng and Dredze, 2016). The baseline model of this work is mostly closed to (Yang et al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embeddin"
N18-2056,D17-1018,0,0.0228023,"duction Entity Recognition (ER) is a fundamental task in Natural Language Processing (NLP). The task includes named entity recognition and nominal entity recognition. ER is the building blocks for higher level applications such as natural language understanding, question answering, machine reading comprehension, etc. They are usually treated as sequence labeling problems. Although the topics have been studied extensively for the past several decades, development of neural network and deep learning based methods in recent years (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Kenton Lee and Zettlemoyer, 2017; Xinchi Chen, 2017) significantly improves the previous state-of-the-art. ∗ Multi-Task Data Selection To ensure homogeneity between source and target training data, adaptive training data selection is applied to source data during multi-task learning, to filter out instances with different distribution and misaligned annotation guideline. Data selection is interleaved with model training iteratively, and this training process terminates until convergence. Constrained Decoding using Knowledge Base Knowledge-based constraints are enforced at decoding time. The goal is to capture document level"
N18-2056,P17-1114,0,0.0260452,"chitecture for ER is BiLSTM-CRF (Lample et al., 2016). The architecture has been shown to achieve best performance on many sequence labeling tasks. In addition, the architecture can be easily extended to model different sources of training data. In real world applications, it is important to include external data sources for model training, because using only domain-specific data for training is usually not enough to achieve best performance. For example, in the case of KBP 2016 tracks, both the 1st and the 2nd teams (ranking in the NERC evaluation) use external data source (Liu et al., 2016; Xu et al., 2017) for model training. The challenge here is to transfer knowledge from external data source to target data source. Multi-Task (MT) BiLSTM-CRF architecture (Yang et al., 2017) is designed for this knowledge transfer. In this work, we develop an ER model based on the MT BiLSTM-CRF architecture, with additional entity embeddings and domain adaption. Two novel methods are proposed to further improve the model performance. Entity recognition is a widely benchmarked task in natural language processing due to its massive applications. The state-of-the-art solution applies a neural architecture named B"
N18-2056,H93-1052,0,0.195758,"ple et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Peng and Dredze, 2016). The baseline model of this work is mostly closed to (Yang et al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The g"
P18-1252,P16-1231,0,0.0506344,"Missing"
P18-1252,D12-1133,0,0.0569982,"Missing"
P18-1252,P12-1071,1,0.896274,"or boosting parsing performance. Though under different linguistic theories or annotation guidelines, the treebanks are painstakingly developed to capture the syntactic structures of the same language, thereby having a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treeb"
P18-1252,D14-1082,0,0.0998588,"we propose two simple yet effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre"
P18-1252,P16-1033,1,0.654377,"iSeqLSTM at wk , denoted as hseq k , is fed into two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task"
P18-1252,P07-1033,0,0.462705,"Missing"
P18-1252,P15-1033,0,0.104829,"t effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre (2012) by 0.97 (93."
P18-1252,C16-1002,0,0.486022,"ing a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1"
P18-1252,N13-1013,0,0.0187158,"of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1, the goal of this work is to convert the under tree that follows the HIT-CDT guideline (Che et al., 2012) into the upper one that follows our new guideline. However, due to the lack 2 Johansson (2013) applies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion"
P18-1252,P13-2105,0,0.0200075,"treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li et al. (2012) to th"
P18-1252,I17-1007,0,0.0141555,"two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task learning. We treat the source-side and target-side"
P18-1252,P16-1105,0,0.141086,"is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, which is consistent with Miwa and Bansal. We did not implement the in-between subtree treeLSTM. 2710 score(i ← j) consistent: i ← j h↑a wa grand: i ← k ← j sibling: i ← k → j reverse: i → j Biaffine wi h↓i reverse grand: i → k → j e"
P18-1252,P09-1006,0,0.036991,"pplies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofi"
P18-1252,D14-1162,0,0.079859,"Missing"
P18-1252,C14-1026,0,0.0706607,"Missing"
P18-1252,P15-1150,0,0.0608474,"←j rpat ⊕ eli ⊕ elj ⊕ ela i←j = e (3) Through rpat i←j , the extended word representaH tions, i.e., rD i,i←j and rj,i←j , now contain the structural information of wi and wj in dsrc . The remaining parts of the biaffine parser is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, w"
P18-1252,telljohann-etal-2004-tuba,0,0.122222,"Missing"
P18-1252,L16-1034,0,0.0229828,"Missing"
P18-1252,P15-1117,0,0.081965,"Missing"
P18-1252,P11-2126,0,0.0209418,"2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li e"
P19-1113,S17-2082,0,0.242504,"Missing"
P19-1113,D17-1134,0,0.0345098,"They studied information credibility and various features. Stance classification is also an active research area that has been studied in previous work (Ranade et al., 2013; Chuang and Hsieh, 2015; Lukasik et al., 2016; Zubiaga et al., 2016; Kochkina et al., 2017). Several studies have employed neural networks on rumor verification (Ma et al., 2016; Kochkina et al., 2017; Ma et al., 2017), and they mainly focus on analyzing the information propagation structure. Multi-task learning has been used in various NLP tasks, including rumor verification (Collobert et al., 2011; Aguilar et al., 2017; Lan et al., 2017; Ma et al., 2018a; Kochkina et al., 2018). Kochkina et al. (2018) proposed a multi-task method without task specific layer for rumor verification. MT-ES is a multi-task approach using Gated Recurrent Unit (GRU) (Cho et al., 2014) with a task specific layer for each task (Ma et al., 2018a). MT-ES has no attention mechanism, and it does not use user information. Ma et al. (2018b) proposed a model based on tree-structured recursive neural networks. 3 3.1 The Proposed Model The Multi-task Network Structure Figure 1 presents the high-level structure of our proposed multi-task learning approach. Th"
P19-1113,P16-2064,0,0.0314265,"ning network, in this study, we focus on the main task, rumor detection, so the experiments are conducted for evaluating the performance of rumor detection. Our experiments show that our approach outperforms the state-of-the-art methods. 2 Related Studies Many existing algorithms (Liu et al., 2015; Wu et al., 2015; Yang et al., 2012) for debunking rumors followed the work of Castillo et al. (2011). They studied information credibility and various features. Stance classification is also an active research area that has been studied in previous work (Ranade et al., 2013; Chuang and Hsieh, 2015; Lukasik et al., 2016; Zubiaga et al., 2016; Kochkina et al., 2017). Several studies have employed neural networks on rumor verification (Ma et al., 2016; Kochkina et al., 2017; Ma et al., 2017), and they mainly focus on analyzing the information propagation structure. Multi-task learning has been used in various NLP tasks, including rumor verification (Collobert et al., 2011; Aguilar et al., 2017; Lan et al., 2017; Ma et al., 2018a; Kochkina et al., 2018). Kochkina et al. (2018) proposed a multi-task method without task specific layer for rumor verification. MT-ES is a multi-task approach using Gated Recurrent Un"
P19-1113,S16-1003,0,0.132786,"Missing"
P19-1113,D18-1003,0,0.0458682,"Missing"
P19-1113,D11-1147,0,0.651997,"always pose authentic information. Rumors sometimes may spread quickly over these platforms, and they usually spread fear or hate. Therefore, rumor detection and verification has gained great interest recently. Social media platforms and government authorities are also taking great efforts to defeat the negative impacts of rumors. Rumor Detection: Rumor definition varies over different publications. The lack of consistency makes it difficult to do a head-tohead comparison between existing methods. In this paper, a rumor is defined as a statement whose truth value is true, unverified or false (Qazvinian et al., 2011). When a rumor’s veracity value is false, some studies call it “false rumor” or “fake news”. However, many previous studies give “fake news” a stricter definition: fake news is a news article published by a news outlet that is intentionally and verifiably false (Shu et al., 2017; Zubiaga et al., 2018). The focus of this study is rumor on social media, not fake news. There are also different rumor detection is defined as determining if a story or online post is a rumor or non-rumor (i.e. a real story, a news article), and the task of determining the veracity of a rumor (true, false or unverifie"
P19-1113,W13-4008,0,0.0168464,"detection is included in the multi-task learning network, in this study, we focus on the main task, rumor detection, so the experiments are conducted for evaluating the performance of rumor detection. Our experiments show that our approach outperforms the state-of-the-art methods. 2 Related Studies Many existing algorithms (Liu et al., 2015; Wu et al., 2015; Yang et al., 2012) for debunking rumors followed the work of Castillo et al. (2011). They studied information credibility and various features. Stance classification is also an active research area that has been studied in previous work (Ranade et al., 2013; Chuang and Hsieh, 2015; Lukasik et al., 2016; Zubiaga et al., 2016; Kochkina et al., 2017). Several studies have employed neural networks on rumor verification (Ma et al., 2016; Kochkina et al., 2017; Ma et al., 2017), and they mainly focus on analyzing the information propagation structure. Multi-task learning has been used in various NLP tasks, including rumor verification (Collobert et al., 2011; Aguilar et al., 2017; Lan et al., 2017; Ma et al., 2018a; Kochkina et al., 2018). Kochkina et al. (2018) proposed a multi-task method without task specific layer for rumor verification. MT-ES is"
P19-1141,N13-1006,0,0.176139,"Missing"
P19-1141,D15-1141,0,0.026907,"hese coefficients are used to define the amount of contribution from each type of structural information (the gazetteers and the character sequence) for our task. In our model, an adapted GGNN architecture is utilized to learn the node representations. The (0) initial state hv of a node v is defined as follows:  W g (v) v ∈ Vs ∪ Ve (0) hv = c > bi > > [W (v) , W (v) ] v ∈ Vc (1) where W c and W g are lookup tables for the character or the gazetteer the node represents. In the case of character nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task (Chen et al., 2015). The structural information of the graph is stored in the adjacency matrix A which serves to retrieve the states of neighboring nodes at each step. To adapt to the multi-digraph structure, A is extended to include edges of different labels, A = [A1 , ..., A|L |]. The contribution coefficients are transformed into weights of edges in A: [wc , wg1 , . . . , wgm ] = σ([αc , αg1 , . . . , αgm ]) (2) Edges of the same label share the same weight. Next, the hidden states are updated by GRU. The basic recurrence for this propagation network is: H= (t−1) [h1 (t−1) , . . . , h|V |]> (3) > > > a(t) v ="
P19-1141,Q16-1026,0,0.117445,"ed of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually created by previous studies (Zamin and Oxley, 2011). Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase (Bollacker et al., 2008)) or commercial data sources (e.g., product catalogues of e-commence websites). While such background knowledge can be helpful, in practice the gazetteers may also contain irrelevant and even erroneous information which harms the system’s performance (Chiu and Nichols, 2016). This is especially the case for Chinese NER, where enormous errors can be introduced due to wrongly matched entities. Chinese language is inherently ambiguous since the granularity of words is less well defined than other languages (such as English). Thus massive wrongly matched entities can be generated with the use of gazetteers. As we can see from the example shown in Figure 1, matching a simple 9-character sentence with 4 gazetteers may result in 6 matched entities, among which 2 are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existin"
P19-1141,N16-1030,0,0.165921,"s maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or heuristics, we believe it is possible to take a data-driven approach here to learn how to combine gazetteer knowledge. To this end, we propose a novel multi-digraph structure which can explicitly model the interaction of the characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conflicts based on contextual information. We summarize our contributions as follows: 1) we propose a novel multi-digraph model to learn how to combine the gazetteer information and to resolve conflicting matches in learning with contexts. To the best of our knowledge, we are the first neural approach to NER that models the gazetteer information with a graph structure; 2) experimental results show that our model significantly outperforms previous methods of using gazetteers"
P19-1141,W06-0115,0,0.622162,"through adjacent nodes. Equations 5, 6, 7, and 8 combine the information from adjacent nodes and the current hidden state of the nodes to compute the new hidden state at time step t. After T steps, we have (T ) our final state hv for the node v. BiLSTM-CRF. The learned feature representa(T ) tions of characters {hv |v ∈ Vc } are then fed to a standard BiLSTM-CRF following the character order in the original sentence, to produce the output sequence. 3 3.1 Experiments Experimental Setup Dataset. The three public datasets used in our experiments are OntoNotes 4.0 (Weischedel et al., 2010), MSRA (Levow, 2006), and Weibo-NER (Peng and Dredze, 2016). OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as Che et al. (2013) and Peng and Dredze (2016) on OntoNotes and on WeiboNER. To demonstrate the effectiveness of our model in the e-commerce domain, we further constructed a new dataset by crawling and manually annotating the NEs of two types, namely PROD (“products”) and BRAN (“brands”). We name our dataset as “E-commerce-NER”. The NER task in the e-commerce domain is more challenging. The NEs of interest are usually the n"
P19-1141,W09-1119,0,0.126111,"proach based on graph neural networks with a multidigraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches. 1 PER2 PER2 Three At Zhang San PER1 LOC1 North Capital Human Beijing Zhang Sanzai The actual translation: Zhang San is at the Beijing People’s Park LOC1 People Public LOC2 Park People’s Park Beijing citizen Wrong matches Correct matches Figure 1: Example of Entity Matching Introduction Previous work (Ratinov and Roth, 2009) shows that NER is a knowledge intensive task. Background knowledge is often incorporated into an NER system in the form of named entity (NE) gazetteers (Seyler et al., 2018). Each gazetteer is typically a list containing NEs of the same type. Many earlier research efforts show that an NER model can benefit from the use of gazetteers (Li et al., 2005). On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually cre"
P19-1141,E14-4016,0,0.0265063,"are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, Qi et al. (2019) defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence (Shang et al., 2018), or maximum matching with rules (Sassano, 2014). Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure 1, 1462 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462–1467 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or heuristics, we believe it is possible to take a da"
P19-1141,P18-2039,0,0.0478745,"is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches. 1 PER2 PER2 Three At Zhang San PER1 LOC1 North Capital Human Beijing Zhang Sanzai The actual translation: Zhang San is at the Beijing People’s Park LOC1 People Public LOC2 Park People’s Park Beijing citizen Wrong matches Correct matches Figure 1: Example of Entity Matching Introduction Previous work (Ratinov and Roth, 2009) shows that NER is a knowledge intensive task. Background knowledge is often incorporated into an NER system in the form of named entity (NE) gazetteers (Seyler et al., 2018). Each gazetteer is typically a list containing NEs of the same type. Many earlier research efforts show that an NER model can benefit from the use of gazetteers (Li et al., 2005). On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually created by previous studies (Zamin and Oxley, 2011). Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase (Bollacker et al., 2008)) or commerc"
P19-1141,D18-1230,0,0.0260449,"eers may result in 6 matched entities, among which 2 are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, Qi et al. (2019) defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence (Shang et al., 2018), or maximum matching with rules (Sassano, 2014). Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure 1, 1462 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462–1467 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or he"
P19-1141,P18-4013,0,0.0528297,"Missing"
P19-1141,W06-0126,0,0.378098,"Missing"
P19-1141,P18-1144,0,0.424648,"Missing"
P19-1141,P16-2025,0,0.276512,"ions 5, 6, 7, and 8 combine the information from adjacent nodes and the current hidden state of the nodes to compute the new hidden state at time step t. After T steps, we have (T ) our final state hv for the node v. BiLSTM-CRF. The learned feature representa(T ) tions of characters {hv |v ∈ Vc } are then fed to a standard BiLSTM-CRF following the character order in the original sentence, to produce the output sequence. 3 3.1 Experiments Experimental Setup Dataset. The three public datasets used in our experiments are OntoNotes 4.0 (Weischedel et al., 2010), MSRA (Levow, 2006), and Weibo-NER (Peng and Dredze, 2016). OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as Che et al. (2013) and Peng and Dredze (2016) on OntoNotes and on WeiboNER. To demonstrate the effectiveness of our model in the e-commerce domain, we further constructed a new dataset by crawling and manually annotating the NEs of two types, namely PROD (“products”) and BRAN (“brands”). We name our dataset as “E-commerce-NER”. The NER task in the e-commerce domain is more challenging. The NEs of interest are usually the names of products 1464 OntoNotes Models"
P19-1229,P16-1231,0,0.0859338,"Missing"
P19-1229,K18-2005,0,0.0589861,"1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of"
P19-1229,D14-1082,0,0.145105,"ency from the head wh to the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the"
P19-1229,D18-1217,0,0.105638,"Missing"
P19-1229,P07-1033,0,0.607207,"Missing"
P19-1229,N19-1423,0,0.066859,"Missing"
P19-1229,P15-1033,0,0.0218671,"the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation"
P19-1229,N09-1068,0,0.0306646,"meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially lab"
P19-1229,W15-2202,0,0.0307514,"shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-domain constituent parsing. Both results report large improvement and show the usefulness of even small amount of target-domain annotation, showing the great potential of semi-supervised domain adaptation for parsing. 6 Conclusions This work addresses the task of semi-supervised domain adaptation for Chinese dependency parsing, based on our two newl"
P19-1229,I11-1100,0,0.0717653,"Missing"
P19-1229,C16-1002,0,0.0777974,"erse annotation guideline) for a language. Inspired by their work, we propose to concatenate each word position with an extra domain embedding to indicate which domain this training sentence comes from, as illustrated in Figure 3. In this way, we expect the model can fully utilize both training datasets, since most parameters are shared except the two domain embedding vectors, and learn to distinguish the domain-specific and general features as well. (3) Multi-task learning (MTL) aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) first employ MTL to improve parsing performance by utilizing multiple heterogeneous treebanks and treating each treebank as a separate task. As shown in Figure 4, we make a straightforward extension to the biaffine parser to realize multi-task learning. The sourcedomain and target-domain parsing are treated as xi ... Figure 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the traini"
P19-1229,D18-1498,0,0.0496972,"ubset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labe"
P19-1229,P18-1252,1,0.921905,"this work, we choose two typical domain-aware web texts for annotation, i.e., product blogs and web fictions. This section introduces the details about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guidelin"
P19-1229,P18-1110,0,0.0770222,"Missing"
P19-1229,R13-1046,0,0.0528327,"Missing"
P19-1229,P17-1060,0,0.0488226,", which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-do"
P19-1229,Q16-1023,0,0.122391,"Missing"
P19-1229,P18-1249,0,0.0262471,"g has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
P19-1229,D14-1108,0,0.0607182,"Missing"
P19-1229,P14-1043,1,0.843831,"e 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the training of the shared parameters. The corpus weighting strategy. For all above three approaches, the target-domain labeled data would be overwhelmed by the source-domain data during training if directly combined, since there usually exists a very big gap in their scale. Therefore, we employ the simple corpus weighting strategy (Li et al., 2014) as a useful trick. Before each iteration, we randomly sample training sentences separately from the target- and sourcedomain training data in the proportion of 1 : M . Then we merge and randomly shuffle the sampled data for one-iteration training. We treat M ≥ 1 as a hyper-parameter tuned on the dev data. 3.3 Utilizing Unlabeled Data Besides labeled data, how to exploit unlabeled data, both target- and source-domain, has been an interesting and important direction for crossdomain parsing for a long time, as discussed in Section 5. Recently, Peters et al. (2018) introduce embeddings from langu"
P19-1229,P18-1130,0,0.0228167,"product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the majo"
P19-1229,P06-1043,0,0.131945,"arch, we try to give a brief (and far from complete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting researc"
P19-1229,N10-1004,0,0.035199,"s from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labele"
P19-1229,D07-1111,0,0.070654,"the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of usin"
P19-1229,D14-1122,0,0.0233107,"sume there is no labeled target-domain training data and thus focus on unsupervised domain adaptation. So far, approaches in this direction have made limited progress, due to the intrinsic difficulty of both domain adaptation and parsing (see discussions in Section 5). On the other hand, due to the extreme complexity and heavy cost, progress on syntactic data annotation on new-domain texts has been very slow, and only several small-scale datasets on web texts have been built, mostly as evaluation data for cross-domain parsing (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). To meet the above challenges, this paper presents two newly-annotated large-scale domainaware datasets (over 12K sentences), and try to tackle the task of semi-supervised domain adaptation for Chinese dependency parsing. With the access of both labeled and unlabeled targetdomain data, we propose and evaluate several simple approaches and conduct error analysis in order to investigate the following three questions: Q1: How to effectively combine the source- and target-domain labeled training data? Q2: How to utilize the target-domain unlabeled data for further improvements? Q3: Given a certai"
P19-1229,W15-2201,0,0.286589,"lete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the i"
P19-1229,E14-1062,0,0.0554062,"about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guideline released by Jiang et al. (2018) based on three considerations. First, their guideline contains 20 relations specifically designed to capture Chin"
P19-1229,P15-1117,0,0.0182641,"ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, w"
P19-1229,N18-1202,0,0.26759,"inese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to a"
P19-1229,P11-1157,0,0.0874358,"Missing"
P19-1345,D17-1047,0,0.161136,"e-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentimen"
P19-1345,P14-2009,0,0.0298397,"QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platforms (e.g., Amazon and Taobao). Compared to traditional non-interactive cust"
P19-1345,P11-1016,0,0.0880512,"ent Classiﬁcation Towards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Cust"
P19-1345,D16-1011,0,0.0706496,"Missing"
P19-1345,C18-1079,0,0.0249668,"Missing"
P19-1345,D13-1171,0,0.358181,"Missing"
P19-1345,S15-2082,0,0.190919,"Missing"
P19-1345,S14-2004,0,0.52776,"oposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platfor"
P19-1345,P13-4009,0,0.0428117,"Missing"
P19-1345,D18-1401,1,0.853282,"Missing"
P19-1345,D16-1021,0,0.411895,"wards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answe"
P19-1345,P08-1036,0,0.0510394,"entence-level text classiﬁcation which aims to incorporate aspect information into a model. Recently, Wang et al. (2016); Ma et al. (2017) propose an attention based LSTM to ASC by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016b), Chen et al. (2017) and Wang et al. (2018b) employ memory networks to model the context and aspect. Wang and Lu (2018) propose a segmentation attention to capture structural dependency between target and opinion terms. Document-level ASC aims to predict sentiment ratings for aspects inside a long text. Traditional studies (Titov and McDonald, 2008; Wang et al., 2010; Pontiki et al., 2016) solve document-level ASC as a sub-problem by utilizing heuristic based methods or topic models. Recently, Lei et al. (2016) focus on extracting rationales for aspects in a document. Li et al. (2018) propose an useraware attention approach to document-level ASC. Yin et al. (2017) model document-level ASC as a machine comprehension problem, of which the input is also a parallel unit, i.e., question and answer. However, their question texts are pseudo and artiﬁcially constructed. This disaccords with the fact that real-world question texts also possibly"
P19-1345,P18-1088,0,0.34373,"text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has beco"
P19-1345,D16-1058,0,0.427641,"determine the polarity towards each aspect category discussed in a QA text pair. 4.2 Baselines For comparison, we implement several state-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decod"
P19-1345,P18-1234,0,0.0124307,"e a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentiment classiﬁcation towards QA style reviews. 7) MAMC (Yin et al., 2017). This is a QA matching"
P19-1345,D17-1217,0,0.0355952,"Missing"
S18-1110,P16-2011,0,0.0333104,"Missing"
S18-1110,P08-1030,0,0.114817,"Missing"
S18-1110,N16-1034,0,0.0299996,"Missing"
S18-1110,D16-1085,0,0.0421415,"Missing"
S18-1110,S18-1009,0,0.0313857,"3. 1 2 System Description We developed a pipeline system for the task, including question parsing, document feature generation, document event type classifications, document retrieval and document clustering. 2.1 Question Properties In this task, each question contains three components: the event type and two event properties. The two event properties provided are either the time, the location or the participant of the event. And specifications for these properties can vary in granularity (e.g. day/month/year, city/state, first/last/full name). Details can refer to official task description (Postma et al., 2018). In this task, we consider four event types (i.e. killing, injuring, fire burning, job firing). But in training data, only killing and injuring events are provided. Our system first processes each question to extract the question event type and propeties. Later each question and document will be paired (q-d pair) and assigned question properties as binary features. For instance, if a question asks for killing event(s) that happened at specific location and time, the features related to the asked event type and properties (i.e. ask killing, ask time, ask location) will be 1 and others Introduc"
S18-1114,D14-1162,0,0.0810053,"Missing"
S18-1114,J92-4003,0,0.655373,"(Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (part-of-speech Tagging), which attaches each word of a sentence a part of speech tag 1 Chunking Labels Similar words have similar distributions of words to their immediate left and right. Motivated by this intuition, Brown Clustering algorithm (Brown et al., 1992) gives an unsupervised class label to a word. Our system uses a C++ implementation3 of the Brown clustering algorithm (Liang, 2005) and sets cluster number as 50. The Brown clusters was trained on a large corpus of APT reports4 provided by the organizer. Feature Extraction 2.2.1 NER Labels Text chunking divides a text into phrases in such a way that syntactically related words become member of the same phrase. For instance, ”technology organizations” is a noun phrase, our system annotates ”technology” as ”B-NP” and ”organizations” as ”I-NP”. Data Preprocessing 2.2 Dependency Labels 3 https://s"
S18-1114,S18-1113,0,0.0334989,"Missing"
S18-1114,N16-1030,0,0.0942385,"d task. Our system is based on RNN network and ranked first in both token level and phrase level. Most existing high performance sequence labeling methods are linear statistical models, such as HMM (Hidden Markov Models) (Eddy, 1996) and CRF (Conditional Random Fields) (Lafferty et al., 2001). In the past few years, neural networks have been widely used to solve NLP problems. Specially, several RNN-based neural networks have been proposed to handle sequence labeling tasks including Chinese word segmentation (Yao and Huang, 2016), POS tagging (Huang et al., 2015), NER (Chiu and Nichols, 2015) (Lample et al., 2016), which achieved outstanding performance against traditional methods. In this paper, we simple derive the result of SubTask1 from SubTask2 and regard SubTask 2 as the preorder. Namely, our system firstly outputs sequence labels of a given sentence, and then checks whether some target labels turn out, such as Action, Entity, Modifier. Sentences which have Introduction As a growing number of mobile devices and facilities are getting connected and digitized, malware attacks become increasingly rampant and dangerous. CybersecUrity attracts more public attention but few NLP research and efforts. A"
S18-1114,P17-1143,0,0.0304662,"Missing"
S18-1114,P16-1101,0,0.0258725,"omly. We take four parts as training set and the rest as development set. 2.2.5 Based upon many previous work on sequence labeling, our system incorporates 5 types of features: POS tags, dependency parsing, NER labels, Chunking labels and Brown clustering. All features are generated automatically. In detail, we use Stanford CoreNLP (Manning et al., 2014) 1 to annotate POS tags, dependency parsing, NER labels, and use Apache OpenNLP 2 to annotate Chunking labels. Brown clustering labels are generated by an open source implementation. 2.3 2 Brown Clustering Labels Model Introduction Similar to (Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (part-of-speech Tagging), which attaches each word of a sentence a part of speech tag 1 Chunking Labels Similar words have similar distributions of words to their immediate left and right. Motivated by this intuition, Brown Clustering algorithm (Brown et al., 1992)"
S18-1114,P14-5010,0,0.0047801,"m data provided by the organizers, we use a python program to correct spelling mistakes and unreadable characters. After that, in order to avoid data distribution problem, we mix the training set development set, and then shuffle and split them into five parts randomly. We take four parts as training set and the rest as development set. 2.2.5 Based upon many previous work on sequence labeling, our system incorporates 5 types of features: POS tags, dependency parsing, NER labels, Chunking labels and Brown clustering. All features are generated automatically. In detail, we use Stanford CoreNLP (Manning et al., 2014) 1 to annotate POS tags, dependency parsing, NER labels, and use Apache OpenNLP 2 to annotate Chunking labels. Brown clustering labels are generated by an open source implementation. 2.3 2 Brown Clustering Labels Model Introduction Similar to (Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (par"
S18-1148,D16-1041,0,0.150504,"rd, we get its hypernym lists by computing the similarities between the hyponym word and words in the training data, and fill the test word’s hypernym lists with the hypernym list in the training set of the nearest similarity distance to the test word. In SemEval 2018 task9, our results, achieve 1st on Spanish, 2nd on Italian, 6th on English in the metric of MAP. 1 2 Hypernymy relationship plays a critical role in language understanding because it enables generalization, which lies at the core of human cognition (Yu et al. (2015)). It has been widely used in various NLP applications (Espinosa Anke et al. (2016)), from word sense disambiguation (Agirre et al. (2014)) to information retrieval (Varelas et al. (2005)) , question answering (Prager (2006)) and textual entailment (Glickman et al. (2005)). To date, the hypernymy relation also plays an important role in Knowledge Base Construction task. In the past SemEval contest (SemEval-2015 task 171 , SemEval-2016 task 132 ), the “Hypernym Detection” task was treated as a classfication task, i.e., given a (hyponym, hypernym) pair, deciding whether the pair is a true hypernymic relation or not. This has led to criticisms regarding its oversimplification ("
S18-1148,P14-1113,0,0.402129,"tool3 on the preprocessed corpus. We employ the skipgram model since the skip-gram model is shown to perform best in identifying semantic relations among words. The trained word embeddings are used in the projection learning and nearestneighbour based method. 3.3 Method based on Projection Learning The intuition of this method is to assume that there is a linear transformation in the embedding space which maps hyponyms to their correspondent hypernyms. We first learn a projection matrix from the training data, then apply the matrix to the test data. Our method is similar to that described in Fu et al. (2014), the main idea can be summarized as follows: Hyponym-hypernym Discovery method 3.1 Word Embedding 1. Give a word x and its hypernym y, assuming there exists a linear projection matrix Φ to meet y = Φx. We need to learn a approximate Φ using the following equation to minimize the MSE loss: Preprocessing For the corpus and the train/gold/test data, we have two preprocessing steps: 1) Lowercase all the words; 2) Concatenate the phrases (hyponym or hypernym composed with more than one word) which occur in the training set or the test set with underline, i.e., “executive president” is replaced by"
S18-1148,C14-1097,0,0.4072,"the hyponym word and words in the training data, and fill the test word’s hypernym lists with the hypernym list in the training set of the nearest similarity distance to the test word. In SemEval 2018 task9, our results, achieve 1st on Spanish, 2nd on Italian, 6th on English in the metric of MAP. 1 2 Hypernymy relationship plays a critical role in language understanding because it enables generalization, which lies at the core of human cognition (Yu et al. (2015)). It has been widely used in various NLP applications (Espinosa Anke et al. (2016)), from word sense disambiguation (Agirre et al. (2014)) to information retrieval (Varelas et al. (2005)) , question answering (Prager (2006)) and textual entailment (Glickman et al. (2005)). To date, the hypernymy relation also plays an important role in Knowledge Base Construction task. In the past SemEval contest (SemEval-2015 task 171 , SemEval-2016 task 132 ), the “Hypernym Detection” task was treated as a classfication task, i.e., given a (hyponym, hypernym) pair, deciding whether the pair is a true hypernymic relation or not. This has led to criticisms regarding its oversimplification (Levy et al., 2015). In the SemEval 2018 Task 9 (Camacho"
S18-1148,C92-2082,0,0.258674,"(SemEval-2015 task 171 , SemEval-2016 task 132 ), the “Hypernym Detection” task was treated as a classfication task, i.e., given a (hyponym, hypernym) pair, deciding whether the pair is a true hypernymic relation or not. This has led to criticisms regarding its oversimplification (Levy et al., 2015). In the SemEval 2018 Task 9 (Camacho-Collados et al., 2018), the task has shifted to “Hypernym Discovery” , i.e., 2 Related Work The work of identifying hypernymy relationship can be categorized from different aspects according to the learning methods and the task formulization. The earlier work (Hearst (1992)) formalized the task as an unsupervised hypernym discovery task, i.e., none hyponym-hypernyms pairs (x, y) are given as the training data. Hearst (1992) handcrafted a set of lexico-syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus. Snow et al. (2004) trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym-hypernym pairs. Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus. Variations of Snow et al. (2004) were later us"
S18-1148,E14-4008,0,0.0713615,"his limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 909–913 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.2 measures to detect hypernym in an unsupervised manner. Weeds and Weir (2003); Kotlerman et al. (2010) employed directional measures based on the distributional inclusion hypothesis. More recent work (Santus et al. (2014); Rimell (2014)) introduces new measures, based on the distributional informativeness hypothesis. Yu et al. (2015); Tuan and Ng (2016); Nguyen et al. (2017) learn directly the word embeddings which are optimized for capturing the hypernymy relationship. The supervised methods include Baroni and Lenci (2011); Roller et al. (2014); Weeds and Weir (2003). These methods were originally wordcount-based, but can be easily adapted using word embeddings (Mikolov et al. (2013a); Pennington et al. (2014)). However, it was criticized that the supervised methods only learn prototypical hypernymy (Levy et"
S18-1148,D10-1108,0,0.026444,"k, i.e., none hyponym-hypernyms pairs (x, y) are given as the training data. Hearst (1992) handcrafted a set of lexico-syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus. Snow et al. (2004) trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym-hypernym pairs. Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus. Variations of Snow et al. (2004) were later used in tasks such as taxonomy construction (Snow et al. (2006); Kozareva and Hovy (2010); Carlson et al. (2010)), analogy identification (Turney (2006)), and definition extraction (Borg et al. (2009); Navigli and Velardi (2010)). A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs. Distributional methods are developed to overcome this limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)"
S18-1148,N15-1098,0,0.200004,"), from word sense disambiguation (Agirre et al. (2014)) to information retrieval (Varelas et al. (2005)) , question answering (Prager (2006)) and textual entailment (Glickman et al. (2005)). To date, the hypernymy relation also plays an important role in Knowledge Base Construction task. In the past SemEval contest (SemEval-2015 task 171 , SemEval-2016 task 132 ), the “Hypernym Detection” task was treated as a classfication task, i.e., given a (hyponym, hypernym) pair, deciding whether the pair is a true hypernymic relation or not. This has led to criticisms regarding its oversimplification (Levy et al., 2015). In the SemEval 2018 Task 9 (Camacho-Collados et al., 2018), the task has shifted to “Hypernym Discovery” , i.e., 2 Related Work The work of identifying hypernymy relationship can be categorized from different aspects according to the learning methods and the task formulization. The earlier work (Hearst (1992)) formalized the task as an unsupervised hypernym discovery task, i.e., none hyponym-hypernyms pairs (x, y) are given as the training data. Hearst (1992) handcrafted a set of lexico-syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus."
S18-1148,P06-1101,0,0.0530956,"pernym discovery task, i.e., none hyponym-hypernyms pairs (x, y) are given as the training data. Hearst (1992) handcrafted a set of lexico-syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus. Snow et al. (2004) trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym-hypernym pairs. Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus. Variations of Snow et al. (2004) were later used in tasks such as taxonomy construction (Snow et al. (2006); Kozareva and Hovy (2010); Carlson et al. (2010)), analogy identification (Turney (2006)), and definition extraction (Borg et al. (2009); Navigli and Velardi (2010)). A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs. Distributional methods are developed to overcome this limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic"
S18-1148,D16-1039,0,0.0121549,"cri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 909–913 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.2 measures to detect hypernym in an unsupervised manner. Weeds and Weir (2003); Kotlerman et al. (2010) employed directional measures based on the distributional inclusion hypothesis. More recent work (Santus et al. (2014); Rimell (2014)) introduces new measures, based on the distributional informativeness hypothesis. Yu et al. (2015); Tuan and Ng (2016); Nguyen et al. (2017) learn directly the word embeddings which are optimized for capturing the hypernymy relationship. The supervised methods include Baroni and Lenci (2011); Roller et al. (2014); Weeds and Weir (2003). These methods were originally wordcount-based, but can be easily adapted using word embeddings (Mikolov et al. (2013a); Pennington et al. (2014)). However, it was criticized that the supervised methods only learn prototypical hypernymy (Levy et al. (2015)). 3 We train our word embedding models using the Google word2vec (Mikolov et al. (2013a,b)) tool3 on the preprocessed corpu"
S18-1148,J06-3003,0,0.0769022,"a. Hearst (1992) handcrafted a set of lexico-syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus. Snow et al. (2004) trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym-hypernym pairs. Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus. Variations of Snow et al. (2004) were later used in tasks such as taxonomy construction (Snow et al. (2006); Kozareva and Hovy (2010); Carlson et al. (2010)), analogy identification (Turney (2006)), and definition extraction (Borg et al. (2009); Navigli and Velardi (2010)). A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs. Distributional methods are developed to overcome this limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 909–913 New Orleans, Louisiana, June 5–6, 2018. ©2018 A"
S18-1148,P10-1134,0,0.140858,"at connect the joint occurrences of x and y which indicate hypernymy in a large corpus. Snow et al. (2004) trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym-hypernym pairs. Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus. Variations of Snow et al. (2004) were later used in tasks such as taxonomy construction (Snow et al. (2006); Kozareva and Hovy (2010); Carlson et al. (2010)), analogy identification (Turney (2006)), and definition extraction (Borg et al. (2009); Navigli and Velardi (2010)). A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs. Distributional methods are developed to overcome this limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 909–913 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.2 measures to detect hypernym in"
S18-1148,W03-1011,0,0.344545,"relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs. Distributional methods are developed to overcome this limitation. Lin (1998) developed symmetric similarity Introduction 1 Luo Si Alibaba Group, China luo.si@alibaba-inc.com http://alt.qcri.org/semeval2015/task17/ http://alt.qcri.org/semeval2016/task13/ 909 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 909–913 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.2 measures to detect hypernym in an unsupervised manner. Weeds and Weir (2003); Kotlerman et al. (2010) employed directional measures based on the distributional inclusion hypothesis. More recent work (Santus et al. (2014); Rimell (2014)) introduces new measures, based on the distributional informativeness hypothesis. Yu et al. (2015); Tuan and Ng (2016); Nguyen et al. (2017) learn directly the word embeddings which are optimized for capturing the hypernymy relationship. The supervised methods include Baroni and Lenci (2011); Roller et al. (2014); Weeds and Weir (2003). These methods were originally wordcount-based, but can be easily adapted using word embeddings (Mikol"
S18-1148,D14-1162,0,0.0994577,"(2010) employed directional measures based on the distributional inclusion hypothesis. More recent work (Santus et al. (2014); Rimell (2014)) introduces new measures, based on the distributional informativeness hypothesis. Yu et al. (2015); Tuan and Ng (2016); Nguyen et al. (2017) learn directly the word embeddings which are optimized for capturing the hypernymy relationship. The supervised methods include Baroni and Lenci (2011); Roller et al. (2014); Weeds and Weir (2003). These methods were originally wordcount-based, but can be easily adapted using word embeddings (Mikolov et al. (2013a); Pennington et al. (2014)). However, it was criticized that the supervised methods only learn prototypical hypernymy (Levy et al. (2015)). 3 We train our word embedding models using the Google word2vec (Mikolov et al. (2013a,b)) tool3 on the preprocessed corpus. We employ the skipgram model since the skip-gram model is shown to perform best in identifying semantic relations among words. The trained word embeddings are used in the projection learning and nearestneighbour based method. 3.3 Method based on Projection Learning The intuition of this method is to assume that there is a linear transformation in the embedding"
S19-2148,P17-1066,0,0.128151,"Zubiaga et al., 2016; Kochkina et al., 2017). A time sequence classification technique has been proposed for 855 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 855–859 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics . detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in tree-structured conversations for classifying stance. Several studies have applied neural networks on the verification of rumors (Ma et al., 2016; Kochkina et al., 2017; Ma et al., 2017); They mainly focus on analyzing the information propagation structure, and have not utilized much information on user credibility. User stance plays an important role in rumor detection. Recent works have employed multi-task learning approaches to jointly learn stance detection and veracity prediction, in order to improve classification accuracy by utilizing the interdependence between them (Ma et al.,; 2018; Kochkina et al., 2018). 3 used to build the embedding model. Totally, 3 billion words are processed, and word embeddings are generated for 3.5 million unique terms using the word2vec mod"
S19-2148,S19-2147,0,0.105707,"dels, such as SVM, and a neural network model, using the language features extracted from the message text. Task B predicts the veracity of a rumor: true, false, or unverified (i.e., its veracity cannot be verified based on the given information). Each rumor consists of a source post that makes a claim, and a set of replies, directly or indirectly towards the source post. We also employed an ensemble approach on this task, which uses multiple models together to do the veracity prediction. For more details about these two tasks, please check the task description paper from the task organizers (Gorrell et al., 2019). Abstract This paper describes our system for SemEval 2019 RumorEval: Determining rumor veracity and support for rumors (SemEval 2019 Task 7). This track has two tasks: Task A is to determine a user’s stance towards the source rumor, and Task B is to detect the veracity of the rumor: true, false or unverified. For stance classification, a neural network model with language features is utilized. For rumor verification, our approach exploits information from different dimensions: rumor content, source credibility, user credibility, user stance, event propagation path, etc. We use an ensemble ap"
S19-2148,S16-1003,0,0.122671,"Missing"
S19-2148,S17-2083,0,0.223578,"tion of this issue started from two special case studies on rumor propagation during natural disasters like earthquakes and hurricanes (Gupta et. al., 2013; Mendoza et al., 2010). Many existing algorithms (Liu et al., 2015; Wu et. al., 2015; Yang et. al., 2012) for debunking rumors followed the work of Castillo et al. (2011). They studied information credibility and proposed a set of features that are able to retrospectively predict if an event is credible. Stance classification is also an active research area that has been studied in previous work (Lukasik et al., 2016; Zubiaga et al., 2016; Kochkina et al., 2017). A time sequence classification technique has been proposed for 855 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 855–859 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics . detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in tree-structured conversations for classifying stance. Several studies have applied neural networks on the verification of rumors (Ma et al., 2016; Kochkina et al., 2017; Ma et al., 2017); They mainly focus on analyz"
S19-2156,W06-0116,0,0.107088,"Missing"
S19-2156,N18-1202,0,0.0138998,"quently help to improve the performance 2.2.1 Detection in Main Body We recast the problem the Toponym Detection in main body as a Named Entity Recognition task and we make use of the BiLSTM-CRF model with the contextual information as input. To alleviate over-fitting, we apply model averaging training strategies. Finally, a voting method is utilized to benefit from multiple models. Input Information Based upon our previous work (Ma et al., 2018) on sequence labeling, our system incorporates four types of linguistic information: Part-of-Speech (POS) tags, NER labels, Chunking labels and ELMo (Peters et al., 2018). The former three are generated by open source tools. In detail, we use Stanford CoreNLP (Manning et al., 2014) to annotate POS tags and NER labels, and use OpenNLP 2 to annotate Chunking labels. These information are represented as distributional vectors which are randomly initialized and trained with the entire model. ELMo 2 https://opennlp.apache.org/ 918 • They usually begin with the word ’Table’. in NER and POS tagging. The input layer generates the final representation of each word by concatenating three types of vectors, the pre-trained word embedding, the word vector given by the char"
S19-2156,N13-1122,0,0.0325343,"date Name, Candidate Starts Mention Name, Jaccard Similarity, Levenshtein Similarity. All names are lowercased in advance and the name of candidate may change into its alternate names if exist. 1. Alternative names recorded in GeoNames dump files, including allCountries, alternatenames, countryInfo. 2. Candidate Attributes This set of features are based on target KB’s (GeoNames) records and capture some priority of candidate, including Popularity, Number of Ancestors, Code Level. 2. Abbreviations of state names in America given by Wikipedia4 . 3. Contextual Features Inspired by previous work (Guo et al., 2013), We designed this set of features to measure the contextual similarity between the mention and the candidate. Firstly, for mentions, we take multiple levels of context around mentions in documents as mention-side context, including sentenceparagraph and document level. Secondly, since target KB (GeoNames) lacks context information, we resort to Wikipedia to request candidate’s page via API 5 . Considering computation efficiency and avoiding the noise introduced by whole wiki page, we just use the summary (first description paragraph) of the page as candidate-side context, instead of multiple"
S19-2156,D11-1072,0,0.12454,"Missing"
S19-2156,N16-1030,0,0.274021,"takes a sequence of character (characters in a word) embedding as input and concatenates the final hidden states (forward and backward) as the representation of the word. Designing a neural network architecture with character representation as input is appealing for several reasons. Firstly, words which have the same morphological properties (like the prefix or suffix of a word) often share the same grammatical function or meaning. Secondly, a character-level analysis can help to address the out-of-vocabulary problem, Thirdly, capitalization may provide additional information. A recent study (Lample et al., 2016) has shown that BiLSTM is an effective approach to extract morphological information from characters of words, and consequently help to improve the performance 2.2.1 Detection in Main Body We recast the problem the Toponym Detection in main body as a Named Entity Recognition task and we make use of the BiLSTM-CRF model with the contextual information as input. To alleviate over-fitting, we apply model averaging training strategies. Finally, a voting method is utilized to benefit from multiple models. Input Information Based upon our previous work (Ma et al., 2018) on sequence labeling, our sys"
S19-2156,S18-1114,1,0.844791,"ormation. A recent study (Lample et al., 2016) has shown that BiLSTM is an effective approach to extract morphological information from characters of words, and consequently help to improve the performance 2.2.1 Detection in Main Body We recast the problem the Toponym Detection in main body as a Named Entity Recognition task and we make use of the BiLSTM-CRF model with the contextual information as input. To alleviate over-fitting, we apply model averaging training strategies. Finally, a voting method is utilized to benefit from multiple models. Input Information Based upon our previous work (Ma et al., 2018) on sequence labeling, our system incorporates four types of linguistic information: Part-of-Speech (POS) tags, NER labels, Chunking labels and ELMo (Peters et al., 2018). The former three are generated by open source tools. In detail, we use Stanford CoreNLP (Manning et al., 2014) to annotate POS tags and NER labels, and use OpenNLP 2 to annotate Chunking labels. These information are represented as distributional vectors which are randomly initialized and trained with the entire model. ELMo 2 https://opennlp.apache.org/ 918 • They usually begin with the word ’Table’. in NER and POS tagging."
S19-2156,P14-5010,0,0.00299574,"n in main body as a Named Entity Recognition task and we make use of the BiLSTM-CRF model with the contextual information as input. To alleviate over-fitting, we apply model averaging training strategies. Finally, a voting method is utilized to benefit from multiple models. Input Information Based upon our previous work (Ma et al., 2018) on sequence labeling, our system incorporates four types of linguistic information: Part-of-Speech (POS) tags, NER labels, Chunking labels and ELMo (Peters et al., 2018). The former three are generated by open source tools. In detail, we use Stanford CoreNLP (Manning et al., 2014) to annotate POS tags and NER labels, and use OpenNLP 2 to annotate Chunking labels. These information are represented as distributional vectors which are randomly initialized and trained with the entire model. ELMo 2 https://opennlp.apache.org/ 918 • They usually begin with the word ’Table’. in NER and POS tagging. The input layer generates the final representation of each word by concatenating three types of vectors, the pre-trained word embedding, the word vector given by the character BiLSTM and the vector of linguistic information (POS label, NE label, chunking label and ELMo vector). The"
W18-6465,P15-4020,0,0.202615,"Missing"
W18-6465,W17-4717,0,0.0467387,"Missing"
W18-6465,W17-4761,0,0.0586933,"Missing"
W18-6465,W16-2378,0,0.0218062,"r case for simplicity. Since not every single model in the ensemble is always needed for the optimized prediction, it is appropriate to select a subset from all candidate models. We follow the greedy ensemble selection algorithm, Focused Ensemble Selection (FES ) (Partalas et al., 2008), to reduce the size of averaging ensembles but improve its efficiency and predictive performance. 4.1.2 Data for Quality Estimation Model The data for quality estimation contains two parts: (i) real QE data provided by WMT QE organizers; (ii) artificial QE data generated by the roundtrip translation technique (Junczys-Dowmunt and Grundkiewicz, 2016). We first combined the real QE data with the artificial QE data to train a baseline quality estimation model, then fine tuned the model with the real QE data alone. The English-German IT domain artificial QE data can be obtained directly from the additional resources of WMT18 Auto Post-Editing task5 created by Junczys-Dowmunt and Grundkiewicz (2016). We applied the English-German artificial QE data on In the sentence level, FES’s output is averaging HTER scores of selected single models. However, in the word level, the ensemble can be made by majority voting of the binary predictions for sele"
W18-6465,W17-4763,0,0.0838264,"eling respectively. Traditional baseline models in WMT 12-17 have two modules: human-crafted rulebased feature extraction model via QuEst++ (Specia et al., 2015) (sentence-level task) or Marmot1 (word-level task); and an SVM regression with an RBF kernel as well as grid search algorithms for predicting how much effort is needed to fix translations to acceptable results (sentence-level task) or a sequence-labeling model with CRFSuit toolkit to predict which word in the translation output needs to be edited (word-level task). A recently proposed predictor-estimator model with stack propagation (Kim et al., 2017) is a recurrent neural network (RNN) based feature extractor and quality prediction model that ranked first place in WMT17. Another novel method is to train an Automatic Post-Editing (APE) system and adapt it to predict sentence-level quality scores and word-level quality labels (Martins et al., 2017). A promising APE system can serve as a guidance to QE system by explicitly explaining errors in the translation output. Our submitted system for sentence and word level QE tasks in WMT18, named QE Brain has two phases: feature extraction and quality estimation. In the phase of feature extraction,"
W18-6465,W16-2385,0,0.0389784,"nd prevents overfitting. To further enhance our model’s performance, we use a greedy algorithm based ensemble selection method to decrease the individual error among a bunch of single quality estimation models. 2 3 Boosting the QE Model Performance 3.1 QE Brain Baseline Model Human-crafted Features Along with the features produced by the Bilingual Expert model, we extract another 17 QE baseline features for the sentence-level task using QuEst++ and additional resources (source and target corpora, language models, ngram counts and lexical translation tables) provided on the WMT18 QE website2 . Kozlova et al. (2016) verifies the significance of these features using Random Forest (Breiman, 2001). Four of them are the most crucial among all according to their degrees of importance. QE Brain base single model contains a feature extractor and a quality estimator. The feature extractor relies on the Bilingual Expert model to extract features representing latent semantic information of the source and translation pair. These features will be fed into a quality estimator to estimate the translation quality. The Bilingual Expert model uses self-attention mechanism and transformer neural networks to construct a bi"
W18-6465,W17-4764,0,0.0947762,"cting how much effort is needed to fix translations to acceptable results (sentence-level task) or a sequence-labeling model with CRFSuit toolkit to predict which word in the translation output needs to be edited (word-level task). A recently proposed predictor-estimator model with stack propagation (Kim et al., 2017) is a recurrent neural network (RNN) based feature extractor and quality prediction model that ranked first place in WMT17. Another novel method is to train an Automatic Post-Editing (APE) system and adapt it to predict sentence-level quality scores and word-level quality labels (Martins et al., 2017). A promising APE system can serve as a guidance to QE system by explicitly explaining errors in the translation output. Our submitted system for sentence and word level QE tasks in WMT18, named QE Brain has two phases: feature extraction and quality estimation. In the phase of feature extraction, it extracts high-level latent joint semantics and alignment information between the source and the translation output, relying on the “neural Bilingual Expert model” introduced by Fan et al. (2018) as a prior knowledge model, which is trained on a large parallel corpus. The high-level latent semantic"
W18-6465,P02-1040,0,0.102559,"experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018. 1 Introduction Quality Estimation (QE) is a task to estimate the quality of a Machine Translation (MT) system without the presence of any manually annotated reference translations. It can serve in a variety of computer-aided scenarios such as translation results screening before release or translation quality comparison between different MT systems. Currently, the classical and widely-used method to evaluate an MT system is measured by BLEU (Papineni et al., 2002), a statistical language-independent metric that requires human golden references for validation. What if we expect to efficiently get the detailed quality evaluation feedbacks (e.g. sentence or token-wise scoring) from an extremely large number of machine translation outputs? An automatic method with no access to any reference is highly appreciated. 1 ∗ * indicates equal contribution. 809 https://github.com/qe-team/marmot Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 809–815 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Assoc"
