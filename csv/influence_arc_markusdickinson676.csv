C08-1026,C04-1080,0,0.0191197,"nts, 1997) could aid in developing better disambiguation models, and perhaps also a better sense of what categories are useful to induce (e.g., a broader category Verb in (12) for paid). 6 Representations for disambiguation We have shown that local lexical context provides a generally unambiguous context for corpus tags, given sufficient information about the word to be disambiguated. The information need not be very abstract, either: frames using ambiguity class nuclei only require a word’s category possibilities. Even for many unsupervised situations, this is available from a lexicon (e.g., Banko and Moore, 2004; Goldberg et al., 2008). We have only looked at cases with variation in tagging; fully gauging the accuracy of such a data representation for disambiguation requires more of the framed nuclei from the corpus, including those without variation. For this, we could take all framed nuclei from a corpus and compare the level of ambiguity for differing abstractions. However, most framed nuclei occur only once, and it is not clear how meaningful it is to say that these are unambiguous. Thus, we examine framed nuclei which occur at least twice and report in table 1 206 Abstraction Word Complete AC No"
C08-1026,E03-1009,0,0.228917,"n needed to disambiguate a word in a local context, when using corpus categories. Specifically, we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties, namely the set of categories it can potentially have. This work thus provides insights into the relation of corpus categories to categories derived from local contexts. 1 Introduction and Motivation Category induction techniques generally rely on local contexts, i.e., surrounding words, to cluster word types together (e.g., Clark, 2003; Sch¨utze, 1995), using information of a kind also found in human category acquisition tasks (e.g., Mintz, 2002, 2003). Such information is also at the core of standard part-of-speech (POS) tagging, or disambiguation, methods (see, e.g., Manning and Sch¨utze, 1999, ch. 10), with the contexts generally abstracted to POS tags. The contextual information is similar in both tasks because induction is founded in part upon the notion that local contexts are useful for disambiguation: one morphosyntactically clusters words which should have the same category in the same contexts. But which contexts"
C08-1026,W96-0102,0,0.616686,"ance, are comparable. Other cases are not: one/CD and shown/VBN can never have the same category. We need to find classes of words that, within the same context, should not vary in their annotation, and it makes sense to compare words in context if they have the same category possibilities. 4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve POS tagging (e.g., Cutting et al., 1992; Daelemans et al., 1996; Dickinson, 2007; Goldberg et al., 2008; Tseng et al., 2005). Only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context. As an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3). There are at least 27 4 One could group affixes by ambiguity class for languages like Chinese (cf. CTBMorph features in Tseng et al., 2005). 203 different VBN (past participle) verbs appearing between being and by (3a), but none of these verbs ever appear as VBD here, even though all of th"
C08-1026,E03-1068,1,0.931842,"Missing"
C08-1026,P08-1085,0,0.19945,": one/CD and shown/VBN can never have the same category. We need to find classes of words that, within the same context, should not vary in their annotation, and it makes sense to compare words in context if they have the same category possibilities. 4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve POS tagging (e.g., Cutting et al., 1992; Daelemans et al., 1996; Dickinson, 2007; Goldberg et al., 2008; Tseng et al., 2005). Only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context. As an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3). There are at least 27 4 One could group affixes by ambiguity class for languages like Chinese (cf. CTBMorph features in Tseng et al., 2005). 203 different VBN (past participle) verbs appearing between being and by (3a), but none of these verbs ever appear as VBD here, even though all of them could be VBD. Two other VBD/VBN verbs"
C08-1026,J93-2004,0,0.030049,"Missing"
C08-1026,E95-1020,0,0.767251,"Missing"
C08-1026,I05-3005,0,0.0176379,"can never have the same category. We need to find classes of words that, within the same context, should not vary in their annotation, and it makes sense to compare words in context if they have the same category possibilities. 4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve POS tagging (e.g., Cutting et al., 1992; Daelemans et al., 1996; Dickinson, 2007; Goldberg et al., 2008; Tseng et al., 2005). Only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context. As an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3). There are at least 27 4 One could group affixes by ambiguity class for languages like Chinese (cf. CTBMorph features in Tseng et al., 2005). 203 different VBN (past participle) verbs appearing between being and by (3a), but none of these verbs ever appear as VBD here, even though all of them could be VBD. Two other VBD/VBN verbs, rejected (3b) and p"
C08-1026,A92-1018,0,\N,Missing
C10-1030,boyd-2010-eagle,0,0.0309136,"of reading. Figure 1: Error taxonomy icon incompleteness (see section 4.2.2). If можут (2c) is generated and is not in the lexicon, we do not know whether it is misformed or simply unattested. In this paper, we group together such cases, since this allows for a simpler and more quickly-derivable lexicon. We have added syntactic errors, whereas Dickinson and Herring (2008) focused on strictly morphological errors. Learners make syntactic errors (e.g., Rubinstein, 1995; Rosengrant, 1987), and when creating errors, a well-formed word may result. In the future, syntactic errors can be subdivided (Boyd, 2010). This classification is of possible errors, making no claim about the actual distribution of learner errors, and does not delve into issues such as errors stemming from first language interference (Rubinstein, 1995). Generating errors from the possible types allows one to investigate which types are plausible in which contexts. 260 It should be noted that we focus on inflectional morphology in Russian, meaning that we focus on suffixes. Prefixes are rarely used in Russian as inflectional markers; for example, prefixes mark semantically-relevant properties for verbs of motion. The choice of pr"
C10-1030,A00-1031,0,0.433022,"-relevant properties for verbs of motion. The choice of prefix is thus related to the overall word choice, an issue discussed under Random stem generation in section 4.2.4. 3 Enriching a POS lexicon To create errors, we need a segmented lexicon with morphological information, as in (3). Here, the word могу (mogu, ‘I am able to’) is split into stem and suffix, with corresponding POS tags.2 (3) a. мог,Vm-----a-p,у,Vmip1s-a-p b. мож,Vm-----a-p,ет,Vmip3s-a-p c. мог,Vm-----a-p,NULL,Vmis-sma-p The freely-available POS lexicon from Sharoff et al. (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information. Working with such a rich database, we only need segmentation, providing a quickly-obtained lexicon (cf. five years for a German lexicon in Geyken and Hanneforth, 2005). In the future, one could switch to a different tagset, such as that in Hana and Feldman (2010), which includes reflexivity, animacy, and aspect features. One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997), as suggested by Feldman and Hana (2010). Still, our lexicon continues the trend of linking tra"
C10-1030,C08-1017,0,0.0188809,"f the tag is x and the word ends with y, make y the suffix. Such rules are easily and quickly derivable from a textbook listing of paradigms. For certain exceptional 2 POS tags are from the compositional tagset in Sharoff et al. (2008). A full description is at: http:// corpus.leeds.ac.uk/mocky/msd-ru.html. 3 This lexicon now includes lemma information, but each word is not segmented (Erjavec, 2010). cases, we write word-specific rules. Additionally, we remove word, tag pairs indicating punctuation or non-words (PUNC, SENT, -). One could use a sophisticated method for lemmatizing words (e.g., Chew et al., 2008; Schone and Jurafsky, 2001), but we would likely have to clean the lexicon later; as Feldman and Hana (2010) point out, it is difficult to automatically guess the entries for a word, without POS information. Essentially, we write precise rules to specify part of the Russian system of suffixes; the lexicon then provides the stems for free. We use the lexicon for generating errors, but it should be compatible with analysis. Thus, we focus on suffixes for beginning and intermediate learners. We can easily prune or add to the rule set later. From an analysis perspective, we need to specify that c"
C10-1030,W08-0901,1,0.57474,"ogical errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context. 1 Introduction Work on detecting grammatical errors in the language of non-native speakers covers a range of errors, but it has largely focused on syntax in a small number of languages (e.g., Vandeventer Faltin, 2003; Tetreault and Chodorow, 2008). In more morphologically-rich languages, learners naturally make many errors in morphology (Dickinson and Herring, 2008). Yet for many languages, there is a major bottleneck in system development: there are not enough error-annotated learner corpora which can be mined to discover the nature of learner errors, let alone enough data to train or evaluate a system. Our perspective is that one can speed up the process of determining the nature of learner errors via semi-automatic means, by generating plausible errors. We set out to generate linguistically-plausible morphological errors for Russian, a language with rich inflections. Generating learner-like errors has practical and theoretical benefits. First, there i"
C10-1030,erjavec-2010-multext,0,0.0135962,"categories used for tagging with deeper analyses (Sharoff et al., 2008; Hana and Feldman, 2010).3 3.1 Finding segments/morphemes We use a set of hand-crafted rules to segment words into morphemes, of the form: if the tag is x and the word ends with y, make y the suffix. Such rules are easily and quickly derivable from a textbook listing of paradigms. For certain exceptional 2 POS tags are from the compositional tagset in Sharoff et al. (2008). A full description is at: http:// corpus.leeds.ac.uk/mocky/msd-ru.html. 3 This lexicon now includes lemma information, but each word is not segmented (Erjavec, 2010). cases, we write word-specific rules. Additionally, we remove word, tag pairs indicating punctuation or non-words (PUNC, SENT, -). One could use a sophisticated method for lemmatizing words (e.g., Chew et al., 2008; Schone and Jurafsky, 2001), but we would likely have to clean the lexicon later; as Feldman and Hana (2010) point out, it is difficult to automatically guess the entries for a word, without POS information. Essentially, we write precise rules to specify part of the Russian system of suffixes; the lexicon then provides the stems for free. We use the lexicon for generating errors, b"
C10-1030,W09-2112,0,0.0431969,"n expert—e.g., a language teacher—to search through, expanding the taxonomy with new error types or subtypes and/or deprecating error types which are unlikely. Given the lack of real learner data, this has the potential to speed up error categorization and subsequent system development. Furthermore, error generation techniques can be re-used, adjusting the errors for different learner levels, first languages, and so forth. The error generation process can benefit by using linguistic properties to mimic learner variations. This can lead to more realistic errors, a benefit for machine learning (Foster and Andersen, 2009), and can also provide feedback for the linguistic representation used to generate errors by, e.g., demonstrating under which linguistic conditions certain error types are generated and under which they are not. We are specifically interested in generating Russian morphological errors. To do this, we need a knowledge base representing Russian morphology, allowing us to manipulate linguistic properties. After outlining the coarse error taxonomy 259 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 259–267, Beijing, August 2010 0. Correct: The wor"
C10-1030,hana-feldman-2010-positional,0,0.0179568,"o’) is split into stem and suffix, with corresponding POS tags.2 (3) a. мог,Vm-----a-p,у,Vmip1s-a-p b. мож,Vm-----a-p,ет,Vmip3s-a-p c. мог,Vm-----a-p,NULL,Vmis-sma-p The freely-available POS lexicon from Sharoff et al. (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information. Working with such a rich database, we only need segmentation, providing a quickly-obtained lexicon (cf. five years for a German lexicon in Geyken and Hanneforth, 2005). In the future, one could switch to a different tagset, such as that in Hana and Feldman (2010), which includes reflexivity, animacy, and aspect features. One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997), as suggested by Feldman and Hana (2010). Still, our lexicon continues the trend of linking traditional categories used for tagging with deeper analyses (Sharoff et al., 2008; Hana and Feldman, 2010).3 3.1 Finding segments/morphemes We use a set of hand-crafted rules to segment words into morphemes, of the form: if the tag is x and the word ends with y, make y the suffix. Such rules are easily and quickly derivable from a textbo"
C10-1030,W04-3229,0,0.0442434,"Missing"
C10-1030,J97-3003,0,0.0210575,"-available POS lexicon from Sharoff et al. (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information. Working with such a rich database, we only need segmentation, providing a quickly-obtained lexicon (cf. five years for a German lexicon in Geyken and Hanneforth, 2005). In the future, one could switch to a different tagset, such as that in Hana and Feldman (2010), which includes reflexivity, animacy, and aspect features. One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997), as suggested by Feldman and Hana (2010). Still, our lexicon continues the trend of linking traditional categories used for tagging with deeper analyses (Sharoff et al., 2008; Hana and Feldman, 2010).3 3.1 Finding segments/morphemes We use a set of hand-crafted rules to segment words into morphemes, of the form: if the tag is x and the word ends with y, make y the suffix. Such rules are easily and quickly derivable from a textbook listing of paradigms. For certain exceptional 2 POS tags are from the compositional tagset in Sharoff et al. (2008). A full description is at: http:// corpus.leeds."
C10-1030,N10-1018,0,0.0504774,"Missing"
C10-1030,N01-1024,0,0.0424566,"the word ends with y, make y the suffix. Such rules are easily and quickly derivable from a textbook listing of paradigms. For certain exceptional 2 POS tags are from the compositional tagset in Sharoff et al. (2008). A full description is at: http:// corpus.leeds.ac.uk/mocky/msd-ru.html. 3 This lexicon now includes lemma information, but each word is not segmented (Erjavec, 2010). cases, we write word-specific rules. Additionally, we remove word, tag pairs indicating punctuation or non-words (PUNC, SENT, -). One could use a sophisticated method for lemmatizing words (e.g., Chew et al., 2008; Schone and Jurafsky, 2001), but we would likely have to clean the lexicon later; as Feldman and Hana (2010) point out, it is difficult to automatically guess the entries for a word, without POS information. Essentially, we write precise rules to specify part of the Russian system of suffixes; the lexicon then provides the stems for free. We use the lexicon for generating errors, but it should be compatible with analysis. Thus, we focus on suffixes for beginning and intermediate learners. We can easily prune or add to the rule set later. From an analysis perspective, we need to specify that certain grammatical propertie"
C10-1030,C08-1109,0,0.020017,"anguages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context. 1 Introduction Work on detecting grammatical errors in the language of non-native speakers covers a range of errors, but it has largely focused on syntax in a small number of languages (e.g., Vandeventer Faltin, 2003; Tetreault and Chodorow, 2008). In more morphologically-rich languages, learners naturally make many errors in morphology (Dickinson and Herring, 2008). Yet for many languages, there is a major bottleneck in system development: there are not enough error-annotated learner corpora which can be mined to discover the nature of learner errors, let alone enough data to train or evaluate a system. Our perspective is that one can speed up the process of determining the nature of learner errors via semi-automatic means, by generating plausible errors. We set out to generate linguistically-plausible morphological errors for Russian"
C10-1030,sharoff-etal-2008-designing,0,\N,Missing
C12-1038,boyd-2010-eagle,0,0.0439876,"of string distance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into ac"
C12-1038,D11-1010,0,0.0149267,"12: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this p"
C12-1038,N12-1067,0,0.0368454,"a string-based definition can refer to both words as a single error. (2) a. The book in my class inspire me. b. The book in my class inspires me. c. The books in my class inspire me. There are several issues involved in choosing the unit size. First, the definition of unit size affects whether a system is given credit for finding all and only the errors, as these examples illustrate. Identifying inspire as an error may or may not be sufficient; identifying both words may be overkill. Defining error detection metrics in terms of edit distance mitigates this problem for evaluation comparisons (Dahlmeier and Ng, 2012), as correcting inspire to inspires is handled the same as book ... inspire corrected to book ... inspires. In essence, edit distance measures (EDMs) compare the system output to the correct string, ignoring exactly how it was derived. 619 Moreover, EDMs naturally handle multiple, overlapping errors, a problem for systems that target only a specific error type (Gamon, 2010; Rozovskaya and Roth, 2010a). Taking an example from Dahlmeier and Ng (2012), the sequence ... development set similar with test set ... can be corrected as a preposition selection error with → to and an adjacent article omi"
C12-1038,W12-2006,0,0.47295,"et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaningful comparisons between"
C12-1038,W10-4236,0,0.0232724,"r even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaning"
C12-1038,dale-narroway-2012-framework,0,0.0329995,"Missing"
C12-1038,W11-1410,1,0.932988,"ror detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and com"
C12-1038,dickinson-ledbetter-2012-annotating,1,0.843068,"t true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely done in Dale and Narroway (2011))."
C12-1038,N10-1019,0,0.0422643,"se in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy,"
C12-1038,W11-1422,0,0.0290787,"2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will enable readers to calculate other measures that the authors of a particular paper did"
C12-1038,han-etal-2010-using,1,0.627698,"e, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single"
C12-1038,W10-1802,0,0.053335,"istance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (w"
C12-1038,N12-1029,1,0.845321,"ccuracy (A), Precision (P), Recall (R), true-negative rate (TNR), and F-score (F1 ), as shown in Figure 2. System prediction Comma (+) No comma (-) Annotation (Gold Standard) Comma (+) No comma (-) TP FP FN TN Figure 1: The basis for typical NLP system evaluation T P+T N T P+T N +F P+F N TP T P+F P TP T P+F N Accuracy (A) = Precision (P)= Recall (R)= True Negative Rate (TNR) = F-measure (F1 ) = 2 · P·R P+R TN T N +F P Figure 2: Evaluation metrics 3.2 Error Detection and the Three-Way Contingency Table Now consider a task that is similar to comma restoration, the task of comma error detection (Israel et al., 2012), in which a system seeks to find and correct errors in the writer’s usage of commas. For this task, the positive class is not the presence of a comma but rather an error of the writer’s that involves a comma. Therefore, it is necessary to compare what the writer has written to an annotator’s judgment, and only if there is a mismatch between the two do we have an error (the positive class); when writer and annotator agree, the case is a non-error (the negative class). The traditional 2x2 table is no longer sufficient to represent all of the contingencies, which must instead be laid out in the"
C12-1038,P08-1021,0,0.0160292,"luation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential fo"
C12-1038,W12-3617,1,0.849254,"o way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely do"
C12-1038,W03-0209,0,0.0191481,"task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such as the Test of English as a Foreign Language (TOEFL). Improving writing quality To d"
C12-1038,P11-2089,1,0.69726,"Missing"
C12-1038,P11-1094,0,0.0174037,". Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will e"
C12-1038,W11-2111,1,0.821062,"n order to properly model behavior (Amaral and Meurers, 2007). For both purposes, error detection is needed to detect errors, suggest corrections, and provide information about the linguistic properties of the writer’s mistakes. Applications within NLP Grammatical error detection can be a useful component in correcting and evaluating text generated in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma),"
C12-1038,E12-1035,0,0.325261,"ects (TP) to the total number of errors in the Annotated gold standard (TP+FN); P compares TP to the total number of errors that the System reports (TP+FP); and F1 is the harmonic mean of R and P. Unfortunately, all three measures are affected by the proportion of cases that are annotated as errors in the gold standard (referred to as the prevalence of the errors, which is equal to (TP+FN)/N, where N is the total number of cases, i.e., N = TP+TN+FP+FN) and by the proportion of cases that are reported by the System as errors (referred to as the bias of the System, which is equal to (TP+FP)/N). Powers (2012) demonstrates how a system that performs no better than chance will nonetheless show an increase in R when prevalence increases and an increase in P when bias increases. To understand this behavior, we must consider what it means to perform at chance. If the class labels Error and No Error are assigned to cases independently by the Annotator and the System, then these labels are expected to match a proportion of the time by chance alone a proportion equal to the product of their probabilities. For example, the expected proportion of TP matches is equal to the product of the proportion of cases"
C12-1038,W10-1004,0,0.314583,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,N10-1018,0,0.101343,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,P11-1093,0,0.0376992,"sures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon"
C12-1038,N03-1029,0,0.0228136,"ed in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma), a comparison between the system’s output and the annotator’s judgments (the gold standard) can be organized as a two-by-two contingency table, shown in Figure 1. Presence of a comma is the target or positive class, and absence is the negative class. Positions in the text where both the system and the gold standard indicate that there should be a co"
C12-1038,W08-1205,1,0.759732,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,C08-1109,1,0.657801,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,P10-2065,1,0.845196,"use, as shown above, it does not reflect A, P, or R. This limits its value for decisions about whether a system is ready for use in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some resear"
C12-1038,P11-1019,0,0.0332827,"uation. Certain metrics are more or less appropriate depending on the type of task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such"
C12-2094,P11-1121,0,0.305105,", COLING 2012, Mumbai, December 2012. 965 1 Introduction Learner corpora are increasingly gaining attention due to the potential wealth of data they present for a variety of purposes, including the investigation of different aspects of interlanguage, the developing language of second language learners. Interlanguage (IL) often differs from the target language (L2), and the annotation of such corpora is an important means of accessing its unique characteristics (Granger, 2003). Such annotation has practical benefits for developing error detection systems and intelligent tutoring systems (e.g., Nagata et al., 2011; Rozovskaya and Roth, 2010), by providing data and insights for the parsing of learner language. While many benefits have been derived from error annotation, a recent approach to annotating learner language is to annotate the linguistic properties of a text to provide direct access to grammatical properties of interest (e.g., Díaz-Negrillo et al., 2010; Dickinson and Ragheb, 2009; Rastelli, 2009; Pienemann, 1992). To account for IL, multiple layers of analysis—e.g., three separate part-of-speech (POS) layers—have been proposed to capture learner innovations. These layers have yet to be proper"
C12-2094,N10-1049,0,0.0414838,"ctual form. (5) I must play with he. For cases of agreement, the head may not be the locus of agreement. Consider (6), where the subject-verb disagreement affects the forms of both tokens. In this case, the verb is the head, but the subject could be considered the source of the agreement features. This is why, instead of being head-driven, we speak of one token predicting another token’s properties. (6) He play by toys. The exact treatment for cases of agreement depends upon defining the source of agreement in one’s syntactic theory—as annotation depends upon the theory employed (Leech, 2004; Rambow, 2010). For most label inventories, there is no distinction for agreement, but if there were (e.g., SUBJ3s vs. SUBJp), one could choose to use the relation driven by the dependent (SUBJ3s), since the prediction works “backwards.” This would directly contradict the headdriven subcategorization (SUBJpl), specifying that the L2 requires a verb which agrees. The interaction with morphosyntax—which would underspecify (to SUBJ or to nothing)—is then similar to the case of adjuncts, as in section 6 (see the discussion around He runs quick). Adjuncts Adjuncts select for their heads (Pollard and Sag, 1994),"
C12-2094,W10-1004,0,0.0617817,", December 2012. 965 1 Introduction Learner corpora are increasingly gaining attention due to the potential wealth of data they present for a variety of purposes, including the investigation of different aspects of interlanguage, the developing language of second language learners. Interlanguage (IL) often differs from the target language (L2), and the annotation of such corpora is an important means of accessing its unique characteristics (Granger, 2003). Such annotation has practical benefits for developing error detection systems and intelligent tutoring systems (e.g., Nagata et al., 2011; Rozovskaya and Roth, 2010), by providing data and insights for the parsing of learner language. While many benefits have been derived from error annotation, a recent approach to annotating learner language is to annotate the linguistic properties of a text to provide direct access to grammatical properties of interest (e.g., Díaz-Negrillo et al., 2010; Dickinson and Ragheb, 2009; Rastelli, 2009; Pienemann, 1992). To account for IL, multiple layers of analysis—e.g., three separate part-of-speech (POS) layers—have been proposed to capture learner innovations. These layers have yet to be properly defined for syntax, howev"
C12-2094,W07-0604,0,0.0738808,"Missing"
C12-2094,P10-2065,0,0.0773224,"he importance of providing a linguistic description of learner text for second language acquisition (SLA) (Ragheb and Dickinson, 2011). To see the need more broadly, consider that there has been very little work investigating POS tagging (Thouësny, 2009; van Rooy and Schäfer, 2002; de Haan, 2000) or parsing (Rehbein et al., 2012; Krivanek and Meurers, 2011; Ott and Ziai, 2010) of learner language, due to a lack of annotated data or clear standards. Furthermore, the studies on parsing often first map to a target form, while many situations—such as extracting parse features for error detection (Tetreault et al., 2010) or identifying criterial features indicating learner proficiency level (Hawkins and Buttery, 2010)—require direct parsing of learner language. Defining and applying syntactic annotation provides a clearer picture of the goal for parsing learner language, and evaluation data to do so. Only by developing such annotation can research into POS tagging and syntactic parsing for learner language make serious advancements. As mentioned, proposals for linguistic annotation split categories into multiple layers. In proliferating categories, however, we must ask what these categories denote and whether"
dickinson-jochim-2008-simple,J93-2004,0,\N,Missing
dickinson-jochim-2008-simple,E95-1029,0,\N,Missing
dickinson-jochim-2008-simple,W00-1308,0,\N,Missing
dickinson-jochim-2008-simple,E03-1068,1,\N,Missing
dickinson-jochim-2008-simple,D07-1066,0,\N,Missing
dickinson-jochim-2008-simple,P95-1039,0,\N,Missing
dickinson-jochim-2008-simple,dejean-2000-evaluate,0,\N,Missing
dickinson-jochim-2008-simple,I05-3005,0,\N,Missing
dickinson-jochim-2010-evaluating,N04-1043,0,\N,Missing
dickinson-jochim-2010-evaluating,zeman-2008-reusable,0,\N,Missing
dickinson-jochim-2010-evaluating,dickinson-jochim-2008-simple,1,\N,Missing
dickinson-jochim-2010-evaluating,J93-2004,0,\N,Missing
dickinson-jochim-2010-evaluating,bosco-etal-2000-building,0,\N,Missing
dickinson-jochim-2010-evaluating,E03-1009,0,\N,Missing
dickinson-jochim-2010-evaluating,W09-0905,1,\N,Missing
dickinson-jochim-2010-evaluating,P08-1085,0,\N,Missing
dickinson-jochim-2010-evaluating,A00-1031,0,\N,Missing
dickinson-jochim-2010-evaluating,dejean-2000-evaluate,0,\N,Missing
dickinson-jochim-2010-evaluating,P07-1094,0,\N,Missing
dickinson-jochim-2010-evaluating,P05-1044,0,\N,Missing
dickinson-ledbetter-2012-annotating,W10-1004,0,\N,Missing
dickinson-ledbetter-2012-annotating,W10-1802,0,\N,Missing
dickinson-ledbetter-2012-annotating,boyd-2010-eagle,0,\N,Missing
dickinson-lee-2008-detecting,burchardt-etal-2006-salsa,0,\N,Missing
dickinson-lee-2008-detecting,C04-1100,0,\N,Missing
dickinson-lee-2008-detecting,E03-1068,1,\N,Missing
dickinson-lee-2008-detecting,W04-3212,0,\N,Missing
dickinson-lee-2008-detecting,C04-1080,0,\N,Missing
dickinson-lee-2008-detecting,P05-1040,1,\N,Missing
dickinson-lee-2008-detecting,W06-0609,0,\N,Missing
dickinson-lee-2008-detecting,P98-1013,0,\N,Missing
dickinson-lee-2008-detecting,C98-1013,0,\N,Missing
dickinson-lee-2008-detecting,P07-1086,0,\N,Missing
dickinson-lee-2008-detecting,W05-0620,0,\N,Missing
dickinson-lee-2008-detecting,P05-1073,0,\N,Missing
dickinson-lee-2008-detecting,P98-2164,0,\N,Missing
dickinson-lee-2008-detecting,C98-2159,0,\N,Missing
dickinson-lee-2008-detecting,P03-1002,0,\N,Missing
dickinson-lee-2008-detecting,J02-3001,0,\N,Missing
dickinson-lee-2008-detecting,J01-2002,0,\N,Missing
dickinson-lee-2008-detecting,J05-1004,0,\N,Missing
dickinson-lee-2008-detecting,2006.iwslt-evaluation.11,0,\N,Missing
dickinson-lee-2008-detecting,W04-2412,0,\N,Missing
dickinson-lee-2008-detecting,W06-0600,0,\N,Missing
E03-1068,W99-0606,0,0.0129456,"Missing"
E03-1068,brants-2000-inter,0,0.0923122,"Missing"
E03-1068,C00-1046,0,0.045323,"Missing"
E03-1068,C94-1103,0,0.0288718,"tment of Linguistics The Ohio State University dickinso@ling.osu.edu Abstract We propose a new method for detecting errors in ""gold-standard"" part-ofspeech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finitestate tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Treebank. 1 Introduction Part-of-speech (pos) annotated reference corpora, such as the British National Corpus (Leech et al., 1994), the Penn Treebank (Marcus et al., 1993), or the German Negra Treebank (Skut et al., 1997) play an important role for current work in computational linguistics. They provide training material for research on tagging algorithms and they serve as a gold standard for evaluating the performance of such tools. High quality, pos-annotated text is also relevant as input for syntactic processing, for practical applications such as information extraction, and for linguistic research making use of pos-based corpus queries. The gold-standard pos-annotation for such large reference corpora is generally o"
E03-1068,J93-2004,0,0.0280505,"Missing"
E03-1068,C02-1131,0,0.0379374,"Missing"
E03-1068,W96-0213,0,0.283799,"d, is independent of the particular language and tagset of the corpus, and requires no additional language resources such as lexica. We showed that an instance of this method based on identity of words in the variation contexts, so-called variation n-grams, successfully detects a variety of errors in the WSJ corpus. The usefulness of the notion of a variation ngram relies on a particular word to appear several times in a corpus, with different annotations. It thus works best for large corpora and hand-annotated or hand-corrected corpora, or corpora involving other sources of inconsistency. As Ratnaparkhi (1996) points out, interannotator bias creates inconsistencies which a completely automatically-tagged corpus does not have. And Baker (1997) makes the point that a human posteditor also decreases the internal consistency of the tagged data since he will spot a mistake made by an automatic tagger for some but not all of its occurrences. As a result, our variation n-gram approach is well suited for the gold-standard annotations generally resulting from a combination of automatic annotation and manual post-editing. A case in point is that we recently applied the variation n-gram algorithm to the BNC-s"
E03-1068,A97-1014,0,0.0172525,"ew method for detecting errors in ""gold-standard"" part-ofspeech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finitestate tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Treebank. 1 Introduction Part-of-speech (pos) annotated reference corpora, such as the British National Corpus (Leech et al., 1994), the Penn Treebank (Marcus et al., 1993), or the German Negra Treebank (Skut et al., 1997) play an important role for current work in computational linguistics. They provide training material for research on tagging algorithms and they serve as a gold standard for evaluating the performance of such tools. High quality, pos-annotated text is also relevant as input for syntactic processing, for practical applications such as information extraction, and for linguistic research making use of pos-based corpus queries. The gold-standard pos-annotation for such large reference corpora is generally obtained using an automatic tagger to produce a first annotation, followed by human post-edi"
E03-1068,E95-1029,0,0.1335,"Missing"
E03-1068,W02-1015,0,\N,Missing
E03-1068,W00-1907,0,\N,Missing
E06-1034,A00-1031,0,0.212303,"Missing"
E06-1034,A92-1018,0,0.145771,"out behaves differently than the rest of its class—but the altered contextual probabilities, unlike a lexicalized tagger, bring general IN/RB/RP class information to bear on this tagging situation. Combining the two, we get the correct tag RB at this position. Since variation errors are errors for words with prominent ambiguity classes, zeroing in on these ambiguity classes should provide more accurate probabilities. For this to work, however, we have to ensure that we have the most effective ambiguity class for every word. 5.2 Assigning complex ambiguity tags In the tagging literature (e.g., Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word. For correction, using every possible tag for an ambiguity class will result in too many classes, for two reasons: 1) there are erroneous tags which should not be part of the ambiguity class, and 2) some classes are irrelevant for disambiguating variation positions. Guided by these considerations, we use the procedure below to assign complex ambiguity tags to all words in the corpus, based on whether a word is a non-fringe variation nucleus and thus flagged as a potential error by the variation n-gram method (ch"
E06-1034,dejean-2000-evaluate,0,0.0358733,"Missing"
E06-1034,E03-1068,1,0.893663,"linguistics, and as a source of data for theoretical linguists searching for relevant language patterns. However, they contain annotation errors, and such errors provide unreliable training and evaluation data, as has been previously shown (see ch. 1 of Dickinson (2005) and references therein). Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics. Research has gone into automatically detecting annotation errors for part-of-speech annotation (van Halteren, 2000; Kvˇetˇon and Oliva, 2002; Dickinson and Meurers, 2003), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.1 1 Oliva (2001) specifies hand-written rules to detect and Automatic correction can speed up corpus improvement efforts and provide new data for NLP technology training on the corpus. Additionally, an investigation into automatic correction forces us to re-evaluate the technology using the corpus, providing new insights into such technology. We propose in this paper to automatically correct part-of-speech (POS) annotation errors in corpora, by adapting existing technology for POS dis"
E06-1034,J93-2004,0,0.0321562,"Missing"
E06-1034,P97-1031,0,0.0620781,"Missing"
E06-1034,W00-1907,0,0.393173,"Missing"
E09-1023,E06-1034,1,0.82808,"harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form In order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases. The data representation must therefore be such that it clearly allows for the specific identification of errors between words. Thus, we explore relatively simple models of the data, emphasizing small substructures (se"
E09-1023,P05-1040,1,0.837814,"pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R). This is tantamount to handling pairs of words as single entries in a “lexicon” and provides a natural way to talk of ambiguities. Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickinson and Meurers, 2005). A major issue in generating a lexicon is how to handle pairs of words which are not dependencies. We follow Boyd et al. (2008) and generate NIL labels for those pairs of words which also occur as a true labeled relation. In other words, only word pairs which can be relations can also be NILs. For every sentence, then, when we produce feature lists (see section 3.3), we produce them for all word pairs that are related or could potentially be related, but not those which have never been observed as a dependency pair. This selection of NIL items works because there are no unknown words. We use"
E09-1023,D07-1116,0,0.0137596,"een words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in whi"
E09-1023,P07-1086,0,0.0313565,"elations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully"
E09-1023,P02-1017,0,0.0120764,"r as a genuine ambiguity. The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus. The nucleus with its repeated surrounding context is referred to as a variation n-gram. While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008). This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002). The approach can detect both bracketing and labeling errors in constituency annotation, and we already saw a labeling error for next Tuesday. As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL. The method for detecting annotation errors can be extended to discontinuous constituency annota3 3.1 Modeling the data The data For our data set, we use the written portion (sections P and G) of the Swedish Talbanken05 t"
E09-1023,W06-2932,0,0.0194188,"order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases. The data representation must therefore be such that it clearly allows for the specific identification of errors between words. Thus, we explore relatively simple models of the data, emphasizing small substructures (see section 3.2). This simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system. Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006). Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193–201, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computa"
E09-1023,D07-1013,0,0.0305983,", in order to learn the true patterns underlying the data. This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006). Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R). This is tantamount to handling pairs of words as single entries in a “lexicon” and provides a natural way to talk of ambiguities. Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickinson and Meurers, 2005). A major issue in generating a lexico"
E09-1023,E06-1011,0,0.0170232,"nd in dependency parsing (e.g., Canisius et al., 2006; Chen et al., 2008). Most directly, Canisius et al. (2006) integrate such a representation into a memory-based dependency parser, treating each pair individually, with words and POS tags as features. Individual relations Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data. This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006). Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the"
E09-1023,P08-1108,0,0.060827,"Missing"
E09-1023,nivre-etal-2006-talbanken05,0,0.0764406,"Missing"
E09-1023,P98-2164,0,0.047981,"el to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus"
E09-1023,D08-1017,0,0.018437,"ection 4.2. On the other hand, one can use one set of features to constrain another set, as described in section 5. Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification. This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951). The output from one classifier is used as a feature in the next one (see also Torres Martins et al., 2008). Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1. 4 4.1 (1) k¨arlekens v¨ag a¨ r/AV en l˚ang v¨ag/NN och love’s a long way and way is ... ... Secondly, in a surprising number of cases where there is a majority tag (122 out of the 917 tokens we have a correction for), a non-majority label is actual"
E09-1023,W97-1016,0,\N,Missing
E09-1023,C08-1015,0,\N,Missing
E09-1023,C98-2159,0,\N,Missing
E09-1023,W06-2924,0,\N,Missing
E09-1023,P08-1000,0,\N,Missing
eom-etal-2012-using,W98-0710,0,\N,Missing
eom-etal-2012-using,D08-1027,0,\N,Missing
eom-etal-2012-using,S07-1006,0,\N,Missing
eom-etal-2012-using,D09-1046,0,\N,Missing
eom-etal-2012-using,N09-5005,0,\N,Missing
eom-etal-2012-using,C96-2140,0,\N,Missing
eom-etal-2012-using,P10-1023,0,\N,Missing
eom-etal-2012-using,P11-2089,0,\N,Missing
eom-etal-2012-using,P09-1002,0,\N,Missing
eom-etal-2012-using,I11-1099,0,\N,Missing
eom-etal-2012-using,W11-0122,0,\N,Missing
eom-etal-2012-using,D11-1143,0,\N,Missing
I13-1199,C12-1038,1,0.908659,"error detection, but employ Korean-specific features and filters; likewise, output analysis and the omission correction system illustrate how unique properties of Korean, such as the distinct types of particles used, need to be accounted for in adapting the system, thereby moving the field one step closer to robust multi-lingual methods. 1 Introduction Grammatical error detection is useful to produce an improved final document for writing assistance, provide feedback to language learners, provide features for automatic essay scoring, and postedit machine translation output (see references in Chodorow et al., 2012, sec. 2). Within this growing field, most of the work has focused on English, but there has been a small community of researchers working on other languages. We continue this trend by advancing the state-of-the-art in detecting errors in Korean particle usage. Expanding to other languages and language families obviously presents new challenges, such as being able to handle word segmentation and greater morphological complexity (e.g., Basque (de Ilarraza et al., 2008), Korean (Lee et al., 2012), Hungarian (Dickinson and Ledbetter, 2012), Japanese (Mizumoto et al., 2011)); greater varieties of"
I13-1199,W07-1604,0,0.119326,"able 1: Annotated corpus statistics (sentences, tokens, nouns, particles, errors, omissions) Particle errors are marked as omissions, insertions (comissions), or substitutions, in a multilayered framework. Spacing and spelling errors are corrected before the target form and correct segmentation are marked, segmentation being necessary since nouns and particles are written as a single orthographic unit. For our experiments, we use the correctly-spelled layer, mitigating the effect of spelling errors for testing an error detection system, as done for English (e.g., Tetreault and Chodorow, 2008; Chodorow et al., 2007). All particles (erroneous or correct) are labeled as to their function (e.g., locative), allowing us to group particles into categories, to see how classifier performance differs. Figure 1 provides the four groups we consider (cf. tables 5 and 6). Additionally, some nominals require multiple particles in sequence (Seq.), and some of the annotations allow for particles from more than one category as a correct answer, i.e., a set of correct answers (Set). 4.3 Learner Error Analysis Lee et al. (2009) annotate another corpus of learner Korean, divided using the same four-way split among learner l"
I13-1199,W12-2006,0,0.0769373,"Missing"
I13-1199,C08-2008,0,0.215817,"Missing"
I13-1199,W10-1502,1,0.873342,"Missing"
I13-1199,W11-1410,1,0.883773,"Missing"
I13-1199,dickinson-ledbetter-2012-annotating,1,0.512726,"coring, and postedit machine translation output (see references in Chodorow et al., 2012, sec. 2). Within this growing field, most of the work has focused on English, but there has been a small community of researchers working on other languages. We continue this trend by advancing the state-of-the-art in detecting errors in Korean particle usage. Expanding to other languages and language families obviously presents new challenges, such as being able to handle word segmentation and greater morphological complexity (e.g., Basque (de Ilarraza et al., 2008), Korean (Lee et al., 2012), Hungarian (Dickinson and Ledbetter, 2012), Japanese (Mizumoto et al., 2011)); greater varieties of word order (Czech (Hana et al., 2010), Sun-Hee Lee Wellesley College slee6@wellesley.edu German (Boyd, 2012)); case ending errors (Czech, German, Hungarian); differing definitions of function words (Korean, Japanese, Basque); and so forth. Investing in methods which apply across languages will make techniques more robust and applicable for even more languages. An additional challenge for many of these languages is the lack of resources. Much previous work on detecting errors in Korean, for example, focused less on techniques and more on"
I13-1199,I08-1059,0,0.19543,"Missing"
I13-1199,W10-1802,0,0.148531,"Missing"
I13-1199,P12-2076,0,0.386435,"instances of a single particle (wo). Mizumoto et al. (2011) use statistical machine translation (SMT) techniques to detect and correct all errors within Japanese, using a “parallel” cor1420 pus of ill-formed and correctly-formed Japanese, based on correction logs from a collaborative language learning website. Our paradigm is much different, basing our method only on a correct model of the target language, given a relative lack of corrected data available in Korean and other lesserresourced languages. We are, however, able to use some correction logs for building confusion sets (section 7.1). Imamura et al. (2012) correct Japanese particle errors using an approach similar to SMT ones, relying on a corpus of generated errors to learn a model of alignment to correct forms. We could explore generated errors in the future, but rely only on a model of correct Korean here. Suzuki and Toutanova (2006) predict case markers in Japanese for an MT system, basing their techniques on semantic role labeling. They predict 18 case particles, a subset of all Japanese particles. They use a two-stage classifier, first identifying whether case is needed and then assigning the particular case ending, training the second cl"
I13-1199,N12-1029,1,0.812801,"n training size and complexity. Secondly, many errors can be found at this stage, as a lot of errors stem from learners omitting necessary particles (see section 4.3). Nearly half of the learner errors could be detected with an accurate omission particle detection system at this step. Thus, this classifier can provide useful feedback to learners, 6.1 CRF Classifier Conditional Random Fields (CRFs) have been utilized in a variety of NLP tasks in the last few years, and have been used recently for leaner error detection tasks, especially those which can be seen as sequence labeling tasks (e.g., Israel et al., 2012; Tajiri et al., 2012; Imamura et al., 2012). We use the comma error detection work in Israel et al. (2012) as a basis, and employ CRF++3 to set up a binary classifier at this step based on 1.5 million instances from our web corpus. Here we consider all nominals, as annotated in the corpus, as possible candidates for particle insertion. When we derive features based on POS tags (section 6.2), however, we rely on an automatic POS tagger. 6.2 Features The feature set for particle omission detection is mainly composed of words and POS tags in the surrounding context, where tags are derived from a"
I13-1199,W12-3617,1,0.849354,"features for automatic essay scoring, and postedit machine translation output (see references in Chodorow et al., 2012, sec. 2). Within this growing field, most of the work has focused on English, but there has been a small community of researchers working on other languages. We continue this trend by advancing the state-of-the-art in detecting errors in Korean particle usage. Expanding to other languages and language families obviously presents new challenges, such as being able to handle word segmentation and greater morphological complexity (e.g., Basque (de Ilarraza et al., 2008), Korean (Lee et al., 2012), Hungarian (Dickinson and Ledbetter, 2012), Japanese (Mizumoto et al., 2011)); greater varieties of word order (Czech (Hana et al., 2010), Sun-Hee Lee Wellesley College slee6@wellesley.edu German (Boyd, 2012)); case ending errors (Czech, German, Hungarian); differing definitions of function words (Korean, Japanese, Basque); and so forth. Investing in methods which apply across languages will make techniques more robust and applicable for even more languages. An additional challenge for many of these languages is the lack of resources. Much previous work on detecting errors in Korean, for exam"
I13-1199,I11-1017,0,0.0464517,"utput (see references in Chodorow et al., 2012, sec. 2). Within this growing field, most of the work has focused on English, but there has been a small community of researchers working on other languages. We continue this trend by advancing the state-of-the-art in detecting errors in Korean particle usage. Expanding to other languages and language families obviously presents new challenges, such as being able to handle word segmentation and greater morphological complexity (e.g., Basque (de Ilarraza et al., 2008), Korean (Lee et al., 2012), Hungarian (Dickinson and Ledbetter, 2012), Japanese (Mizumoto et al., 2011)); greater varieties of word order (Czech (Hana et al., 2010), Sun-Hee Lee Wellesley College slee6@wellesley.edu German (Boyd, 2012)); case ending errors (Czech, German, Hungarian); differing definitions of function words (Korean, Japanese, Basque); and so forth. Investing in methods which apply across languages will make techniques more robust and applicable for even more languages. An additional challenge for many of these languages is the lack of resources. Much previous work on detecting errors in Korean, for example, focused less on techniques and more on acquiring training data (Dickinso"
I13-1199,D10-1094,0,0.154878,"Missing"
I13-1199,P06-1132,0,0.0322695,"tive language learning website. Our paradigm is much different, basing our method only on a correct model of the target language, given a relative lack of corrected data available in Korean and other lesserresourced languages. We are, however, able to use some correction logs for building confusion sets (section 7.1). Imamura et al. (2012) correct Japanese particle errors using an approach similar to SMT ones, relying on a corpus of generated errors to learn a model of alignment to correct forms. We could explore generated errors in the future, but rely only on a model of correct Korean here. Suzuki and Toutanova (2006) predict case markers in Japanese for an MT system, basing their techniques on semantic role labeling. They predict 18 case particles, a subset of all Japanese particles. They use a two-stage classifier, first identifying whether case is needed and then assigning the particular case ending, training the second classifier only on instances where a case marker was required. This breakdown and parts of their feature sets are similar to ours, but: a) they use (gold standard) parse features and treat the problem as one of predicting markers for phrases; and b) they correct machine errors, while we"
I13-1199,P12-2039,0,0.0491162,"omplexity. Secondly, many errors can be found at this stage, as a lot of errors stem from learners omitting necessary particles (see section 4.3). Nearly half of the learner errors could be detected with an accurate omission particle detection system at this step. Thus, this classifier can provide useful feedback to learners, 6.1 CRF Classifier Conditional Random Fields (CRFs) have been utilized in a variety of NLP tasks in the last few years, and have been used recently for leaner error detection tasks, especially those which can be seen as sequence labeling tasks (e.g., Israel et al., 2012; Tajiri et al., 2012; Imamura et al., 2012). We use the comma error detection work in Israel et al. (2012) as a basis, and employ CRF++3 to set up a binary classifier at this step based on 1.5 million instances from our web corpus. Here we consider all nominals, as annotated in the corpus, as possible candidates for particle insertion. When we derive features based on POS tags (section 6.2), however, we rely on an automatic POS tagger. 6.2 Features The feature set for particle omission detection is mainly composed of words and POS tags in the surrounding context, where tags are derived from a POS tagger (Han and"
I13-1199,C08-1109,0,0.0720435,". Om. 103 51 430 234 533 285 Table 1: Annotated corpus statistics (sentences, tokens, nouns, particles, errors, omissions) Particle errors are marked as omissions, insertions (comissions), or substitutions, in a multilayered framework. Spacing and spelling errors are corrected before the target form and correct segmentation are marked, segmentation being necessary since nouns and particles are written as a single orthographic unit. For our experiments, we use the correctly-spelled layer, mitigating the effect of spelling errors for testing an error detection system, as done for English (e.g., Tetreault and Chodorow, 2008; Chodorow et al., 2007). All particles (erroneous or correct) are labeled as to their function (e.g., locative), allowing us to group particles into categories, to see how classifier performance differs. Figure 1 provides the four groups we consider (cf. tables 5 and 6). Additionally, some nominals require multiple particles in sequence (Seq.), and some of the annotations allow for particles from more than one category as a correct answer, i.e., a set of correct answers (Set). 4.3 Learner Error Analysis Lee et al. (2009) annotate another corpus of learner Korean, divided using the same four-w"
P05-1040,E03-1068,1,0.932893,"pressions (e.g., Rayson et al., 2004), as well as for spoken language corpora or corpora with multiple layers of annotation which cross boundaries (e.g., Blache and Hirst, 2000). In this paper, we present an approach to the detection of errors in discontinuous structural annotation. We focus on syntactic annotation with potentially discontinuous constituents and show that the approach successfully deals with the discontinuous syntactic annotation found in the TIGER treebank (Brants et al., 2002). 2 The variation n-gram method Our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003a,b). The basic idea behind that approach is that a string occurring more than once can occur with different labels in a corpus, which we refer to as variation. Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options, or ii) error: the tagging of a string is inconsistent across comparable occurrences. 1 The ordinary way of marking a constituent with brackets is inadequate for discontinuous constituents, so we instead boldface and underline the words belonging t"
P05-1040,J93-2004,0,0.0254548,"Missing"
P05-1040,A97-1014,0,0.0764029,"Missing"
P05-1040,W00-1907,0,\N,Missing
P05-1040,P98-2164,0,\N,Missing
P05-1040,C98-2159,0,\N,Missing
P05-1040,J01-2002,0,\N,Missing
P08-1042,P05-1022,0,0.0329721,"Missing"
P08-1042,J05-1003,0,0.0234133,"n scheme. Phrases starting with due to are sometimes annotated with this rule, but they also occur as ADJP or ADVP or with due as RB. If PP → JJ PP is correct, identifying this rule actually points to other erroneous rules. 365 Bigram scoring The other method of detecting ad hoc rules calculates reliability scores by focusing specifically on what the classes do not have in common. Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). We abstract to bigrams, including added START and END tags, as longer sequences risk missing generalizations; e.g., unary rules would have no comparable rules. We score rule types as follows: 1. Map a rule to its equivalence class, resulting in a reduced rule. 2. Calculate the frequency of each <mother,bigram&gt; pair in a reduced rule: for every reduced rule token with the same pair, add a score of 1 for that bigram pair. 3. Assign the score of the least-frequent bigram as the score of the rule. We assign the score of the lowest-scoring bigram because we are interested in anomalous sequences."
P08-1042,W06-2305,0,0.0432021,"w data. This is why low-frequency rules often do not extend to new data: if they were only used once, it was likely for a specific reason, not something we would expect to see again. Ungeneralizable rules, however, do not extend to new text for a variety of reasons, not all of which can be captured strictly by frequency. Introduction and Motivation When extracting rules from constituency-based treebanks employing flat structures, grammars often limit the set of rules (e.g., Charniak, 1996), due to the large number of rules (Krotov et al., 1998) and “leaky” rules that can lead to mis-analysis (Foth and Menzel, 2006). Although frequency-based criteria are often used, these are not without problems because low-frequency rules can be valid and potentially useful rules (see, e.g., Daelemans et al., 1999), and high-frequency rules can be erroneous (see., e.g., Dickinson and Meurers, 2005). A key issue in determining the rule set is rule generalizability: will these rules be needed to analyze new data? This issue is of even more importance when considering the task of porting a parser trained on one genre to another genre (e.g., Gildea, 2001). Infrequent rules in one genre may be quite frequent in While there"
P08-1042,W01-0521,0,0.136676,"al., 1998) and “leaky” rules that can lead to mis-analysis (Foth and Menzel, 2006). Although frequency-based criteria are often used, these are not without problems because low-frequency rules can be valid and potentially useful rules (see, e.g., Daelemans et al., 1999), and high-frequency rules can be erroneous (see., e.g., Dickinson and Meurers, 2005). A key issue in determining the rule set is rule generalizability: will these rules be needed to analyze new data? This issue is of even more importance when considering the task of porting a parser trained on one genre to another genre (e.g., Gildea, 2001). Infrequent rules in one genre may be quite frequent in While there are simply phenomena which, for various reasons, are rarely used (e.g., long coordinated lists), other ungeneralizable phenomena are potentially more troubling. For example, when ungrammatical or non-standard text is used, treebanks employ rules to cover it, but do not usually indicate ungrammaticality in the annotation. These rules are only to be used in certain situations, e.g., for typographical conventions such as footnotes, and the fact that the situation is irregular would be useful to know if the purpose of an induced"
P08-1042,P07-1086,0,0.0160202,"e.g., Metcalf and Boyd, 362 Proceedings of ACL-08: HLT, pages 362–370, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2006), then one needs to isolate rules for ungrammatical data, to be able to distinguish grammatical from ungrammatical input. Detecting ad hoc rules can also reveal issues related to rule quality. Many ad hoc rules exist because they are erroneous. Not only are errors inherently undesirable for obtaining an accurate grammar, but training on data with erroneous rules can be detrimental to parsing performance (e.g., Dickinson and Meurers, 2005; Hogan, 2007) As annotation schemes are not guaranteed to be completely consistent, other ad hoc rules point to non-uniform aspects of the annotation scheme. Thus, identifying ad hoc rules can also provide feedback on annotation schemes, an especially important step if one is to use the treebank for specific applications (see, e.g., Vadas and Curran, 2007), or if one is in the process of developing a treebank. Although statistical techniques have been employed to detect anomalous annotation (Ule and Simov, 2004; Eskin, 2000), these methods do not account for linguistically-motivated generalizations across"
P08-1042,P98-1115,0,0.0268938,"r constructions specific to one data set and unlikely to be used on new data. This is why low-frequency rules often do not extend to new data: if they were only used once, it was likely for a specific reason, not something we would expect to see again. Ungeneralizable rules, however, do not extend to new text for a variety of reasons, not all of which can be captured strictly by frequency. Introduction and Motivation When extracting rules from constituency-based treebanks employing flat structures, grammars often limit the set of rules (e.g., Charniak, 1996), due to the large number of rules (Krotov et al., 1998) and “leaky” rules that can lead to mis-analysis (Foth and Menzel, 2006). Although frequency-based criteria are often used, these are not without problems because low-frequency rules can be valid and potentially useful rules (see, e.g., Daelemans et al., 1999), and high-frequency rules can be erroneous (see., e.g., Dickinson and Meurers, 2005). A key issue in determining the rule set is rule generalizability: will these rules be needed to analyze new data? This issue is of even more importance when considering the task of porting a parser trained on one genre to another genre (e.g., Gildea, 20"
P08-1042,J93-2004,0,0.0315782,"Missing"
P08-1042,A97-1015,0,0.720118,"Missing"
P08-1042,ule-simov-2004-unexpected,0,0.0287456,"data with erroneous rules can be detrimental to parsing performance (e.g., Dickinson and Meurers, 2005; Hogan, 2007) As annotation schemes are not guaranteed to be completely consistent, other ad hoc rules point to non-uniform aspects of the annotation scheme. Thus, identifying ad hoc rules can also provide feedback on annotation schemes, an especially important step if one is to use the treebank for specific applications (see, e.g., Vadas and Curran, 2007), or if one is in the process of developing a treebank. Although statistical techniques have been employed to detect anomalous annotation (Ule and Simov, 2004; Eskin, 2000), these methods do not account for linguistically-motivated generalizations across rules, and no full evaluation has been done on a treebank. Our starting point for detecting ad hoc rules is also that they are dissimilar to the rest of the grammar, but we rely on a notion of equivalence which accounts for linguistic generalizations, as described in section 2. We generalize equivalence in a corpus-independent way in section 3 to detect ad hoc rules, using two different methods to determine when rules are dissimilar. The results in section 4 show the success of the method in identi"
P08-1042,P07-1031,0,0.022197,"o rule quality. Many ad hoc rules exist because they are erroneous. Not only are errors inherently undesirable for obtaining an accurate grammar, but training on data with erroneous rules can be detrimental to parsing performance (e.g., Dickinson and Meurers, 2005; Hogan, 2007) As annotation schemes are not guaranteed to be completely consistent, other ad hoc rules point to non-uniform aspects of the annotation scheme. Thus, identifying ad hoc rules can also provide feedback on annotation schemes, an especially important step if one is to use the treebank for specific applications (see, e.g., Vadas and Curran, 2007), or if one is in the process of developing a treebank. Although statistical techniques have been employed to detect anomalous annotation (Ule and Simov, 2004; Eskin, 2000), these methods do not account for linguistically-motivated generalizations across rules, and no full evaluation has been done on a treebank. Our starting point for detecting ad hoc rules is also that they are dissimilar to the rest of the grammar, but we rely on a notion of equivalence which accounts for linguistic generalizations, as described in section 2. We generalize equivalence in a corpus-independent way in section 3"
P08-1042,D07-1012,0,0.0236545,"Missing"
P08-1042,C98-1111,0,\N,Missing
P08-1042,P02-1016,0,\N,Missing
P08-1042,A00-2020,0,\N,Missing
P10-1075,afonso-etal-2002-floresta,0,0.0691254,"Missing"
P10-1075,E09-1060,0,0.0301935,"F → SV AA:VV and +F → ++:++ AA:VV have very little support in the training data, AA:VV receives a low score. Note that the subrule count C(sub, c) is different than counting the number of rules containing a subrule, as can be seen with identical elements. For example, for SS → VN ET:PR ET:PR, C(VN ET:PR, SS) = 2, in keeping with the fact that there are 2 pieces of evidence for its legitimacy. 3.4 3.4.1 Implementation 4 Additional information The methods presented so far have limited definitions of comparability. As using complementary information has been useful in, e.g., POS error detection (Loftsson, 2009), we explore other simple comparable properties of a dependency grammar. Namely, we include: a) frequency information of an overall dependency rule and b) information on how likely each dependent is to be in a relation with its head, described next. Bigram anomalies 4.1 Motivation Including POS information Consider PA → SS:NN XX:XX HV OO:VN, as illustrated in figure 5 for the sentence in (8). This rule is entirely correct, yet the XX:XX position has low whole rule and bigram scores. The bigram method examines relationships between adjacent sisters, complementing the whole rule method by focusi"
P10-1075,N07-1049,0,0.415195,"otation checks, we need an approach which relies on more general properties of the dependency structures, in order to develop techniques which work for automatically-parsed corpora. Developing techniques to detect errors in parses in a way which is independent of corpus and parser has fairly broad implications. By using only the information available in a training corpus, the methods we explore are applicable to annotation error detection for either hand-annotated or automatically-parsed corpora and can also provide insights for parse reranking (e.g., Hall and Nov´ak, 2005) or parse revision (Attardi and Ciaramita, 2007). Although we focus only on detecting errors in automatically-parsed data, similar techniques have been applied for hand-annotated data (Dickinson, 2008; Dickinson and Foster, 2009). Our general approach is based on extracting a grammar from an annotated corpus and comparing dependency rules in a new (automaticallyannotated) corpus to the grammar. Roughly speaking, if a dependency rule—which represents all the dependents of a head together (see section 3.1)— does not fit well with the grammar, it is flagged as potentially erroneous. The methods do not have to be retrained for a given parser’s"
P10-1075,E06-1011,0,0.0982867,"Missing"
P10-1075,D07-1120,0,0.0267806,"th Annual Meeting of the Association for Computational Linguistics, pages 729–738, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of arguments and adjuncts (cf. Przepi´orkowski, 2006)—that is, a head and all its dependents. The methods work because we expect there to be regularities in valency structure in a treebank grammar; non-conformity to such regularities indicates a potential problem. Campbell and Johnson, 2002), but work by comparing any tree to what is in the training grammar (cf. also approaches stacking hand-written rules on top of other parsers (Bick, 2007)). We propose to flag erroneous parse rules, using information which reflects different grammatical properties: POS lookup, bigram information, and full rule comparisons. We build on a method to detect so-called ad hoc rules, as described in section 2, and then turn to the main approaches in section 3. After a discussion of a simple way to flag POS anomalies in section 4, we evaluate the different methods in section 5, using the outputs from two different parsers. The methodology proposed in this paper is easy to implement and independent of corpus, language, or parser. 3 Ad hoc rule detection"
P10-1075,W06-2920,0,0.0488845,"le to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process. 1 Introduction and Motivation Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). Furthermore, parsing accuracy degrades unless sufficient amounts of labeled training data from the same domain are available (e.g., Gildea, 2001; Sekine, 1997), and thus we need larger and more varied annotated treebanks, covering a wide range of domains. However, there is a bottleneck in obtaining annotation, due to the need for manual intervention in annotating a treebank. One approach is to develop automatically-parsed corpora (van Noord and Bouma, 2009), but a natural disadvantage with such data is that it contains parsing errors. Identifying the most problematic parses for human post-pr"
P10-1075,P09-1022,0,0.0287072,"ora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process. 1 Introduction and Motivation Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). Furthermore, parsing accuracy degrades unless sufficient amounts of labeled training data from the same domain are available (e.g., Gildea, 2001; Sekine, 1997), and thus we need larger and more varied annotated treebanks, covering a wide range of domains. However, there is a bottleneck in obtaining annotation, due to the need for manual intervention in annotating a treebank. One approach is to develop automatically-parsed corpora (van Noord a"
P10-1075,W02-0306,0,0.0290903,"detect errors in automatically-parsed data. If annotated corpora are to grow in scale and retain a high quality, annotation errors which arise 729 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 729–738, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of arguments and adjuncts (cf. Przepi´orkowski, 2006)—that is, a head and all its dependents. The methods work because we expect there to be regularities in valency structure in a treebank grammar; non-conformity to such regularities indicates a potential problem. Campbell and Johnson, 2002), but work by comparing any tree to what is in the training grammar (cf. also approaches stacking hand-written rules on top of other parsers (Bick, 2007)). We propose to flag erroneous parse rules, using information which reflects different grammatical properties: POS lookup, bigram information, and full rule comparisons. We build on a method to detect so-called ad hoc rules, as described in section 2, and then turn to the main approaches in section 3. After a discussion of a simple way to flag POS anomalies in section 4, we evaluate the different methods in section 5, using the outputs from t"
P10-1075,P08-1042,1,0.915269,"-parsed corpora. Developing techniques to detect errors in parses in a way which is independent of corpus and parser has fairly broad implications. By using only the information available in a training corpus, the methods we explore are applicable to annotation error detection for either hand-annotated or automatically-parsed corpora and can also provide insights for parse reranking (e.g., Hall and Nov´ak, 2005) or parse revision (Attardi and Ciaramita, 2007). Although we focus only on detecting errors in automatically-parsed data, similar techniques have been applied for hand-annotated data (Dickinson, 2008; Dickinson and Foster, 2009). Our general approach is based on extracting a grammar from an annotated corpus and comparing dependency rules in a new (automaticallyannotated) corpus to the grammar. Roughly speaking, if a dependency rule—which represents all the dependents of a head together (see section 3.1)— does not fit well with the grammar, it is flagged as potentially erroneous. The methods do not have to be retrained for a given parser’s output (e.g., We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their"
P10-1075,A97-1015,0,0.0710344,"omise for speeding up the annotation process. 1 Introduction and Motivation Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). Furthermore, parsing accuracy degrades unless sufficient amounts of labeled training data from the same domain are available (e.g., Gildea, 2001; Sekine, 1997), and thus we need larger and more varied annotated treebanks, covering a wide range of domains. However, there is a bottleneck in obtaining annotation, due to the need for manual intervention in annotating a treebank. One approach is to develop automatically-parsed corpora (van Noord and Bouma, 2009), but a natural disadvantage with such data is that it contains parsing errors. Identifying the most problematic parses for human post-processing could combine the benefits of automatic and manual annotation, by allowing a human annotator to efficiently correct automatic errors. We thus set out in"
P10-1075,W01-0521,0,0.0453917,"ropose show promise for speeding up the annotation process. 1 Introduction and Motivation Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). Furthermore, parsing accuracy degrades unless sufficient amounts of labeled training data from the same domain are available (e.g., Gildea, 2001; Sekine, 1997), and thus we need larger and more varied annotated treebanks, covering a wide range of domains. However, there is a bottleneck in obtaining annotation, due to the need for manual intervention in annotating a treebank. One approach is to develop automatically-parsed corpora (van Noord and Bouma, 2009), but a natural disadvantage with such data is that it contains parsing errors. Identifying the most problematic parses for human post-processing could combine the benefits of automatic and manual annotation, by allowing a human annotator to efficiently correct automatic errors. We"
P10-1075,W09-0107,0,0.151577,"Missing"
P10-1075,W05-1505,0,0.0676281,"Missing"
P10-1075,N09-1028,0,0.0136579,"s output (e.g., We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process. 1 Introduction and Motivation Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). Furthermore, parsing accuracy degrades unless sufficient amounts of labeled training data from the same domain are available (e.g., Gildea, 2001; Sekine, 1997), and thus we need larger and more varied annotated treebanks, covering a wide range of domains. However, there is a bottleneck in obtaining annotation, due to the need for manual interv"
P10-1075,E09-1097,0,\N,Missing
P10-1075,E09-1055,0,\N,Missing
R13-1046,A00-1031,0,0.271796,"Missing"
R13-1046,W08-1301,0,0.0874013,"Missing"
R13-1046,R11-1057,0,0.0184636,"ing, pages 357–364, Hissar, Bulgaria, 7-13 September 2013. Indeed, Petrov and McDonald (2012) mention that for the shared task, “[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems.” Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001), as well as for other genres, such as questions (Dima and Hinrichs, 2011). One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b), e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012). There are no reports about the impact of the quality of POS tags for training— i.e., whether worse, automatically-derived tags might be an improvement over gold tags—though Søgaard and Plank (2012) note"
R13-1046,N10-1060,0,0.0438138,"sentences from the Penn Treebank is beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). 2 Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16"
R13-1046,I11-1100,0,0.157089,"Missing"
R13-1046,W01-0710,0,0.0520135,"7 Proceedings of Recent Advances in Natural Language Processing, pages 357–364, Hissar, Bulgaria, 7-13 September 2013. Indeed, Petrov and McDonald (2012) mention that for the shared task, “[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems.” Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001), as well as for other genres, such as questions (Dima and Hinrichs, 2011). One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b), e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012). There are no reports about the impact of the quality of POS tags for training— i.e., whether worse, automatically-derived tags m"
R13-1046,W13-1101,1,0.830387,"Missing"
R13-1046,J93-2004,0,0.0421844,"ques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16 000 sen358 Train WSJ EWT WSJ WSJ+EWT (balanced) tences from weblogs, newsgroups, emails, reviews, and question-answers. Note that our data sets are different from the ones in Khan et al. (2013) since in the previous work we had removed sentences with POS labels AFX and GW. To create training and test sets, we broke the data into the following sets: sibling, context, and non-local features, employing information from words and POS tags. We use its default settings for all experiments. • WSJ testing: sect"
R13-1046,P11-1163,0,0.0311811,"Missing"
R13-1046,P05-1012,0,0.18425,"Missing"
R13-1046,E06-1011,0,0.0827335,"Missing"
R13-1046,W11-2917,0,0.0203212,"generally fits the testing domain, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010), where dissimilar sentences are selected for hand-annotation to improve parsing. 1. Answer: where can I get morcillas in tampa bay , I will like the argentinian type , but I will try anothers please ? 2. Email: Michael : &lt;s&gt; Thanks for putting the paperwork together . &lt;s&gt; I would have interest in meeting if you can present unique investment opportunities that I do n’t have access to now . 3. News: complete with original Magnavox tubes all tubes have been tested they are all good - stereo amp 4. Review: Buyer Beware !! &lt;s&gt; Rusted out and unsafe cars sold here ! 5."
R13-1046,N10-1120,0,0.0194146,"Missing"
R13-1046,C12-2088,0,0.0589846,"s beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). 2 Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16 000 sen358 Train WSJ EWT WSJ WSJ+EWT (balanced) t"
R13-1046,P10-1037,0,0.0222344,"main, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010), where dissimilar sentences are selected for hand-annotation to improve parsing. 1. Answer: where can I get morcillas in tampa bay , I will like the argentinian type , but I will try anothers please ? 2. Email: Michael : &lt;s&gt; Thanks for putting the paperwork together . &lt;s&gt; I would have interest in meeting if you can present unique investment opportunities that I do n’t have access to now . 3. News: complete with original Magnavox tubes all tubes have been tested they are all good - stereo amp 4. Review: Buyer Beware !! &lt;s&gt; Rusted out and unsafe cars sold here ! 5. Blog: The Supreme Court annou"
R13-1046,N13-1039,0,\N,Missing
S14-2060,P03-1021,0,0.00422352,"near model, taking the log of each score – the direct and inverse translation probabilities, the LM probability, and the surface and POS SIM scores – and producing a weighted sum. Since the original scores are either probabilities or probability-like (in the range [0, 1]), their logs are negative numbers, and at translation time we return the translation (or n-best) with the highest (least negative) score. This leaves us with the question of how to set the weights for the log-linear model; in this work, we use the ZMERT package (Zaidan, 2009), which implements the MERT optimization algorithm (Och, 2003), iteratively tuning the feature weights by repeatedly requesting n-best lists from the system. We used ZMERT with its default settings, optimizing our system’s BLEU scores on the provided development set. We chose, for convenience, BLEU as a stand-in for the wordlevel accuracy score, as BLEU scores are maximized when the system output matches the reference translations. 4 5 Conclusion We have described our entry for the initial running of the “L2 Writing Assistant” task and explained some possible extensions to our base loglinear model system. In developing the SIM extensions, we faced some i"
S14-2060,E12-1009,0,0.0202846,"Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and the"
S14-2060,J13-1004,0,0.0143479,"edings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and then extract phrases with t"
S14-2060,C10-1011,0,0.0263733,"ional License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section"
S14-2060,de-marneffe-etal-2006-generating,0,0.0750578,"Missing"
S14-2060,S13-1035,0,0.0138536,"n, in cases where an L1 phrase contained words that were neither in our training data nor BabelNet (and thus were simply outof-vocabulary for our system), we took the first translation for that phrase, without regard to context, from Google Translate, through the semiautomated Google Docs interface. This approach is not particularly scalable or reproducible, but simulates what a user might do in such a situation. Google Books Syntactic N-grams For English, we also obtained dependency relationships for our word similarity statistics using the arcs dataset of the Google Books Syntactic N-Grams (Goldberg and Orwant, 2013), which has 919M items, each of which is a small “syntactic n-gram”, a term Goldberg and Orwant use to describe short dependency chains, each of which may contain several tokens. This data set does not contain the actual parses of books from the Google Books corpus, but counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating"
S14-2060,W11-2123,0,0.0168098,"built with the training scripts packaged with Moses (Koehn et al., 2007). These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 context, we score candidates with an n-gram lan1 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 357 guage model (LM) trained on a large sample of target-language text. Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011). Here our models were trained on the Wikipedia text mentioned previously (without filtering long sentences), with KenLM set to 5-grams and the default settings. fitness of the phrase “eat with chopsticks” would be computed as: 3.3 Since we consider the heads and dependents of a target phrase component, these may be situated inside or outside the phrase. Both cases are included in our calculation, thus enabling us to consider a broader, syntactically determined local context of the phrase. By basing the calculation on a single word’s head and dependents, we attempt to avoid data sparseness iss"
S14-2060,P03-1054,0,0.00464979,"or German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and then extract phrases with the grow-diag-final-and heuristic. At translation time, we look for the given source-language phrase in the phrase table, and if it is found, we take all translations of that phrase as our candidates. When translating a phrase that is not found in the phrase"
S14-2060,P07-2045,0,0.00570498,"counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating many possible candidate translations, then scoring and ranking them in the targetlanguage context. 3.1 Constructing Candidate Translations As a starting point, we use phrase tables constructed in typical SMT fashion, built with the training scripts packaged with Moses (Koehn et al., 2007). These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 context, we score candidates with an n-gram lan1 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 357 guage model (LM) trained on a large sample of target-language text. Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011). Here our models were trained on the Wikipedia text ment"
S14-2060,2005.mtsummit-papers.11,0,0.00753831,"ng collocational relationships between tokens anywhere in the L2 context and the proposed fragment translations. Our system proceeds in several stages: (1) looking up or constructing candidate translations for the L1 fragment, (2) scoring candidate translations via a language model of the L2, (3) scoring candidate translations with a dependency-driven word similarity measure (Lin, 1998) (which we call SIM), and (4) combining the previous scores in a log-linear model to arrive at a final n-best list. Step 1 models transfer knowledge between Europarl The Europarl Parallel Corpus (Europarl, v7) (Koehn, 2005) is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments. From this corpus, we build phrase tables for English-Spanish, English-German, FrenchEnglish, Dutch-English. BabelNet In the cases where the constructed phrase tables do not contain a translation for a source phrase, we need to back off to smaller phrases and find candidate translations for these components. To better handle sparsity, we extend look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. This work is"
S14-2060,P00-1056,0,0.048067,"Missing"
W05-0103,W02-0105,0,0.062929,"sification tasks, such as language identification) • Writers’ aids (Spelling and grammar correction) • Machine translation (2 weeks) • Dialogue systems (2 weeks) • Computer-aided language learning • Social context of language technology use In contrast to the courses of which we are aware that offer computational linguistics to undergraduates, our Language and Computers is supposed to be accessible without prerequisites to students from every major (a requirement for GEC courses). For example, we cannot assume any linguistic background or language awareness. Like Lillian Lee’s Cornell course (Lee, 2002), the course cannot presume programming 17 • Reasoning about finite-state automata and regular expressions (in the contexts of web searching and of information management). Students reason about relationships between specific and general search terms. • Reasoning about more elaborate syntactic representations (such as context-free grammars) and semantic representations (such as predicate calculus), in order to better understand grammar checking and machine translation errors. • Reasoning about the interaction between components of natural language systems (in the contexts of machine translatio"
W08-0901,A00-1031,0,0.0620296,"ption of an input symbol without transition to a new state (D ELETION). 5 It is worth noting here that GPARS was actually a sentencelevel system; it is for the word-level morphological analysis discussed here that we expect the most gain from our approach. SERTION ), is appropriate to the context. The FSA analyzer will provide a list of possible analyses (i.e., augmented POS tags) for each input item (ranked, if need be). We can explore using a third-party tagger to narrow down this output list to analyses that make sense in context. We are considering both the Hidden Markov Model tagger TnT (Brants, 2000) and the Decision Tree Tagger (Schmid, 1997), with parameter files from Sharoff et al. (2008). Both of these taggers use local context, but, as they provide potentially different types of information, the final system may use both in parallel, weighing the output of each to the degree which each proves useful in trial runs to make its decision. Since POS tagging does not capture every syntactic property that we might need access to, we are not sure how accurate error detection can be. Thus, to supplement its contextual information, we intend to use shallow syntactic processing methods, perhaps"
W09-0905,P08-1068,0,0.0709674,"University Bloomington, IN USA md7@indiana.edu Charles Jochim Indiana University Bloomington, IN USA cajochim@indiana.edu Abstract ambiguation models as a foundation for induction. We aim in this paper to thoroughly investigate what category properties contexts can or cannot distinguish by themselves. With this approach, we are able to more thoroughly examine the categories used for evaluation. Evaluation of induction methods is difficult, due to the variety of corpora and tagsets in existence (see discussion in Clark, 2003) and the variety of potential purposes for induced categories (e.g., Koo et al., 2008; Miller et al., 2004). Yet improving the evaluation of category induction is vital, as evaluation does not match up well with grammar induction evaluation (Headden III et al., 2008). For many evaluations, POS tags have been mapped to a smaller tagset (e.g., Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008), but there have been few criteria for evaluating the quality of these mappings. By isolating contexts, we can investigate how each mapping affects the accuracy of a method and the lexicon. Using corpus annotation also allows us to explore the relation between induced categories an"
W09-0905,J93-2004,0,0.0337839,"Missing"
W09-0905,W08-2112,0,0.0275659,"Missing"
W09-0905,E95-1020,0,0.178255,"Missing"
W09-0905,P05-1044,0,0.0542897,"Missing"
W09-0905,J92-4003,0,0.0618035,"Missing"
W09-0905,W00-0717,0,0.0823438,"Missing"
W09-0905,E03-1009,0,0.680955,"cal Contexts as a Step in Grammatical Category Induction Markus Dickinson Indiana University Bloomington, IN USA md7@indiana.edu Charles Jochim Indiana University Bloomington, IN USA cajochim@indiana.edu Abstract ambiguation models as a foundation for induction. We aim in this paper to thoroughly investigate what category properties contexts can or cannot distinguish by themselves. With this approach, we are able to more thoroughly examine the categories used for evaluation. Evaluation of induction methods is difficult, due to the variety of corpora and tagsets in existence (see discussion in Clark, 2003) and the variety of potential purposes for induced categories (e.g., Koo et al., 2008; Miller et al., 2004). Yet improving the evaluation of category induction is vital, as evaluation does not match up well with grammar induction evaluation (Headden III et al., 2008). For many evaluations, POS tags have been mapped to a smaller tagset (e.g., Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008), but there have been few criteria for evaluating the quality of these mappings. By isolating contexts, we can investigate how each mapping affects the accuracy of a method and the lexicon. Using c"
W09-0905,C08-1026,1,0.915501,"ocal context of a word has proven to be a reliable indicator of its grammatical category, or part of speech (e.g., Mintz, 2002, 2003; Redington et al., 1998). Likewise, category induction techniques cluster word types together (e.g., Clark, 2003; Sch¨utze, 1995), using similar information, i.e., distributions of local context information. These methods are successful and useful (e.g. Koo et al., 2008), but in both cases it is not always clear whether errors in lexical classification are due to a problem in the induction algorithm or in what contexts count as identifying the same category (cf. Dickinson, 2008). The question we ask, then, is: what role does the context on its own play in defining a grammatical category? Specifically, when do two contexts identify the same category? Many category induction experiments start by trying to categorize words, and Parisien et al. (2008) categorize word usages, a combination of a word and its context. But to isolate the effect the context has on the word, we take the approach of categorizing contexts as a first step towards clustering words. By separating out contexts for word clustering, we can begin to speak of better disProceedings of the EACL 2009 Works"
W09-0905,P08-1085,0,0.0929438,"model, the context be identifies nouns, adjectives, and verbs, among others. Viewed in this way, it is important to gauge the precision of contexts for distinguishing a category (cf. also Dickinson, 2008). In other words, how often does the same context identify the same category? And how fine-grained is the category that the context distinguishes? To test whether a frame defines a single category in non-childdirected speech, we focus on which categorical properties frames define, and for this we use a POS-annotated corpus. Due to its popularity for unsupervised POS induction research (e.g., Goldberg et al., 2008; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008) and its often-used tagset, for our initial research, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), with 36 tags (plus 9 punctuation tags), and we use sections 00-18, leaving held-out data for future experiments.4 Defining frequent frames as those occurring at Category Determiner Adjective Noun Adverb Verb Wh-Det. Corpus tags DT, PDT, PRP$ JJ, JJR, JJS NN, NNS, PRP, NNP, NNPS RB, RBR, RBS MD, VB, VBD, VBG, VBN, VBP, VBZ WDT, WP$ Table 1: Tag mappings into basic categories These broader categ"
W09-0905,P07-1094,0,0.292907,"t distinguish by themselves. With this approach, we are able to more thoroughly examine the categories used for evaluation. Evaluation of induction methods is difficult, due to the variety of corpora and tagsets in existence (see discussion in Clark, 2003) and the variety of potential purposes for induced categories (e.g., Koo et al., 2008; Miller et al., 2004). Yet improving the evaluation of category induction is vital, as evaluation does not match up well with grammar induction evaluation (Headden III et al., 2008). For many evaluations, POS tags have been mapped to a smaller tagset (e.g., Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008), but there have been few criteria for evaluating the quality of these mappings. By isolating contexts, we can investigate how each mapping affects the accuracy of a method and the lexicon. Using corpus annotation also allows us to explore the relation between induced categories and computationally or theoretically-relevant categories (e.g., Elworthy, 1995). While human category acquisition results successfully divide a lexicon into categories, these categories are not necessarily ones which are appropriate for many computational purposes or match theoretical synt"
W09-0905,C08-1042,0,0.0329576,"Missing"
W09-0905,N04-1043,0,\N,Missing
W09-0905,dickinson-jochim-2008-simple,1,\N,Missing
W09-0905,P03-1009,0,\N,Missing
W10-1502,baroni-bernardini-2004-bootcat,0,0.496738,"instructor, etc. By isolating a particular domain, we can hope for greater degrees of accuracy; see, for example, the high accuracies for domain-specific grammar correction in Lee and Seneff (2006). In this situation, we face the challenge of obtaining data which is appropriate both for: a) the topic the learners are writing about, and b) the linguistic construction of interest, i.e., containing enough relevant instances. In the ideal case, one could build a corpus directly for the types of learner data to analyze. Luckily, using the web as a data source can provide such specialized corpora (Baroni and Bernardini, 2004), in addition to larger, more general corpora (Sharoff, 2006). A crucial question, though, is how one goes about designing the right web corpus for analyzing learner language (see, e.g., Sharoff, 2006, for other contexts) The area of difficulty for language learners which we focus on is that of Korean post-positional particles, akin to English prepositions (Lee et al., 2009; Ko et al., 2004). Korean is an important language to develop NLP techniques for (see, e.g., discussion in Dickinson et al., 2008), presenting a variety of features which are less prevalent in many Western languages, such a"
W10-1502,C08-1022,0,0.0756056,"Missing"
W10-1502,C08-1109,0,0.451608,"step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction. 1 Introduction Applications for assisting second language learners can be extremely useful when they make learners more aware of the non-native characteristics in their writing (Amaral and Meurers, 2006). Certain constructions, such as English prepositions, are difficult to characterize by grammar rules and thus are wellsuited for machine learning approaches (Tetreault and Chodorow, 2008; De Felice and Pulman, 2008). Machine learning techniques are relatively portable to new languages, but new languages bring issues in terms of defining the language learning problem and in terms of acquiring appropriate data for training a machine learner. We focus in this paper mainly on acquiring data for training a machine learning system. In particular, we are interested in situations where the task is constant—e.g., detecting grammatical errors in particles—but the domain might fluctuate. This is the case when a learner is asked to write an essay on Sun-Hee Lee Wellesley College slee6@we"
W10-1502,han-etal-2002-development,0,0.106523,"d method for similar NLP problems like detecting errors in English preposition use. For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning, with 10 million sentences in their training set. In expanding the paradigm to other languages, one problem is a dearth of data. It seems like a large data set is essential for moving forward. For Korean, there are at least two corpora publicly available right now, the Penn Korean Treebank (Han et al., 2002), with hundreds of thousands of words, and the Sejong Corpus (a.k.a., The Korean National Corpus, The National Institute of Korean Language, 2007), with tens of millions of words. While we plan to include the Sejong corpus in future data, there are several reasons we pursue a different tack here. First, not every language has such resources, and we want to work towards a languageindependent platform of data acquisition. Secondly, these corpora may not be a good model for the kinds of topics learners write about. For example, news texts are typically written more formally than learner writing."
W10-1502,R09-1085,0,0.148309,"Missing"
W10-1502,I05-1052,1,\N,Missing
W10-1805,J03-1002,0,0.00313923,"are given the special label, NIL-C. • (For phrases) Constituent phrases which are unary branches are given a single, normalized label representing all target strings. 2. Generate NIL alignments for string tokens which occur in SL, but have no alignment to TL, using the method described in Dickinson and Meurers (2005). 3. Find SL strings which have variation in labeling. 4. Filter the variations from step 3, based on likelihood of being an error (see section 4). Alignment Types 4 Aligned corpora often specify additional information about each alignment, e.g., a “sure” or “possible” alignment (Och and Ney, 2003). In S MUL TRON , for instance, an EXACT alignment means that the strings are considered direct translation equivalents outside the current sentence context, whereas a FUZZY one is not as strict an equivalent. For example, something in English EXACTaligns with etwas in German. However, if something and irgend etwas (‘something or other’) are constituents on the phrase level, &lt;something, irgend etwas&gt; is an acceptable alignment (since the corpus aligns as much as possible), but is FUZZY. Since EXACT alignments are the ones we expect to consistently align with the same string across the corpus,"
W10-1805,C08-1026,1,0.877965,"starting point for an error detection method for aligned corpora, we use the variation n-gram approach for syntactic annotation (Dickinson and Meurers, 2003, 2005). The approach is based on detecting strings which occur multiple times in the corpus with varying annotation, the so-called VARIATION NUCLEI . The nucleus with repeated surrounding context is referred to as a VARIATION n- GRAM. The basic heuristic for detecting annotation errors requires one word of recurring context on each side of the nucleus, which is sufficient for detecting errors in grammatical annotation with high precision (Dickinson, 2008). The approach detects bracketing and labeling errors in constituency annotation. For example, the variation nucleus last month occurs once in the Penn Treebank (Taylor et al., 2003) with the label NP and once as a non-constituent, handled through a special label NIL. As a labeling error example, next Tuesday occurs three times, twice as NP and once as PP (Dickinson and Meurers, 2003). The method works for discontinuous constituency annotation (Dickinson and Meurers, 2005), allowing one to apply it to alignments, which may span over several words. 2.2 3 Consistency of Alignment To adapt the va"
W10-1805,P09-3009,0,0.0258632,"Samuelsson Stockholm University yvonne.samuelsson@ling.su.se Abstract to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for already rare linguistic phenomena (e.g., Meurers and M¨uller, 2008). Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. This paper explores ways to detect errors in aligned corpora, using very little technology. In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predi"
W10-1805,P98-2164,0,0.111721,"Missing"
W10-1805,P05-1040,1,0.900473,"e each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. 1 We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation units with equivalent meanings. Introduction Parallel corpora—texts and their translations— have become essential in the development of machine translation (MT) systems. Alignment quality is crucial to these corpora; as Tiedemann (2003) states, “[t]he most important feature of texts and their translations is the correspondence between source"
W10-1805,J93-1004,0,\N,Missing
W10-1805,D07-1116,0,\N,Missing
W10-1805,ule-simov-2004-unexpected,0,\N,Missing
W10-1805,C08-1139,0,\N,Missing
W10-1805,E09-1060,0,\N,Missing
W10-1805,W08-0411,0,\N,Missing
W10-1805,C98-2159,0,\N,Missing
W10-1805,W07-1514,0,\N,Missing
W11-1410,baroni-bernardini-2004-bootcat,0,0.0162558,"r relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant se82 Obtaining the most relevant instances Using sub-corpora Web corpora can be built by searching for a set of seed terms, extracting documents with those terms (Baroni and Bernardini, 2004). One way to improve such corpora is to use better seeds, namely, those which are: 1) domain-appropriate (e.g., about traveling), and 2) of an appropriate level. In Dickinson et al. (2010), we show that basic terms result in poor quality Korean, but slightly more advanced terms on the same topics result in better-formed data. Rather than use all of the seed terms to create a single corpus, we divide the seed terms into 13 separate sets, based on the individual topics from our learner corpus. The sub-corpora are then combined to create a cohesive corpus covering all the topics. For example, we"
W11-1410,W07-1604,0,0.0585666,"l functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, location, instrument, possession, and so forth. 1 Sun-Hee Lee Wellesley College slee6@wellesley.edu We use the Yale Romanization scheme for writing Korean. We treat the task of particle error detection as one of particle selection, and we use machine learning because it has proven effective in similar tasks for other languages (e.g., Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). A"
W11-1410,W10-1406,0,0.0146339,"s there a particle? (Yes/No); and 2) What is the exact particle? Using two steps eases the task of actual particle prediction: with a successful classification of negative and positive instances, there is no need to handle nominals that have no particle in step 2. To evaluate our parameters for obtaining the most relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data. For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant se82 Obtaining the most relevant instances Using sub-corpora Web corpora can be built by searching for a set of seed terms, extracting documents with those terms (Baroni and Bernardini, 2004). One way to improve"
W11-1410,C08-2008,0,0.431259,"Missing"
W11-1410,W10-1502,1,0.881597,"stribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization. 1 ‘Sumi waited for John for (the whole) two hours in his house.’ Introduction A growing area of research in analyzing learner language is to detect errors in function words, namely categories such as prepositions and articles (see Leacock et al., 2010, and references therein). This work has mostly been for English, and there are issues, such as greater morphological complexity, in moving to other languages (see, e.g., de Ilarraza et al., 2008; Dickinson et al., 2010). Our goal is to build a machine learning system for detecting errors in postpositional particles in Korean, a significant source of learner errors (Ko et al., 2004; Lee et al., 2009b). Korean postpositional particles are morphemes that attach to a preceding nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role).1 Similar to English prepositions, particles can also have modifier functions, adding meanings of time, loca"
W11-1410,N10-1019,0,0.150342,"ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). 2 See Dickinson and Lee (2009); de Ilarraza et al. (2008); Oyama (2010) for related work in other languages. 81 Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81–86, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics 2 2.1 Particle error detection Pre-processing Korean is an agglutinative language: Korean words (referred to as ecels) are usually composed of a root with a number of functional affixes. We thus first segment and POS tag the text, for both training and testing, using a hybrid (trigram + ruleba"
W11-1410,I08-1059,0,0.045258,"Chodorow et al., 2007; Oyama, 2010). Training on a corpus of well-formed Korean, we predict which particle should appear after a given nominal; if this is different from the learner’s, we have detected an error. Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules, especially with the complex set of linguistic relationships motivating particle selection.2 We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). As the task is understudied, the work is preliminary, but it nonetheless is able to highlight the primary areas of focus for future work. Secondly, we improve upon the training data, in particular doing a better job of selecting relevant instances for the machine learner. Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). 2 See Dickinson and Lee (2009); de Ilarraza et al. (2008); Oyama (2010) for related work in other languages. 81 Proceedings of the Si"
W11-1410,C08-1109,0,0.366093,"s no need to handle nominals that have no particle in step 2. To evaluate our parameters for obtaining the most relevant instances, we keep the task simple and perform only step 1, as this step provides information about the usability of the training data. For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010). We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008). Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant se82 Obtaining the most relevant instances Using sub-corpora Web corpora can be built by searching for a set of seed terms, extracting documents with those terms (Baroni and Bernardini, 2004). One way to improve such corpora is to use better seeds, namely, those which are: 1) domain-appropriate (e.g., about traveling), and 2) of an appropriate level. In Dickinson et al. (2010), we show that basic terms result in poor qual"
W11-1410,R09-1085,0,0.0670881,"Missing"
W11-2929,P10-3018,0,0.0231708,"Missing"
W11-2929,W10-2927,0,0.0226548,"Missing"
W11-2929,ambati-etal-2010-high,0,0.0436006,"Missing"
W11-2929,W09-3828,0,0.0659196,"Missing"
W11-2929,N07-1049,0,0.060004,"uilding annotated corpora—corpora for new languages, corpora of a much larger nature than previously created, corpora in a new domain, etc.—the assumptions of one’s resources can be quite different. We sketch different possibilities here. 3.1 Grammar-Based Error Detection The reason we phrase error detection in terms of grammars is that some grammar is always available, whereas we cannot always assume a modifiable parser. Error detection based on coarse grammars is applicable to any of these scenarios, as opposed to, e.g., methods which rely on details of how a parser is likely to fail (e.g., Attardi and Ciaramita, 2007; Goldberg and Elhadad, 2010b).2 Additionally, because we can always obtain a grammar from parsed data, we will have access to the frequency of occurrence of each rule.3 Types of Grammars Our methods are based on having a grammar to compare parse rules to. Automatically-parsed data can be obtained in different ways, affecting grammar resources. First, parsed data can be obtained from a parser trained on gold data. In this case, a GOLD GRAMMAR can be extracted from the gold data. For error detection, we can then compare the parsed data to this fixed grammar (section 5). One subtype of this is t"
W11-2929,W07-2416,0,0.0311731,"Missing"
W11-2929,W04-1901,0,0.0437554,"Missing"
W11-2929,W06-2920,0,0.0247223,"a is partially hand-corrected. In either case, a NOISY GRAMMAR can be extracted from the parsed data (section 6). This leads to the possibility of hybrid grammars, where some rules have been hand-checked and others have not—i.e., a concatenation of a 4 Methods of Comparison In this section, we develop the best methods of rule comparison (section 4.1) and introduce a new, orthogonal way of flagging possible errors (section 4.2). In order to compare directly to the previous work, we use the Swedish Talbanken corpus (Nilsson and Hall, 2005) with the same data split as in the CoNLL-X Shared Task (Buchholz and Marsi, 2006); in section 5 and beyond, we switch the training and testing data. In all experiments, we use gold standard POS tags. To keep the data sets clear across different training regiments, we refer to them as the large and small Talbanken data sets. The large Talbanken data has 11,042 sentences, 191,467 words, 96,517 (non-unary) rule tokens and 26,292 rule types. The small data set has 389 sentences, 5,656 words, 3,107 rule tokens and 1,284 rule types. As we show in section 5, even such a small grammar can be highly effective in detecting parse errors. 2 Those methods are of course well-suited to t"
W11-2929,J93-2004,0,0.0382628,"Missing"
W11-2929,P10-1075,1,0.720788,"only needs a small grammar. 1 Introduction and Motivation There is a need for high-quality dependency annotation for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006), ideally large amounts of annotated data. This is a lofty goal for any language, especially languages with few, if any, annotated resources. Citing Abeill´e (2003), Hwa et al. (2005) say: “it appears that acquiring 20,000-40,000 sentences — including the work of building style guides, redundant manual annotation for quality checking, and so forth – can take from four to seven years.” As pointed out by Dickinson (2010), a major bottleneck in obtaining annotation involves the need for human correction, leading to the following process: 1) automatically parse corpora (van Noord and Bouma, 2009), which will contain errors, and 2) identify problematic parses for human post-processing. We develop this second step of detecting errors. In particular, there is the problem of having little annotated data to work with, as in the cases of: lesser-resourced languages (e.g., Ambati et al., 2010; Simpson et al., 2009), new annotation schemes, and new domains with limited in-domain 241 Proceedings of the 12th Internationa"
W11-2929,W06-2932,0,0.0253001,"Missing"
W11-2929,P10-1111,0,0.0228227,"Missing"
W11-2929,W09-3408,0,0.0143375,"manual annotation for quality checking, and so forth – can take from four to seven years.” As pointed out by Dickinson (2010), a major bottleneck in obtaining annotation involves the need for human correction, leading to the following process: 1) automatically parse corpora (van Noord and Bouma, 2009), which will contain errors, and 2) identify problematic parses for human post-processing. We develop this second step of detecting errors. In particular, there is the problem of having little annotated data to work with, as in the cases of: lesser-resourced languages (e.g., Ambati et al., 2010; Simpson et al., 2009), new annotation schemes, and new domains with limited in-domain 241 Proceedings of the 12th International Conference on Parsing Technologies, pages 241–252, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. flagged. The intuition is that there should be regularities in dependency structure; non-conformity to regularities indicates a potential problem. Dickinson (2010) compares pairs of dependency relations and POS tags (instead of using only, e.g., dependencies), and we follow suit. Relatedly, although scores can be obtained for each unit in its role"
W11-2929,W10-2105,0,0.0272457,"Missing"
W11-2929,W09-0107,0,0.0626668,"Missing"
W11-2929,N10-1059,0,0.0351129,"Missing"
W11-2929,N06-2033,0,0.0761277,"Missing"
W11-2929,D07-1111,0,0.0339188,"Missing"
W11-2929,P10-1037,0,0.0400645,"Missing"
W11-2929,N10-1091,0,\N,Missing
W11-2929,P05-1012,0,\N,Missing
W11-2929,D09-1060,0,\N,Missing
W11-2929,N10-1115,0,\N,Missing
W12-2011,H01-1052,0,0.0816575,"Missing"
W12-2011,boyd-2010-eagle,0,0.0296141,"nce Hebrew is a less commonly taught language (LCTL), we have few placement exams from which to learn correspondences. Compounding the data sparsity problem is that each piece of data is complex: if a learner produces an erroneous answer, there are potentially a number of ways to analyze it (cf. e.g. (Dickinson, 2011)). An error could feature, for instance, a letter inserted in an irregular verb stem, or between two nouns; any of these properties may be relevant to describing the error (cf. how errors are described in different taxonomies, e.g. (D´ıaz-Negrillo and Fern´andez-Dom´ınguez, 2006; Boyd, 2010)). Specific error types are unlikely to recur, making sparsity even more of a concern. We thus need to develop methods which generalize well, finding the most useful aspects of the data. Our system is an online system to be used at the Hebrew Language Program at our university. The system is intended to semi-automatically place incoming students into the appropriate Hebrew course, i.e., level. As with many exams, the main purpose is to “reduce the number of students who attend an oral interview” (Fulcher, 1997). 2 The Data Exercise type We focus on a scrambled sentence exercise, in which learn"
W12-2011,W11-2838,0,0.0335701,"tures in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels. There is only little work investigating this correspondence formally (see (Hawkins and Filipovi´c, 2010; Alexopoulou e"
W12-2011,W11-1410,1,0.845426,"ation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels. There is only little work investigating this correspondence formally (see (Hawkins and Filipovi´c, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora. For this reason, we follow a data-driv"
W12-2011,P11-2124,0,0.0139688,"ammatical sentence. Our goal is to move towards freer language production and to analyze language proficiency through more variables, but, in the interest of practicality, we start in a more restricted way. For lesser-resourced languages, there is generally little data and few NLP resources available. For Hebrew, for example, we must create our own pool of 95 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95–104, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions. For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses. Then, the system aligns the learner answer to the gold standard answer and determines the types of deviations. Since Hebrew is a less commonly taught language (LCTL), we have few placement exams from which to learn correspondences. Compounding the data sparsity problem is that each piece of data is complex: if a learner produces an erroneous"
W12-2011,C10-1046,0,0.0281701,"Missing"
W12-2011,P11-1093,0,0.0206146,"nd linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequen"
W12-2011,C08-1109,0,0.0156809,"which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner lev"
W12-2011,P11-1019,0,0.270144,"e account for in our feature selection, learning algorithm, and in the setup. Specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human,"
W12-2038,eom-etal-2012-using,1,0.88314,"Missing"
W12-2038,P05-3014,0,0.0818761,"Missing"
W12-2038,C96-2140,0,0.0939772,"these questions with a group of 60 ESL learners is given in section 4, and the results are discussed in section 5. 2 Background While there are many studies in second language acquisition (SLA) on providing vocabulary and reading assistance (e.g., Prichard, 2008; Luppescu and Day, 1993), we focus on outlining intelligent computer-assisted language learning (ICALL) systems here (see also discussion in Dela Rosa and Eskenazi, 2011). Such systems hold the promise of alleviating some problems of acquiring words while reading by providing information specific to each word as it is used in context (Nerbonne and Smit, 1996; Kulkarni et al., 2008). The GLOSSER-RuG system (Nerbonne and Smit, 1996) disambiguates on the basis of part of speech (POS). This is helpful in distinguishing verbal and nominal uses, for example, but is, of course, ineffective when a word has more than one sense in the same POS (e.g., face). More effective is the REAP Tutor (Heilman et al., 2006), which uses word sense disambiguation to provide lexicographic information and has 317 been shown to benefit learners by providing sensespecific lexical information (Dela Rosa and Eskenazi, 2011; Kulkarni et al., 2008). We build from this work by f"
W12-2038,N09-5005,0,0.23193,"Missing"
W12-2038,N03-1033,0,0.00689935,"Missing"
W12-3617,abuhakema-etal-2008-annotating,0,0.0222747,"sometimes optional, influenced by surrounding errors, and can be interchangeable. Thirdly, Korean particles have a wide range of functions, including modification and case-marking. Annotation, and by extension the task of particle error detection, must account for these issues. We focus on the utility of annotation in evaluating particle error detection systems, ensuring that it can support the automatic task of predicting the correct particle (or no particle) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of “developing best practices for annotation and evaluOne area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been for English. We thus aim"
W12-3617,boyd-2010-eagle,0,0.105655,"ct particle is ey. If we do not correct the verb, the learner’s particle is, in a syntactic sense, appropriate for the verb, even if the verb’s selectional restrictions are not followed. (4) a. O: nay insayng-i i sihem-ul uycihanta my life-SBJ this exam-OBJ lean-on b. C: nay insayng-i i sihem-ey tallyeissta my life-SBJ this exam-ON depend ‘My life depends on this exam’ we do not correct it to (4b). Previous work has corrected sets of errors (Rozovskaya and Roth, 2010), eliminated sentences with nested or adjacent errors (Gamon, 2010), or built multiple layers of annotation (Hana et al., 2010; Boyd, 2010). Our decision makes the particle-selection task for machine learning more attainable and is easily extendible with multi-layered annotation (section 4.1). 3.3 Classifying particles For every particle in the learner corpus, error or not, we mark its specific category, e.g., GOAL. This categorization helps because learners can make different kinds of mistakes with different kinds of particles, and systems can be developed, evaluated, or optimized with respect to a particular kind of particle. 4 Putting it together The previous discussion outlines the type of annotation needed for evaluating Kor"
W12-3617,C08-1022,0,0.102369,"Missing"
W12-3617,C08-2008,0,0.256215,"Missing"
W12-3617,W11-1410,1,0.790967,"orrect particle (or no particle) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of “developing best practices for annotation and evaluOne area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been for English. We thus aim to further the development of methods for detecting errors in functional elements across languages, by developing annotation for postpositional particles in Korean, a significant source of error for learners (Ko et al., 2004; Lee et al., 2009) and an area of interest for computer-assisted language learning (CALL) (Dickinson et al., 2008). As there is at present very little work on annotated learner corpora for morphologically-r"
W12-3617,N10-1019,0,0.0327788,". If we correct the verb to tallyeissta (‘depend’), as in (4b), the correct particle is ey. If we do not correct the verb, the learner’s particle is, in a syntactic sense, appropriate for the verb, even if the verb’s selectional restrictions are not followed. (4) a. O: nay insayng-i i sihem-ul uycihanta my life-SBJ this exam-OBJ lean-on b. C: nay insayng-i i sihem-ey tallyeissta my life-SBJ this exam-ON depend ‘My life depends on this exam’ we do not correct it to (4b). Previous work has corrected sets of errors (Rozovskaya and Roth, 2010), eliminated sentences with nested or adjacent errors (Gamon, 2010), or built multiple layers of annotation (Hana et al., 2010; Boyd, 2010). Our decision makes the particle-selection task for machine learning more attainable and is easily extendible with multi-layered annotation (section 4.1). 3.3 Classifying particles For every particle in the learner corpus, error or not, we mark its specific category, e.g., GOAL. This categorization helps because learners can make different kinds of mistakes with different kinds of particles, and systems can be developed, evaluated, or optimized with respect to a particular kind of particle. 4 Putting it together The previ"
W12-3617,W10-1802,0,0.0714484,"Missing"
W12-3617,hanaoka-etal-2010-japanese,0,0.0292596,"question, as they are sometimes optional, influenced by surrounding errors, and can be interchangeable. Thirdly, Korean particles have a wide range of functions, including modification and case-marking. Annotation, and by extension the task of particle error detection, must account for these issues. We focus on the utility of annotation in evaluating particle error detection systems, ensuring that it can support the automatic task of predicting the correct particle (or no particle) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of “developing best practices for annotation and evaluOne area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been"
W12-3617,W10-1004,0,0.0720723,") is wrong, because it requires an animate object and sihem (‘exam’) is inanimate. If we correct the verb to tallyeissta (‘depend’), as in (4b), the correct particle is ey. If we do not correct the verb, the learner’s particle is, in a syntactic sense, appropriate for the verb, even if the verb’s selectional restrictions are not followed. (4) a. O: nay insayng-i i sihem-ul uycihanta my life-SBJ this exam-OBJ lean-on b. C: nay insayng-i i sihem-ey tallyeissta my life-SBJ this exam-ON depend ‘My life depends on this exam’ we do not correct it to (4b). Previous work has corrected sets of errors (Rozovskaya and Roth, 2010), eliminated sentences with nested or adjacent errors (Gamon, 2010), or built multiple layers of annotation (Hana et al., 2010; Boyd, 2010). Our decision makes the particle-selection task for machine learning more attainable and is easily extendible with multi-layered annotation (section 4.1). 3.3 Classifying particles For every particle in the learner corpus, error or not, we mark its specific category, e.g., GOAL. This categorization helps because learners can make different kinds of mistakes with different kinds of particles, and systems can be developed, evaluated, or optimized with respec"
W12-3617,C08-1109,0,0.407883,"tection systems, ensuring that it can support the automatic task of predicting the correct particle (or no particle) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of “developing best practices for annotation and evaluOne area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been for English. We thus aim to further the development of methods for detecting errors in functional elements across languages, by developing annotation for postpositional particles in Korean, a significant source of error for learners (Ko et al., 2004; Lee et al., 2009) and an area of interest for computer-assisted language learning (CALL) (Dickinson et al., 2008). As there is"
W12-3617,W10-1006,0,0.0205208,"article) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of “developing best practices for annotation and evaluOne area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been for English. We thus aim to further the development of methods for detecting errors in functional elements across languages, by developing annotation for postpositional particles in Korean, a significant source of error for learners (Ko et al., 2004; Lee et al., 2009) and an area of interest for computer-assisted language learning (CALL) (Dickinson et al., 2008). As there is at present very little work on annotated learner corpora for morphologically-rich languages, this repr"
W13-1101,N07-1049,0,0.152133,"age in Social Media (LASM 2013), pages 1–10, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics from the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and Dell’Orletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has a positive impact, although spell checking has mixed results, and that a simple tree"
W13-1101,N09-2066,0,0.0598623,"Missing"
W13-1101,W08-1301,0,0.0136027,"to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008).2 The EWT is comprised of approximately 16,000 sentences from weblogs, newsgroups, emails, reviews, and question-answers. Instead of examining each group individually, we chose to treat all web 1 <<_ -LRB--LRB-_ 2 punct _ _ 2 File _ NN NN _ 0 root _ _ 3 : _ : : _ 2 punct _ _ 4 220b _ GW GW _ 11 dep _ _ 5 -_ GW GW _ 11 dep _ _ 6 dg _ GW GW _ 11 dep _ _ 7 -_ GW GW _ 11 dep _ _ 8 Agreement _ GW GW _ 11 dep _ _ 9 for _ GW GW _ 11 dep _ _ 10 Recruiting _ GW GW _ 11 dep _ _ 11 Services.doc _ NN NN _ 2 dep _ _ 12 &gt;&gt;_ -RRB--RRB-_ 2 punct _ _ 13 <<_ -LRB--LRB-_ 14 punct _ _ 14 File _ NN NN _ 2 dep _ _"
W13-1101,P10-1075,1,0.860962,"Fix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a low confidence score, it may be revised, while high confidence dependencies are not considered for revision. 6 Reviser #1: machine learning model Reviser #2: simple tree anomaly model Another method we use for building parse revisions is based on a method to detect anomalies in parse structures (APS) using n-gram sequences of dependency structures (Dickinson and Smith, 2011; Dickinson, 2010). The method checks whether the same head category (e.g., verb) has a set of dependents similar to others of the same category (Dickinson, 2010). To see this, consider the partial tree in figure 2, from the dependency-converted EWT.7 This tree is converted to a rule as in (2), where all dependents of a head are realized. dobj det ... DT IN ... (2) dobj → det:DT NN prep:IN DT/det=determiner, dobj=direct object 4 NN Figure 2: A sketch of a basic dependency tree 7 http://sourceforge.net/projects/ mstparser/ prep NN=noun, IN/prep=preposition, This rule is then broken down into its component n-gram"
W13-1101,W11-2929,1,0.934888,"data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has a positive impact, although spell checking has mixed results, and that a simple tree anomaly detection method (Dickinson and Smith, 2011) outperforms a machine learning reviser (Attardi and Ciaramita, 2007), especially when integrated with confidence scores from the parser itself. In addition to the machine learner requiring a weak baseline parser, some of the main differences include the higher recall of the simple method at positing revisions and the fact that it detects odd structures, which parser confidence can then sort out as incorrect or not. 2 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bi"
W13-1101,P05-1045,0,0.00718594,"Missing"
W13-1101,N10-1060,0,0.0907689,"logs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps to perform tagging and parsing, with an eye towards providing a shallow semantic analysis. To process such data, with its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem with parsing social media data is the data situation: parsers can be trained on the standard training set, the Penn Treebank (Marcus et al., 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 1–10, c Atlanta, Georgia, June 13 2013. 2013 Association for"
W13-1101,I11-1100,0,0.110438,"Missing"
W13-1101,D11-1113,0,0.016201,": http://www.youtube.com/watch? v=eHSpHCprXLA 1 Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 1–10, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics from the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse revision techniques for dependency parsers. As far as we know, despite being efficient and trainable on a small amount of data, parse revision (Henestroza Anguiano and Candito, 2011; Cetinoglu et al., 2011; Attardi and Dell’Orletta, 2009; Attardi and Ciaramita, 2007) has not been used for web data, or more generally for adapting a parser to out-of-domain data; an investigation of its strengths and weaknesses is thus needed. We describe the data sets used in our experiments in section 2 and the process of normalization in section 3 before turning to the main task of parsing in section 4. Within this section, we discuss our main parser as well as two different parse revision methods (sections 4.2 and 4.3). In the evaluation in section 5, we will find that normalization has"
W13-1101,J93-2004,0,0.0479503,"h its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem with parsing social media data is the data situation: parsers can be trained on the standard training set, the Penn Treebank (Marcus et al., 1993), which has a sufficient size for training a statistical parser, but has the distinct downside of modeling language that is very dissimilar 1 Taken from: http://www.youtube.com/watch? v=eHSpHCprXLA 1 Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 1–10, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics from the target. Or one can train parsers on the English Web Treebank (Bies et al., 2012), which covers web language, including social media data, but is rather small. Our focus on improving parsing for such data is on exploring parse rev"
W13-1101,P11-1163,0,0.0304087,"data (Petrov and McDonald, 2012), which is not a good fit for social media data. The language used in social media does not follow standard conventions (e.g., containing many sentence fragments), is largely unedited, and tends to be on different topics than standard NLP technology is trained for. At the same time, there is a clear need to develop even basic NLP technology for a variety of types of social media and contexts (e.g., Twitter, Facebook, YouTube comments, discussion forums, blogs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps to perform tagging and parsing, with an eye towards providing a shallow semantic analysis. To process such data, with its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are beneficial under which circumstances. A second problem"
W13-1101,P05-1012,0,0.0528236,"eriments, DeSR generally only considers the most frequent rules (e.g., 20), as these cover most of the errors. For best results, the reviser should: a) be trained on extra data other than the data the base parser is trained on, and b) begin with a relatively poor base parsing model. As we will see, using a fairly strong base parser presents difficulties for DeSR. 4.3 4.1 Basic parser MSTParser (McDonald and Pereira, 2006)6 is a freely available parser which reaches state-of-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005), using a variety of feature sets, i.e., edge, sibling, context, and nonlocal features, employing information from words and POS tags. We use its default settings for all experiments. We use MST as our base parser, training it in different conditions on the WSJ and the EWT. Also, MST offers the possibility to retrieve confidence scores for each dependency edge: We use the KDFix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a l"
W13-1101,E06-1011,0,0.376076,"in, Aspell suggested Mukden, which has a score of 1.0 and is thus rejected. Since we do not consider context or any other information besides edit distance, spell checking is not perfect and is subject to making errors, but the number of errors is considerably smaller than the number of correct revisions. For example, lol would be changed into Lil if it were not listed in the extended lexicon. Additionally, since the errors are consistent throughout the data, they result in normalization even when the spelling is wrong. 4 Parser revision We use a state of the art dependency parser, MSTParser (McDonald and Pereira, 2006), as our main parser; and we use two parse revision methods: a machine learning model and a simple tree anomaly model. The goal is to be able to learn where the parser errs and to adjust the parses to be more appropriate given the target domain of social media texts. 4.2 We use DeSR (Attardi and Ciaramita, 2007) as a machine learning model of parse revision. DeSR uses a tree revision method based on decomposing revision actions into basic graph movements and learning sequences of such movements, referred to as a revision rule. For example, the rule -1u indicates that the reviser should change"
W13-1101,N12-1068,0,0.0168023,"available parser which reaches state-of-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005), using a variety of feature sets, i.e., edge, sibling, context, and nonlocal features, employing information from words and POS tags. We use its default settings for all experiments. We use MST as our base parser, training it in different conditions on the WSJ and the EWT. Also, MST offers the possibility to retrieve confidence scores for each dependency edge: We use the KDFix edge confidence scores discussed by Mejer and Crammer (2012) to assist in parse revision. As described in section 4.4, the scores are used to limit which dependencies are candidates for revision: if a dependency has a low confidence score, it may be revised, while high confidence dependencies are not considered for revision. 6 Reviser #1: machine learning model Reviser #2: simple tree anomaly model Another method we use for building parse revisions is based on a method to detect anomalies in parse structures (APS) using n-gram sequences of dependency structures (Dickinson and Smith, 2011; Dickinson, 2010). The method checks whether the same head catego"
W13-1101,N10-1120,0,0.01905,"ifficult, as parsers are generally trained on news data (Petrov and McDonald, 2012), which is not a good fit for social media data. The language used in social media does not follow standard conventions (e.g., containing many sentence fragments), is largely unedited, and tends to be on different topics than standard NLP technology is trained for. At the same time, there is a clear need to develop even basic NLP technology for a variety of types of social media and contexts (e.g., Twitter, Facebook, YouTube comments, discussion forums, blogs, etc.). To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011), it helps to perform tagging and parsing, with an eye towards providing a shallow semantic analysis. To process such data, with its non-standard words, we first develop techniques for normalizing the text, so as to be able to accommodate the wide range of realizations of a given token, e.g., all the different spellings and intentional misspellings of cute. While previous research has shown the benefit of text normalization (Foster et al., 2011; Gadde et al., 2011; Foster, 2010), it has not teased apart which parts of the normalization are bene"
W13-1101,C12-2088,0,0.155624,"Missing"
W13-1702,S12-1051,0,0.0145942,"quate or what it might be missing. Our scenario is based on images, not text, but our future processing will most likely need to include similar elements, e.g., determining lexical relations from WordNet. 3 Data Collection The data involved in this study shares much in common with other investigations into semantic analysis of descriptions of images and video, such as the Microsoft Research Video Description Corpus (MSRvid; Chen and Dolan (2011)) and the SemEval-2012 Semantic Textual Similarity (STS) task utilizing MSRvid as training data for assigning similarity scores to pairs of sentences (Agirre et al., 2012). However, because our approach requires both native speaker (NS) and non-native speaker (NNS) responses and necessitates constraining both the form and content of responses, we assembled our own small corpus of NS and NNS responses to a PDT. Research in SLA often relies on the ability of task design to induce particular linguistic behavior (Skehan et al., 1998), and the PDT should induce more interactive behavior. Moreover, the use of the PDT as a reliable language research tool is well-established in areas of study ranging from SLA to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venner"
W13-1702,P11-1020,0,0.0276562,"n attempts to align elements of the sentence with those of the (similarly annotated) reading prompt, the question, and target answers to determine whether a response is adequate or what it might be missing. Our scenario is based on images, not text, but our future processing will most likely need to include similar elements, e.g., determining lexical relations from WordNet. 3 Data Collection The data involved in this study shares much in common with other investigations into semantic analysis of descriptions of images and video, such as the Microsoft Research Video Description Corpus (MSRvid; Chen and Dolan (2011)) and the SemEval-2012 Semantic Textual Similarity (STS) task utilizing MSRvid as training data for assigning similarity scores to pairs of sentences (Agirre et al., 2012). However, because our approach requires both native speaker (NS) and non-native speaker (NNS) responses and necessitates constraining both the form and content of responses, we assembled our own small corpus of NS and NNS responses to a PDT. Research in SLA often relies on the ability of task design to induce particular linguistic behavior (Skehan et al., 1998), and the PDT should induce more interactive behavior. Moreover,"
W13-1702,W07-1604,0,0.0665651,"e sentence text and instead add the word to the dependency label between content words; and 2) propagate relations across any conjunctions. These decisions are important to consider for any semanticallyinformed processing of learner language. 1 http://nlp.stanford.edu/software/ lex-parser.shtml 2 http://nlp.stanford.edu/software/ dependencies_manual.pdf To see the impetus for removing prepositions, consider the learner response (1), where the preposition with is relatively unimportant to collecting the meaning. Additionally, learners often omit, insert, or otherwise use the wrong preposition (Chodorow et al., 2007). The default parser would present a prep relation between played and with, obscuring what the object is; with the options set as above, however, the dependency representation folds the preposition into the label (prep with), instead of keeping it in the parsed string, as shown in Figure 2. (1) The boy played with a ball. root prep with nsubj det vroot The boy det played with a ball Figure 2: The dependency parse of (1) This is a very lenient approach to prepositions, as prepositions certainly carry semantic meaning— e.g., the boy played in a ball means something quite different than what (1)"
W13-1702,W12-2006,0,0.0454296,"Missing"
W13-1702,de-marneffe-etal-2006-generating,0,0.125192,"Missing"
W13-1702,W12-2039,0,0.0753818,"ing of interactive statements: there is much to be gained with a small amount of computational effort, but much work needs to go into delineating a proper set of gold standard forms. There are several ways to take this work. First, 19 given the preponderance of spelling errors in NNS data and its effect on downstream processing, the effect of automatic spelling correction must be taken into account. Secondly, we only investigated transitive verbs, and much needs to be done to investigate interactions with other types of constructions, including the definition of more elaborate semantic forms (Hahn and Meurers, 2012). Finally, to better model ILTs and the interactions found in activities and games, one can begin by modeling more complex visual prompts. By using video description tasks or story retell tasks, we can elicit more complex narrative responses. This would allow us to investigate the possibility of extending our current approach to tasks that involve greater learner interaction. Acknowledgments We would like to thank the task participants, David Stringer for assistance in developing the task, Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for recruitment help, and Ross Israel for evaluat"
W13-1702,P03-1054,0,0.0120639,"elations, rather than constituents or phrase structure, it easily finds the subject, verb and object of a sentence, which can then map to a semantic form (K¨ubler et al., 2009). Our approach must eventually account for other relations, such as negation and adverbial modification, but at this point, since we focus on transitive verbs, we take an na¨ıve approach in which subject, verb and object are considered sufficient for deciding whether or not a response accurately describes the visual prompt. We use the Stanford Parser for this task, trained on the Penn Treebank (de Marneffe et al., 2006; Klein and Manning, 2003).1 Using the parser’s options, we set the output to be Stanford typed dependencies, a set of labels for dependency relations. The Stanford parser has a variety of options to choose from for the specific parser ouput, e.g., how one wishes to treat prepositions (de Marneffe and Manning, 2012). We use the CCPropagatedDependencies / CCprocessed option to accomplish two things:2 1) omit prepositions and conjunctions from the sentence text and instead add the word to the dependency label between content words; and 2) propagate relations across any conjunctions. These decisions are important to consi"
W13-1702,J03-4003,0,\N,Missing
W13-1723,J08-4004,0,0.147216,"f annotator 1 and annotator 2 both mark an empty set, 5 173 6 Since our sets tend to be small (rarely bigger than two), we do not expect much change with a full MASI calculation. Figure 2: Example of the annotation interface ROOT JCT PUNCT POBJ DET root In my opinion DET , My PRED JCT SUBJ Age is Very Young Figure 3: A mistaken arrow (JCT) leading to two dependencies for is ((0,ROOT),(1,JCT)) we count full agreement for MASI, i.e., a score of 1; for GCM, nothing gets added to the totals. We could, of course, report various coefficients commonly used in IAA studies, such as kappa or alpha (see Artstein and Poesio, 2008), but, given the large number of classes and lack of predominant classes, chance agreement seems very small. 4.1.2 Dependency-specific issues As a minor point: for dependencies, we calculate agreements for matches in only attachment or labeling. Consider (7), where there is one match only in attachment ((24,OBJ)-(24,JCT)), counting towards UAA, and one only in labeling ((24,SUBJ)(22,SUBJ)) for LOA. Importantly, we have to ensure that (24,SUBJ) and (24,JCT) are not linked. (7) A1: {(24,SUBJ), (24,OBJ)} A2: {(22,SUBJ), (24,JCT)} In general, we prioritize identical attachment over labeling, if a"
W13-1723,A00-1031,0,0.0314378,"fferences, to see whether they could reach a consensus. Each annotator fixed their own file based on the results of this discussion. At each point, we took a snapshot of the data, but at no point did we provide feedback to the annotators on their decisions. (6) 3.5 Sentence 2, word 1: relation ... JCT NJCT Annotation interface 4.1.1 The annotation is done via the Brat rapid annotation tool (Stenetorp et al., 2012).4 This online interface, shown in figure 2, allows an annotator to drag an arrow between words to create a dependency. Annotators were given automatically-derived POS tags from TnT (Brants, 2000), trained on the SUSANNE corpus (Sampson, 1995), but created the dependencies from scratch.5 Subcategorizations, lemmas, and lexical violations are annotated within one of the POS layers; lemmas are noted by the blue shading, and the presence of other layers is noted by asterisks, an interface point discussed in section 4.2.3. Annotators liked the tool, but complained of its slowness. 4 Evaluation 4.1 agreement (LAA) requires both the attachment and labeling to be the same to count as an agreement. Label only agreement (LOA) ignores the head a token attaches to and only compares labels. All th"
W13-1723,cer-etal-2010-parsing,0,0.0794888,"Missing"
W13-1723,W12-3617,1,0.716128,"e are numerous studies investigating inter-annotator agreement between coders for different types of grammatical annotation schemes, focusing on part-of-speech, syntactic, or semantic annotation (e.g., Passonneau et al., 2006; Babarczy et al., 2006; Civit et al., 2003). For learner language, a 169 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169–179, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics number of error annotation projects include measures of interannotator agreement, (see, e.g., Boyd, 2012; Lee et al., 2012; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008; Bonaventura et al., 2000), but as far as we are aware, there have been no studies on IAA for grammatical annotation. We have conducted an IAA study to investigate the quality and robustness of our annotation scheme, as reported in section 3. In section 4, we report quantitative results and a qualitative analysis of this study to tease apart disagreements due to inherent ambiguity or text difficulty from those due to the annotation scheme and/or the guidelines. The study has already reaped benefits by helping us to revise our annotation"
W13-1723,P11-1121,0,0.0641936,"re able to obtain generally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme. We pinpoint some of the problems in obtaining full agreement, including annotation scheme vagueness for certain learner innovations, interface design issues, and difficult syntactic constructions. In the process, we also develop ways to calculate agreements for sets of dependencies. 1 Introduction Learner corpora have been essential for developing error correction systems and intelligent tutoring systems (e.g., Nagata et al., 2011; Rozovskaya and Roth, 2010). So far, error annotation has been the main focus, to the exclusion of corpora and annotation for more basic NLP development, despite the need for parse information for error detection (Tetreault et al., 2010), learner proficiency identification (Hawkins and Buttery, 2010), and acquisition research (Ragheb and Dickinson, 2011). Indeed, there is very little work on POS tagging (Thou¨esny, 2009; van Rooy and Sch¨afer, 2002; de Haan, 2000) Probing grammatical annotation can lead to advancements in research on POS tagging and syntactic parsing of learner language, for"
W13-1723,W12-2011,1,0.884419,"Missing"
W13-1723,passonneau-etal-2006-inter,0,0.186975,"n Rooy and Sch¨afer, 2002; de Haan, 2000) Probing grammatical annotation can lead to advancements in research on POS tagging and syntactic parsing of learner language, for it shows what can be annotated reliably and what needs additional diagnostics. We specifically report on inter-annotator agreement (IAA) for the annotation scheme described in section 2, focusing on dependency annotation. There are numerous studies investigating inter-annotator agreement between coders for different types of grammatical annotation schemes, focusing on part-of-speech, syntactic, or semantic annotation (e.g., Passonneau et al., 2006; Babarczy et al., 2006; Civit et al., 2003). For learner language, a 169 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169–179, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics number of error annotation projects include measures of interannotator agreement, (see, e.g., Boyd, 2012; Lee et al., 2012; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008; Bonaventura et al., 2000), but as far as we are aware, there have been no studies on IAA for grammatical annotation. We have conducted an IAA study to"
W13-1723,C12-2094,1,0.907321,"of the sentence (cf. was admitted). In this case, basing the annotation on syntactic evidence makes for a more straightforward task. Moreover, adhering to a syntactic analysis helps outline the grammatical properties of a learner’s interlanguage and can thus assist in automatic tasks such as native language identification (e.g., Tetreault et al., 2012), and proficiency level determination (Yannakoudakis et al., 2011). (1) When I admit to Korea University, I decide ... 170 Now I take very hard my personal stuffs. Initial annotation layers Using ideas developed for annotating learner language (Ragheb and Dickinson, 2012, 2011; D´ıazNegrillo et al., 2010; Dickinson and Ragheb, 2009), we annotate several layers before targeting dependencies: 1) lemmas (i.e., normalized forms), 2) morphological part-of-speech (POS), 3) distributional POS, and 4) lexical violations. The idea for lemma annotation is to normalize a word to its dictionary form. In (3), for example, the misspelled excersice is normalized to the correctlyspelled exercise for the lemma annotation. We specify that only “reasonable” orthographic or phonetic changes are allowed; thus, for prison, it is lemmaannotated as prison, not person. In this case,"
W13-1723,W10-1004,0,0.146945,"erally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme. We pinpoint some of the problems in obtaining full agreement, including annotation scheme vagueness for certain learner innovations, interface design issues, and difficult syntactic constructions. In the process, we also develop ways to calculate agreements for sets of dependencies. 1 Introduction Learner corpora have been essential for developing error correction systems and intelligent tutoring systems (e.g., Nagata et al., 2011; Rozovskaya and Roth, 2010). So far, error annotation has been the main focus, to the exclusion of corpora and annotation for more basic NLP development, despite the need for parse information for error detection (Tetreault et al., 2010), learner proficiency identification (Hawkins and Buttery, 2010), and acquisition research (Ragheb and Dickinson, 2011). Indeed, there is very little work on POS tagging (Thou¨esny, 2009; van Rooy and Sch¨afer, 2002; de Haan, 2000) Probing grammatical annotation can lead to advancements in research on POS tagging and syntactic parsing of learner language, for it shows what can be annotat"
W13-1723,E12-2021,0,0.0914215,"Missing"
W13-1723,C12-1158,0,0.0143628,"es even if the meaning of the sentence or clause is unclear within the particular grammatical analysis. For example, in the learner sentence (1), the verb admit clearly occurs in the form of an active verb, and is annotated as such, regardless of the (passive) meaning of the sentence (cf. was admitted). In this case, basing the annotation on syntactic evidence makes for a more straightforward task. Moreover, adhering to a syntactic analysis helps outline the grammatical properties of a learner’s interlanguage and can thus assist in automatic tasks such as native language identification (e.g., Tetreault et al., 2012), and proficiency level determination (Yannakoudakis et al., 2011). (1) When I admit to Korea University, I decide ... 170 Now I take very hard my personal stuffs. Initial annotation layers Using ideas developed for annotating learner language (Ragheb and Dickinson, 2012, 2011; D´ıazNegrillo et al., 2010; Dickinson and Ragheb, 2009), we annotate several layers before targeting dependencies: 1) lemmas (i.e., normalized forms), 2) morphological part-of-speech (POS), 3) distributional POS, and 4) lexical violations. The idea for lemma annotation is to normalize a word to its dictionary form. In ("
W13-1723,W08-1205,0,0.0309815,"notator agreement between coders for different types of grammatical annotation schemes, focusing on part-of-speech, syntactic, or semantic annotation (e.g., Passonneau et al., 2006; Babarczy et al., 2006; Civit et al., 2003). For learner language, a 169 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169–179, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics number of error annotation projects include measures of interannotator agreement, (see, e.g., Boyd, 2012; Lee et al., 2012; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008; Bonaventura et al., 2000), but as far as we are aware, there have been no studies on IAA for grammatical annotation. We have conducted an IAA study to investigate the quality and robustness of our annotation scheme, as reported in section 3. In section 4, we report quantitative results and a qualitative analysis of this study to tease apart disagreements due to inherent ambiguity or text difficulty from those due to the annotation scheme and/or the guidelines. The study has already reaped benefits by helping us to revise our annotation scheme and guidelines, and the insights gained here shou"
W13-1723,P10-2065,0,0.111467,"Missing"
W13-1723,P11-1019,0,0.022064,"thin the particular grammatical analysis. For example, in the learner sentence (1), the verb admit clearly occurs in the form of an active verb, and is annotated as such, regardless of the (passive) meaning of the sentence (cf. was admitted). In this case, basing the annotation on syntactic evidence makes for a more straightforward task. Moreover, adhering to a syntactic analysis helps outline the grammatical properties of a learner’s interlanguage and can thus assist in automatic tasks such as native language identification (e.g., Tetreault et al., 2012), and proficiency level determination (Yannakoudakis et al., 2011). (1) When I admit to Korea University, I decide ... 170 Now I take very hard my personal stuffs. Initial annotation layers Using ideas developed for annotating learner language (Ragheb and Dickinson, 2012, 2011; D´ıazNegrillo et al., 2010; Dickinson and Ragheb, 2009), we annotate several layers before targeting dependencies: 1) lemmas (i.e., normalized forms), 2) morphological part-of-speech (POS), 3) distributional POS, and 4) lexical violations. The idea for lemma annotation is to normalize a word to its dictionary form. In (3), for example, the misspelled excersice is normalized to the cor"
W14-3504,W12-2006,0,0.0186625,"any context where image descriptions or some expected content is available, but not necessarily expected linguistic forms. KEYWORDS: picture description task, semantic analysis, spelling correction, language modeling. Levi King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards"
W14-3504,de-marneffe-etal-2006-generating,0,0.172252,"Missing"
W14-3504,W12-2012,0,0.0688163,"Missing"
W14-3504,W12-2039,0,0.0254411,"ning texts for the LM, which was shown to have serious biases against the contents of the PDT responses, which tend to describe physical actions or scenarios not common in newspaper text. Finding training texts that contain the necessary kinds of sentences but also the sheer volume needed to cover the variability of NNS responses is a challenge for future experiments in this area. Given that this study primarily investigated transitive verbs, research on this problem will need to examine interactions with other types of constructions, including the definition of more elaborate semantic forms (Hahn and Meurers, 2012). Moving to a wider range of sentence types may require the use of a semantic role labeler or similar tools and has the potential to increase the complexity of spelling correction, due to, e.g., longer sentences. Acknowledgments We would like to thank the task participants, David Stringer for assistance in developing the task, and Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for help in recruiting participants. We also thank Abigail Elston and Alex Rudnick for their helpful advice during the system development. Finally, for their insightful feedback, we would like to thank the two a"
W14-3504,W13-1702,1,0.348717,"e feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner’s expected content, and thus also their word forms. Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools that maximize free interaction, we have to deal with removing impediments to interaction. Given the preponderance of spelling errors in learner data, and specifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely, language modeling for spelling correction—to make the meaning of a learner’s sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have ri"
W14-3504,P03-1054,0,0.00636355,"ers in an intensive English as a Second Language program at Indiana University. This data set contains responses from 53 informants, including native speakers (NSs) (14 NSs, 39 NNSs), for a total of 530 sentences. The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish. 3.2 Method As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from root, and the logical object from nsubjpass. This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed:"
W14-3504,P14-5010,0,0.00478286,"(de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from root, and the logical object from nsubjpass. This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed: the parser is pre-built; the decision tree is small; and the extraction rules are minimal. Note, too, that certain relations (e.g., det) are completely ignored in the extraction. (1) A bird is shot by a man. root nsubjpass det agent auxpass vroot A bird is det shot by a man Figure 2: The dependency parse of (1) One is able to use little effort in part due to the constraints in the pictures. For figure 1, for example, the artist, the man in the beret, and the man are all acceptable subjects, whereas if there were multiple men in the picture, the man would not be"
W14-3504,W14-1801,0,0.188064,"sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have right now, on picture description tasks (PDTs), connects not only with a desire for more interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and 3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used. This then raises the question of whether we are modeling what the learner said (modulo some spelling variation), what the learner intended, or what the learner should have intended, an issue we take up in section 4, after covering the background in section 3. The methods are covered in section 5 and the evaluat"
W15-0604,E12-1007,0,0.0131391,", 1994; Ozlem C¸etinoˇglu and Kuhn, 2013; Aduriz et al., 2000). These are robust, but the process is not quickly adaptable to other languages, as every rule is language-specific, and there is no clear way to handle learner innovations. (Section 4). Morphdb (Tr´on et al., 2006), a lexical database for Hungarian, encodes only irregularities and uses features on the appropriate lexical items to apply the proper phonological and morphological processes during analysis. These various tools have been incorporated into a variety of other Hungarian systems (Hal´acsy et al., 2006; Bohnet et al., 2013; Farkas et al., 2012; Zsibrita et al., 2013). For approaches like Hunmorph and Morphdb that rely on a dictionary, unknown words are the main problem— also a crucial issue for innovative learner forms. 2.3 Grammatical error detection There is some work exploring morphological derivations in learner language. Dickinson (2011) looks for stem-suffix mismatches to identify potential errors (for Russian) and uses heuristics to sort through multiple analyses. There is, however, no evaluation on learner data. We focus on building a small grammar to explicitly license combinations and provide a variety of evaluations on r"
W15-0604,Q13-1034,0,0.0176139,"nniemi, 1983; Oflazer, 1994; Ozlem C¸etinoˇglu and Kuhn, 2013; Aduriz et al., 2000). These are robust, but the process is not quickly adaptable to other languages, as every rule is language-specific, and there is no clear way to handle learner innovations. (Section 4). Morphdb (Tr´on et al., 2006), a lexical database for Hungarian, encodes only irregularities and uses features on the appropriate lexical items to apply the proper phonological and morphological processes during analysis. These various tools have been incorporated into a variety of other Hungarian systems (Hal´acsy et al., 2006; Bohnet et al., 2013; Farkas et al., 2012; Zsibrita et al., 2013). For approaches like Hunmorph and Morphdb that rely on a dictionary, unknown words are the main problem— also a crucial issue for innovative learner forms. 2.3 Grammatical error detection There is some work exploring morphological derivations in learner language. Dickinson (2011) looks for stem-suffix mismatches to identify potential errors (for Russian) and uses heuristics to sort through multiple analyses. There is, however, no evaluation on learner data. We focus on building a small grammar to explicitly license combinations and provide a variet"
W15-0604,H92-1022,0,0.517861,"hich the system can be improved. The rich morphology of Hungarian necessitates taking the morpheme as the basic unit of analysis. A single morpheme can convey a wealth of information (e.g. person, number, definiteness on verb suffixes), and a sufficiently extensive set of phonological and morphological features must be used, particularly if one is to capture individual variation. 2 For Hungarian, HuMor (High-speed Unification Morphology) (Pr´osz´eky and Kis, 1999) uses a bank of pre-encoded knowledge in the form of a dictionary and feature-based rules. Megyesi (1999) extends the Brill tagger (Brill, 1992), a rule-based tagger, with simple lexical templates. Tron et al. (2005) derive a morphological analyzer, Hunmorph, from a language-independent spelling corrector, using a recursive affix-stripping algorithm that relies on a dictionary to remove affixes one by one until a root morpheme is found. The dictionary is customizable to other languages, and the idea of using affixremoval to identify stems is similar to our technique 2.1 Background and Previous Work Hungarian Hungarian is an agglutinative language belonging to the Finno-Ugric family. It has a rich inflectional and derivational morpholo"
W15-0604,W07-1604,0,0.0398046,"o the idea of constraint relaxation and constraint ranking (e.g., Menzel, 2006; Schwind, 1995), wherein grammatical constraints are defeasible (see Leacock et al., 2014, ch. 2). In the case of morphology, the primary process of relaxing constraints is in allowing stems and affixes to combine which are generally not allowed to do so (see also Section 4). There is a wealth of research on statistical error detection and correction of grammatical errors for language learners (Leacock et al., 2014), including for Hungarian (Durst et al., 2014; Vincze et al., 2014). As has been argued before (e.g., Chodorow et al., 2007; Tetreault and Chodorow, 2008), statistical methods are ideal for parts of the linguistic system difficult to encode via rules. Since Hungarian morphology is a highly rule-governed domain of the language and since we want detailed linguistic infor33 mation for feedback, we do not focus on statistical methods here. We hope, however, to eventually obtain an appropriate distribution of errors in order to incorporate probabilities into the analysis. The emphasis on rule-based error detection allows one to connect the work to broader techniques for modeling learner behavior, in the context of ICAL"
W15-0604,halacsy-etal-2006-using,0,0.0440566,"Missing"
W15-0604,I13-1199,1,0.898087,"Missing"
W15-0604,W04-1903,0,0.803916,"Missing"
W15-0604,C08-2008,0,0.0547871,"Missing"
W15-0604,dickinson-ledbetter-2012-annotating,1,0.632451,"eginners) within proficiency levels, and a single learner’s data over time can also be analyzed. Additionally, passages are often longer and feature more descriptive language than those produced for grammatical exercises. 3.2 Annotation Each journal has been transcribed manually and annotated for errors with EXMARaLDA (Schmidt, 2010).1 The text is segmented on morpheme boundaries, and errors are identified in four different tiers, matched to a target form. The annotation scheme is specifically for Hungarian, but the principles behind it can be extended to other morphologically rich languages (Dickinson and Ledbetter, 2012). The annotation marks different types of errors re1 http://www.exmaralda.org/en_index.html flecting different levels of linguistic analysis. For instance, for (2), the annotation shows a CL (vowel length) error on the verb stem and an MAD (definiteness) error on the verb suffix—i.e. the definite suffix does not agree with the indefinite noun complements—as shown in Figure 1. (2) Ajanl -om bor -t , nem s¨or -t recommend 1 SG . DF wine ACC , not beer ACC ‘I recommend wine, not beer.’ TXT SEG CHA MOR TGT Ajanlom bort , nem s¨ort . Ajanl om bor t , nem s¨or t . CL MAD Aj´anl ok bor t , nem s¨or t"
W15-0604,W99-0633,0,0.836234,"s the evaluation also points to ways in which the system can be improved. The rich morphology of Hungarian necessitates taking the morpheme as the basic unit of analysis. A single morpheme can convey a wealth of information (e.g. person, number, definiteness on verb suffixes), and a sufficiently extensive set of phonological and morphological features must be used, particularly if one is to capture individual variation. 2 For Hungarian, HuMor (High-speed Unification Morphology) (Pr´osz´eky and Kis, 1999) uses a bank of pre-encoded knowledge in the form of a dictionary and feature-based rules. Megyesi (1999) extends the Brill tagger (Brill, 1992), a rule-based tagger, with simple lexical templates. Tron et al. (2005) derive a morphological analyzer, Hunmorph, from a language-independent spelling corrector, using a recursive affix-stripping algorithm that relies on a dictionary to remove affixes one by one until a root morpheme is found. The dictionary is customizable to other languages, and the idea of using affixremoval to identify stems is similar to our technique 2.1 Background and Previous Work Hungarian Hungarian is an agglutinative language belonging to the Finno-Ugric family. It has a rich"
W15-0604,P99-1034,0,0.881247,"Missing"
W15-0604,C08-1109,0,0.0215257,"t relaxation and constraint ranking (e.g., Menzel, 2006; Schwind, 1995), wherein grammatical constraints are defeasible (see Leacock et al., 2014, ch. 2). In the case of morphology, the primary process of relaxing constraints is in allowing stems and affixes to combine which are generally not allowed to do so (see also Section 4). There is a wealth of research on statistical error detection and correction of grammatical errors for language learners (Leacock et al., 2014), including for Hungarian (Durst et al., 2014; Vincze et al., 2014). As has been argued before (e.g., Chodorow et al., 2007; Tetreault and Chodorow, 2008), statistical methods are ideal for parts of the linguistic system difficult to encode via rules. Since Hungarian morphology is a highly rule-governed domain of the language and since we want detailed linguistic infor33 mation for feedback, we do not focus on statistical methods here. We hope, however, to eventually obtain an appropriate distribution of errors in order to incorporate probabilities into the analysis. The emphasis on rule-based error detection allows one to connect the work to broader techniques for modeling learner behavior, in the context of ICALL exercises (Thou¨esny and Blin"
W15-0604,W05-1106,0,0.634797,"Missing"
W15-0604,tron-etal-2006-morphdb,0,0.686588,"Missing"
W15-0604,W13-1708,0,0.383299,"rror detection and correction (Leacock et al., 2014), this work has a few (admitted) limitations: 1) it has largely focused on a few error types (e.g., prepositions, articles, collocations); 2) it has largely been for English, with only a few explorations into other languages (e.g., Basque (de Ilarraza et al., 2008), Korean (Israel et al., 2013)); and 3) it has often focused on errors to the exclusion of broader patterns of learner productions—a crucial link if one wants to develop intelligent computer-assisted language learning (ICALL) (Heift and Schulze, 2007) or proficiency classification (Vajjala and Loo, 2013; Hawkins and Buttery, 2010) applications or connect to second language acquisition (SLA) research (Ragheb, 2014). We focus on Hungarian morphological analysis for learner language, attempting to build a system that: 1) works for a variety of morWe hope to make the analysis of Hungarian morphology maximally useful. Consider ICALL system development, for example: successful systems not only provide meaningful feedback for learners but also model learner behavior (e.g., Amaral and Meurers, 2008). To do this requires tracking correct and incorrect use of different linguistic phenomena (e.g., case"
W15-0604,vincze-etal-2014-automatic,0,0.0509834,"Missing"
W15-0604,W12-0206,0,0.168447,"rian morphology is a highly rule-governed domain of the language and since we want detailed linguistic infor33 mation for feedback, we do not focus on statistical methods here. We hope, however, to eventually obtain an appropriate distribution of errors in order to incorporate probabilities into the analysis. The emphasis on rule-based error detection allows one to connect the work to broader techniques for modeling learner behavior, in the context of ICALL exercises (Thou¨esny and Blin, 2011; Heift, 2007) or in mapping and understanding development (cf. Vajjala and Loo, 2013; Vyatkina, 2013; Yannakoudakis et al., 2012). Our evaluation thus focuses on multiple facets of the output and its use (Section 5). 3 Data and Annotation 3.1 Corpus The corpus was collected from L1 English students of Hungarian at Indiana University and is divided into three levels of proficiency (Beginner, Intermediate, Advanced) as determined by course placement in one of three two-semester sequences. The corpus consists of journal entries, each a minimum ten sentences in length on a topic selected by the student. The corpus at present contains data for 14 learners (9 Beginner, 1 Intermediate, 4 Advanced), 9391 sentences total, with 1"
W15-0604,R13-1099,0,0.404649,"ˇglu and Kuhn, 2013; Aduriz et al., 2000). These are robust, but the process is not quickly adaptable to other languages, as every rule is language-specific, and there is no clear way to handle learner innovations. (Section 4). Morphdb (Tr´on et al., 2006), a lexical database for Hungarian, encodes only irregularities and uses features on the appropriate lexical items to apply the proper phonological and morphological processes during analysis. These various tools have been incorporated into a variety of other Hungarian systems (Hal´acsy et al., 2006; Bohnet et al., 2013; Farkas et al., 2012; Zsibrita et al., 2013). For approaches like Hunmorph and Morphdb that rely on a dictionary, unknown words are the main problem— also a crucial issue for innovative learner forms. 2.3 Grammatical error detection There is some work exploring morphological derivations in learner language. Dickinson (2011) looks for stem-suffix mismatches to identify potential errors (for Russian) and uses heuristics to sort through multiple analyses. There is, however, no evaluation on learner data. We focus on building a small grammar to explicitly license combinations and provide a variety of evaluations on real learner data. Prior"
W15-0604,W13-2808,0,\N,Missing
W15-0604,C00-1001,0,\N,Missing
W15-0604,W13-3704,0,\N,Missing
W15-1619,D14-1108,0,0.0728968,"Missing"
W15-1619,W12-3617,1,0.780293,"es are not specific to this annotation, but it illustrates the difficulties in applying native categories to learner data. That is, the SALLE annotation scheme (Dickinson and Ragheb, 2013) helps define questions of what constitutes appropriate linguistic annotation for interlanguage.2 3 Grammatical Annotation When annotating learner data, it is important to know what is meant by grammatical. For error annotation, for example, this defines what an error is; e.g., in Korean, a missing postpositional particle may be an error or not depending on the level of formality underpinning grammaticality (Lee et al., 2012). The SALLE framework assumes a grammar based on the target language as an underpinning to the annotation (section 3.1), but, in the face of innovative learner usage, has focused on annotating the language as it appears and not on whether each sentence deviates from that grammar, i.e., is ungrammatical or not (section 3.2). 3.1 Target language grammar To see the need to make clear the source of grammaticality, consider morphological POS annotation (section 2). In a verbal sequence like can promotes, for example, promotes intuitively has the morphological evidence of a third person singular ver"
W15-1619,C12-2094,1,0.810646,"Guidelines at: http://cl.indiana.edu/˜salle/ 160 to reference these morphological properties requires some notion of how these properties are defined, e.g., how -s stands for third person singular. One obvious source of information is that “third person singular” comes from the definition of the -s morpheme in English. To annotate this way means referencing grammatical concepts from the target language (L2). If a different grammar is chosen to define categories, such as the learner’s first language (L1), one might posit, e.g., -et as an indicator of “third person singular” (cf. Russian). In (Ragheb and Dickinson, 2012), we argue for using the L2 as the source of the grammar, as learners share many aspects of development in the L2 (Ellis, 2008) and as this can ensure annotation reliability. 3.2 Emerging categories Annotation deals with the way facts from the grammar interact with phenomena occuring within a sentence. Consider objects, for example: a constellation of properties allows one to specify that two different sentences both contain them. Objects can be defined as: a) occurring, roughly speaking, after a verb (syntactic distribution); b) fitting into the argument structure of a verb, typically as a pa"
W15-1619,N10-1049,0,0.0246756,"Missing"
W16-0512,W08-0913,0,0.622553,"om a set of native speaker (NS) responses. In more exploratory work, we examine the variability in both NS and NNS responses, and how different system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language. 1 Introduction and Motivation Although much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Leacock et al., 2014), there is a growing body of work covering varying kinds of semantic analysis (e.g., Meurers et al., 2011; Bailey and Meurers, 2008; King and Dickinson, 2014, 2013; Petersen, 2010), including assessment-driven work (e.g., Somasundaran et al., 2015; Somasundaran and Chodorow, 2014). One goal of such work is to facilitate intelligent language tutors (ILTs) and language assessment tools that maximize communicative interaction, as suggested by research in second language instruction (cf. Celce-Murcia, 1991, 2002; Larsen-Freeman, 2002). Whether for feedback or for assessment, however, there are lingering Focusing on semantic analysis requires a sense of what counts as a semantically appropriate utterance from a language learne"
W16-0512,de-marneffe-etal-2006-generating,0,0.0267924,"Missing"
W16-0512,N15-1174,0,0.0196853,"ation to be derived from a relatively small set of native speaker responses, as opposed to deriving them by hand, in 112 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 112–121, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics addition to allowing for a range of sentence types. We should note, in this context, that we are discussing semantic analysis given a gold standard of native sentences. Image description tasks can often rely on breaking images into semantic primitives (see, e.g., Gilberto Mateos Ortiz et al., 2015, and references therein), but for learner data, we want to ensure that we can account not just for correct semantics (the what of a picture), but natural expressions of the semantics (the how of expressing the content). In other words, we want to reason about meaning based on specific linguistic forms. A second issue regarding semantic analysis, beyond correctness, stems from using an incomplete gold standard, namely: assessing the degree of semantic variability, both for native speakers (NSs) and non-native speakers (NNSs). In addition to providing insight into theoretical research on variab"
W16-0512,W13-1702,1,0.941593,"ntically correct answer have to sound nativelike or only convey the correct facts? 2) Which facts from the picture are more or less relevant? 3) Are responses strictly correct or not, or is it better to treat correctness as a gradable phenomenon? Additionally, a gold standard of correct responses cannot capture all possible variations of saying the correct content (cf. paraphrases, Barzilay, 2003). We thus must address the specific question of how one can reason about semantic correctness from a (necessarily) incomplete gold standard of answers. In this paper, we build from our previous work (King and Dickinson, 2013, 2014) and move towards finding a “sweet spot” of semantic analysis (cf. Bailey and Meurers, 2008) for such imagebased learner productions. In particular, using available NLP tools, we move away from specific correct semantic representations and an exact definition of correctness, to more abstract data representations and more gradable notions of correctness (section 4). A benefit of more abstract representations is to allow correct and relevant information to be derived from a relatively small set of native speaker responses, as opposed to deriving them by hand, in 112 Proceedings of the 11t"
W16-0512,W14-3504,1,0.856954,"(NS) responses. In more exploratory work, we examine the variability in both NS and NNS responses, and how different system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language. 1 Introduction and Motivation Although much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Leacock et al., 2014), there is a growing body of work covering varying kinds of semantic analysis (e.g., Meurers et al., 2011; Bailey and Meurers, 2008; King and Dickinson, 2014, 2013; Petersen, 2010), including assessment-driven work (e.g., Somasundaran et al., 2015; Somasundaran and Chodorow, 2014). One goal of such work is to facilitate intelligent language tutors (ILTs) and language assessment tools that maximize communicative interaction, as suggested by research in second language instruction (cf. Celce-Murcia, 1991, 2002; Larsen-Freeman, 2002). Whether for feedback or for assessment, however, there are lingering Focusing on semantic analysis requires a sense of what counts as a semantically appropriate utterance from a language learner. Consider when a learner"
W16-0512,P03-1054,0,0.00822706,"ontents. Somasundaran et al. (2015) present similar work analyzing responses to sequences of pictures. While they score via a machine learning system, we stick closer to the original forms in trying to find an appropriate way to analyze the data; the notion of overlap for relevance, however, is very similar in spirit to our count-based methods (section 4.2). We build directly from King and Dickinson (2013, 2014), where the method to obtain a semantic form from a NNS production is: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford Parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. It is this method we attempt to generalize (section 4). 3 Data Collection Because our approach requires both NS and NNS responses and necessitates constraining both the form and content of responses, we previously assembled a small corpus of NS and NNS responses to a PDT (King and Dickinson, 2013). Research in SLA often relies on the ability of task design to induce particular linguistic behavior (Skehan et al., 1998), and the PDT should induce context-focused communicative behavior. Moreover, the use of the"
W16-0512,J93-2004,0,0.0579228,"dition to the four approaches, we have term representations and two sets of parameters, listed below, to vary, resulting in a total of 60 settings for processing responses (see also Table 2). Term form. As discussed in section 4.1, the terms can take one of five representations: ldh, xdh, lxh, ldx, or xdx. Scoring approach. As discussed in section 4.2, the NNS responses can be compared with the GS via models FA, TA, FC, or TC. Reference corpus. The reference corpus for deriving tf-idf scores can be either the Brown Corpus 116 (Kucera and Francis, 1967) or the Wall Street Journal (WSJ) Corpus (Marcus et al., 1993). These are abbreviated as B and W in the results below; na indicates the lack of a reference corpus, as this is only relevant to approaches TA and TC. The corpora are divided into as many documents as originally distributed (W: 1640, B: 499). The WSJ is larger, but Brown has the benefit of containing more balance in its genres (vs. newstext only). Considering the narrative nature of PDT responses, a reference corpus of narrative texts would be ideal, but we choose manually parsed reference corpora as they are more reliable than automatically parsed data. NNS source. Each response has an origi"
W16-0512,W14-1801,0,0.388162,"t system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language. 1 Introduction and Motivation Although much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Leacock et al., 2014), there is a growing body of work covering varying kinds of semantic analysis (e.g., Meurers et al., 2011; Bailey and Meurers, 2008; King and Dickinson, 2014, 2013; Petersen, 2010), including assessment-driven work (e.g., Somasundaran et al., 2015; Somasundaran and Chodorow, 2014). One goal of such work is to facilitate intelligent language tutors (ILTs) and language assessment tools that maximize communicative interaction, as suggested by research in second language instruction (cf. Celce-Murcia, 1991, 2002; Larsen-Freeman, 2002). Whether for feedback or for assessment, however, there are lingering Focusing on semantic analysis requires a sense of what counts as a semantically appropriate utterance from a language learner. Consider when a learner has to describe the contents of a picture (see section 3). There are a number of questions to address in such a situation:"
W16-0512,W15-0605,0,0.38706,"responses, and how different system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language. 1 Introduction and Motivation Although much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Leacock et al., 2014), there is a growing body of work covering varying kinds of semantic analysis (e.g., Meurers et al., 2011; Bailey and Meurers, 2008; King and Dickinson, 2014, 2013; Petersen, 2010), including assessment-driven work (e.g., Somasundaran et al., 2015; Somasundaran and Chodorow, 2014). One goal of such work is to facilitate intelligent language tutors (ILTs) and language assessment tools that maximize communicative interaction, as suggested by research in second language instruction (cf. Celce-Murcia, 1991, 2002; Larsen-Freeman, 2002). Whether for feedback or for assessment, however, there are lingering Focusing on semantic analysis requires a sense of what counts as a semantically appropriate utterance from a language learner. Consider when a learner has to describe the contents of a picture (see section 3). There are a number of question"
W16-0512,J03-4003,0,\N,Missing
W16-0523,W04-1903,0,0.0277502,"Missing"
W16-0523,dickinson-ledbetter-2012-annotating,1,0.821769,"ngh et al., 2006; Shrivastava and Bhattacharyya, 2008; Alfter, 2014). Details of the system are in section 3.2. 3.1 Data To develop and evaluate a system, we rely on both native language and learner data. The corpus of learner data was collected from L1 English students of Hungarian, divided into three levels of proficiency (Beginner, Intermediate, Advanced) as determined by course placement in one of three two-semester sequences. The corpus consists of journal entries, each a minimum of ten sentences in length on a topic selected by the student. The data and error annotation are described in Dickinson and Ledbetter (2012). For native data, we use the Szeged Corpus of Hungarian (Csendes et al., 2004). For development of all our improvements, we use the same 1000 words of native data and 1024 words of learner data that were used in Ledbetter and Dickinson (2015). The native data is taken from compositions, so as to be relatively comparable to the journal entries found in the learner corpus. For evaluation (section 5), we use new approximately samesized (1000, 1032) sections of data, i.e., the test sets are comprised of new compositions never before seen by the analyzer. Note that there is no training data, as we"
W16-0523,P15-2139,0,0.0223429,", outlining extensions to a resourcelight system that can be developed by different types of experts. Specifically, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible. 1 Introduction There is much work on developing technology and corpora for lesser-resourced languages (LRLs), involving varying assumptions about the amount of available data (e.g., Feldman and Hana, 2010; Duong et al., 2015; McDonald et al., 2011; Garrette et al., 2013; Garrette and Baldridge, 2013; Lynn et al., 2013; Smith and Dickinson, 2014). To our knowledge, though, there has been very little work focusing on tools for automatically analyzing learner language. Yet LRLs present many challenges and opportunities, not least of which is the chance, through language learning, to increase awareness and usage of the language. In that light, we build from our work in Ledbetter and Dickinson (2015) to develop a morphological analyzer for learner Hungarian in a resource-light way, extending the framework to handle a"
W16-0523,N13-1014,0,0.0211342,"oped by different types of experts. Specifically, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible. 1 Introduction There is much work on developing technology and corpora for lesser-resourced languages (LRLs), involving varying assumptions about the amount of available data (e.g., Feldman and Hana, 2010; Duong et al., 2015; McDonald et al., 2011; Garrette et al., 2013; Garrette and Baldridge, 2013; Lynn et al., 2013; Smith and Dickinson, 2014). To our knowledge, though, there has been very little work focusing on tools for automatically analyzing learner language. Yet LRLs present many challenges and opportunities, not least of which is the chance, through language learning, to increase awareness and usage of the language. In that light, we build from our work in Ledbetter and Dickinson (2015) to develop a morphological analyzer for learner Hungarian in a resource-light way, extending the framework to handle a wider range of phenomena and to increase accuracy, addressing the tradeoffs"
W16-0523,P13-1057,0,0.0201016,"ystem that can be developed by different types of experts. Specifically, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible. 1 Introduction There is much work on developing technology and corpora for lesser-resourced languages (LRLs), involving varying assumptions about the amount of available data (e.g., Feldman and Hana, 2010; Duong et al., 2015; McDonald et al., 2011; Garrette et al., 2013; Garrette and Baldridge, 2013; Lynn et al., 2013; Smith and Dickinson, 2014). To our knowledge, though, there has been very little work focusing on tools for automatically analyzing learner language. Yet LRLs present many challenges and opportunities, not least of which is the chance, through language learning, to increase awareness and usage of the language. In that light, we build from our work in Ledbetter and Dickinson (2015) to develop a morphological analyzer for learner Hungarian in a resource-light way, extending the framework to handle a wider range of phenomena and to increase accur"
W16-0523,E09-2008,0,0.0265619,"ection 4) is then evaluated separately with respect to this baseline. Since we are concerned about time, we should note that the rule compiler is around 3000-4000 lines of Python code (depending on how one counts comments, data handling, printing of intermediate output, system-internal checks, etc.). We estimate it 208 would take 1–2 months for someone to build, but as it relies on a CYK parser (see details in Ledbetter and Dickinson, 2015) and as one may choose different kinds of rule compilers for the same effect (e.g., a constraint grammar compiler (Didriksen, 2016) or an FST-based system (Hulden, 2009)), this time could be significantly less. 4 Improvements The morphological analyzer is designed with lowresource scenarios in mind, starting with as little as a rule compiler and however much grammatical information one has time to specify in rules (i.e., affixes). We want to improve beyond this simple design, but we need to determine the best ways to improve. Ideal improvements: a) require little time to implement; b) contribute (significantly) to better performance; and c) are as transparent as possible. As mentioned in section 1, improvements to the analyzer are divided into four categories"
W16-0523,W15-0604,1,0.540397,"lesser-resourced languages (LRLs), involving varying assumptions about the amount of available data (e.g., Feldman and Hana, 2010; Duong et al., 2015; McDonald et al., 2011; Garrette et al., 2013; Garrette and Baldridge, 2013; Lynn et al., 2013; Smith and Dickinson, 2014). To our knowledge, though, there has been very little work focusing on tools for automatically analyzing learner language. Yet LRLs present many challenges and opportunities, not least of which is the chance, through language learning, to increase awareness and usage of the language. In that light, we build from our work in Ledbetter and Dickinson (2015) to develop a morphological analyzer for learner Hungarian in a resource-light way, extending the framework to handle a wider range of phenomena and to increase accuracy, addressing the tradeoffs between effort and performance. Indeed, there is a convenient connection between LRLs and the automatic analysis of learner language, one enabling a rule-based approach. In a low-resourced setting, there is a lack of data for datadriven modeling, and one can write rules in a short amount of time; because such rules are linguistically motivated, they are relatively easy to connect to tasks such as prov"
W16-0523,W15-0602,0,0.0179866,"nect to tasks such as providing feedback on learner input or modeling learner behavior, i.e., tasks requiring linguistic analysis. Systems for learner language are also amenable to low-resource development in that the vocabulary is often restricted and the requirement for high precision dovetails nicely with using linguistic rules. Such an approach allows for quick, transparent development, helping identify parts of the linguistic system that the tool gets (in)correct. Given the need to interact with NLP researchers, educators, and learners, this linguistic transparency is a key property (cf. Loukina et al., 2015). In this work, we propose a number of different kinds of extensions to a system, appropriate for different aspects of analysis. The system is rule-based, which— in addition to being appropriate for Hungarian mor206 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 206–216, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics phology in a resource-light setting—makes it feasible to plug in different components and thus allows one to test the effect of different extensions. We envision different kinds of expe"
W16-0523,W13-4901,0,0.0345267,"Missing"
W16-0523,D11-1006,0,0.0331093,"Missing"
W16-0523,W99-0633,0,0.143623,"ness of vowels (represented by the feature [+/- BK]) matches during affixation (T¨orkenczy, 2008); e.g., the front vowel variant -ben is inappropriate for (1a). (1) 3 a. f´a -k -ban tree[+ BK] -PL -INESS [+ BK ] ‘in trees’ b. felejt -ett -em forget[- BK] -PST [- BK ] -1 SG . INDEF [- BK ] ‘I forgot’ c. a´ lm -atlan -s´ag sleep[+ BK ] -NEG . ADJ [+ BK ] -NOM [+ BK ] ‘sleeplessness/insomnia’ Framework We build from the analyzer in Ledbetter and Dickinson (2015). Given the nature of Hungarian, the morphological analyzer follows in the tradition of rule-based approaches (Pr´osz´eky and Kis, 1999; Megyesi, 1999; Tr´on et al., 2005, 2006), as a statistical approach relying on large amounts of data for training would be more challenging in a resourcelight setting. Data sparsity, common with agglutinative languages in which a given wordform appears few times in training data, compounds this problem. The amount and type of resources necessary for our work is comparable to other research on morphologically-rich languages in low-resource settings, e.g. for some Indian languages (Singh et al., 2006; Shrivastava and Bhattacharyya, 2008; Alfter, 2014). Details of the system are in section 3.2. 3.1 Data To de"
W16-0523,oravecz-etal-2014-hungarian,0,0.0580551,"Missing"
W16-0523,P99-1034,0,0.240952,"Missing"
W16-0523,P06-2100,0,0.0303301,"Hungarian, the morphological analyzer follows in the tradition of rule-based approaches (Pr´osz´eky and Kis, 1999; Megyesi, 1999; Tr´on et al., 2005, 2006), as a statistical approach relying on large amounts of data for training would be more challenging in a resourcelight setting. Data sparsity, common with agglutinative languages in which a given wordform appears few times in training data, compounds this problem. The amount and type of resources necessary for our work is comparable to other research on morphologically-rich languages in low-resource settings, e.g. for some Indian languages (Singh et al., 2006; Shrivastava and Bhattacharyya, 2008; Alfter, 2014). Details of the system are in section 3.2. 3.1 Data To develop and evaluate a system, we rely on both native language and learner data. The corpus of learner data was collected from L1 English students of Hungarian, divided into three levels of proficiency (Beginner, Intermediate, Advanced) as determined by course placement in one of three two-semester sequences. The corpus consists of journal entries, each a minimum of ten sentences in length on a topic selected by the student. The data and error annotation are described in Dickinson and Le"
W16-0523,W05-1106,0,0.0503864,"Missing"
W16-0523,tron-etal-2006-morphdb,0,0.0843732,"Missing"
W16-0523,R13-1099,0,0.0419079,"nts to determine the overall effectiveness. The test sets, as mentioned in section 3, are new excerpts from the target (1000 tokens) and learner corpora (1032 tokens). The analyzer produces as output one or more morphological tags for each word in the input. During evaluation, this list of output tags is compared to a list of gold standard tags. For native data, the gold tag list from the target language corpus contains the correct contextspecific tag as well as a list of possible tags based on morphology. The annotation of the target language corpus uses corrected output from the magyarlanc (Zsibrita et al., 2013) tool. For learner data, we annotate a single gold standard tag for each word. We evaluate how many tags from the analyzer match possible tags from the gold standard and whether the correct context-specific tag appears in the output. Specifically, we report: • Precision (P): the number of tags in the output that appear in the gold standard, divided by the number of tags in the output. • Recall (R): the number of tags in the output that appear in the gold standard, divided by the number of tags in the gold standard. • Accuracy (A): the number of correctly identified context-specific tags divide"
W16-2020,D09-1070,0,0.0662346,"Missing"
W16-2020,N09-1024,0,0.0246942,"t in practice. 1 Introduction It is well-known that Semitic languages pose problems for the unsupervised learning of morphology (ULM). For example, Hebrew morphology exhibits both agglutinative and fusional processes, in addition to non-concatenative root-and-pattern morphology. This diversity in types of morphological processes presents unique challenges not only for unsupervised morphological learning, but for morphological theory in general. Many previous ULM approaches either handle the concatenative parts of the morpholgy (e.g., Goldsmith, 2001; Creutz and Lagus, 2007; Moon et al., 2009; Poon et al., 2009) or, less often, the non-concatenative parts (e.g., Botha and Blunsom, 2013; Elghamry, 2005). We present an approach to clustering morphologically related words that addresses both concatenative and non-concatenative morphology via the same learning mechanism, namely the Multiple Cause Mixture Model (MCMM) (Saund, 1993, 1994). This type of learning has direct connections to autosegmental theories of morphology 1 We follow the transliteration scheme of the Hebrew Treebank (Sima’an et al., 2001). 131 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phono"
W16-2020,P98-1012,0,0.116875,"Hebrew alphabet, this amounts to 22 × s × 2 positional features. A precedence feature indicates, for two characters a and b, whether a precedes b within a certain distance (or number of characters). This distance is the system parameter δ. We define δ as the difference between the indices of the characters a and b. For example, if δ = 1, then characters a and b are adjacent. The number of precedence features is the length of the alphabet squared (222 = 484). 4.3 puravg (U, V ) = 1 X maxn |um ∩ vn | (4) M M m∈M Given this bias, we incorporate other metrics: BCubed precision and BCubed recall (Bagga and Baldwin, 1998) compare the cluster mappings of x with those of y, for every pair of data points x and y. These metrics are well-suited to cases of overlapping clusters (Artiles and Verdejo, 2009). Suppose x and y share m clusters and n categories. BCubed precision measures the extent to which m ≤ n. It is 1 as long there are not more clusters than gold-standard categories. BCubed Recall measures the extent to which m ≥ n. See Artiles and Verdejo (2009) for calculation details. 4.4 Results With a cut-off point at K = 100 clusters, we ran the MCMM at different valuations of s and δ. The results are given in t"
W16-2020,D13-1034,0,0.100242,"pose problems for the unsupervised learning of morphology (ULM). For example, Hebrew morphology exhibits both agglutinative and fusional processes, in addition to non-concatenative root-and-pattern morphology. This diversity in types of morphological processes presents unique challenges not only for unsupervised morphological learning, but for morphological theory in general. Many previous ULM approaches either handle the concatenative parts of the morpholgy (e.g., Goldsmith, 2001; Creutz and Lagus, 2007; Moon et al., 2009; Poon et al., 2009) or, less often, the non-concatenative parts (e.g., Botha and Blunsom, 2013; Elghamry, 2005). We present an approach to clustering morphologically related words that addresses both concatenative and non-concatenative morphology via the same learning mechanism, namely the Multiple Cause Mixture Model (MCMM) (Saund, 1993, 1994). This type of learning has direct connections to autosegmental theories of morphology 1 We follow the transliteration scheme of the Hebrew Treebank (Sima’an et al., 2001). 131 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 131–140, c Berlin, Germany, August 11, 2016. 20"
W16-2020,J08-3005,0,0.0802465,"Missing"
W16-2020,W13-2603,0,0.06021,"Missing"
W16-2020,J01-2001,0,0.209205,"on Hebrew data show that this theoretical soundness bears out in practice. 1 Introduction It is well-known that Semitic languages pose problems for the unsupervised learning of morphology (ULM). For example, Hebrew morphology exhibits both agglutinative and fusional processes, in addition to non-concatenative root-and-pattern morphology. This diversity in types of morphological processes presents unique challenges not only for unsupervised morphological learning, but for morphological theory in general. Many previous ULM approaches either handle the concatenative parts of the morpholgy (e.g., Goldsmith, 2001; Creutz and Lagus, 2007; Moon et al., 2009; Poon et al., 2009) or, less often, the non-concatenative parts (e.g., Botha and Blunsom, 2013; Elghamry, 2005). We present an approach to clustering morphologically related words that addresses both concatenative and non-concatenative morphology via the same learning mechanism, namely the Multiple Cause Mixture Model (MCMM) (Saund, 1993, 1994). This type of learning has direct connections to autosegmental theories of morphology 1 We follow the transliteration scheme of the Hebrew Treebank (Sima’an et al., 2001). 131 Proceedings of the 14th Annual SI"
W16-2020,J11-2002,0,0.0214973,"us Dickinson Indiana University md7@indiana.edu Abstract (McCarthy, 1981), and at the same time raises questions about the meaning of morphological units (cf. Aronoff, 1994). Consider the Hebrew verbs zwkr1 (‘he remembers’) and mzkir (‘he reminds’), which share the root z.k.r. In neither form does this root appear as a continuous string. Moreover, each form interrupts the root in a different way. Many ULM algorithms ignore non-concatenative processes, assuming word formation to be a linear process, or handle the non-concatenative processes separately from the concatenative ones (see survey in Hammarstrom and Borin, 2011). By separating the units of morphological structure from the surface string of phonemes (or characters), however, the distinction between non-concatenative and concatenative morphological processes vanishes. We apply the Multiple Cause Mixture Model (MCMM) (Saund, 1993, 1994), a type of autoencoder that serves as a disjunctive clustering algorithm, to the problem of morphological learning. An MCMM is composed of a layer of hidden nodes and a layer of surface nodes. Like other generative models, it assumes that some subset of hidden nodes is responsible for generating each instance of observed"
W16-2020,C96-2112,0,0.168187,"inctions in form (see section 4). 2 morpheme tiers segmental tier µ2 g,d,l µ3 i C V C C V C (a) Multi-linear approach single tier Morphology and MCMMs ma|gd|i|l (b) Linear approach In this section, we will examine autosegemental (or multi-linear) morphology (McCarthy, 1981), to isolate the property that allows it to handle nonconcatenative morphology. We will then show that because an MCMM has this same property, it is an appropriate computational model for learning non-concatenative morphology. First, we note some previous work connecting autosegmental morphology to computation. For example, Kiraz (1996) provides a framework for autosegmental morphology within two-level morphology, using hand-written grammars. By contrast, Fullwood and O’Donnell (2013) provide a learning algorithm in the spirit of autosegmental morphology. They sample templates, roots, and residues from Pitmor-Yor processes, where a residue consists of a word’s non-root phonemes, and a template specifies word length and the wordinternal positions of root phonemes. Botha and Blunsom (2013) use mildly context-free grammars with crossing branches to generate words with discontiguous morphemes. The present work, in contrast, assu"
W18-0510,J08-4004,0,0.219487,"Missing"
W18-0510,W11-2401,0,0.0198519,"and NNS groups, and the potential use of NS responses to evaluate NNS responses. There is a long-standing notion that systems processing learner data would be wise to constrain the data in some way (e.g., Heift and Schulze, 2007; Somasundaran et al., 2015), but we do not know how much constraint is needed—or whether we sacrifice the possibility of observing particular learner behavior for the sake of a constraint— without knowing more about the ways in which variation happens (cf. Bailey and Meurers, 2008). The corpus presented here bears some similarities to other task-based learner corpora. Meurers et al. (2011) examined German learner responses to short-answer reading comprehension questions. A target answer was produced by an expert, and annotators used this target to label the meaning of responses as correct or incorrect, along with a more detailed set of labels related to form, meaning, and task appropriateness. In our own previous work (King and Dickinson, 2016, 2013), we annotated a small set of PDT responses as correct or incorrect, with incorrect responses further labeled as errors of form or meaning. Somasundaran and Chodorow (2014) presented work on PDT reGiven that all users of a language"
W18-0510,W08-0913,0,0.0436449,"bility, we are interested in how variable linguistic behavior is for the same content, both within and between NS and NNS groups, and the potential use of NS responses to evaluate NNS responses. There is a long-standing notion that systems processing learner data would be wise to constrain the data in some way (e.g., Heift and Schulze, 2007; Somasundaran et al., 2015), but we do not know how much constraint is needed—or whether we sacrifice the possibility of observing particular learner behavior for the sake of a constraint— without knowing more about the ways in which variation happens (cf. Bailey and Meurers, 2008). The corpus presented here bears some similarities to other task-based learner corpora. Meurers et al. (2011) examined German learner responses to short-answer reading comprehension questions. A target answer was produced by an expert, and annotators used this target to label the meaning of responses as correct or incorrect, along with a more detailed set of labels related to form, meaning, and task appropriateness. In our own previous work (King and Dickinson, 2016, 2013), we annotated a small set of PDT responses as correct or incorrect, with incorrect responses further labeled as errors of"
W18-0510,W14-6106,0,0.0147513,"ated features, which we hope will be useful for mapping to holistic scores. For example, a response may present the main content of an item correctly but add imaginary details, while another may address background information not asked about in the prompt (see section 3). The acceptability of a response is thus taken as a function of several interacting features, most of which relate the text to the known semantic content. Relating to known content is distinct from typical grammatical error correction (GEC) (Leacock et al., 2014) and from more linguistically driven work such as parsing (e.g., Cahill et al., 2014; Ragheb and Dickinson, 2014), but providing the dimensions of acceptability and elucidating how they are applied provides insight for any enterprise desiring to connect learner text with semantic content, in addition to unpacking the sources of variation and of difficulty in processing a range of learner data. In section 2 we outline the picture description task (PDT) we use, designed with items that elicit specific types of linguistic behavior. Section 3 outlines the annotation, tackling the five-dimensional scheme; inter-anntotator agreement results are in section 4. While agreement seems r"
W18-0510,W14-1801,0,0.0198093,"nted here bears some similarities to other task-based learner corpora. Meurers et al. (2011) examined German learner responses to short-answer reading comprehension questions. A target answer was produced by an expert, and annotators used this target to label the meaning of responses as correct or incorrect, along with a more detailed set of labels related to form, meaning, and task appropriateness. In our own previous work (King and Dickinson, 2016, 2013), we annotated a small set of PDT responses as correct or incorrect, with incorrect responses further labeled as errors of form or meaning. Somasundaran and Chodorow (2014) presented work on PDT reGiven that all users of a language can be creative in their language usage, the overarching goal of this work is to investigate issues of variability and acceptability in written text, for both non-native speakers (NNSs) and native speakers (NSs). We control for meaning by collecting a dataset of picture description task (PDT) responses from a number of NSs and NNSs, and we define and annotate a handful of features pertaining to form and meaning, to capture the multi-dimensional ways in which responses can vary and can be acceptable. By examining the decisions made in"
W18-0510,W15-0605,0,0.0862865,"s—our intended use. By examining the decisions made in this corpus development, we highlight the questions facing anyone working with learner language properties such as variability, acceptability and native-likeness. Given the form-meaning aspect of variability, we are interested in how variable linguistic behavior is for the same content, both within and between NS and NNS groups, and the potential use of NS responses to evaluate NNS responses. There is a long-standing notion that systems processing learner data would be wise to constrain the data in some way (e.g., Heift and Schulze, 2007; Somasundaran et al., 2015), but we do not know how much constraint is needed—or whether we sacrifice the possibility of observing particular learner behavior for the sake of a constraint— without knowing more about the ways in which variation happens (cf. Bailey and Meurers, 2008). The corpus presented here bears some similarities to other task-based learner corpora. Meurers et al. (2011) examined German learner responses to short-answer reading comprehension questions. A target answer was produced by an expert, and annotators used this target to label the meaning of responses as correct or incorrect, along with a more"
W18-0510,W08-1205,0,0.085774,"Missing"
W18-0510,W13-1702,1,0.892934,"Missing"
W18-0510,C08-1109,0,0.105973,"Missing"
W18-0510,W16-0512,1,0.801179,"g particular learner behavior for the sake of a constraint— without knowing more about the ways in which variation happens (cf. Bailey and Meurers, 2008). The corpus presented here bears some similarities to other task-based learner corpora. Meurers et al. (2011) examined German learner responses to short-answer reading comprehension questions. A target answer was produced by an expert, and annotators used this target to label the meaning of responses as correct or incorrect, along with a more detailed set of labels related to form, meaning, and task appropriateness. In our own previous work (King and Dickinson, 2016, 2013), we annotated a small set of PDT responses as correct or incorrect, with incorrect responses further labeled as errors of form or meaning. Somasundaran and Chodorow (2014) presented work on PDT reGiven that all users of a language can be creative in their language usage, the overarching goal of this work is to investigate issues of variability and acceptability in written text, for both non-native speakers (NNSs) and native speakers (NSs). We control for meaning by collecting a dataset of picture description task (PDT) responses from a number of NSs and NNSs, and we define and annotate"
