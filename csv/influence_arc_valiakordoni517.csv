2013.mtsummit-wmwumttt.9,W03-1809,0,0.0874026,"Missing"
2013.mtsummit-wmwumttt.9,D07-1091,0,0.0390818,"ata was preprocessed with TreeTagger (Schmid, 1994), which provides part-of-speech tag and lemma information for each word. Similar annotations were automatically produced for the Bulgarian data with the help of the BTB-LPP tagger (Savkov et al., 2012). This is a necessary preliminary step for both the PV identification module and for translation. The PV identification system detects PVs in running text using lexicon look-up. Therefore in order for all occurrences to be detected it needs to operate on the lemma, instead of word level. The translation step employs a factored translation model (Koehn and Hoang, 2007), a suitable choice for this language pair and translation direction due to the rich morphology of Bulgarian. 2 http://www.setimes.com http://opus.lingfil.uu.se/ 4 http://www.bultreebank.org/EMP/ 5 http://webclark.org/ 3 65 for a transitive separable phrasal verb are a noun phrase appearing between the verb and particle, or a noun phrase following the verb and particle. The grammar is thus able to mark cases like (b) as unsafe (in this case due to missing direct object). preprocessing POS Tagging + Lemmatisation POS Tagging + Lemmatisation (TreeTagger) (TreeTagger) pv detection PV Candidate Id"
2013.mtsummit-wmwumttt.9,P07-2045,0,0.0114839,"Missing"
2013.mtsummit-wmwumttt.9,N10-1029,0,0.633841,"s, following the evaluations in (Kordoni et al., 2012). The development set was used for refining the constraint grammar for PV candidate filtering. A phrase-based translation system was built with the following tools and settings: the Moses open source toolkit (Koehn et al., 2007) was used to build a factored translation model. The parallel data was aligned with the help of GIZA++ (Och and Ney, 2003). Two 5-gram language models were built with the SRI Language Modeling http://en.wiktionary.org/wiki/Category:English phrasal verbs http://www.phrasalverbdemon.com/ 66 8 terminology adopted from (Carpuat and Diab, 2010). The dynamic strategy is slightly altered to use binary features. Toolkit (SRILM9 ) (Stolcke, 2002) on the preprocessed monolingual data from the Bulgarian National Reference Corpus to model word and partof-speech tag n-gram information. This choice of translation model is motivated by data sparsity issues due to the rich morphology of Bulgarian. When translating between a language with poor morphology and a highly inflected language, traditional translation models which use only word information often produce poor results because inflected forms of the same word are treated as separate token"
2013.mtsummit-wmwumttt.9,W11-0818,0,0.158928,"t (Fellbaum, 1998), the COMLEX Syntax dictionary (Macleod et al., 1998), and the gold standard data used for the experiments in (McCarthy et al., 2003) and (Baldwin, 2008). Most of these resources contain additional linguistic information about each PV, such as whether it is transitive or intransitive, separable or inseparable. This information was extracted together with the PVs where available and used to tackle the problem of ambiguous PP-attachments in the PV detection step. PV candidates are detected in the source data with the help of the library for multiword expression detection jMWE (Kulkarni and Finlayson, 2011; Finlayson and Kulkarni, 2011). An additional module is employed as a post-processing step to filter out the spurious PV candidates. It is implemented in the form of a constraint grammar (Karlsson et al., 1995), and makes use of shallow parsing techniques, as well as the additional linguistic information extracted about the entries in the lexicon. The main idea behind the filtering mechanism is to define a number of positive contexts in which valid PV candidates would occur within a sentence. For example, valid contexts 6 7 data set test development tune train number of sentences 800 100 2000"
2013.mtsummit-wmwumttt.9,P03-1065,0,0.0838482,"Missing"
2013.mtsummit-wmwumttt.9,J09-1005,0,0.0293699,"Missing"
2013.mtsummit-wmwumttt.9,W03-1810,0,0.162236,"addition to serve as basis for comparison between these techniques. Figure 1: Pipeline of the experiment including phrasal verb detection and integration into the English part of the parallel corpora. The PV detection step makes use of a lexicon of phrasal verbs, which was constructed from a number of resources. These include the English Phrasal Verbs section of Wiktionary6 , the Phrasal Verb Demon7 dictionary, the CELEX Lexical Database (Baayen et al., 1995), WordNet (Fellbaum, 1998), the COMLEX Syntax dictionary (Macleod et al., 1998), and the gold standard data used for the experiments in (McCarthy et al., 2003) and (Baldwin, 2008). Most of these resources contain additional linguistic information about each PV, such as whether it is transitive or intransitive, separable or inseparable. This information was extracted together with the PVs where available and used to tackle the problem of ambiguous PP-attachments in the PV detection step. PV candidates are detected in the source data with the help of the library for multiword expression detection jMWE (Kulkarni and Finlayson, 2011; Finlayson and Kulkarni, 2011). An additional module is employed as a post-processing step to filter out the spurious PV c"
2013.mtsummit-wmwumttt.9,W11-0805,0,0.0277057,"Syntax dictionary (Macleod et al., 1998), and the gold standard data used for the experiments in (McCarthy et al., 2003) and (Baldwin, 2008). Most of these resources contain additional linguistic information about each PV, such as whether it is transitive or intransitive, separable or inseparable. This information was extracted together with the PVs where available and used to tackle the problem of ambiguous PP-attachments in the PV detection step. PV candidates are detected in the source data with the help of the library for multiword expression detection jMWE (Kulkarni and Finlayson, 2011; Finlayson and Kulkarni, 2011). An additional module is employed as a post-processing step to filter out the spurious PV candidates. It is implemented in the form of a constraint grammar (Karlsson et al., 1995), and makes use of shallow parsing techniques, as well as the additional linguistic information extracted about the entries in the lexicon. The main idea behind the filtering mechanism is to define a number of positive contexts in which valid PV candidates would occur within a sentence. For example, valid contexts 6 7 data set test development tune train number of sentences 800 100 2000 the remaining (≈151K) Table 1:"
2013.mtsummit-wmwumttt.9,J03-1002,0,0.00716072,"Missing"
2013.mtsummit-wmwumttt.9,E09-3008,0,0.036986,"Missing"
2013.mtsummit-wmwumttt.9,W10-3707,0,0.0339858,"Missing"
2013.mtsummit-wmwumttt.9,C10-2144,0,0.0250066,"Missing"
2013.mtsummit-wmwumttt.9,P02-1040,0,0.0884316,"Missing"
2013.mtsummit-wmwumttt.9,D11-1077,0,0.0236152,"Missing"
2013.mtsummit-wmwumttt.9,pearce-2002-comparative,0,0.0417483,"Missing"
2013.mtsummit-wmwumttt.9,D07-1110,1,0.874019,"Missing"
2013.mtsummit-wmwumttt.9,W08-2107,0,0.0422611,"Missing"
2013.mtsummit-wmwumttt.9,W09-2907,0,0.2861,"Missing"
2013.mtsummit-wmwumttt.9,W06-1206,1,0.870136,"Missing"
ben-gera-etal-2010-semantic,copestake-flickinger-2000-open,0,\N,Missing
ben-gera-etal-2010-semantic,A00-2018,0,\N,Missing
ben-gera-etal-2010-semantic,J93-2004,0,\N,Missing
ben-gera-etal-2010-semantic,P97-1003,0,\N,Missing
ben-gera-etal-2010-semantic,W07-2207,1,\N,Missing
ben-gera-etal-2010-semantic,C02-2025,0,\N,Missing
ben-gera-etal-2010-semantic,J05-1003,0,\N,Missing
ben-gera-etal-2010-semantic,J01-2004,0,\N,Missing
ben-gera-etal-2010-semantic,P08-1067,0,\N,Missing
ben-gera-etal-2010-semantic,P02-1035,0,\N,Missing
ben-gera-etal-2010-semantic,P95-1037,0,\N,Missing
ben-gera-etal-2010-semantic,I05-1015,0,\N,Missing
ben-gera-etal-2010-semantic,W07-1204,0,\N,Missing
C10-2166,W03-2401,0,0.0682874,"Missing"
C10-2166,W09-3032,1,0.901765,"Missing"
C10-2166,J93-2004,0,0.0351072,"Missing"
C10-2166,C02-2025,0,0.254126,"Missing"
C10-2166,W97-1502,0,\N,Missing
C10-2166,J96-2004,0,\N,Missing
D07-1110,W02-2001,1,0.844682,"Missing"
D07-1110,baldwin-etal-2004-road,0,0.0343885,"Missing"
D07-1110,A97-1052,0,0.0539433,"Missing"
D07-1110,C02-2025,0,0.0124496,"wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model predicted for each occurrence of the head words a most plausible lexical type in that context. Only those predictions that occurred 5 times or more were taken into consideration for the generation of the new lexical entries. As a result, we obtained 21 new lexical entries. These new lexical entries were later merged into the ERG lexicon. To evaluate the grammar performance with and without these new lexical entries, we 1. parsed the sub-corpus with/without new lexical entries and compared the grammar coverage; 2. inspected the parse"
D07-1110,pearce-2002-comparative,0,0.705957,"ring Aline Villavicencio♣♠ , Valia Kordoni♦ , Yi Zhang♦ , Marco Idiart♥ and Carlos Ramisch♣ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♠ Department of Computer Sciences, Bath University (UK) ♦ Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br Abstract Another difficulty for work on MWE identification is that of the evaluation of the results obtained (Pearce, 2002; Evert and Krenn, 2005), starting from the lack of consensus about a precise definition for MWEs (Villavicencio et al., 2005). This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), χ2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiat"
D07-1110,zhang-kordoni-2006-automated,1,0.854157,"candidates are used in 2 The combination of the “word with space” approach of Zhang et al. (2006) with the constructional approach we propose here is an interesting topic that we want to investigate in future research. 1040 this experiment. We used simple heuristics in order to extract the head words from these MWEs: • the n-grams are POS-tagged with an automatic tagger; • finite verbs in the n-grams are extracted as head words; • nouns are also extracted if there is no verb in the n-gram. Occasionally, the tagger errors might introduce wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model p"
D07-1110,W06-1206,1,0.248015,"se corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted. 1 Introduction The task of automatically identifying Multiword Expressions (MWEs) like phrasal verbs (break down) and compound nouns (coffee machine) using statistical measures has been the focus of considerable investigative effort, (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Given the heterogeneousness of the different phenomena that are considered to be MWEs, there is no consensus about which method is best suited for which type of MWE, and if there is a single method that can be successfully used for any kind of MWE. In this paper we investigate some of the issues involved in the evaluation of automatically extracted MWEs, from their extraction to their subsequent use in an NLP task. In order to do that, we present a discussion of different statistical measures, and the influence that the size and quality of different data sources have. We then perform a comp"
D07-1110,P04-1057,0,\N,Missing
D07-1110,W06-1208,0,\N,Missing
D14-1024,W11-0818,0,0.077048,"Missing"
D14-1024,2005.mtsummit-posters.11,0,0.335816,"Missing"
D14-1024,W03-1810,0,0.153301,"Missing"
D14-1024,J03-1002,0,0.0037542,"adin.cholakov,kordonieva}@anglistik.hu-berlin.de Abstract training data. Various versions of this strategy are applied in Lambert and Banchs (2005), Carpuat and Diab (2010), and Simova and Kordoni (2013). In all cases there is some improvement in translation quality, caused mainly by the better treatment of separable PVs, such as in turn the light on. Another strategy, which is referred to as dynamic, is to modify directly the SMT system. Ren et al. (2009), for example, treat bilingual MWEs pairs as parallel sentences which are then added to training data and subsequently aligned with GIZA++ (Och and Ney, 2003). Other approaches perform feature mining and modify directly the automatically extracted translation table. Ren et al. (2009) and Simova and Kordoni (2013) employ Moses1 to build and train phrase-based SMT systems and then, in addition to the standard phrasal translational probabilities, they add a binary feature which indicates whether an MWE is present in a given source phrase or not. Carpuat and Diab (2010) employ the same approach but the additional feature indicates the number of MWEs in each phrase. All studies report improvements over a baseline system with no MWE knowledge but these i"
D14-1024,P02-1040,0,0.0922186,".02 5.92 6.01 5.73 5.76 5.54 5.74 all nist baseline static dynamic-1 dynamic-4 6.14 6.18 6.02 6.16 good acceptable incorrect 0.21 0.25 0.24 0.3 0.41 0.5 0.51 0.5 0.38 0.25 0.25 0.2 Table 2: Automatic evaluation of translation quality. Table 3: Manual evaluation of translation quality. 5 which such verbs were correctly identified during the identification step. The human subject takes into account the target PV and a limited context around it and judges the translation as: Results and Discussion Automatic Evaluation. Table 2 presents the results from the automatic evaluation, in terms of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores, of 4 system setups. The baseline has no MWE knowledge, while the static and the dynamic-1 system setups are reproduced from the experiments described in Simova and Kordoni (2013). Dynamic-1 includes only a single binary feature which indicates the presence of a PV while our method, dynamic-4, includes the 4 features described in Table 1. Our method outperforms all other setups in terms of BLEU score, thus proving our point that adding features describing the linguistic properties of PVs improves SMT even further. Also, the results for the 400 sentences with"
D14-1024,W09-2907,0,0.0889704,"tistical Machine Translation through Linguistic Treatment of Phrasal Verbs Kostadin Cholakov and Valia Kordoni Humboldt-Universit¨at zu Berlin, Germany {kostadin.cholakov,kordonieva}@anglistik.hu-berlin.de Abstract training data. Various versions of this strategy are applied in Lambert and Banchs (2005), Carpuat and Diab (2010), and Simova and Kordoni (2013). In all cases there is some improvement in translation quality, caused mainly by the better treatment of separable PVs, such as in turn the light on. Another strategy, which is referred to as dynamic, is to modify directly the SMT system. Ren et al. (2009), for example, treat bilingual MWEs pairs as parallel sentences which are then added to training data and subsequently aligned with GIZA++ (Och and Ney, 2003). Other approaches perform feature mining and modify directly the automatically extracted translation table. Ren et al. (2009) and Simova and Kordoni (2013) employ Moses1 to build and train phrase-based SMT systems and then, in addition to the standard phrasal translational probabilities, they add a binary feature which indicates whether an MWE is present in a given source phrase or not. Carpuat and Diab (2010) employ the same approach bu"
D14-1024,2013.mtsummit-wmwumttt.9,1,0.957795,"Carpuat and Diab (2010), and Simova and Kordoni (2013). In all cases there is some improvement in translation quality, caused mainly by the better treatment of separable PVs, such as in turn the light on. Another strategy, which is referred to as dynamic, is to modify directly the SMT system. Ren et al. (2009), for example, treat bilingual MWEs pairs as parallel sentences which are then added to training data and subsequently aligned with GIZA++ (Och and Ney, 2003). Other approaches perform feature mining and modify directly the automatically extracted translation table. Ren et al. (2009) and Simova and Kordoni (2013) employ Moses1 to build and train phrase-based SMT systems and then, in addition to the standard phrasal translational probabilities, they add a binary feature which indicates whether an MWE is present in a given source phrase or not. Carpuat and Diab (2010) employ the same approach but the additional feature indicates the number of MWEs in each phrase. All studies report improvements over a baseline system with no MWE knowledge but these improvements are comparable to those achieved by static methods. In this article, we further improve the dynamic strategy by adding features which, unlike al"
D14-1024,N10-1029,0,\N,Missing
E14-1032,H91-1067,0,0.450806,"n Section 2, Section 3 describes our subcategorisation acquisition system, and Section 4 the SCF lexicon that we build using it. In Sections 5 and 6 we evaluate the SCF lexicon on a verb classification task and discuss our results; Section 7 then concludes with directions for future work. 298 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298–307, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Previous work To date, research on SCF acquisition from corpora has mostly targeted English. Brent and Berwick (1991) detect five SCFs by looking for attested contexts where argument slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming"
E14-1032,A97-1052,0,0.207316,"SCF lexicon on a verb classification task and discuss our results; Section 7 then concludes with directions for future work. 298 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298–307, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Previous work To date, research on SCF acquisition from corpora has mostly targeted English. Brent and Berwick (1991) detect five SCFs by looking for attested contexts where argument slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture"
E14-1032,burchardt-etal-2006-salsa,0,0.0411388,"ate a large subcategorisation frame lexicon for German verbs. Our SCF lexicon resource is available at http://amor.cms.hu-berlin. de/˜robertsw/scflex.html. We are performing a manual evaluation of the output of our system, which we will report soon. We plan to continue this work first by expanding our SCF lexicon with case information and selectional preferences, second by using our SCF classifier and lexicon for verbal Multiword Expression identification in German, and last by comparing it to existing verb classifications, either by using available resources for German like the SALSA corpus (Burchardt et al., 2006), or by translating parts of VerbNet into German to create a more extensive gold standard for verb clustering in the spirit of Sun et al. (2010) who found that Levin’s verb classification can be translated to French and still usefully allow generalisation over verb classes. Finally, we plan to perform in vivo evaluation of our SCF lexicon, to determine what benefit it can deliver for NLP applications such as Semantic Role Labelling and Word Sense Disambiguation. Recent research has found that even automatically-acquired verb classifications can be useful for NLP applications (Shutova et al., 2"
E14-1032,P06-1055,0,0.0556071,"er PP complements). Consequently, the main verb empfehlen ‘recommend’ in (8), which has a subject, a dative object, a PP, and an infinitival clausal complement, is assigned the SCF ndi. Another challenging task which relies on edge label information is filtering out clausal adjuncts (relative clauses and parentheticals) so as not to include them in SCFs. The SCF tagger The SCF tagger begins by collecting complements co-occurring with a verb instance using the phrase structure of the sentence. In our system, we obtain phrase structure information for unannotated text using the Berkeley Parser (Petrov et al., 2006), a statistical unlexicalised parser trained on TIGER. Fig. 1 illustrates the phrase structure analysis and edge labels in the TIGER corpus for (6): The 17 rules of the SCF tagger are simple; most of them categorise the complements of a specific verb instance; e.g., if a nominal complement to the verb is edge-labelled as a nominative subject, add n to the verb’s SCF, unless the verb is in the passive, in which case add a to the SCF. Our system was optimised by progressively refining the SCF tagger’s rules through manual error analysis on sentences from TIGER. The result is an automatic SCF tag"
E14-1032,C00-2100,0,0.0543266,"is a counterexample to Manning’s (1993) expectation that freedom of word order should be matched by an increase in case and/or agreement marking. This is due to a very high degree of syncretism (identity of word forms) in German paradigms for nouns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model co"
E14-1032,P02-1029,0,0.029736,"c subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automatically induce a set of semantic verb classes with an unsupervised clustering algorithm. This clustering was evaluated against a small manually created semantic verb classification. Schulte im Walde (2006) continues this work using a larger manual verb classification. The SCFs used in this study are defined at three levels of granularity. The first level (38 different SCFs) lists only the complements in the frame; the second one adds head and case information for PP complements (183 SCFs). The third level examined the effect of"
E14-1032,W06-1601,0,0.0243155,"ilds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called"
E14-1032,D11-1025,0,0.202188,"Missing"
E14-1032,N10-1137,0,0.0162221,"frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called Argument Linking (Levin, 1993; Levin and Rappaport Hovav, 2005). Acquiring SCFs for languages with a very fixed word order like English needs only a simple syntactic analysis, which mainly relies on the predetermined"
E14-1032,P99-1004,0,0.110234,"initialise the centroids by random partitions (each of the n objects is randomly assigned to one of k clusters, and the centroids are then computed as the means of these random partitions). Because the random initial centroids influence the final clustering, we repeat the clustering a number of times. We also initialise the k-means cluster centroids using agglomerative hierarchical clustering, a deterministic iterative bottom-up process. Hierarchical clustering initially assigns verbs to singleton clusters; the two clusters which are “nearest” to Schulte im Walde (2006) takes α = 0.9 although Lee (1999) recommends α = 0.99 or higher values in her original description of skew divergence. 6 Evaluation This section presents the results of evaluating the unsupervised verb clustering based on our SCF lexica against the gold standard described in Sec. 5.1. (9) irad(p, q) = D(pk 4 each other are then joined together, and this process is repeated until the desired number of clusters is obtained. Hierarchical clustering is performed to group the verbs into k clusters; the centroids of these clusters are then used to initialise the kmeans algorithm. While there exist several variants of hierarchical c"
E14-1032,schulte-im-walde-2002-subcategorisation,0,0.691537,"cusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automa"
E14-1032,J06-2001,0,0.438309,"ords of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automatically induce a set of semantic verb classes with an unsupervised clustering algorithm. This clustering was evaluated against a small manually created semantic verb classification. Schulte im Walde (2006) continues this work using a larger manual verb classification. The SCFs used in this study are defined at three levels of granularity. The first level (38 different SCFs) lists only the complements in the frame; the second one adds head and case information for PP complements (183 SCFs). The third level examined the effect of adding selectional preferences, but results were inconclusive. A recent paper (Scheible et al., 2013) describes a system similar to ours, built on a statistical dependency parser, and using some of the same kinds of rules as we describe in Section 3.1; this system is eva"
E14-1032,C10-1113,0,0.174849,"Missing"
E14-1032,A97-1014,0,0.0708646,"pronoun, and x for expletive es (‘it’) subject. Clausal complements can be infinite (i); finite ones can have the verb in second position (S-2) or include the complementiser dass ‘that’ (S-dass). Complements can be combined as in na (transitive verb); for PPs in SCFs, the head is specified, e.g., p:f¨ ur 1 for PP complements headed by f¨ur ‘for’ . Due to the free word order, simple phrase structure like that used for analysis of English is not enough to specify the syntax of German sentences. Therefore we use the annotation scheme in the manually constructed German treebanks NEGRA and TIGER (Skut et al., 1997; Brants et al., 2002), which decorate parse trees with edge labels specifying the syntactic roles of constituents. We automatically annotate the parse trees from our statistical parser using a simple machine learning model. In the next section, we illustrate the operation of the SCF tagger with reference to examples; then in Section 3.2 we describe our edge labeller. The rule-based SCF tagger handles auxiliary and modal verb constructions, passive alternations, separable verb prefixes, and raising and control constructions. E.g., the subject sie ‘they’ of anfangen ‘begin’ in (7) doubles as th"
E14-1032,C10-1119,0,0.0620243,"Missing"
E14-1032,E12-1003,0,0.0143976,"n (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called Argument Linking (Levin, 1993; Levin and Rappaport Hovav, 2005). Acquiring SCFs for languages with a very fixed word order like English needs only a simple syntactic analysis, which mainly relies on the predetermined sequencing of arguments in th"
E14-1032,P93-1032,0,\N,Missing
I11-1086,J99-2004,0,0.0686756,"rd Prediction Methods Kostadin Cholakov† , Gertjan van Noord† , Valia Kordoni‡ , Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during par"
I11-1086,P98-1014,0,0.0567531,"ncreases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the"
I11-1086,W00-0740,0,0.0124041,"of having lower accuracy than the baseline. The application of the adapted C& V N method, on the other hand, increases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details"
I11-1086,P08-1070,1,0.93808,"en, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a"
I11-1086,W06-1620,0,0.0154973,"s applied with the GG and compares the results to the results reported previously for the suppertagging methods for this corpus. Section 7 explores the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they sha"
I11-1086,A00-1031,0,0.0596308,"Missing"
I11-1086,I05-1015,0,0.0592628,"Missing"
I11-1086,R09-1012,1,0.894147,"Missing"
I11-1086,C10-2018,1,0.799003,"Missing"
I11-1086,W08-1708,1,0.695363,"e actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the effect the method has on the"
I11-1086,C04-1041,0,0.034145,"Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and"
I11-1086,copestake-flickinger-2000-open,0,0.0436578,"the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where va"
I11-1086,E03-1041,0,0.0230605,"mpared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the parsing coverage a"
I11-1086,J03-3001,0,0.0370393,"Missing"
I11-1086,W02-2018,0,0.0211703,"of search hits Yahoo! returns for each form in a given paradigm is combined with some simple heuristics to disambiguate the output of the morphology and to determine the correct paradigm(s). We also apply heuristics to guess the gender for words with generated noun paradigms and to determine if a word which is assigned a verb paradigm starts with a separable particle. (1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary features from the context and < Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 2 shows the features for Aufgabe. Since the stem of the unknown word is added to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved to be less informative for LA. One could argue that there is a simpler approach for mapping the various forms of the unknown word to its stem. For instance, the TreeTagger provides both POS and stem information with high accuracy. However, the generation of the paradigms allows us to consider contexts in which ot"
I11-1086,nicholson-etal-2008-evaluating,1,0.870798,"entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the ef"
I11-1086,W02-1210,0,0.0115253,"es in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunker"
I11-1086,2006.jeptalnrecital-invite.2,1,0.833811,"Missing"
I11-1086,zhang-kordoni-2006-automated,1,0.955308,"isation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create"
I11-1086,N10-1090,0,0.0140834,"Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a particular input. Th"
I11-1086,W05-1008,0,\N,Missing
I11-1086,C98-1014,0,\N,Missing
J09-2001,J07-4002,0,0.0149345,"used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Mannin"
J09-2001,P98-1013,0,0.0554213,"Missing"
J09-2001,baldwin-etal-2004-road,1,0.714541,"eposition PVs allow limited coordination of PP objects (e.g., refer to the book and to the DVD), and the NP object of the selected preposition can be passivized (e.g., the book they referred to). Even subtler are the syntactic effects observed for determinerless PPs. The singular noun in the PP–D is often strictly countable (e.g., off screen, on break), resulting in syntactic markedness as, without a determiner, the noun does not constitute a saturated NP. This in turn dictates the need for a dedicated analysis in a linguistically motivated grammar in order to be able to avoid parse failures (Baldwin et al. 2004; van der Beek 2005). Additionally, there is considerable variation in the internal modiﬁability of determinerless PPs, with some not permitting any internal modiﬁcation (e.g., of course) and others allowing optional internal modiﬁcation (e.g., at considerable length). There are also, however, cases of obligatory internal modiﬁcation (e.g., at considerable/great expense vs. *at expense) and highly restricted internal modiﬁcation (e.g., at long last vs. *at great/short last). Balancing up these different possibilities in terms of over- and undergeneration in a grammar is far from trivial (Baldw"
J09-2001,W02-2001,1,0.68867,"Missing"
J09-2001,J04-4004,0,0.00886265,"ation. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Manning, and Ng 2004; Olteanu and Moldovan 2005) generally improves parser accuracy. In addition, they developed a variant of the RRR data set (RRR-sent) which contains full sentential contexts of possible PP attachment ambiguity. Others who have successfully built PP re-attachment models for speciﬁc parsers are Olteanu (2004) and Foth and Menzel (2006). Agirre, Baldwin, ¨ and Martinez"
J09-2001,W07-1604,0,0.0628822,"Missing"
J09-2001,copestake-etal-2002-multiword,1,0.504905,"Missing"
J09-2001,J01-1005,0,0.00769076,"for text classiﬁcation, and demonstrated that dependency tuples incorporating prepositions are a more effective document representation than simple words. In a direct challenge to the prevalent “stop word” perception of prepositions in information retrieval, Hansen (2005) and Lassen (2006) placed emphasis on not only prepositions but preposition semantics in a music retrieval system and ontology-based text search system, respectively. Information extraction is one application where prepositions are uncontroversially crucial to system accuracy, in terms of the role they play in named entities (Cucchiarelli and Velardi 2001; Toral 2005; Kozareva 2006) and in IE patterns, in linking the elements in a text (Appelt et al. 1993; Muslea 1999; Ono et al. 2001; Leroy and Chen 2002). Benamara (2005) used preposition semantics in a cooperative question answering system. In the context of cross-language question answering (CLQA), Hartrumpf, Helbig, and Osswald (2006) used MultiNet to interpret the semantics of German prepositions, and demonstrated that in instances where the answer passage contained a different preposition to that included in the original question, preposition semantics boosted the performance of their CL"
J09-2001,W07-1607,0,0.0580473,"Missing"
J09-2001,J09-1005,0,0.0399341,"Missing"
J09-2001,W07-1603,0,0.0220521,"1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed the aspectual and path properties of goal-marking postpositions in Japanese, and proposed an analysis based on predicate and event restrictions. Boonthum, Toida, and Levinstein (2005, 2006) deﬁned a generalpurpose sense inventory of seven prepositions (but purportedly applicable to all"
J09-2001,P06-2029,0,0.0121475,"repositional MWE types that cause the greatest syntactic problems in English, in terms of their relative frequency and tendency for syntactic variation, are: 1. verb-particle constructions (VPCs), where the verb selects for an intransitive preposition (e.g., chicken out or hand in: Deh´e et al. [2002]); 2. prepositional verbs (PVs), where the verb selects for a transitive preposition (e.g., rely on or refer to: Huddleston and Pullum [2002]); 3. determinerless PPs (PP–Ds), where a PP is made up of a preposition and singular noun without a determiner (e.g., at school, off screen: Baldwin et al. [2006]). All three MWE types undergo limited syntactic variation (Sag et al. 2002). For example, transitive verb particle constructions generally undergo the particle alternation, 125 Computational Linguistics Volume 35, Number 2 whereby the particle may occur either adjacent to the verb (e.g., tear up the letter), or be separated from the verb by the NP complement (e.g., tear the letter up). Some VPCs readily occur with both orders (like tear up), while others have a strong preference for a particular order (e.g., take off —under the interpretation of having the day off—tends to occur in the partic"
J09-2001,I08-1059,0,0.00951014,"Missing"
J09-2001,W99-0614,0,0.022322,"uple is eatv , pizzan1 , with p , chopsticksn2 . The high degree of interest in PP attachment stems from it being a common phenomenon when parsing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazi"
J09-2001,W06-2105,0,0.127742,"Missing"
J09-2001,W07-1608,0,0.045722,"Missing"
J09-2001,W07-1601,0,0.0955897,"tual reality, in the context of interpreting the spatial information of prepositions. For example, Xu and Badler (2000) developed a geometric deﬁnition of the motion trajectories of prepositions, whereas Tokunaga, Koyama, and Saito (2005) use potential functions to estimate the spatial extent of Japanese spatial nouns (which combine with postpositions to have a similar syntactic and semantic proﬁle to English spatial prepositions). Kelleher and van Genabith (2003) proposed a method for interpreting in front of and behind in a virtual reality environment based on different frames of reference. Hying (2007) carried out an analysis of preposition semantics in the HRCR Map Task corpus, and used it to evaluate two models of projective prepositions. Kelleher and Kruijff (2005) developed a model for grounding spatial expressions in visual perception and also for modeling proximity, and Reichelt and Verleih (2005) developed the B3D system for generating a computational representation of prepositions in geospatial applications. Furlan, Baldwin, and Klippel (2007) used preposition occurrence in Web data as a means of classifying landmarks for use in route directions. Finally, Kelleher and Costello (2009"
J09-2001,isahara-etal-2008-development,0,0.0127084,"tions in MT, and automatic error correction of preposition usage in non-native speaker text. Following the lead of Saint-Dizier (2006b), Jørgensen and Lønning (2009), and others, we also hope to see more crosslinguistic and typological research on the lexical semantics of prepositions. Although there has been a steady proliferation of WordNets for different languages, linked variously to English WordNet (e.g., EuroWordNet for several European languages [Vossen 1998], BALKANET for Balkan languages [Stamou et al. 2002], HowNet for Chinese [Dong and Dong 2006], and Japanese WordNet for Japanese [Isahara et al. 2008]), they have tended to follow the lead of English WordNet and focus exclusively on content words. Given the increasing maturity of resources such as the Preposition Project and PrepNet, the time seems right to develop preposition sense inventories for more languages, linked back to English. On the basis of currently available resources and future efforts such as these, we believe there will be a steady lowering of the barrier to including a more systematic handling of prepositions in NLP applications. 137 Computational Linguistics Volume 35, Number 2 The purpose of this article has been to hi"
J09-2001,W04-2604,0,0.162756,"Missing"
J09-2001,P03-1054,0,0.0059965,"en a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in"
J09-2001,W06-2102,0,0.0282577,"Missing"
J09-2001,W02-0802,0,0.135776,"S representation for the directional sense of up (as in up the stairs) is: (4) (toward Loc (nil 2) (UP Loc (nil 2) (∗ Thing 6))) where the numbers indicate the logical arguments of the predicates. This representation indicates that the logical subject of the PP (indexed by “2”; e.g., the piano in move the piano up the stairs) is relocated up in the direction of the logical argument (indexed by “6”; e.g., the stairs in our example), which is in turn a concrete thing. The LCS Lexicon was developed from a theoretical point of view and isn’t directly tied to corpus usage. The Preposition Project (Litkowski 2002; Litkowski and Hargraves 2005, 2006) is an attempt to develop a comprehensive semantic database for English prepositions, intended for NLP applications. The project took the New Oxford Dictionary of English (Pearsall 1998) as its source of preposition sense deﬁnitions, which it then ﬁne-tuned based on cross-comparison with both functionally tagged prepositions in FrameNet (Baker, Fillmore, and Lowe 1998) and the account of preposition semantics in a descriptive grammar of English (Quirk et al. 1985); it also draws partially on Dorr’s LCS deﬁnitions of prepositions. Importantly, the Prepositio"
J09-2001,W06-2106,0,0.216037,"Missing"
J09-2001,S07-1005,0,0.462645,"a decision tree classiﬁer based on a set of contextual features similar to those used in WSD systems. O’Hara and Wiebe (2009) is an updated version of this original research, using a broader range of resources. Ye and Baldwin (2006) also built on the earlier research, in attempting to enhance the accuracy of semantic role labeling with dedicated PP disambiguation. 131 Computational Linguistics Volume 35, Number 2 They demonstrated the potential for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, an"
J09-2001,U05-1008,1,0.776051,"ll work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in Polish (Tseng 2004), and the"
J09-2001,J93-2004,0,0.0342879,"Missing"
J09-2001,W03-1810,0,0.0601999,"Missing"
J09-2001,E03-1079,0,0.011156,"relative to a baseline parser. As part of this effort, they developed a standardized data set for exploration of the interaction between lexical semantics and parsing/PP attachment accuracy. 3 Because it was automatically extracted, the RRR data set is notoriously noisy. For instance, Pantel and Lin (2000) observed that 133 tuples contain the as either n1 or n2 . 124 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications One signiﬁcant variation on the classic binary PP attachment task which attempts to generate a richer semantic characterisation of the PP is the work of Merlo (2003) and Merlo and Esteve Ferrer (2006), who included classiﬁcation of the PP as an argument or adjunct, making for a four-way classiﬁcation task. In this context, they found that PP attachment resolution for argument PPs is considerably easier than is the case for adjunct PPs. Returning to our observation that PP attachment can occur in multiple syntactic conﬁgurations, Merlo, Crocker, and Berthouzoz (1997) applied backed-off estimation to the problem of multiple PP attachment, in the form of 14 discrete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the bas"
J09-2001,W97-0317,0,0.0285464,"Missing"
J09-2001,J06-3002,0,0.0586485,"Missing"
J09-2001,odijk-2004-reusable,0,0.0197692,"Missing"
J09-2001,W03-0411,0,0.483873,"Missing"
J09-2001,J09-2002,0,0.213341,"Missing"
J09-2001,H05-1035,0,0.0667112,"P parser (Briscoe, Carroll, and Watson 2006) is highly effective at VPC identiﬁcation, and (b) that the incorporation of lexicalized models of selectional preferences can lead to modest improvements in parser accuracy. In terms of English PV extraction, Baldwin (2005b) proposed a method based on a combination of statistical measures and linguistic diagnostics, and demonstrated that the combination of statistics with linguistic diagnostics achieved the best extraction performance. Research on prepositional MWEs in languages other than English includes Krenn and Evert (2001) and Evert and Krenn (2005) on the extraction of German PP–verb collocations (which are similar to verbal idioms/verb–noun combinations in English [Fazly, Cook, and Stevenson 2009]) based on a range of lexical association measures. Pecina (2008) further extended this work using a much broader set of lexical association ¨ measures and classiﬁer combination. Looking at German, Domges et al. (2007) analyzed the productivity of PP–Ds headed by unter, and used their results to motivate a syntactic analysis of the phenomenon. For Dutch, van der Beek (2005) worked on the extraction of PP–Ds from the output of a parser, once ag"
J09-2001,P00-1014,0,0.0776453,"), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the"
J09-2001,S07-1040,0,0.127872,"Missing"
J09-2001,W02-0312,0,0.0166734,"Missing"
J09-2001,P05-1034,0,0.00722033,"Missing"
J09-2001,H94-1048,0,0.238432,"Missing"
J09-2001,W06-2109,0,0.0511325,"Missing"
J09-2001,W93-0307,0,0.114962,"f PP attachment (e.g., cases of n1 being a pronoun [high attachment], or the PP post-modifying n1 in subject position [low attachment]) to derive smoothed estimates of Prhigh ( p|v), Prhigh ( NULL|n) (i.e., the probability of n not being post-modiﬁed by a PP), and Prlow ( p|n), which then form the basis of Prhigh ( p|v, n) and Prlow ( p|v, n), respectively. The proposed method was signiﬁcant in demonstrating the effectiveness of simple co-occurrence probabilities, without explicit semantics or discourse processing, and also in its ability to operate without explicitly annotated training data. Resnik and Hearst (1993) observed that PP attachment preferences are also conditioned on the semantics of the noun object of the preposition in the PP, as can be seen in our earlier example of Kim eats pizza with chopsticks/anchovies where chopsticks leads to verb attachment and anchovies to noun attachment. Although they were unable to come up with a model which was empirically superior to existing methods which did not represent the semantics of the noun object, this paved the way for a new wave of research using the full 4-tuple of v, n1 , p, n2 . 123 Computational Linguistics Volume 35, Number 2 The ﬁrst to suc"
J09-2001,E95-1040,0,0.0174849,", classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed"
J09-2001,saint-dizier-2006-prepnet,0,0.389946,"s an attempt to develop a compositional account of preposition semantics which interfaces with the semantics of the predicate (e.g., verb or predicative noun). Similarly to the English LCS Lexicon, it uses LCS as the descriptive language, in conjunction with typed λ-calculus and underspeciﬁed representations. Noteworthy elements of PrepNet are that it attempts to capture selectional constraints, metaphorical sense extension, and complex arguments. PrepNet was originally developed over French prepositions, but has since been applied to the analysis of instrumentals across a range of languages (Saint-Dizier 2006b). VerbNet (Kipper, Dang, and Palmer 2000; Kipper Schuler 2005) contains a shallow hierarchy of 50 spatial prepositions, classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyde"
J09-2001,I08-2106,0,0.148755,"Missing"
J09-2001,W04-2118,0,0.060867,"Missing"
J09-2001,W06-3312,0,0.0198173,"ete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the basic V NP PP case, due to increased ambiguity and data sparseness. Mitchell (2004) similarly performed an extensive analysis of the Penn Treebank to investigate the different contexts PP attachment ambiguities occur in, and the relative ability of different PP attachment methods to disambiguate each. There have also been domain-speciﬁc methods proposed for PP attachment, for example, in the area of biomedicine (Hahn, Romacker, and Schulz 2002; Pustejovsky et al. 2002; Leroy, Chen, and Martinez 2003; Schuman and Bergler 2006). 2.2 The Syntax of Prepositional Multiword Expressions Prepositions are also often found as part of multiword expressions (MWEs), such as verb-particle constructions (break down), prepositional verbs (rely on), determinerless PPs (in hospital), complex prepositions (by means of ) and compound nominals (affairs of state). MWEs are lexical items which are composed of more than one word and are lexically, syntactically, semantically, pragmatically, and/or statistically idiosyncratic in some way (Sag et al. 2002). In this section, we present a brief overview of the syntax of the key prepositional"
J09-2001,2003.mtsummit-papers.44,0,0.0189971,"Missing"
J09-2001,P93-1042,0,0.0400489,"exposure in the NLP literature but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finn"
J09-2001,P98-2201,0,0.0394995,"Missing"
J09-2001,J87-1007,0,0.395324,"Missing"
J09-2001,A00-1034,0,0.133018,"Missing"
J09-2001,W97-0109,0,0.0185413,"elemans, and Veenstra 1997; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in"
J09-2001,W08-1205,0,0.0228208,"Missing"
J09-2001,C08-1109,0,0.0128719,"Missing"
J09-2001,W00-1308,0,0.0182022,"ure but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short a"
J09-2001,N07-1007,0,0.059034,"Missing"
J09-2001,W06-2103,0,0.0494386,"Missing"
J09-2001,E03-1051,0,0.0846401,"(Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the ava"
J09-2001,C02-1004,0,0.0113591,"(Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of"
J09-2001,W06-2112,0,0.0587411,"sing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazier 1979; Schutze 1995). For example, Minimal Attachment was the strategy of choosing the attachment site which “minimizes” the parse tree, a"
J09-2001,P90-1004,0,0.024064,"this would be unable to disambiguate between Examples (2) and (3) as they both contain the same number of nodes. Late Attachment, on the other hand, was the strategy of attaching “low” in the parse tree, corresponding to Example (2). Ford, Bresnan, and Kaplan (1982) proposed an alternative heuristic strategy, based on the existence of p in a subcategorization frame for v. In later research, Pereira (1985) described a method for incorporating Right Association and Minimal Attachment 122 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications into a shift-reduce parser, and Whittemore and Ferrara (1990) developed a rule-based algorithm to combine various attachment preferences on the basis of empirical evaluation of the predictive power of each. Syntactic preferences were of course a blunt instrument in dealing with PP attachment, and largely ineffectual in predicting the difference in PP attachment between Kim eats pizza with chopsticks (verb attachment) and Kim eats pizza with anchovies (noun attachment), for example. This led to a shift away from syntactic methods in the 1980s towards AI-inspired techniques which used world knowledge to resolve PP attachment ambiguity. In the case of Exam"
J09-2001,S07-1051,1,0.50376,"tial for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decisi"
J09-2001,P98-2234,0,0.0307679,"97; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and d"
J09-2001,S07-1044,0,0.00886363,"position labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decision trees to d"
J09-2001,W97-1016,0,0.0754138,"Missing"
J09-2001,W99-0606,0,\N,Missing
J09-2001,J82-3004,0,\N,Missing
J09-2001,J87-3005,0,\N,Missing
J09-2001,J93-1005,0,\N,Missing
J09-2001,W99-0628,0,\N,Missing
J09-2001,E03-1044,1,\N,Missing
J09-2001,W06-2110,1,\N,Missing
J09-2001,W07-1605,0,\N,Missing
J09-2001,W06-1207,0,\N,Missing
J09-2001,W01-0707,0,\N,Missing
J09-2001,W06-2107,0,\N,Missing
J09-2001,W04-2608,0,\N,Missing
J09-2001,W06-2113,0,\N,Missing
J09-2001,A97-1052,0,\N,Missing
J09-2001,W04-0403,0,\N,Missing
J09-2001,J09-2003,0,\N,Missing
J09-2001,P08-1037,1,\N,Missing
J09-2001,calzolari-etal-2002-towards,0,\N,Missing
J09-2001,C98-1013,0,\N,Missing
J09-2001,C98-2229,0,\N,Missing
J09-2001,P03-2026,0,\N,Missing
J09-2001,P93-1032,0,\N,Missing
J09-2001,C98-2196,0,\N,Missing
J09-2001,W03-1812,1,\N,Missing
J09-2001,J10-4006,0,\N,Missing
J09-2001,P06-4020,0,\N,Missing
J09-2001,J09-2004,0,\N,Missing
J09-2001,J09-2005,0,\N,Missing
J09-2001,E06-3004,0,\N,Missing
J09-2001,P03-1065,0,\N,Missing
J09-2001,W07-1602,1,\N,Missing
J09-2001,W02-0804,0,\N,Missing
J09-2001,P98-2127,0,\N,Missing
J09-2001,C98-2122,0,\N,Missing
J09-2001,P07-1072,0,\N,Missing
J09-2001,W06-2104,0,\N,Missing
jakob-etal-2010-mapping,copestake-flickinger-2000-open,0,\N,Missing
jakob-etal-2010-mapping,W07-1210,0,\N,Missing
jakob-etal-2010-mapping,W07-1207,0,\N,Missing
jakob-etal-2010-mapping,W08-0325,0,\N,Missing
konstantopoulos-etal-2012-task,C10-2166,1,\N,Missing
konstantopoulos-etal-2012-task,M98-1004,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1012,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1014,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1021,0,\N,Missing
konstantopoulos-etal-2012-task,callmeier-etal-2004-deepthought,0,\N,Missing
konstantopoulos-etal-2012-task,I05-1033,0,\N,Missing
konstantopoulos-etal-2012-task,fragkou-etal-2008-boemie,1,\N,Missing
konstantopoulos-etal-2012-task,C10-1066,0,\N,Missing
konstantopoulos-etal-2012-task,C94-1070,0,\N,Missing
konstantopoulos-etal-2012-task,W09-1101,0,\N,Missing
konstantopoulos-etal-2012-task,W07-2207,0,\N,Missing
konstantopoulos-etal-2012-task,P05-1053,0,\N,Missing
konstantopoulos-etal-2012-task,P03-1054,0,\N,Missing
konstantopoulos-etal-2012-task,H05-1045,0,\N,Missing
kordoni-neu-2004-creating,C02-2025,0,\N,Missing
kordoni-neu-2004-creating,W02-1210,0,\N,Missing
kordoni-neu-2004-creating,W02-1502,0,\N,Missing
kordoni-neu-2004-creating,P01-1019,0,\N,Missing
kordoni-simova-2014-multiword,W03-1810,0,\N,Missing
kordoni-simova-2014-multiword,P02-1040,0,\N,Missing
kordoni-simova-2014-multiword,D07-1091,0,\N,Missing
kordoni-simova-2014-multiword,P07-2045,0,\N,Missing
kordoni-simova-2014-multiword,N10-1029,0,\N,Missing
kordoni-simova-2014-multiword,J03-1002,0,\N,Missing
kordoni-simova-2014-multiword,W11-0818,0,\N,Missing
kordoni-zhang-2010-disambiguating,A00-2021,0,\N,Missing
kordoni-zhang-2010-disambiguating,J93-2004,0,\N,Missing
kordoni-zhang-2010-disambiguating,W07-2201,0,\N,Missing
kordoni-zhang-2010-disambiguating,C02-2025,0,\N,Missing
kordoni-zhang-2010-disambiguating,P08-1039,0,\N,Missing
kordoni-zhang-2010-disambiguating,P07-1031,0,\N,Missing
L16-1003,W09-1904,0,0.0383851,"in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation and ranking process the"
L16-1003,2015.mtsummit-papers.19,1,0.892142,"i et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-g"
L16-1003,2005.mtsummit-papers.11,0,0.0298517,"onsist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are ta"
L16-1003,P02-1040,0,0.0956189,"EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (millio"
L16-1003,P11-1138,0,0.0602764,"Missing"
L16-1003,steinberger-etal-2006-jrc,0,0.0272335,"experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz da"
L16-1003,tiedemann-2012-parallel,0,0.0663706,"known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz data that are far from straightforward to"
L16-1003,abdelali-etal-2014-amara,0,0.284209,"Missing"
L16-1003,D09-1030,0,0.252764,"Missing"
L16-1003,2012.eamt-1.60,0,0.0756545,"king field. The targeted crowds consist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality a"
L16-1003,N12-1047,0,0.0685374,"Missing"
L16-1003,N13-1001,0,0.0129109,"TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 20"
L16-1003,W10-0713,0,0.0292656,", and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation an"
L16-1003,D08-1089,0,0.0114402,"ments, slides, and other course materials. The forum data of TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the hi"
L16-1003,N12-1006,0,0.0606257,"Missing"
L16-1003,ambati-etal-2010-active,0,\N,Missing
L18-1073,abdelali-etal-2014-amara,0,0.0313475,"has the main benefit of access to people speaking the different languages. 3. Data Selection Our aim was to collect wikification annotations for 500 to 1,000 sentences from parallel educational texts for each language pair. We used three existing parallel text resources as the basis for the sentence selection so as to cover a broad range of online courses and to cover all eleven language pairs. In particular, we use parallel texts from course material of the Coursera MOOC platform, the Iversity MOOC platform, and the QCRI Educational Domain (QED) Corpus (formerly known as QCRI AMARA Corpus) (Abdelali et al., 2014). The Iversity data consists of (i) manually translated MOOC data, and (ii) MT output of English MOOC data produced by the first MT software prototype that was developed in the first year of the TraMOOC project. Both Coursera and QED material consist of MOOC subtitles that were translated using crowdsourcing in other projects unrelated to TraMOOC. The QED corpus consists of a large collection of files and each file contains a number of subtitles from MOOC video lectures in a particular language. The aligned files (parallel corpus) share the same first part of the file name. However, not all fi"
L18-1073,S16-1081,0,0.0258532,"We use the data set for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit"
L18-1073,S17-2001,0,0.03374,"for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evalua"
L18-1073,W08-0336,0,0.024088,"of speech (such as “mmm”, [NOISE], or [MUSIC]). Also, particularly long paragraphs were not selected, so as to maintain the microtasking nature of the activity. The selected source and target sentences were automatically tokenized using the multilingual tokenizer Ucto1 (van Gompel et al., 2017). Ucto has language-specific rules for the tokenization of several languages including Dutch, English, German, Italian, Portuguese, and Russian. For the other languages, generic language-independent settings of Ucto were used except for the Chinese language where we applied the Stanford Word Segmenter (Chang et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016"
L18-1073,D13-1184,0,0.0441826,"Missing"
L18-1073,N16-1104,0,0.0316436,"ng et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task"
L18-1073,S15-2049,0,0.0331012,"e also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data set is intended as both tuning material for the developed implicit machine translation system evaluation tool and for testing the final machine translation systems. The data set also gives us insights into the coverage of"
L18-1073,P11-1138,0,0.042925,"2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data"
L18-1073,sabou-etal-2014-corpus,0,0.0251481,"ared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task design, task completion time, quality control, task monitoring and crowd evaluation. Given pairs of aligned texts, the entity annotation (wikification) task consists of identifying and annotating"
L18-1073,N16-1072,0,0.0225831,"an be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those"
L18-1075,W10-0710,0,0.0342275,"lume aimed at, the quality assurance of the experimental process, and the related crowdsourcing workflow issues. 2. Related Work The creation of parallel data on a large scale for new language-pairs requires intensive human effort and availability of experts. For most language-pairs, the small number of expert translators available or the lack of access to fluent bilingual speakers makes it difﬁcult and expensive to create parallel corpora for training machine translation systems. Recent research has looked at obtaining translations via crowdsourcing, in particular for low resource languages (Ambati & Vogel, 2010; ; Zaidan & Callison-Burch, 2011; Post, Callison-Burch, Osborne, 2012). Crowdsourcing as an approach to activate or use the knowledge and skills of a large group of people in order to solve problems has existed for a long time (cf. Ellis, 2014). Nowadays, it leverages Web 2.0 tools (O&apos;Reilly, 2007) in order to take a job normally performed by a designated person and having it done by a large, undefined, and dispersed number of participants (Howe, 2008). In the area of translation, crowdsourcing has actually been used widely in the past years for the translation of online content. As Jiménez-C"
L18-1075,L18-1528,1,0.863099,"Missing"
L18-1075,L16-1003,1,0.435367,"Missing"
L18-1075,W10-0734,0,0.0769409,"Missing"
L18-1075,W12-3152,0,0.042812,"Missing"
L18-1075,P11-1122,0,0.473351,"Missing"
L18-1528,abdelali-etal-2014-amara,0,0.150329,"Missing"
L18-1528,D09-1030,0,0.139864,"t properties. Achieving high-quality machine translation in these conditions therefore requires significant amounts of in-domain data for training and testing. Creating such data by hiring professional translators would be expensive, especially considering that it would have to be done for eleven target languages. Therefore we turn our attention to crowdsourcing as a cost-saving alternative. The impact of non-expert input, i.e. translations provided by non-professional translators, on the development and the evaluation of machine translation engines has been investigated in previous research (Callison-Burch, 2009; Zaidan and Callison-Burch, 2011; Ambati, 2012). Crowdsourcing has been used to this end, and the main research concern has been whether input by the general crowd, that has no expertise in linguistics or translation studies, can improve the quality of large-scale machine In this work we use crowdsourcing with carefully designed quality controls to collect translations of MOOC material from English to the eleven project target languages. We combine this data with pre-existing indomain educational data and we use it to build translation systems in a transfer learning approach: we first train n"
L18-1528,2012.eamt-1.60,0,0.16079,"Missing"
L18-1528,2005.mtsummit-papers.11,0,0.12737,"Missing"
L18-1528,W17-3204,0,0.035658,"hat we have access to varying amounts of pre-existing data that we treat as “in-domain”, with varying distance to our actual target domain. Despite all confounding variables, if we consider that the amount of crowdsourced training data is 100-1,000 times smaller than the amount of out-of-domain training data used, and consistently smaller than the amount of preexisting in-domain data, we conclude that the improvements that we observe from adding crowdsourced data cannot just be attributed to having more training data available. Based on the log-linear learning curves reported in related work (Koehn and Knowles, 2017; Miceli Barone et al. 2017), we would expect small or negligible improvements in translation quality if we added the same amount of out-of-domain training data, or pre-existing indomain data, to the systems without crowdsourced data. This confirms the relevance of obtaining in-domain training data that is similar in terms of domain and genre 3346 to the texts that are to be translated, and that the crowdsourced training data is of high value to the MT system. 4. Conclusion We collected crowdsourced translations from English to eleven languages to create a parallel corpus for the educational d"
L18-1528,L16-1003,1,0.854754,"Missing"
L18-1528,D17-1156,1,0.895442,"ystems Our baseline translation systems are GRU attentive sequence-to-sequence neural machine translation models (Bahdanau, 2015). For training, we used the same configuration as the Edinburgh’s submission to the WMT-17 news translation task (Sennrich et al., 2017), which provides a strong baseline. 6 3345 https://translate.yandex.ru/corpus Figure 1: Number of trusted segments collected for each target language for training and testing/tuning. We adapt the baseline systems to the in-domain MOOC data using continued training of the baseline system with MAP-L2 and dropout regularization (Miceli Barone et al. 2017). 3.4 Results Evaluation results are shown in Table 3. We can see that domain adaptation via fine-tuning is effective for all language pairs. Baseline + Preexisting + Crowdsourced en-bg 22.91 23.57 25.89 en-cs 29.86 31.06 32.06 en-de 29.29 32.14 33.69 en-el 35.54 38.01 40.76 en-hr 23.36 23.70 26.43 en-it 32.15 36.19 38.53 en-nl 35.59 38.04 40.07 en-pl 27.16 28.41 30.97 en-pt 39.44 47.68 48.71 en-ru 26.41 29.08 29.78 en-zh 27.93 28.51 29.77 avg 29.97 32.40 34.24 Table 3: Translation quality (BLEU) of baseline system, and systems adapted to domain with/without crowdsourced data. Using only pre-e"
L18-1528,L18-1075,1,0.862976,"Missing"
L18-1528,steinberger-etal-2012-dgt,0,0.0514095,"Missing"
L18-1528,steinberger-etal-2006-jrc,0,0.140295,"Missing"
L18-1528,tiedemann-2012-parallel,0,0.137194,"Missing"
L18-1528,P11-1122,0,0.394868,"Missing"
L18-1528,L16-1561,0,0.0380301,"Missing"
N10-1002,I05-1015,0,0.0145244,"000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carroll and Oepen (2005) and Zhang et al. (2007b) have proposed efficient k-best unpacking algorithms that can selectively extract the most probable readings from the packed parse forest according to a discriminative parse disambiguation model, by minimising the number of potential unifications. The algorithm can be applied to unpack any passive edges. Because of the dynamic programming used in the algorithm and the hierarchical structure of the edges, the cost of the unpacking routine is empirically linear in the number of desired readings, and O(1) when invoked more than once on the same edge. The other challenge c"
N10-1002,W09-2609,0,0.0339964,"Missing"
N10-1002,P99-1069,0,0.0532038,"ired readings, and O(1) when invoked more than once on the same edge. The other challenge concerns the selection of informative and representative pieces of knowledge from the massive sea of partial analyses in the parsing chart. How to effectively extract the indicative features for a specific language phenomenon is a very task-specific question, as we will show in the context of the VPC extraction task in Section 3.2. However, general strategies can be applied to generate parse ranking scores on each passive edge. The most widely used parse ranking model is the loglinear model (Abney, 1997; Johnson et al., 1999; Toutanova et al., 2002). When the model does not use non-local features, the accumulated score on a sub-tree under a certain (unpacked) passive edge can be used to approximate the probability of the partial analysis conditioned on the sub-string within that span.3 3.2 The Application: Acquiring Features for VPC Extraction As stated above, the target task we use to illustrate the capabilities of our chart mining method is VPC extraction. The grammar we apply our chart mining method to in this paper is the English Resource Grammar (ERG, Flickinger (2002)), a large-scale precision HPSG for Engl"
N10-1002,P99-1061,0,0.00941937,"merican Chapter of the ACL, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be"
N10-1002,W09-1410,1,0.834536,"tures mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with precision grammars less"
N10-1002,W02-2018,0,0.0128608,"candidate VPC v - le:4, v np le:3, v p le:1, v p-np le:2 LE:M AX C ONS LE:M AX CR ANK PARTICLE off Table 1: Chart mining features used for VPC extraction S3−subjh(.875) S1−subjh(.125) S2−subjh(.925) VP5−hcomp VP1−hadj VP2−hadj(.325) VP3−hcomp v_−_le v_np_le v_p_le VP4−hcomp PP−hcomp v_p−np_le PRTL PREP NP1 NP2 DUMMY−V shows the boy 0 off 2 3 his new toys 4 7 Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG category classification: non-VPC, transitive VPC, or intransitive VPC. For the parameter estimation of the ME model, we use the TADM open source toolkit (Malouf, 2002). The token-level predictions are then combined with a simple majority voting to derive the type-level prediction for the VPC candidate. In the case of a tie, the method backs off to the na¨ıve baseline model described in Section 4.2, which relies on the combined probability of the verb and particle forming a VPC. We have also experimented with other ways of deriving type-level predictions from token-level classification results. For instance, we trained a separate classifier that takes the token-level prediction as input in order to determine the type-level VPC predic14 tion. Our results indi"
N10-1002,A00-2022,0,0.0296662,"que. First, there is potentially a huge number of parsing edges in the chart. For instance, when parsing with a large precision grammar like the HPSG English Resource Grammar (ERG, Flickinger (2002)), it is not unusual for a 20-word sentence to receive over 10,000 passive edges. In order to achieve high efficiency in parsing (as well as generation), ambiguity packing is usually used to reduce the number of productive passive edges on the parsing chart (Tomita, 1985). For constraint-based grammar frameworks like LFG and HPSG, subsumption-based packing is used to achieve a higher packing ratio (Oepen and Carroll, 2000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carr"
N10-1002,2004.tmi-1.2,0,0.0187101,"Missing"
N10-1002,J93-1007,0,0.167573,"ston and Pullum, 2002; Baldwin and Kim, 2009); for the purposes of our dataset, we assume that all particles are prepositional—by far the most common and productive of the three types—and further restrict our attention to single-particle VPCs (i.e., we ignore VPCs such as get along together). 11 One aspect of VPCs that makes them a particularly challenging target for lexical acquisition is that the verb and particle can be non-contiguous (for instance, hand the paper in and battle right on). This sets them apart from conventional collocations and terminology (cf., Manning and Sch¨utze (1999), Smadja (1993) and McKeown and Radev (2000)) in that they cannot be captured effectively using ngrams, due to their variability in the number and type of words potentially interceding between the verb and the particle. Also, while conventional collocations generally take the form of compound nouns or adjective–noun combinations with relatively simple syntactic structure, VPCs occur with a range of valences. Furthermore, VPCs are highly productive in English and vary in use across domains, making them a prime target for lexical acquisition (Deh´e, 2002; Baldwin, 2005; Baldwin and Kim, 2009). In the VPC datas"
N10-1002,P04-1057,0,0.0524824,"Missing"
N10-1002,zhang-kordoni-2006-automated,1,0.860652,"Determine whether each verb–preposition combination is a VPC or not, and further predict its valence(s) (i.e. unknown if VPC, and unknown valence(s)) VPC Determine whether each verb–preposition combination is a VPC or not ignoring valence (i.e. unknown if VPC, and don’t care about valence) Table 2: Definitions of the three DLA tasks 2001). We use a slightly modified version of the ERG in our experiments, based on the nov-06 release. The modifications include 4 newly-added dummy lexical entries for the verb DUMMY- V and the corresponding inflectional rules, and a lexical type prediction model (Zhang and Kordoni, 2006) trained on the LOGON Treebank (Oepen et al., 2004) for unknown word handling. The parse disambiguation model we use is also trained on the LOGON Treebank. Since the parser has no access to any of the verbs under investigation (due to the DUMMYV substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model’s performance. The chart mining feature extraction process took over 10 CPU days, and collected a total of 44K events for 4,090 candidate VPC triples.4 5-fold cross validation is used to train/test the model. As stated above (Section 2), the VPC triples at"
N10-1002,zhang-kordoni-2008-robust,1,0.826589,"es over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with"
N10-1002,W07-1217,1,0.887544,"Missing"
N10-1002,W07-2207,1,0.950451,"L, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be extracted (see Section"
N10-1002,A00-2018,0,\N,Missing
N10-1002,J97-4005,0,\N,Missing
N10-1002,P03-1059,1,\N,Missing
nicholson-etal-2008-evaluating,copestake-flickinger-2000-open,0,\N,Missing
nicholson-etal-2008-evaluating,zhang-kordoni-2006-automated,1,\N,Missing
nicholson-etal-2008-evaluating,W07-1220,1,\N,Missing
nicholson-etal-2008-evaluating,W02-1210,0,\N,Missing
nicholson-etal-2008-evaluating,W06-1206,1,\N,Missing
nicholson-etal-2008-evaluating,P04-1057,0,\N,Missing
nicholson-etal-2008-evaluating,W05-1008,1,\N,Missing
nicholson-etal-2008-evaluating,baldwin-etal-2004-road,1,\N,Missing
P08-1070,baldwin-etal-2004-road,0,0.0487406,"Missing"
P08-1070,W05-1008,0,0.056017,"information can slow the parser down, causing efficiency problems. This paper describes experiments aimed at Background In all heavily lexicalised formalisms, such as LTAG, CCG, LFG and HPSG, the lexicon plays a key role in parsing. But a lexicon can never hope to contain all words in open domain text, and so lexical coverage is a central issue in boosting parser robustness. Some systems use heuristics based on numbers, capitalisation and perhaps morphology to guess the category of the unknown word (van Noord and Malouf, 2004), while others have focused on automatically expanding the lexicon (Baldwin, 2005; Hockenmaier et al., 2002; O’Donovan et al., 2005). Another method, described in Section 4, uses external resources such as part-of-speech (POS) tags to select generic lexical entries for out-of-vocabulary words. In all cases, we lose some of the depth of information the hand-crafted lexicon would provide, but an analysis is still produced, though possibly less than fully specified. The central position of these detailed lexicons causes problems, not only of robustness, but also of efficiency and ambiguity. Many words may have five, six or more lexicon entries associated with them, and this c"
P08-1070,C94-1024,0,0.0652043,"for the parser. Various means of filtering this search space have been attempted. Kiefer et al. (1999) describes a method of filtering lexical items by specifying and checking for required prefixes and particles 613 Proceedings of ACL-08: HLT, pages 613–621, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics which is particularly effective for German, but also applicable to English. Other research has looked at using dependencies to restrict the parsing process (Sagae et al., 2007), but the most well known filtering method is supertagging. Originally described by Bangalore and Joshi (1994) for use in LTAG parsing, it has also been used very successfully for CCG (Clark, 2002). Supertagging is the process of assigning probable ‘supertags’ to words before parsing to restrict parser ambiguity, where a supertag is a tag that includes more specific information than the typical POS tags. The supertags used in each formalism differ, being elementary trees in LTAG and CCG categories for CCG. Section 3.2 describes an experiment akin to supertagging for HPSG, where the supertags are HPSG lexical types. Unlike elementary trees and CCG categories, which are predominantly syntactic categorie"
P08-1070,P06-2006,0,0.013491,"xical item is not available. One method of doing this, built in to the PET parser, is to use POS tags to select generic lexical items, and hence allow a (less than fully specified) parse to be built. The six data sets used for these experiments were chosen to give a range of languages and genres. Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the treebanks released with the ERG; a00 which is taken from the BNC and consists of factsheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. The last two data sets are German text: clef700 consisting of German questions taken from the CLEF competition and eiche564 a sample of sentences taken from a treebank parsed with the German HPSG grammar, GG and consisting of transcribed German speech data concerning appointment scheduling from the Verbmobil project. Vital statistics of these data sets are described in Table 3. We used TreeTagger to POS tag the six data sets, with the tagger configured to assign multiple tags, where the probability of the less likely tags was at least half that of the most"
P08-1070,J07-4004,0,0.0185036,"5 70 + 65 0.2 +3 3 2 + 33 3 +2 ? Gold standard POS tags Lexical types (no POS model) Lexical types (with POS model) Unrestricted 0.7 0.6 0.5 0.4 0.3 Average time per sentence (seconds) Precision 95 2 2 90 85 + 80 75 0.2 + 2 + +3 33 3 3 2 3 + 2 ? 3 +2 ? Gold standard POS tags Lexical types (no POS model) Lexical types (with POS model) Unrestricted 0.3 0.4 0.5 0.6 0.7 Average time per sentence (seconds) Figure 1: Coverage and precision varying with time for the three restriction experiments. Gold standard and unrestricted results shown for comparison. While this experiment is similar to that of Clark and Curran (2007), it differs in that their supertagger assign categories to every word, while we look up every word in the lexicon and the tagger is used to filter what the lexicon returns, only if the tagger confidence is sufficiently high. As Table 2 shows, when we use the tags for which the tagger had a low confidence, we lose significant coverage. In order to run as a supertagger rather than a filter, the tagger would need to be much more accurate. While we can look at multi-tagging as an option, we believe much more training data would be needed to achieve a sufficient level of tag accuracy. Increasing e"
P08-1070,copestake-flickinger-2000-open,0,0.0869631,"nimal Recursion Semantics (MRS: Copestake et al. (2001)) as a platform to develop deep natural language processing tools, with a focus on multilinguality. The grammars are designed to be bidirectional (used for generation as well as parsing) and so contain very specific linguistic information. In this work, we focus on techniques to improve parsing, not generation, but, as all the methods involve pre-processing and do not change the grammar itself, we do not affect the generation capabilities of the grammars. We use two of the DELPHIN wide-coverage grammars: the English Resource Grammar (ERG: Copestake and Flickinger (2000)) and a German grammar, GG (M¨uller and Kasper, 2000; Crysmann, 2003). We also use the PET parser, and the [incr tsdb()] system profiler and treebanking tool (Oepen, 2001) for evaluation. 3 Parser Restriction An exhaustive parser, such as PET, by default produces every parse licensed by the grammar. However, in many application scenarios, this is unnecessary and time consuming. The benefits of us1 http://wiki.delph-in.net/ 614 ing a deep parser with a lexicalised grammar are the precision and depth of the analysis produced, but this depth comes from making many fine distinctions which greatly"
P08-1070,P01-1019,0,0.024421,"rmalism differ, being elementary trees in LTAG and CCG categories for CCG. Section 3.2 describes an experiment akin to supertagging for HPSG, where the supertags are HPSG lexical types. Unlike elementary trees and CCG categories, which are predominantly syntactic categories, the HPSG lexical types contain a lot of semantic information, as well as syntactic. In the case study we describe here, the tools, grammars and treebanks we use are taken from work carried out in the DELPH-IN1 collaboration. This research is based on using HPSG along with Minimal Recursion Semantics (MRS: Copestake et al. (2001)) as a platform to develop deep natural language processing tools, with a focus on multilinguality. The grammars are designed to be bidirectional (used for generation as well as parsing) and so contain very specific linguistic information. In this work, we focus on techniques to improve parsing, not generation, but, as all the methods involve pre-processing and do not change the grammar itself, we do not affect the generation capabilities of the grammars. We use two of the DELPHIN wide-coverage grammars: the English Resource Grammar (ERG: Copestake and Flickinger (2000)) and a German grammar,"
P08-1070,P99-1061,0,0.030022,"uch as part-of-speech (POS) tags to select generic lexical entries for out-of-vocabulary words. In all cases, we lose some of the depth of information the hand-crafted lexicon would provide, but an analysis is still produced, though possibly less than fully specified. The central position of these detailed lexicons causes problems, not only of robustness, but also of efficiency and ambiguity. Many words may have five, six or more lexicon entries associated with them, and this can lead to an enormous search space for the parser. Various means of filtering this search space have been attempted. Kiefer et al. (1999) describes a method of filtering lexical items by specifying and checking for required prefixes and particles 613 Proceedings of ACL-08: HLT, pages 613–621, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics which is particularly effective for German, but also applicable to English. Other research has looked at using dependencies to restrict the parsing process (Sagae et al., 2007), but the most well known filtering method is supertagging. Originally described by Bangalore and Joshi (1994) for use in LTAG parsing, it has also been used very successfully for CCG (C"
P08-1070,J05-3003,0,0.0353829,"Missing"
P08-1070,2004.tmi-1.2,0,0.0518849,"Missing"
P08-1070,P07-1079,0,0.0319078,"five, six or more lexicon entries associated with them, and this can lead to an enormous search space for the parser. Various means of filtering this search space have been attempted. Kiefer et al. (1999) describes a method of filtering lexical items by specifying and checking for required prefixes and particles 613 Proceedings of ACL-08: HLT, pages 613–621, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics which is particularly effective for German, but also applicable to English. Other research has looked at using dependencies to restrict the parsing process (Sagae et al., 2007), but the most well known filtering method is supertagging. Originally described by Bangalore and Joshi (1994) for use in LTAG parsing, it has also been used very successfully for CCG (Clark, 2002). Supertagging is the process of assigning probable ‘supertags’ to words before parsing to restrict parser ambiguity, where a supertag is a tag that includes more specific information than the typical POS tags. The supertags used in each formalism differ, being elementary trees in LTAG and CCG categories for CCG. Section 3.2 describes an experiment akin to supertagging for HPSG, where the supertags a"
P08-1070,zhang-kordoni-2006-automated,1,0.948366,"7s 0.63s 0.48s 0.37s 0.31s 0.21s 0.62s 0.48s 0.42s 0.33s 0.23s Table 2: Results obtained when restricting the parser lexicon according to the predicted lexical type, where words are restricted according to a threshold of tag probabilities. Two models, with and without POS tags as features, were used. While POS taggers such as TreeTagger are common, and there some supertaggers are available, notably that of Clark and Curran (2007) for CCG, no standard supertagger exists for HPSG. Consequently, we developed a Maximum Entropy model for supertagging using the OpenNLP implementation.2 Similarly to Zhang and Kordoni (2006), we took training data from the gold–standard lexical types in the treebank associated with ERG (in our case, the July-07 version). For each token, we extracted features in two ways. One used features only from the input string itself: four characters from the beginning and end of the target word token, and two words of context (where available) either side of the target. The second used the features from the first, along with POS tags given by TreeTagger for the context tokens. We held back the jh5 section of the treebank for testing the Maximum Entropy model. Again, the lexical items that w"
P08-2048,P98-1013,0,0.130398,"nformed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Generally speaking, such semantic representations should be capable of"
P08-2048,W05-0620,0,0.0602398,"Missing"
P08-2048,2005.mtsummit-papers.22,0,0.15407,"le relations with associated arguments (Copestake et al., 2006). In this paper, the MRS structures are created with the English Resource Grammar (ERG), a HPSG-based broad coverage precision grammar for English. The seman189 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 189–192, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tic predicates and their linguistic behaviour (including the set of semantic roles, indication of optional arguments, and their possible value constraints are specified by the grammar as its semantic interface (SEM-I) (Flickinger et al., 2005). 3 Relating MRS structures to lexical semantic resources 3.1 Feature extraction from linguistic resources The first set of features used to find corresponding lexical semantic roles for the MRS predicate arguments are taken from Robust MRS (RMRS) structures (Copestake, 2006). The general idea of the process is to traverse the bag of elementary predications looking for the verbs in the parsed sentence. When a verb is found, then its arguments are taken from the rarg tags and alternatively from the in-g conjunctions related to the verb. So, given the sentence: (1) Yields on money-market mutual"
P08-2048,W07-1204,0,0.0967429,"oduce errors in the inference phase. In addition, it is reasonable to use the VerbNet class information in the learning and inference phases, which in fact improves slightly the results. The outcomes also show that the use of the SEM algorithm improves accuracy slightly, meaning that the conditional dependency assumptions were reasonable, but still not perfect. The model can be slightly modified for verb class inference, by adding conditional dependencies between the VerbNet class and SEM-I features, which can potentially improve the parse disambiguation task, in a similar way of thinking to (Fujita et al., 2007). For instance, for the following sentence, we derive an incorrect mapping for the verb stay to the VerbNet class EXIST-47.1-1 with the (falsely) favored parse where the PP “in one place” is treated as an adjunct/modifier. For the correct reading where the PP is a complement to stay, the mapping to the correct VerbNet class LODGE -46 is derived, and the correct LOCATION role is identified for the PP. (3) Regardless of whether [T heme you] hike from lodge to lodge or stayLODGE -46 [Location in one place] and take day trips, there are plenty of choices. 5 Conclusions and Future Work In this pape"
P08-2048,J05-1004,0,0.105749,"shift of focus from pure syntactic analyses to more semantically informed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Gener"
P08-2048,N04-3012,0,0.0216275,"RMRS feature extraction was applied and a new verb dependency trees dataset was created. To obtain a correspondence between the SEM-I role labels and the PropBank (or VerbNet) role labels, a procedure which maps these labellings for 190 SEM-I roles Mapped roles Features ARG1 ARG2 Experiencer Theme manager n of propositional m rel Table 2: Alignment instance obtained for the verb expect Since the use of fine-grained features can make the learning process very complex, the WordNet semantic network (Fellbaum, 1998) was also employed to obtain generalisations of nouns. The algorithm described in (Pedersen et al., 2004) was used to disambiguate the sense, given the heads of the verb arguments and the verb itself (by using the mapping from VerbNet senses to WordNet verb senses (Kipper-Schuler, 2005)). Alternatively, a naive model has also been proposed, in which these features are simply generalized as nouns. For prepositions, the ontology provided by the SEM-I was used. Other words like adjectives or verbs in arguments were simply generalised as their corresponding type (e.g., adjectival rel or verbal rel). 3.2 Inference of semantic roles with Bayesian Networks The inference of semantic roles is based on tra"
P08-2048,C98-1013,0,\N,Missing
P10-5003,W09-3006,0,0.0251236,"Missing"
P10-5003,W09-3029,0,\N,Missing
P10-5003,N04-1015,0,\N,Missing
P10-5003,J08-1001,0,\N,Missing
P10-5003,prasad-etal-2008-penn,1,\N,Missing
P13-5005,N10-1089,0,\N,Missing
P13-5005,J13-2003,0,\N,Missing
P13-5005,D07-1110,1,\N,Missing
P13-5005,J13-1009,0,\N,Missing
P13-5005,D11-1060,0,\N,Missing
P18-5005,W15-4002,0,0.0378119,"Missing"
R11-1049,P98-1014,0,0.123822,"Missing"
R11-1049,I05-1015,0,0.0147384,"igate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the classifier. The focus of the experiment is on verbs because of the large number of possibl"
R11-1049,R09-1012,1,0.892635,"Missing"
R11-1049,C10-2018,1,0.7426,"Missing"
R11-1049,D10-1088,1,0.807892,"Missing"
R11-1049,W08-1708,1,0.874614,"Missing"
R11-1049,copestake-flickinger-2000-open,0,0.148867,"Missing"
R11-1049,W00-0740,0,0.0329213,"Missing"
R11-1049,E03-1041,0,0.0367437,"Missing"
R11-1049,P96-1027,0,0.0164094,"luation, extending the evaluation methodology of C& V N, we also investigate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the class"
R11-1049,J03-3001,0,0.0093745,"alization of morphological properties but they proved to be less informative for the classifier. Further, the paradigm generation method outputs a single paradigm for Abfahrten indicating that this word is a singular feminine noun. This information is explicitly used as a feature in the classifier which is shown in row (v) of Table 1. entries in the lexicon mapped onto it and it is assigned to at least 15 distinct words occurring in large corpora parsed with PET and the GG. The parsed corpus we use consists of roughly 2.5M sentences randomly selected from the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, etc. Following these criteria, we have selected 39 open-class types out of the 411 lexical types defined in the GG. As described in Section 2.2, we re-defined the type definitions of the 39 types which resulted in the creation of 68 expanded types. This number is smaller than the 611 types used in the experiments with Alpino because the GG does not have a full form lexicon. Table 2 gives more details"
R11-1049,W02-2018,0,0.00948395,"the word. All other morphological forms are derived by applying various morphological rules defined in the GG to the word stem. For this reason, we employ the paradigm not only as a source of features for the classifier but also as a way to map the unknown word to its stem. The stem for nouns is the singular nominative noun form, for adjectives it is the base nonin(1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary characteristics of the context and &lt; Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 1 shows the features for Abfahrten. Row (i) contains 4 separate features derived from the prefix of the word and 4 other suffix features are 2 357 TADM; http://tadm.sourceforge.net/ given in row (ii). The two features in rows (iii) and (iv) indicate whether the word starts with a separable particle and if it contains a hyphen, respectively. Since it is the stem of the unknown word we add to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved t"
R11-1049,2006.jeptalnrecital-invite.2,1,0.887295,"Missing"
R11-1049,zhang-kordoni-2006-automated,1,0.84341,"Missing"
R11-1049,W05-1008,0,\N,Missing
R11-1049,C98-1014,0,\N,Missing
roberts-kordoni-2012-using,W04-3220,0,\N,Missing
roberts-kordoni-2012-using,H05-1052,0,\N,Missing
roberts-kordoni-2012-using,E09-1005,0,\N,Missing
roberts-kordoni-2012-using,H93-1061,0,\N,Missing
roberts-kordoni-2012-using,P03-1054,0,\N,Missing
roberts-kordoni-2012-using,P03-1007,0,\N,Missing
roberts-kordoni-2012-using,I05-1081,0,\N,Missing
roberts-kordoni-2012-using,P05-1006,0,\N,Missing
U05-1006,W05-1008,0,0.0954397,"Missing"
U05-1006,J99-2004,0,0.138881,"1997) reported an empirical approach towards unknown lexical analysis using morphological and syntactic information. The approach is similar to ours in spirit. However, the experiments were done for a shallow parser with a very limited number of word classes. The applicability to lexicalist deep grammars with lots of lexical types is unknown. In (Malouf and van Noord, 2004), the maximum entropy models were used for wide coverage parsing with the Alpino Dutch grammar (Bouma et al., 2001). But the focus was on parse selection, not unknown words processing. Another related work is supertagging (Bangalore and Joshi, 1999). In supertagging, the lexical items are assigned with rich descriptions (supertags) that impose complex constraints in a local context. Some statistical techniques of assigning supertags to unknown words have been reported. For example, (Bangalore and Joshi, 1999) used a simple method of combining a probability estimate for unknown words P (U KN |Ti ) with a probability estimate based on word features (capitalization, hyphenation, ending of words) by: P (Wi |Ti ) = P (U N K|Ti ) ∗ P (w f eat(Wi )|Ti ) (2) where U N K is a token associated with each supertag and its count NU N K is estimated b"
U05-1006,P98-1014,0,0.0332015,"east) one lexical entry for the unknown, so that the deep processing does not halt at the very beginning. A more important difference is that, while (Baldwin, 2005) focuses on generalizing the method of deriving DLA models on various secondary language resources, our work focuses more on how to utilize the deep grammar itself as a source for enhancing robustness. The Redwoods Treebank is by nature the output of the deep grammar. And the parsing, as well as the disambiguation models are also part of the grammar that has eventually contributed to the unknown word type prediction. (Erbach, 1990; Barg and Walther, 1998; Fouvry, 2003) followed a different approach towards unknown words processing for unification based grammars. The basic idea was to use the underspecified lexical entries, namely TFSs with fewer constraints, in order to generate full parses for the sentences, and then extract the sub-TFS from the parses as a new lexical entry. However, lexical entries generated in this way might be both too general and too specific. And underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing. It gets even worse when 5 We used a text set named rondane for trai"
U05-1006,callmeier-etal-2004-deepthought,0,0.0341559,"Missing"
U05-1006,copestake-flickinger-2000-open,0,0.274171,"Missing"
U05-1006,copestake-etal-2004-lexicon,0,0.0133003,"ludes email correspondence, travel planning dialogs, etc. The 5th growth of Redwoods contains about 16.5K sentences and 122K tokens3 . In all our experiments, we have done a 10fold cross validation on the Redwoods treebank. For each fold, words that do not occur in the training partition are assumed to be unknown. A modified version of the efficient HPSG parser PET (Callmeier, 2000; Callmeier, 2001) has been used to generate the derivation tree fragments of the partial parses. 3 Sentences without a full analysis are neither counted here nor used in experiments. 28 We have also modified LexDB (Copestake et al., 2004) in order to be able to add temporal lexical entries that are only active for specific sentence. The parse disambiguation model we have used is a maximum entropy based model that uses non-lexicalized features with 2 levels of grandparnets (see (Toutanova et al., 2002) for detailed discussion about parse disambiguation models for HPSG grammars). For maximum entropy parameter estimation, we have used (Malouf, 2002)’s MaxEnt package. 5.2 Results For comparison, we have built a baseline system that always assigns a majority type to each unknown according to the POS tag. More specificically, we tag"
U05-1006,E03-1041,0,0.0170058,"for the unknown, so that the deep processing does not halt at the very beginning. A more important difference is that, while (Baldwin, 2005) focuses on generalizing the method of deriving DLA models on various secondary language resources, our work focuses more on how to utilize the deep grammar itself as a source for enhancing robustness. The Redwoods Treebank is by nature the output of the deep grammar. And the parsing, as well as the disambiguation models are also part of the grammar that has eventually contributed to the unknown word type prediction. (Erbach, 1990; Barg and Walther, 1998; Fouvry, 2003) followed a different approach towards unknown words processing for unification based grammars. The basic idea was to use the underspecified lexical entries, namely TFSs with fewer constraints, in order to generate full parses for the sentences, and then extract the sub-TFS from the parses as a new lexical entry. However, lexical entries generated in this way might be both too general and too specific. And underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing. It gets even worse when 5 We used a text set named rondane for training and hike f"
U05-1006,W02-2018,0,0.271539,"of a Maximum entropy model lie in the general feature representation and in no independence assumptions between features. A maximum entropy model can also easily handle thousands of features and large numbers of possible outputs. For our prediction model, the probability of a lexical type t given an unknown word and its context c is: p(t, c) = P exp( i θi fi (t, c)) P 0 0 t ∈T exp( i θi fi (t , c)) P (1) where feature fi (t, c) may encode arbitrary characteristics of the context. The parameters < θ1 , θ2 , . . . &gt; can be evaluated by maximizing the pseudo-likelihood on a training corpus (see (Malouf, 2002)). The basic feature templates used in our MEbased model include the prefix and suffix of the unknown word, the context words within a window size of 5, and their corresponding lexical types. Using Partial Parsing Results as Features Each lexical type is essentially a set of constraints on linguistic objects. If a word has a specific lexical type, it must conform to all the constraints demanded by the type, and hence it can only appear in some specific linguistic context. The constraints concern various linguistic aspects, among which syntactic constraints are predominant. One advantage of usi"
U05-1006,C02-2025,0,0.0499545,"Missing"
U05-1006,W97-0124,0,0.0312127,"written English about tourism in the norwegian mountain area, with an average sentence length of 16 words; hike contains 320 sentences about outdoor hiking in Norway with an average sentence length of 14.3 words. Both contain a lot of unknowns like location names, transliterations, etc. two unknown words occur next to each other, which might allow almost any constituent to be constructed. Also, the underspecified lexical entry significantly increases computational complexity. (van Schagen and Knott, 2004) took a similar approach of interactive unknown word acquisition in a dialogue context. (Thede and Harper, 1997) reported an empirical approach towards unknown lexical analysis using morphological and syntactic information. The approach is similar to ours in spirit. However, the experiments were done for a shallow parser with a very limited number of word classes. The applicability to lexicalist deep grammars with lots of lexical types is unknown. In (Malouf and van Noord, 2004), the maximum entropy models were used for wide coverage parsing with the Alpino Dutch grammar (Bouma et al., 2001). But the focus was on parse selection, not unknown words processing. Another related work is supertagging (Bangal"
U05-1006,U04-1018,0,0.0871669,"Missing"
U05-1006,baldwin-etal-2004-road,0,\N,Missing
W06-1206,baldwin-etal-2004-road,0,0.134886,"Missing"
W06-1206,W05-1008,0,0.0425203,"Missing"
W06-1206,P04-1057,0,0.0200567,"Missing"
W06-1206,1999.tc-1.8,0,0.0293775,"of van Noord (2004), the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of som"
W06-1206,zhang-kordoni-2006-automated,1,0.83552,"rther subtypes. We will call the maximum lexical types after extension the atomic lexical types. Then the lexicon will be a multi-valued mapping from the word stems to the atomic lexical types. Needless to underline here that all we have exp( i θi fi (t, c)) P p(t|c) = P 0 t0 ∈T exp( i θi fi (t , c)) P (3) where feature fi (t, c) may encode arbitrary characteristics of the context. The parameters < θ1 , θ2 , . . . > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). The detailed design and feature selection for the lexical type predictor are described in Zhang and Kordoni (2006). 5 Lexical ambiguity is not considered here for the unknowns. In principle, this constraint can be relaxed by allowing the classifier to return more than one results by, setting a confidence threshold, for example. 42 In the experiment described here, we have used the latest version of the Redwoods Treebank in order to train the lexical type predictor with morphological features and context words/POS tags features 6 . We have then extracted from the BNC 6248 sentences, which contain at least one of the 311 MWE candidates verified with World Wide Web in the way described in the previous sectio"
W06-1206,W02-1030,0,0.0377286,"the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristi"
W06-1206,J03-3001,0,0.157918,"used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristics that make them so challenging for"
W06-1206,W02-2018,0,0.0498277,"hat corresponds to the occurrence of the word in the given context5 . We use a single classifier to predict the atomic lexical type. There are normally hundreds of atomic lexical types for a large grammar. So the classification model should be able to handle a large number of output classes. We choose the Maximum Entropy-based model because it can easily handle thousands of features and a large number of possible outputs. It also has the advantages of general feature representation and no independence assumption between features. With the efficient parameter estimation algorithms discussed by Malouf (2002), the training of the model is now very fast. For our prediction model, the probability of a lexical type t given an unknown word and its context c is: Atomic Lexical Types Lexicalist grammars are normally composed of a limited number of rules and a lexicon with rich linguistic features attached to each entry. Some grammar formalisms have a type inheriting system to encode various constraints, and a flat structure of the lexicon with each entry mapped onto one type in the inheritance hierarchy. The following discussion is based on Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1"
W06-1206,A00-2022,0,0.072191,"Missing"
W07-1217,baldwin-etal-2004-road,0,0.1165,"Missing"
W07-1217,P06-2006,0,0.0472637,"Missing"
W07-1217,P06-4020,0,0.0562837,"Missing"
W07-1217,I05-1015,0,0.0570435,"he local ambiguity packing poses an efficiency and accuracy challenge, as not all the intermediate parsing results are directly available as passive edges on the chart. Without unpacking the ambiguity readings, interesting partial analyses might be lost.2 But exhaustively unpacking all the readings will pay back the efficiency gain by ambiguity packing, and eventually lead to computational intractable results. To efficiently recover the ambiguous readings from packed representations, the selective unpacking algorithm has been recently implemented as an extension to the algorithm described in (Carroll and Oepen, 2005). It is able to recover the top-n best readings of a given passive parser edge based on the score assigned by a maximum entropy parse ranking model. This neat feature largely facilitates the efficient searching for best partial parses described in later sections. 3 Partial Parse Selection A partial parse is a set of partial analyses licensed by the grammar which cover the entire input without overlapping. As shown in the previous section, there are usually more than one possible partial parses for a given input. For deep linguistic processing, a high level of local ambiguity means there are ev"
W07-1217,W06-1106,0,0.173979,"Missing"
W07-1217,P04-1005,0,0.285593,"ation is reasonable performance considering the types of noise in the speech transcript input. As a further step to show the competence of partial parsing, we briefly investigated its application in capturing disfluent regions in speech texts. The state of the art approach in identifying disfluent re3 Lexical prediction was not used here to avoid obfuscating the quality of partial parsing by introducing lexical type prediction errors. 4 The repetition error of “it” is interpreted as a topicalization. 134 gions and potentially capturing meaningful text is a shallow parsing method described in (Johnson and Charniak, 2004), which searches the text string for approximately repeated constituents. We ran their system on our random sample of the Fisher data, and compared its results to the partial parse output of the nine well-segmented partial parses analyses (every utterance of which contained some speaker-induced disfluency) to see how well partial parsing could potentially fare as an approach for identifying disfluent regions of speech text. Often the (Johnson and Charniak, 2004) method identified disfluent regions overlapped with identified fragments found in the partial parse, the removal of which would yield"
W07-1217,P99-1069,0,0.0324247,"ation Functions Generally speaking, the weights of the edges in the shortest path approach represent the quality of the local analyses and their likelihood of appearing in the analysis of the entire input. This is an interesting parallel to the parse selection models for the full analyses, where a goodness score is usually assigned to the full analysis. For example, the parse disambiguation model described in (Toutanova et al., 2002) uses a maximum entropy approach to model the conditional probability of a parse for a given input sequence P (t|w). A similar approach has also been reported in (Johnson et al., 1999; Riezler et al., 2002; Malouf and van Noord, 2004). For a given partial parse Φ = {t1 , . . . , tk }, Ω = 131 k Y P (ti |wi ) (2) i=1 Therefore, the log-probability will be log P (Φ|w) ≈ log P (Ω|w) + k X log P (ti |wi ) (3) i=1 Equation 3 indicates that the log-probability of a partial parse for a given input is the sum of the logprobability of local analyses for the sub-strings, with an additional component − log P (Ω|w) representing the conditional log-probability of the segmentation. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm wi"
W07-1217,W02-2018,0,0.0388681,"n. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm will quickly find the partial parse that maximizes log P (Φ|w) − log P (Ω|w). The probability P (ti |wi ) can be modeled in a similar way to the maximum entropy based full parse selection models: P exp nj=1 λj fj (ti , wi ) Pn P (ti |wi ) = P ′ j=1 λj fj (t , wi ) t′ ∈T exp (4) where T is the set of all possible structures that can be assigned to wi , f1 . . . fn are the features and λ1 . . . λn are the parameters. The parameters can be efficiently estimated from a treebank, as shown by (Malouf, 2002). The only difference from the full parse selection model is that here intermediate results are used to generate events for training the model (i.e. the intermediate nodes are used as positive events if it occurs on one of the active tree, or as negative events if not). Since there is a huge number of intermediate results availalbe, we only randomly select a part of them as training data. This is essentially similar to the approach in (Osborne, 2000), where there is an infeasibly large number of training events, only part of which is used in the estimation step. The exact features used in the"
W07-1217,J93-2004,0,0.0292787,"they are very much specific to the annotation guidelines. Also, the deep grammars we are working with are not automatically extracted from annotated corpora. Therefore, unless there are partial treebanks built specifically for the deep grammars, there is simply no ‘gold’ standard for non-golden partial analyses. Instead, in this paper, we evaluate the partial analyses results on the basis of multiple metrics, from both the syntactic and semantic point of views. Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank (Marcus et al., 1993). A pilot study of applying partial parsing in spontaneous speech text processing is also carried out. The remainder of the paper is organized as follow. Section 2 provides background knowledge about partial analysis. Section 3 presents various partial parse selection models. Section 4 describes the evaluation setup and results. Section 5 concludes the paper. tures (attribute value pairs) and a type inheritance system. Therefore, each passive edge on the parsing chart corresponds to a TFS. A relatively small set of highly generalized rules are used to check the compatibility among smaller TFSe"
W07-1217,A00-2022,0,0.165579,"3 Local Ambiguity Packing There is one more complication concerning the partial parses when the local ambiguity packing is used in the parser. Due to the inherent ambiguity of natural language, the same sequence of input may be analyzed as the same linguistic object in different ways. Such intermediate analyses must be recorded during the processing and recovered in later stages. Without any efficient processing technique, parsing becomes computationally intractable with the combinatory explosion of such local ambiguities. In PET, the subsumption-based ambiguity packing algorithm proposed in (Oepen and Carroll, 2000) is used. This separates the parsing into two phases: forest creation phase and read-out/unpacking phase. In relation to the work on partial parsing in this paper, the local ambiguity packing poses an efficiency and accuracy challenge, as not all the intermediate parsing results are directly available as passive edges on the chart. Without unpacking the ambiguity readings, interesting partial analyses might be lost.2 But exhaustively unpacking all the readings will pay back the efficiency gain by ambiguity packing, and eventually lead to computational intractable results. To efficiently recove"
W07-1217,C02-2025,0,0.0215999,"rses. For full parsers, there are generally two ways of evaluation. For parsers that are trained on a treebank using an automatically extracted grammar, an unseen set of manually annotated data is used as the test set. The parser output on the test set is compared to the gold standard annotation, either with the widely used PARSEVAL measurement, or with more annotation-neutral dependency relations. For parsers based on manually compiled grammars, more human judgment is involved in the evaluation. With the evolution of the grammar, the treebank as the output from the grammar changes over time (Oepen et al., 2002). The grammar writer inspects the parses generated by the grammar and either “accepts” or “rejects” the analysis. In partial parsing for manually compiled grammars, the criterion for acceptable analyses is less evident. Most current treebanking tools are not designed for annotating partial analyses. Large-scale manually annotated treebanks do have the annotation for sentences that deep grammars are not able to fully analyze. And the annotation difference in other language resources makes the comparison less straightforward. More complication is involved with the platform and resources used in"
W07-1217,C00-1085,0,0.0178667,"to wi , f1 . . . fn are the features and λ1 . . . λn are the parameters. The parameters can be efficiently estimated from a treebank, as shown by (Malouf, 2002). The only difference from the full parse selection model is that here intermediate results are used to generate events for training the model (i.e. the intermediate nodes are used as positive events if it occurs on one of the active tree, or as negative events if not). Since there is a huge number of intermediate results availalbe, we only randomly select a part of them as training data. This is essentially similar to the approach in (Osborne, 2000), where there is an infeasibly large number of training events, only part of which is used in the estimation step. The exact features used in the log-linear model can significantly influence the disambiguation accuracy. In this experiment we used the same features as those used in the PCFG-S model in (Toutanova et al., 2002) (i.e., depth-1 derivation trees). The estimation of P (Ω|w) is more difficult. In a sense it is similar to a segmentation or chunking model, where the task is to segment the input into fragments. However, it is difficult to collect training data to directly train such a mo"
W07-1217,P02-1035,0,0.0603517,"lly speaking, the weights of the edges in the shortest path approach represent the quality of the local analyses and their likelihood of appearing in the analysis of the entire input. This is an interesting parallel to the parse selection models for the full analyses, where a goodness score is usually assigned to the full analysis. For example, the parse disambiguation model described in (Toutanova et al., 2002) uses a maximum entropy approach to model the conditional probability of a parse for a given input sequence P (t|w). A similar approach has also been reported in (Johnson et al., 1999; Riezler et al., 2002; Malouf and van Noord, 2004). For a given partial parse Φ = {t1 , . . . , tk }, Ω = 131 k Y P (ti |wi ) (2) i=1 Therefore, the log-probability will be log P (Φ|w) ≈ log P (Ω|w) + k X log P (ti |wi ) (3) i=1 Equation 3 indicates that the log-probability of a partial parse for a given input is the sum of the logprobability of local analyses for the sub-strings, with an additional component − log P (Ω|w) representing the conditional log-probability of the segmentation. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm will quickly find the pa"
W07-1217,P99-1052,0,\N,Missing
W07-1220,baldwin-etal-2004-road,1,0.94279,"s within NLP tasks, to arrive at a detailed (=deep) syntactic and semantic analysis of the data. It is conventionally driven by deep grammars, which encode linguistically-motivated predictions of language behaviour, are usually capable of both parsing and generation, and generate a highlevel semantic abstraction of the input data. While enjoying a resurgence of interest due to advances in parsing algorithms and stochastic parse pruning/ranking, deep grammars remain an underutilised resource predominantly because of their lack of coverage/robustness in parsing tasks. As noted in previous work (Baldwin et al., 2004), a significant cause It is often the case that the different measures lead to significantly different assessments of the quality of DLA, even for a given DLA approach. Additionally, it is far from clear how the numbers generated by these evaluation metrics correlate with actual parsing performance when the output of a given DLA method is used. This makes standardised comparison among the various different approaches to DLA very difficult, if not impossible. It is far from clear which evaluation metrics are more indicative of the true “goodness” of the lexicon. The aim of this research, theref"
W07-1220,W05-1008,1,0.791921,"y may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. The whole procedure gets even more complicated when two unknown words occur next to each other, potentially allowing almost any constituent to be constructed. The evaluation of these proposals has tended to be small-scale and somewhat brittle. No concrete results have been presented relating to the improvement in grammar performance, either for parsing or for generation. Baldwin (2005) took a statistical approach to automated lexical acquisition for deep grammars. Focused on generalising the method of deriving DLA models on various secondary language resources, Baldwin used a large set of binary classifiers to predict whether a given unknown word is of a particular lexical type. This data-driven approach is grammar independent and can be scaled up for large grammars. Evaluation was via type precision, type recall, type F-measure and token accuracy, resulting in different interpretations of the data depending on the evaluation metric used. Zhang and Kordoni (2006) tackled th"
W07-1220,P98-1014,0,0.0260799,"over a carefully designed test suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computational"
W07-1220,E03-1041,0,0.0135372,"est suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. Th"
W07-1220,P06-1064,0,0.0613453,"Missing"
W07-1220,C02-2025,0,0.0585456,"Missing"
W07-1220,W02-1210,0,0.77711,"disation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance. Specifically, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. Our results show convincingly that traditional Fscore-based evaluation of lexical acquisition does not correlate with actual parsing performance. What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms. • Type precision: the proportion of correctly hypothesised lexical entries • Type recall: the proportion of gold-standard lexical entries that are correctly hypothesised • Type F-measure: the harmonic mean of the type precision and type recall • Toke"
W07-1220,P04-1057,0,0.265235,"Missing"
W07-1220,2006.jeptalnrecital-invite.2,0,0.0410557,"Missing"
W07-1220,zhang-kordoni-2006-automated,1,0.930026,"and the Lexicon: Standardising Deep Lexical Acquisition Evaluation Yi Zhang† and Timothy Baldwin‡ and Valia Kordoni† † Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany ‡ Dept of Computer Science and Software Engineering, University of Melbourne, Australia {yzhang,kordoni}@coli.uni-sb.de tim@csse.unimelb.edu.au Abstract of diminished coverage is the lack of lexical coverage. Various attempts have been made to ameliorate the deficiencies of hand-crafted lexicons. More recently, there has been an explosion of interest in deep lexical acquisition (DLA; (Baldwin, 2005; Zhang and Kordoni, 2006; van de Cruys, 2006)) for broad-coverage deep grammars, either by exploiting the linguistic information encoded in the grammar itself (in vivo), or by using secondary language resources (in vitro). Such approaches provide (semi-)automatic ways of extending the lexicon with minimal (or no) human interference. One stumbling block in DLA research has been the lack of standardisation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual pars"
W07-1220,A00-2018,0,\N,Missing
W07-1220,C98-1014,0,\N,Missing
W08-1708,J03-3001,0,0.0459102,"archy. The grammar originates from (M¨uller and Kasper, 2000), but continued to improve after the end of the Verbmobil project (Wahlster, 2000) and it currently consists of 5K types, 115 rules and the lexicon contains approximately 35K entries. These entries belong to 386 distinct lexical types. In the experiments we report here two corpora of different kind and size have been used. The first one has been extracted from the Frankfurter Rundschau newspaper and contains about 614K sentences that have between 5 and 20 tokens. The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, university and science web sites, etc. The German part, named deWaC (Web as Corpus), contains about 93M sentences and 1.65 billion tokens. The subset used in our experiments is extracted by randomly selecting 2.57M sentences that have between 4 and 30 tokens. These corpora have been chosen because it is interesting to observe the grammar performance on a relatively balanced newspaper corpus that does"
W08-1708,P04-1057,0,0.297812,"Missing"
W08-1708,zhang-kordoni-2006-automated,1,0.941759,"and Re-Usability of Lexicalised Grammars Kostadin Cholakov† , Valia Kordoni†‡ , Yi Zhang†‡ † Department of Computational Linguistics, Saarland University, Germany ‡ LT-Lab, DFKI GmbH, Germany {kostadin,kordoni,yzhang}@coli.uni-sb.de Abstract In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and"
W08-1708,baldwin-etal-2004-road,0,0.0130114,"pendent Deep Linguistic Processing: Ensuring Portability and Re-Usability of Lexicalised Grammars Kostadin Cholakov† , Valia Kordoni†‡ , Yi Zhang†‡ † Department of Computational Linguistics, Saarland University, Germany ‡ LT-Lab, DFKI GmbH, Germany {kostadin,kordoni,yzhang}@coli.uni-sb.de Abstract In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as"
W08-1708,W05-1008,0,0.31703,"ldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and (Nicholson et al., 2008) describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English, Dutch and German. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. However, it is our claim that to achieve better and more practically useful results, apart from good learning algorithms, we also need to incorporate into the learning process fine-grained linguistic information which deep grammars i"
W08-1708,A00-1031,0,0.0885319,"Missing"
W08-1708,nicholson-etal-2008-evaluating,1,\N,Missing
W08-1708,copestake-flickinger-2000-open,0,\N,Missing
W09-3032,J93-2004,0,0.0319203,"Missing"
W09-3032,C02-2025,0,0.797141,"completely independent annotation of the WSJ texts built without conversion from the original PTB trees. Another popular alternative way to aid treebank development is to use automatic parsing outputs as guidance. Many state-of-the-art parsers are able to efficiently produce large amount of annotated syntactic structures with relatively high accuracy. This approach has changed the role of human annotation from a labour-intensive task of drawing trees from scratch to a more intelligencedemanding task of correcting parsing errors, or eliminating unwanted ambiguities (cf., the Redwoods Treebank (Oepen et al., 2002)). It is our aim in this on-going project to build a HPSG treebank for the WSJ sections of the PTB based on the hand-written ERG for English. 3 our project for parsing the WSJ sections of the PTB, and [incr tsdb()] (Oepen, 2001), the grammar performance profiling system we are using, which comes with a complete set of GUI-based tools for treebanking. Version control system also plays an important role in this project. 3.2 The sentences from the Wall Street Journal Sections of the Penn Treebank are extracted with their original tokenization, with each word paired with a part-of-speech tag. Each"
W09-3032,zhang-kordoni-2008-robust,1,0.897023,"Missing"
W09-3032,adolphs-etal-2008-fine,0,\N,Missing
W09-3032,2000.iwpt-1.19,0,\N,Missing
W09-3836,W09-3032,1,0.834151,"national Conference on Parsing Technologies (IWPT), pages 226–229, c Paris, October 2009. 2009 Association for Computational Linguistics D1 D2 D3 D4 D5 D6 D7 D8 SUBJH HSPEC FRAG _ NP HSPEC NOUN _ N _ CMPND ... PLUR _ NOUN _ ORULE v_-_le n_-_mc_le of parse disambiguation. The decisions record the fine-grained human judgements in the manual disambiguation process. This is different from the traditional use of treebanks to build parse selection models, where a marked gold tree is picked from the parse forest without concerning detailed selection steps. Recent study on double annotated treebanks (Kordoni and Zhang, 2009) shows that annotators tend to start with the decisions with the most certainty, and delay the “hard” decisions as much as possible. As the decision process goes, many of the “hard” discriminants will receive an inferred value from the certain decisions. This greedy approach helps to guarantee high interannotator agreement. Concerning the statistical parse selection models, the discriminative nature of these treebanking decisions suggests that they are highly effective features, and if properly used, they will contribute to an efficient disambiguation model. the dog ||barks the ||dog barks the"
W09-3836,C02-2025,0,0.016751,"parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. 2 Treebanking decisions One of the defining characteristics of Redwoodsstyle treebanks1 (Oepen et al., 2002) is that the candidate trees are constructed automatically by 1 More details available in http://redwoods.stanford.edu. 226 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226–229, c Paris, October 2009. 2009 Association for Computational Linguistics D1 D2 D3 D4 D5 D6 D7 D8 SUBJH HSPEC FRAG _ NP HSPEC NOUN _"
W09-3836,2004.tmi-1.2,0,0.0279029,"tences. So, for example, if a feature (of a particular feature type) is observed 100 times, then these 100 occurrences are added to the total FHC. • Feature Type Hit Count (FTHC): it is the total number of distinct features (of the corresponding feature type) observed inside the syntactic analyses of all the test sentences. While exact and 5-best match measures show relative informativeness and robustness of the feature types, FHC and FTHC provide a more comprehensive picture of relative efficiencies. Experiment 4.1 Data 4.3 We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentenc"
W09-3836,N04-1012,0,0.0161998,"rmativeness and robustness of the feature types, FHC and FTHC provide a more comprehensive picture of relative efficiencies. Experiment 4.1 Data 4.3 We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. Results and discussion As we can see in Table 1, local configurations achieve highest accuracy among the traditional feature types. They also use higher number of features (almost 2.7 millions). TDFC do better than both n-grams and active edges, even with a lower number of features. Though, local configurations gain more accurac"
W09-3836,W97-1502,0,0.269613,"ls are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features. 1 Introduction State-of-the-art parse disambiguation models are trained on treebanks, which are either fully handannotated or manually disambiguated from the parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. 2 Treebanking decisions One of the defining characteristics of Redwoodsstyle treebanks1 (Oepen et al., 2002) is that th"
W09-3836,A00-2018,0,\N,Missing
W09-3836,P99-1069,0,\N,Missing
W09-4107,W05-1008,0,0.0136693,"yment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English and Dutch. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. We use the ERG and GG grammars for the work we present in this talk, for the ERG is one of the biggest deep linguistic resources to date which has been being used in many (industrial) applications, and GG is to our knowledge one of the most thorough deep grammars of German, a language with rich morphology"
W09-4107,baldwin-etal-2004-road,0,0.0159027,"g robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient method"
W09-4107,copestake-flickinger-2000-open,0,0.0381181,"ency deficiency of large-scale lexicalised grammars, ensuring this way their portability and re-usability and aiming at domain-independent linguistic processing. In particular, we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and ensuring maintainability and re-usability of lexicalised grammars. To this effect, we focus on enhancing robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are"
W09-4107,P04-1057,0,0.0372151,"ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the ta"
W09-4107,zhang-kordoni-2006-automated,1,0.802444,"g maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of le"
W09-4107,P09-1043,1,0.881966,"Missing"
W16-1808,W14-4001,0,0.27297,"., 2005). Previous work (Ren et al. (2009), Carpuat and Diab (2010), Cholakov and Kordoni (2014)) has shown that dedicated techniques for identification of MWEs and their integration into the translation algorithms improve the quality of SMT. Generally, those techniques are based on categorical representations. MWEs are either treated as a single unit or binary features encoding properties of MWEs are added to the translation table. On the other hand, recent works have successfully applied distributional representations of words and phrases in SMT (Mikolov et al. (2013a), Zhang et al. (2014), Alkhouli et al. (2014)). The idea behind is that similar words and phrases in different languages tend to have simiWe perform a case study with an English to Bulgarian SMT system. An English PV is generally translated to a single Bulgarian verb. This manyto-one mapping poses difficulties for SMT. The combined integration of all three strategies presented above outperforms the results reported in previous work both in automated and manual evaluation. Thus we show that word embeddings help SMT to handle better such a challenging linguistic phenomenon as PVs. 56 Proceedings of the 12th Workshop on Multiword Expression"
W16-1808,W09-2907,0,0.170285,"mi-)compositional PVs can be (partially) derived from the meaning of their lexemes, e.g. carry in. Previous work (Cholakov and Kordoni, 2014) treats PVs as either compositional or idiomatic while we handle compositionality as a continuous phenomenon. Introduction Phrasal verbs (PVs) are a type of multiword expressions (MWEs) and as such, their semantics is not predictable, or is only partially predictable, from the semantics of their components. In statistical machine translation (SMT) the word-to-word translation of MWEs often results in wrong translations (Piao et al., 2005). Previous work (Ren et al. (2009), Carpuat and Diab (2010), Cholakov and Kordoni (2014)) has shown that dedicated techniques for identification of MWEs and their integration into the translation algorithms improve the quality of SMT. Generally, those techniques are based on categorical representations. MWEs are either treated as a single unit or binary features encoding properties of MWEs are added to the translation table. On the other hand, recent works have successfully applied distributional representations of words and phrases in SMT (Mikolov et al. (2013a), Zhang et al. (2014), Alkhouli et al. (2014)). The idea behind i"
W16-1808,N10-1029,0,0.100615,"Vs can be (partially) derived from the meaning of their lexemes, e.g. carry in. Previous work (Cholakov and Kordoni, 2014) treats PVs as either compositional or idiomatic while we handle compositionality as a continuous phenomenon. Introduction Phrasal verbs (PVs) are a type of multiword expressions (MWEs) and as such, their semantics is not predictable, or is only partially predictable, from the semantics of their components. In statistical machine translation (SMT) the word-to-word translation of MWEs often results in wrong translations (Piao et al., 2005). Previous work (Ren et al. (2009), Carpuat and Diab (2010), Cholakov and Kordoni (2014)) has shown that dedicated techniques for identification of MWEs and their integration into the translation algorithms improve the quality of SMT. Generally, those techniques are based on categorical representations. MWEs are either treated as a single unit or binary features encoding properties of MWEs are added to the translation table. On the other hand, recent works have successfully applied distributional representations of words and phrases in SMT (Mikolov et al. (2013a), Zhang et al. (2014), Alkhouli et al. (2014)). The idea behind is that similar words and"
W16-1808,E14-1050,0,0.0412779,"Missing"
W16-1808,D14-1024,1,0.494272,"process. Second, we use the vectors learnt to find paraphrases of the original phrase pairs and add those to the translation table. This increases the amount of relevant parallel data. Third, we make use of word embeddings to map a PV onto a continuous-valued compositionality score and add this score as a feature in the SMT model. The score indicates the semantic similarity between a PV and the verb forming that PV, i.e. the degree of compositionality of the PV. The meaning of (semi-)compositional PVs can be (partially) derived from the meaning of their lexemes, e.g. carry in. Previous work (Cholakov and Kordoni, 2014) treats PVs as either compositional or idiomatic while we handle compositionality as a continuous phenomenon. Introduction Phrasal verbs (PVs) are a type of multiword expressions (MWEs) and as such, their semantics is not predictable, or is only partially predictable, from the semantics of their components. In statistical machine translation (SMT) the word-to-word translation of MWEs often results in wrong translations (Piao et al., 2005). Previous work (Ren et al. (2009), Carpuat and Diab (2010), Cholakov and Kordoni (2014)) has shown that dedicated techniques for identification of MWEs and t"
W16-1808,N15-1099,0,0.0388014,"Missing"
W16-1808,P11-2031,0,0.017959,"ositionality feature our 3 strategies combined 0.268 0.270 0.269 0.272 6.00 6.02 6.01 6.02 0.258 0.261 0.260 0.262 6.15 6.18 6.17 6.18 nist good acceptable incorrect baseline 4 binary features 0.21 0.3 0.41 0.5 0.38 0.2 semantic scoring feature paraphrases compositionality feature our 3 strategies combined 0.3 0.31 0.3 0.31 0.54 0.53 0.57 0.57 0.16 0.16 0.13 0.12 Table 2: Manual evaluation of MT quality. Table 1: Automatic evaluation of MT quality. garian was asked to judge the translations of PVs produced by the MT system. A translation was judged as: (AR) test. We used the multeval toolkit (Clark et al., 2011) for evaluation. In the baseline case Moses is run in a standard configuration, i.e. without any explicit MWE knowledge. Table 1 also shows the best results from Cholakov and Kordoni (2014) where 4 binary features indicate: 1) whether a phrase contains a PV; 2) whether a detected PV is transitive or not; 3) whether the particle in a PV is separable or not; and 4) whether a PV is compositional or not. We evaluated the contribution of each of our 3 strategies based on word embeddings as well as various combinations thereof. Note that, for reasons of space, we do not report on the 400 test senten"
W16-1808,S13-1038,0,0.172541,"n table. However, the modifications come from the usage of word embeddings assuming that those allow for a better incorporation of semantic information into SMT. Following the work of Mikolov et al. (2013a), Mikolov et al. (2013b), and Alkhouli et al. (2014), we exploit the idea that vector representations of similar words in different languages are related by a linear transformation. However, we focus on exploring this idea on a specific phenomenon with challenging semantics, namely PVs. Finally, there has been significant research on predicting the compositionality of MWEs (e.g., Schulte im Walde et al. (2013), Salehi et al. (2015)) under the assumption that this could be helpful in applications. Here, we go a step further and prove this assumption correct by integrating compositionality into a real-life application such as SMT. 3 4 In our work, we construct word embeddings of English phrases which contain PVs and of their aligned counterparts in Bulgarian. Then we use those representations to augment the translation table with new features and phrase alignments. The word embeddings are obtained using the word2vec toolkit.3 We used the continuous bagof-words (CBOW) model. Experiments with the skip-"
W16-1808,W11-0818,0,0.0657441,"Missing"
W16-1808,2005.mtsummit-posters.11,0,0.0435455,"anguages. The training data consist of approximately 151,000 sentences. Another 2,000 sentences are used for tuning. The test set consists of 800 sentences, 400 of which contain one or more instances of PVs. We manually identified 138 unique PVs with a total of 403 instances. A language model for the target language is created based on a 50 million words subset of the Bulgarian National Reference Corpus (BNRC).2 Finally, Moses is employed to build a factored phrase-based translation model which operates on lemmas and POS tags due to the rich Bulgarian morphology. Previous work on SMT of MWEs (Lambert and Banchs (2005), Carpuat and Diab (2010), Simova and Kordoni (2013)) suggests training the SMT system on corpora in which each MWE is treated as a single unit, e.g. call off. Ren et al. (2009) treat bilingual MWEs pairs as parallel sentences which are then added to the training data. Other methods (Simova and Kordoni (2013), Cholakov and Kordoni (2014)) perform feature mining and modify directly the translation table. In addition to the standard translational probabilities, those methods add binary features which indicate whether a source phrase contains MWEs and whether an MWE is compositional or idiomatic."
W16-1808,2013.mtsummit-wmwumttt.9,1,0.915092,"151,000 sentences. Another 2,000 sentences are used for tuning. The test set consists of 800 sentences, 400 of which contain one or more instances of PVs. We manually identified 138 unique PVs with a total of 403 instances. A language model for the target language is created based on a 50 million words subset of the Bulgarian National Reference Corpus (BNRC).2 Finally, Moses is employed to build a factored phrase-based translation model which operates on lemmas and POS tags due to the rich Bulgarian morphology. Previous work on SMT of MWEs (Lambert and Banchs (2005), Carpuat and Diab (2010), Simova and Kordoni (2013)) suggests training the SMT system on corpora in which each MWE is treated as a single unit, e.g. call off. Ren et al. (2009) treat bilingual MWEs pairs as parallel sentences which are then added to the training data. Other methods (Simova and Kordoni (2013), Cholakov and Kordoni (2014)) perform feature mining and modify directly the translation table. In addition to the standard translational probabilities, those methods add binary features which indicate whether a source phrase contains MWEs and whether an MWE is compositional or idiomatic. Our work modifies both the training data (via the a"
W16-1808,P14-1011,0,0.0321724,"nslations (Piao et al., 2005). Previous work (Ren et al. (2009), Carpuat and Diab (2010), Cholakov and Kordoni (2014)) has shown that dedicated techniques for identification of MWEs and their integration into the translation algorithms improve the quality of SMT. Generally, those techniques are based on categorical representations. MWEs are either treated as a single unit or binary features encoding properties of MWEs are added to the translation table. On the other hand, recent works have successfully applied distributional representations of words and phrases in SMT (Mikolov et al. (2013a), Zhang et al. (2014), Alkhouli et al. (2014)). The idea behind is that similar words and phrases in different languages tend to have simiWe perform a case study with an English to Bulgarian SMT system. An English PV is generally translated to a single Bulgarian verb. This manyto-one mapping poses difficulties for SMT. The combined integration of all three strategies presented above outperforms the results reported in previous work both in automated and manual evaluation. Thus we show that word embeddings help SMT to handle better such a challenging linguistic phenomenon as PVs. 56 Proceedings of the 12th Workshop"
W16-1808,P02-1040,0,0.0971946,"e and target phrase vectors are learned separately, we do not have an immediate mapping between them. That is why we resort to the phrase table to obtain it. A source and a target vectors are paired if there is a corresponding phrase pair entry in the phrase table. 4.3 Compositionality Score 5 Results Our work is directly comparable to that in Cholakov and Kordoni (2014) since we used the same datasets and MT system setup. Furthermore, we have successfully reproduced the results reported there. Automatic Evaluation. Table 1 presents the results from the automatic evaluation, in terms of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. All results are averages of 3 MERT optimizer runs. Statistical significance is computed using the Approximate Randomization Paraphrases We use the vectors produced for Bulgarian to augment the phrase table with additional entries. Us58 with PVs bleu nist bleu all baseline 4 binary features 0.244 0.267 5.97 6.01 0.237 0.256 6.14 6.16 semantic scoring feature paraphrases compositionality feature our 3 strategies combined 0.268 0.270 0.269 0.272 6.00 6.02 6.01 6.02 0.258 0.261 0.260 0.262 6.15 6.18 6.17 6.18 nist good acceptable incorrect baseline 4 binary fea"
W16-4813,2013.iwslt-evaluation.1,0,0.0459683,"Missing"
W16-4813,W16-3422,1,0.61955,"Missing"
W16-4813,J03-1002,0,0.00529792,"12k parallel segments were extracted, and for English-Serbian about 50k. An interesting observation is that although Croatian is generally better supported in terms of publicly available parallel data,6 Serbian is currently better supported for educational parallel texts. As for the out-of-domain corpus, we used the SETimes news corpus (Tyers and Alperen, 2010) since it is relatively large (200k parallel sentences) and clean. Moses set-ups We trained the statistical phrase-based systems using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with SRILM (Stolcke, 2002). The investigated bilingual training set-ups are: 1. en-hr SEtimes (relatively large clean out-of-domain corpus) 2. en-hr Coursera (small in-domain corpus) 3. en-hr Coursera (small in-domain corpus) + en-sr Coursera (larger in-domain corpus) 4. en-hr Coursera + en-hr’ Coursera 5. en-hr SEtimes + en-hr Coursera + en-hr’ Coursera 6 http://opus.lingfil.uu.se/ 100 sentences Training Dev Test 1) setimes 2) coursera 3) 2+coursera en-sr 4) 2+coursera en-hr’ 5) 1+4 coursera coursera 206k 12k 62k 62k 268k 2935 2091 running words en hr 4."
W16-4813,P02-1040,0,0.0956083,"Missing"
W16-4813,W14-4210,1,0.479533,"Missing"
W16-4813,W15-3049,1,0.871794,"Missing"
W16-4813,W16-3421,1,0.40739,"Missing"
W16-4813,2014.eamt-1.45,0,0.669565,"Missing"
W16-4813,W16-3423,0,0.0385996,"Missing"
zhang-kordoni-2006-automated,copestake-flickinger-2000-open,0,\N,Missing
zhang-kordoni-2006-automated,W96-0213,0,\N,Missing
zhang-kordoni-2006-automated,E03-1041,0,\N,Missing
zhang-kordoni-2006-automated,C02-2025,0,\N,Missing
zhang-kordoni-2006-automated,W00-0740,0,\N,Missing
zhang-kordoni-2006-automated,P04-1057,0,\N,Missing
zhang-kordoni-2006-automated,W05-1008,0,\N,Missing
zhang-kordoni-2006-automated,baldwin-etal-2004-road,0,\N,Missing
zhang-kordoni-2006-automated,W02-2018,0,\N,Missing
zhang-kordoni-2008-robust,W03-2401,0,\N,Missing
zhang-kordoni-2008-robust,callmeier-etal-2004-deepthought,0,\N,Missing
zhang-kordoni-2008-robust,J97-4005,0,\N,Missing
zhang-kordoni-2008-robust,zhang-kordoni-2006-automated,1,\N,Missing
zhang-kordoni-2008-robust,W07-2207,1,\N,Missing
zhang-kordoni-2008-robust,C02-2025,0,\N,Missing
zhang-kordoni-2008-robust,W07-1217,1,\N,Missing
zhang-kordoni-2008-robust,W05-1008,0,\N,Missing
zhang-kordoni-2008-robust,P06-4020,0,\N,Missing
zhang-kordoni-2008-robust,W06-1106,0,\N,Missing
zhang-kordoni-2008-robust,P99-1052,0,\N,Missing
zhang-kordoni-2008-robust,I05-1015,0,\N,Missing
zhang-kordoni-2008-robust,baldwin-etal-2004-road,0,\N,Missing
