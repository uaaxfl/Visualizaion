2021.naacl-main.15,Data Filtering using Cross-Lingual Word Embeddings,2021,-1,-1,4,1,3259,christian herold,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign."
2021.iwslt-1.32,Integrated Training for Sequence-to-Sequence Models Using Non-Autoregressive Transformer,2021,-1,-1,7,0,5801,evgeniia tokarchuk,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"Complex natural language applications such as speech translation or pivot translation traditionally rely on cascaded models. However,cascaded models are known to be prone to error propagation and model discrepancy problems. Furthermore, there is no possibility of using end-to-end training data in conventional cascaded systems, meaning that the training data most suited for the task cannot be used.Previous studies suggested several approaches for integrated end-to-end training to overcome those problems, however they mostly rely on(synthetic or natural) three-way data. We propose a cascaded model based on the non-autoregressive Transformer that enables end-to-end training without the need for an explicit intermediate representation. This new architecture (i) avoids unnecessary early decisions that can cause errors which are then propagated throughout the cascaded models and (ii) utilizes the end-to-end training data directly. We conduct an evaluation on two pivot-based machine translation tasks, namely FrenchâGerman and GermanâCzech. Our experimental results show that the proposed architecture yields an improvement of more than 2 BLEU for FrenchâGerman over the cascaded baseline."
2021.insights-1.10,Recurrent Attention for the Transformer,2021,-1,-1,4,1,3260,jan rosendahl,Proceedings of the Second Workshop on Insights from Negative Results in NLP,0,"In this work, we conduct a comprehensive investigation on one of the centerpieces of modern machine translation systems: the encoder-decoder attention mechanism. Motivated by the concept of first-order alignments, we extend the (cross-)attention mechanism by a recurrent connection, allowing direct access to previous attention/alignment decisions. We propose several ways to include such a recurrency into the attention mechanism. Verifying their performance across different translation tasks we conclude that these extensions and dependencies are not beneficial for the translation performance of the Transformer architecture."
2021.dialdoc-1.8,Cascaded Span Extraction and Response Generation for Document-Grounded Dialog,2021,-1,-1,4,0,11223,nico daheim,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),0,"This paper summarizes our entries to both subtasks of the first DialDoc shared task which focuses on the agent response prediction task in goal-oriented document-grounded dialogs. The task is split into two subtasks: predicting a span in a document that grounds an agent turn and generating an agent response based on a dialog and grounding document. In the first subtask, we restrict the set of valid spans to the ones defined in the dataset, use a biaffine classifier to model spans, and finally use an ensemble of different models. For the second sub-task, we use a cascaded model which grounds the response prediction on the predicted span instead of the full document. With these approaches, we obtain significant improvements in both subtasks compared to the baseline."
2021.acl-srw.1,Investigation on Data Adaptation Techniques for Neural Named Entity Recognition,2021,-1,-1,5,0,5801,evgeniia tokarchuk,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Data processing is an important step in various natural language processing tasks. As the commonly used datasets in named entity recognition contain only a limited number of samples, it is important to obtain additional labeled data in an efficient and reliable manner. A common practice is to utilize large monolingual unlabeled corpora. Another popular technique is to create synthetic data from the original labeled data (data augmentation). In this work, we investigate the impact of these two methods on the performance of three different named entity recognition tasks."
2021.acl-srw.3,Transformer-Based Direct Hidden {M}arkov Model for Machine Translation,2021,-1,-1,4,1,5802,weiyue wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding."
2020.wmt-1.71,Diving Deep into Context-Aware Neural Machine Translation,2020,-1,-1,6,0,13894,jingjing huo,Proceedings of the Fifth Conference on Machine Translation,0,"Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts."
2020.wmt-1.103,Towards a Better Evaluation of Metrics for Machine Translation,2020,-1,-1,3,0,13934,peter stanchev,Proceedings of the Fifth Conference on Machine Translation,0,"An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system- and segment-level are analyzed in detail and their shortcomings are pointed out."
2020.findings-emnlp.155,Multi-Agent Mutual Learning at Sentence-Level and Token-Level for Neural Machine Translation,2020,-1,-1,3,0,19592,baohao liao,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Mutual learning, where multiple agents learn collaboratively and teach one another, has been shown to be an effective way to distill knowledge for image classification tasks. In this paper, we extend mutual learning to the machine translation task and operate at both the sentence-level and the token-level. Firstly, we co-train multiple agents by using the same parallel corpora. After convergence, each agent selects and learns its poorly predicted tokens from other agents. The poorly predicted tokens are determined by the acceptance-rejection sampling algorithm. Our experiments show that sequential mutual learning at the sentence-level and the token-level improves the results cumulatively. Absolute improvements compared to strong baselines are obtained on various translation tasks. On the IWSLT{'}14 German-English task, we get a new state-of-the-art BLEU score of 37.0. We also report a competitive result, 29.9 BLEU score, on the WMT{'}14 English-German task."
2020.eamt-1.5,When and Why is Unsupervised Neural Machine Translation Useless?,2020,50,1,3,1,20831,yunsu kim,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This paper studies the practicality of the current state-of-the-art unsupervised methods in neural machine translation (NMT). In ten translation tasks with various data settings, we analyze the conditions under which the unsupervised methods fail to produce reasonable translations. We show that their performance is severely affected by linguistic dissimilarity and domain mismatch between source and target monolingual data. Such conditions are common for low-resource language pairs, where unsupervised learning works poorly. In all of our experiments, supervised and semi-supervised baselines with 50k-sentence bilingual data outperform the best unsupervised results. Our analyses pinpoint the limits of the current unsupervised NMT and also suggest immediate research directions."
2020.coling-main.386,Unifying Input and Output Smoothing in Neural Machine Translation,2020,-1,-1,3,1,12422,yingbo gao,Proceedings of the 28th International Conference on Computational Linguistics,0,"Soft contextualized data augmentation is a recent method that replaces one-hot representation of words with soft posterior distributions of an external language model, smoothing the input of neural machine translation systems. Label smoothing is another effective method that penalizes over-confident model outputs by discounting some probability mass from the true target word, smoothing the output of neural machine translation systems. Having the benefit of updating all word vectors in each optimization step and better regularizing the models, the two smoothing methods are shown to bring significant improvements in translation performance. In this work, we study how to best combine the methods and stack the improvements. Specifically, we vary the prior distributions to smooth with, the hyperparameters that control the smoothing strength, and the token selection procedures. We conduct extensive experiments on small datasets, evaluate the recipes on larger datasets, and examine the implications when back-translation is further used. Our results confirm cumulative improvements when input and output smoothing are used in combination, giving up to +1.9 BLEU scores on standard machine translation tasks and reveal reasons why these smoothing methods should be preferred."
2020.coling-main.612,Neural Language Modeling for Named Entity Recognition,2020,-1,-1,4,0,21729,zhihong lei,Proceedings of the 28th International Conference on Computational Linguistics,0,"Named entity recognition is a key component in various natural language processing systems, and neural architectures provide significant improvements over conventional approaches. Regardless of different word embedding and hidden layer structures of the networks, a conditional random field layer is commonly used for the output. This work proposes to use a neural language model as an alternative to the conditional random field layer, which is more flexible for the size of the corpus. Experimental results show that the proposed system has a significant advantage in terms of training speed, with a marginal performance degradation."
2020.amta-research.2,Investigation of Transformer-based Latent Attention Models for Neural Machine Translation,2020,-1,-1,3,1,5733,parnia bahar,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),0,None
2020.acl-main.360,Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture,2020,-1,-1,3,0,22848,christopher brix,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85{\%}, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter{'}s initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets."
2020.aacl-main.25,Towards a Better Understanding of Label Smoothing in Neural Machine Translation,2020,-1,-1,5,1,12422,yingbo gao,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In order to combat overfitting and in pursuit of better generalization, label smoothing is widely applied in modern neural machine translation systems. The core idea is to penalize over-confident outputs and regularize the model so that its outputs do not diverge too much from some prior distribution. While training perplexity generally gets worse, label smoothing is found to consistently improve test performance. In this work, we aim to better understand label smoothing in the context of neural machine translation. Theoretically, we derive and explain exactly what label smoothing is optimizing for. Practically, we conduct extensive experiments by varying which tokens to smooth, tuning the probability mass to be deducted from the true targets and considering different prior distributions. We show that label smoothing is theoretically well-motivated, and by carefully choosing hyperparameters, the practical performance of strong neural machine translation systems can be further improved."
2020.aacl-main.41,Predicting and Using Target Length in Neural Machine Translation,2020,-1,-1,4,0,12421,zijian yang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Attention-based encoder-decoder models have achieved great success in neural machine translation tasks. However, the lengths of the target sequences are not explicitly predicted in these models. This work proposes length prediction as an auxiliary task and set up a sub-network to obtain the length information from the encoder. Experimental results show that the length prediction sub-network brings improvements over the strong baseline system and that the predicted length can be used as an alternative to length normalization during decoding."
W19-5338,The {RWTH} {A}achen {U}niversity Machine Translation Systems for {WMT} 2019,2019,0,0,8,1,3260,jan rosendahl,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper describes the neural machine translation systems developed at the RWTH Aachen University for the German-English, Chinese-English and Kazakh-English news translation tasks of the Fourth Conference on Machine Translation (WMT19). For all tasks, the final submitted system is based on the Transformer architecture. We focus on improving data filtering and fine-tuning as well as systematically evaluating interesting approaches like unigram language model segmentation and transfer learning. For the De-En task, none of the tested methods gave a significant improvement over last years winning system and we end up with the same performance, resulting in 39.6{\%} BLEU on newstest2019. In the Zh-En task, we show 1.3{\%} BLEU improvement over our last year{'}s submission, which we mostly attribute to the splitting of long sentences during translation. We further report results on the Kazakh-English task where we gain improvements of 11.1{\%} BLEU over our baseline system. On the same task we present a recent transfer learning approach, which uses half of the free parameters of our submission system and performs on par with it."
W19-5359,{EED}: Extended Edit Distance Measure for Machine Translation,2019,0,3,3,0,13934,peter stanchev,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand."
W19-5205,Generalizing Back-Translation in Neural Machine Translation,2019,19,3,5,1,20832,miguel gracca,Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers),0,"Back-translation {---} data augmentation by translating target monolingual data {---} is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German {\textless}-{\textgreater} English news translation task."
W19-4309,Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron,2019,41,0,6,1,20831,yunsu kim,Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),0,We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.
P19-1120,Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies,2019,53,2,3,1,20831,yunsu kim,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1{\%} BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer."
D19-6503,When and Why is Document-level Context Useful in Neural Machine Translation?,2019,30,0,3,1,20831,yunsu kim,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT."
D19-1080,Pivot-based Transfer Learning for Neural Machine Translation between Non-{E}nglish Languages,2019,0,5,5,1,20831,yunsu kim,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6{\%} BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios."
D19-1133,uniblock: Scoring and Filtering Corpus with {U}nicode Block Information,2019,22,0,3,1,12422,yingbo gao,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language Modeling and Machine Translation, and show the simplicity and effectiveness of our method."
W18-6409,The {RWTH} {A}achen {U}niversity {E}nglish-{G}erman and {G}erman-{E}nglish Unsupervised Neural Machine Translation Systems for {WMT} 2018,2018,0,1,5,1,20832,miguel gracca,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the unsupervised neural machine translation (NMT) systems of the RWTH Aachen University developed for the English â German news translation task of the \textit{EMNLP 2018 Third Conference on Machine Translation} (WMT 2018). Our work is based on iterative back-translation using a shared encoder-decoder NMT model. We extensively compare different vocabulary types, word embedding initialization schemes and optimization methods for our model. We also investigate gating and weight normalization for the word embedding layer."
W18-6426,The {RWTH} {A}achen {U}niversity Supervised Machine Translation Systems for {WMT} 2018,2018,0,2,6,1,23895,julian schamper,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the statistical machine translation systems developed at RWTH Aachen University for the GermanâEnglish, EnglishâTurkish and ChineseâEnglish translation tasks of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use ensembles of neural machine translation systems based on the Transformer architecture. Our main focus is on the GermanâEnglish task where we to all automatic scored first with respect metrics provided by the organizers. We identify data selection, fine-tuning, batch size and model dimension as important hyperparameters. In total we improve by 6.8{\%} BLEU over our last year{'}s submission and by 4.8{\%} BLEU over the winning system of the 2017 GermanâEnglish task. In EnglishâTurkish task, we show 3.6{\%} BLEU improvement over the last year{'}s winning system. We further report results on the ChineseâEnglish task where we improve 2.2{\%} BLEU on average over our baseline systems but stay behind the 2018 winning systems."
W18-6487,The {RWTH} {A}achen {U}niversity Filtering System for the {WMT} 2018 Parallel Corpus Filtering Task,2018,0,4,6,0,24233,nick rossenbach,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the submission of RWTH Aachen University for the DeâEn parallel corpus filtering task of the \textit{EMNLP 2018 Third Conference on Machine Translation} (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2{\%} BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8{\%} BLEU."
W18-6310,Improving Neural Language Models with Weight Norm Initialization and Regularization,2018,0,2,3,1,3259,christian herold,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such matrices in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a grid search. With this method, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on Penn Treebank (PTB) and from 61.45 to 60.13 on WikiText-2 (WT2)."
W18-6318,On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation,2018,25,1,3,1,18813,tamer alkhouli,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"This work investigates the alignment problem in state-of-the-art multi-head attention models based on the transformer architecture. We demonstrate that alignment extraction in transformer models can be improved by augmenting an additional alignment head to the multi-head source-to-target attention component. This is used to compute sharper attention weights. We describe how to use the alignment head to achieve competitive performance. To study the effect of adding the alignment head, we simulate a dictionary-guided translation task, where the user wants to guide translation using pre-defined dictionary entries. Using the proposed approach, we achieve up to 3.8{\%} BLEU improvement when using the dictionary, in comparison to 2.4{\%} BLEU in the baseline case. We also propose alignment pruning to speed up decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of 1.8 without loss in translation performance. We carry out experiments on the shared WMT 2016 EnglishâRomanian news task and the BOLT ChineseâEnglish discussion forum task."
P18-4022,{RETURNN} as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition,2018,25,21,3,0,29000,albert zeyer,"Proceedings of {ACL} 2018, System Demonstrations",0,"We compare the fast training and decoding speed of RETURNN of attention models for translation, due to fast CUDA LSTM kernels, and a fast pure TensorFlow beam search decoder. We show that a layer-wise pretraining scheme for recurrent attention models gives over 1{\%} BLEU improvement absolute and it allows to train deeper recurrent encoder networks. Promising preliminary results on max. expected BLEU training are presented. We are able to train state-of-the-art models for translation and end-to-end models for speech recognition and show results on WMT 2017 and Switchboard. The flexibility of RETURNN allows a fast research feedback loop to experiment with alternative architectures, and its generality allows to use it on a wide range of applications."
P18-2060,Neural Hidden {M}arkov Model for Machine Translation,2018,0,9,5,1,5802,weiyue wang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 GermanâEnglish and ChineseâEnglish translation tasks."
D18-2015,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",2018,0,3,3,1,30412,janthorsten peter,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output.
D18-1101,Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder,2018,0,11,3,1,20831,yunsu kim,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation."
D18-1335,Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation,2018,23,1,3,1,5733,parnia bahar,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German{\textless}-{\textgreater}English."
W17-4711,Biasing Attention-Based Recurrent Neural Networks Using External Alignment Information,2017,20,12,2,1,18813,tamer alkhouli,Proceedings of the Second Conference on Machine Translation,0,None
W17-4734,The {QT}21 Combined Machine Translation System for {E}nglish to {L}atvian,2017,0,0,2,1,30412,janthorsten peter,Proceedings of the Second Conference on Machine Translation,0,None
W17-4735,The {RWTH} {A}achen {U}niversity {E}nglish-{G}erman and {G}erman-{E}nglish Machine Translation System for {WMT} 2017,2017,7,1,8,1,30412,janthorsten peter,Proceedings of the Second Conference on Machine Translation,0,None
P17-2020,Hybrid Neural Network Alignment and Lexicon Model in Direct {HMM} for Statistical Machine Translation,2017,5,2,4,1,5802,weiyue wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Recently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks. Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods. This work proposes a direct HMM with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm. The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0{\%} Bleu scores on two different translation tasks."
E17-2103,Unsupervised Training for Large Vocabulary Translation Using Sparse Lexicon and Word Classes,2017,0,1,3,1,20831,yunsu kim,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We address for the first time unsupervised training for a translation task with hundreds of thousands of vocabulary words. We scale up the expectation-maximization (EM) algorithm to learn a large translation table without any parallel text or seed lexicon. First, we solve the memory bottleneck and enforce the sparsity with a simple thresholding scheme for the lexicon. Second, we initialize the lexicon training with word classes, which efficiently boosts the performance. Our methods produced promising results on two large-scale unsupervised translation tasks."
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,3,1,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
W16-2321,The {RWTH} {A}achen {U}niversity {E}nglish-{R}omanian Machine Translation System for {WMT} 2016,2016,31,2,4,1,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the statistical machine translation system developed at RWTH Aachen University for the English!Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). We combined three different state-ofthe-art systems in a system combination: A phrase-based system, a hierarchical phrase-based system and an attentionbased neural machine translation system. The phrase-based and the hierarchical phrase-based systems make use of a language model trained on all available data, a language model trained on the bilingual data and a word class language model. In addition, we utilized a recurrent neural network language model and a bidirectional recurrent neural network translation model for reranking the output of both systems. The attention-based neural machine translation system was trained using all bilingual data together with the backtranslated data from the News Crawl 2015"
W16-2342,{C}harac{T}er: Translation Edit Rate on Character Level,2016,9,31,4,1,5802,weiyue wang,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2206,Alignment-Based Neural Machine Translation,2016,34,15,6,1,18813,tamer alkhouli,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
W16-2212,A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing,2016,10,1,4,1,20831,yunsu kim,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-2048,Exponentially Decaying Bag-of-Words Input Features for Feed-Forward Neural Network in Statistical Machine Translation,2016,2,1,3,1,30412,janthorsten peter,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
W15-3018,The {RWTH} {A}achen {G}erman-{E}nglish Machine Translation System for {WMT} 2015,2015,23,1,4,1,30412,janthorsten peter,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the statistical machine translation system developed at RWTH Aachen University for the German!English translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). A phrase-based machine translation system was applied and augmented with hierarchical phrase reordering and word class language models. Further, we ran discriminative maximum expected BLEU training for our system. In addition, we utilized multiple feed-forward neural network language and translation models and a recurrent neural network language model for reranking."
W15-3033,Extended Translation Models in Phrase-based Decoding,2015,37,3,5,1,18814,andreas guta,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We propose a novel extended translation model (ETM) to counteract some problems in phrase-based translation: The lack of translation context when using singleword phrases and uncaptured dependencies beyond phrase boundaries. The ETM operates on word-level and augments the IBM models by an additional bilingual word pair and a reordering operation. Its implementation in a phrase-based decoder introduces translation and reordering dependencies for single-word phrases and dependencies across phrase boundaries. More, the model incorporates an explicit treatment of multiple and empty alignments. Its integration outperforms competitive systems that include lexical and phrase translation models as well as hierarchical reordering models on 4 language pairs significantly by 0.7% BLEU on average. Although simpler and using fewer dependencies, the ETM proves to be on par with 7-gram operation sequence models (Durrani et al., 2013b)."
W15-3034,Investigations on Phrase-based Decoding with Recurrent Neural Network Language and Translation Models,2015,30,9,3,1,18813,tamer alkhouli,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This work explores the application of recurrent neural network (RNN) language and translation models during phrasebased decoding. Due to their use of unbounded context, the decoder integration of RNNs is more challenging compared to the integration of feedforward neural models. In this paper, we apply approximations and use caching to enable RNN decoder integration, while requiring reasonable memory and time resources. We analyze the effect of caching on translation quality and speed, and use it to integrate RNN language and translation models into a phrase-based decoder. To the best of our knowledge, no previous work has discussed the integration of RNN translation models into phrase-based decoding. We also show that a special RNN can be integrated efficiently without the need for approximations. We compare decoding using RNNs to rescoring n-best lists on two tasks: IWSLT 2013 Germanxe2x86x92English, and BOLT Arabicxe2x86x92English. We demonstrate that the performance of decoding with RNNs is at least as good as using them in rescoring."
W15-3060,Local System Voting Feature for Machine Translation System Combination,2015,24,5,5,1,3519,markus freitag,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence."
P15-2090,{UNRAVEL}{---}{A} Decipherment Toolkit,2015,-1,-1,3,1,37443,malte nuhn,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,None
N15-1175,A Comparison of Update Strategies for Large-Scale Maximum Expected {BLEU} Training,2015,43,3,5,1,7150,joern wuebker,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This work presents a flexible and efficient discriminative training approach for statistical machine translation. We propose to use the RPROP algorithm for optimizing a maximum expected BLEU objective and experimentally compare it to several other updating schemes. It proves to be more efficient and effective than the previously proposed growth transformation technique and also yields better results than stochastic gradient descent and AdaGrad. We also report strong empirical results on two large scale tasks, namely BOLT Chinese!English and WMT German!English, where our final systems outperform results reported by Setiawan and Zhou (2013) and on matrix.statmt.org. On the WMT task, discriminative training is performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature."
D15-1165,A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences,2015,36,10,5,1,18814,andreas guta,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German!English, WMT German!English and BOLT Chinese!English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU."
2015.iwslt-evaluation.2,The {RWTH} {A}achen machine translation system for {IWSLT} 2015,2015,-1,-1,6,1,30412,janthorsten peter,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
W14-4001,Vector Space Models for Phrase-based Machine Translation,2014,30,6,3,1,18813,tamer alkhouli,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"This paper investigates the application of vector space models (VSMs) to the standard phrase-based machine translation pipeline. VSMs are models based on continuous word representations embedded in a vector space. We exploit word vectors to augment the phrase table with new inferred phrase pairs. This helps reduce out-of-vocabulary (OOV) words. In addition, we present a simple way to learn bilingually-constrained phrase vectors. The phrase vectors are then used to provide additional scoring of phrase pairs, which fits into the standard log-linear framework of phrase-based statistical machine translation. Both methods result in significant improvements over a competitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013."
W14-3310,{EU-BRIDGE} {MT}: Combined Machine Translation,2014,59,18,4,1,3519,markus freitag,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, Germanxe2x86x92English and Englishxe2x86x92German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems."
W14-3317,The {RWTH} {A}achen {G}erman-{E}nglish Machine Translation System for {WMT} 2014,2014,27,3,4,1,27027,stephan peitz,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the German!English translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models. For the phrase-based system, we run discriminative phrase training. In addition, we describe our preprocessing pipeline for German!English."
W14-3359,Unsupervised Adaptation for Statistical Machine Translation,2014,19,3,2,1,2928,saab mansour,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"In this work, we tackle the problem of language and translation models domainadaptation without explicit bilingual indomain training data. In such a scenario, the only information about the domain can be induced from the source-language test corpus. We explore unsupervised adaptation, where the source-language test corpus is combined with the corresponding hypotheses generated by the translation system to perform adaptation. We compare unsupervised adaptation to supervised and pseudo supervised adaptation. Our results show that the choice of the adaptation (target) set is crucial for successful application of adaptation methods. Evaluation is conducted over the German-to-English WMT newswire translation task. The experiments show that the unsupervised adaptation method generates the best translation quality as well as generalizes well to unseen test sets."
W14-0808,{G}erman Compounds and Statistical Machine Translation. Can they get along?,2014,24,3,3,0,4995,carla escartin,Proceedings of the 10th Workshop on Multiword Expressions ({MWE}),0,"This paper reports different experiments created to study the impact of using linguistics to preprocess German compounds prior to translation in Statistical Machine Translation (SMT). Compounds are a known challenge both in Machine Translation (MT) and Translation in general as well as in other Natural Language Processing (NLP) applications. In the case of SMT, German compounds are split into their constituents to decrease the number of unknown words and improve the results of evaluation measures like the Bleu score. To assess to which extent it is necessary to deal with German compounds as a part of preprocessing in SMT systems, we have tested different compound splitters and strategies, such as adding lists of compounds and their translations to the training set. This paper summarizes the results of our experiments and attempts to yield better translations of German nominal compounds into Spanish and shows how our approach improves by up to 1.4 Bleu points with respect to the baseline."
P14-2123,{EM} Decipherment for Large Vocabularies,2014,13,7,2,1,37443,malte nuhn,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper addresses the problem of EMbased decipherment for large vocabularies. Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type. As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are large, so the usualO(NV 2 ) exact EM approach is infeasible. When faced with this situation, many people turn to sampling. However, we propose to use a type of approximate EM and show that it works well. The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice. The subset is different for each iteration of EM. One option is to use beam search to do the subsetting. The second method restricts the successor words that are looked at, for each hypothesis. It does this by consulting pre-computed tables of likely n-grams and likely substitutions."
forster-etal-2014-extensions,Extensions of the Sign Language Recognition and Translation Corpus {RWTH}-{PHOENIX}-Weather,2014,19,34,5,1,39719,jens forster,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper introduces the RWTH-PHOENIX-Weather 2014, a video-based, large vocabulary, German sign language corpus which has been extended over the last two years, tripling the size of the original corpus. The corpus contains weather forecasts simultaneously interpreted into sign language which were recorded from German public TV and manually annotated using glosses on the sentence level and semi-automatically transcribed spoken German extracted from the videos using the open-source speech recognition system RASR. Spatial annotations of the signers{'} hands as well as shape and orientation annotations of the dominant hand have been added for more than 40k respectively 10k video frames creating one of the largest corpora allowing for quantitative evaluation of object tracking algorithms. Further, over 2k signs have been annotated using the SignWriting annotation system, focusing on the shape, orientation, movement as well as spatial contacts of both hands. Finally, extended recognition and translation setups are defined, and baseline results are presented."
E14-4034,Simple and Effective Approach for Consistent Training of Hierarchical Phrase-based Translation Models,2014,24,1,3,1,27027,stephan peitz,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"In this paper, we present a simple approach for consistent training of hierarchical phrase-based translation models. In order to consistently train a translation model, we perform hierarchical phrasebased decoding on training data to find derivations between the source and target sentences. This is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German!English and English!French translation tasks. Our results show improvements of up to 1.6 points BLEU."
E14-2008,{J}ane: Open Source Machine Translation System Combination,2014,14,17,3,1,3519,markus freitag,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they yield. System combination is a method for combining the output of multiple machine translation engines in order to take benefit of the strengths of each of the individual engines. In this work we introduce a novel system combination implementation which is integrated into Jane, RWTHxe2x80x99s open source statistical machine translation toolkit. On the most recent Workshop on Statistical Machine Translation system combination shared task, we achieve improvements of up to 0.7 points in BLEU over the best system combination hypotheses which were submitted for the official evaluation. Moreover, we enhance our system combination pipeline with additional n-gram language models and lexical translation models."
D14-1003,Translation Modeling with Bidirectional Recurrent Neural Networks,2014,42,96,4,0,40094,martin sundermeyer,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This work presents two different translation models using recurrent neural networks. The first one is a word-based approach using word alignments. Second, we present phrase-based translation models that are more consistent with phrasebased decoding. Moreover, we introduce bidirectional recurrent neural models to the problem of machine translation, allowing us to use the full source sentence in our models, which is also of theoretical interest. We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks: IWSLT 2013 German!English, BOLT Arabic!English and Chinese!English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists."
D14-1184,Improved Decipherment of Homophonic Ciphers,2014,9,3,3,1,37443,malte nuhn,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper, we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. (2013): An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred: The search effort is reduced from several hours of computation time to just a few seconds on a single CPU. These improvements allow us to successfully decipher the second part of the famous Beale cipher (see (Ward et al., 1885) and e.g. (King, 1993)): Having 182 different cipher symbols while having a length of just 762 symbols, the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac408 cipher (length 408, 54 different symbols). To the best of our knowledge, this cipher has not been deciphered automatically before."
2014.iwslt-papers.17,Better punctuation prediction with hierarchical phrase-based translation,2014,-1,-1,3,1,27027,stephan peitz,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Punctuation prediction is an important task in spoken language translation and can be performed by using a monolingual phrase-based translation system to translate from unpunctuated to text with punctuation. However, a punctuation prediction system based on phrase-based translation is not able to capture long-range dependencies between words and punctuation marks. In this paper, we propose to employ hierarchical translation in place of phrase-based translation and show that this approach is more robust for unseen word sequences. Furthermore, we analyze different optimization criteria for tuning the scaling factors of a monolingual statistical machine translation system. In our experiments, we compare the new approach with other punctuation prediction methods and show improvements in terms of F1-Score and BLEU on the IWSLT 2014 GermanâEnglish and EnglishâFrench translation tasks."
2014.iwslt-evaluation.7,Combined spoken language translation,2014,55,6,4,1,3519,markus freitag,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the GermanâEnglish spoken language translation (SLT) track as well as to the GermanâEnglish, EnglishâGerman and EnglishâFrench machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system."
2014.iwslt-evaluation.22,The {RWTH} {A}achen machine translation systems for {IWSLT} 2014,2014,-1,-1,4,1,7150,joern wuebker,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2014. We participated in both the MT and SLT tracks for the EnglishâFrench and GermanâEnglish language pairs and applied the identical training pipeline and models on both language pairs. Our state-of-the-art phrase-based baseline systems are augmented with maximum expected BLEU training for phrasal, lexical and reordering models. Further, we apply rescoring with novel recurrent neural language and translation models. The same systems are used for the SLT track, where we additionally perform punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. We are able to improve RWTH{'}s 2013 evaluation systems by 1.7-1.8{\%} BLEU absolute."
2014.amta-researchers.15,Comparison of data selection techniques for the translation of video lectures,2014,30,1,2,1,7150,joern wuebker,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"For the task of online translation of scientific video lectures, using huge models is not possible. In order to get smaller and efficient models, we perform data selection. In this paper, we perform a qualitative and quantitative comparison of several data selection techniques, based on cross-entropy and infrequent n-gram criteria. In terms of BLEU, a combination of translation and language model cross-entropy achieves the most stable results. As another important criterion for measuring translation quality in our application, we identify the number of out-of-vocabulary words. Here, infrequent n-gram recovery shows superior performance. Finally, we combine the two selection techniques in order to benefit from both their strengths."
W13-3908,Improving Continuous Sign Language Recognition: Speech Recognition Techniques and System Design,2013,30,13,5,1,39719,jens forster,Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies,0,"Automatic sign language recognition (ASLR) is a special case of automatic speech recognition (ASR) and computer vision (CV) and is currently evolving from using artificial labgenerated data to using xe2x80x99real-lifexe2x80x99 data. Although ASLR still struggles with feature extraction, it can benefit from techniques developed for ASR. We present a large-vocabulary ASLR system that is able to recognize sentences in continuous sign language and uses features extracted from standard single-view video cameras without using additional equipment. ASR techniques such as the multi-layer-perceptron (MLP) tandem approach, speaker adaptation, pronunciation modelling, and parallel hidden Markov models are investigated. We evaluate the influence of each system component on the recognition performance. On two publicly available large vocabulary databases representing lab-data (25 signer, 455 sign vocabulary, 19k sentence) and unconstrained xe2x80x99real-lifexe2x80x99 sign language (1 signer, 266 sign vocabulary, 351 sentences) we can achieve 22.1% respectively 38.6% WER."
W13-2802,Statistical {MT} Systems Revisited: How much Hybridity do they have?,2013,0,0,1,1,3262,hermann ney,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"The statistical approach to MT started about twenty-five years ago and has now been widely accepted as an alternative to the classical approach with manually designed rules. Among the attractive properties of the statistical approach is its capability to learn the translation models automatically from a (sufficiently) large amount of sourcetarget sentence pairs. Thus the need for the manual design of suitable rules and for human interaction can be reduced dramatically when developing an MT system for a new application or language pair. The idea of hybrid MT is to combine the advantages of both the rule-based and statistical approaches. In practice, most statistical MT systems make use of manually designed rules in order to improve the MT accuracy. We revisit the RWTH systems in order to study the effect of typical preprocessing steps based on manually designed rules. The RWTH systems cover various tasks (e.g. news, patents, lectures) and various languages (e.g. Arabic, Chinese, English, Japanese). The preprocessing steps may include a categorization of numbers, date and time expressions, a word decomposition based on morphological analysis and explicit word re-ordering based on a syntactic analysis. In general, the preprocessing steps may depend heavily on the language pair under consideration. We will also address concepts that aim at a tighter integration of the conventional rule-based and the statistical approaches. We will consider the implications of such a tight integration for the architecture of an MT system."
W13-2223,Joint {WMT} 2013 Submission of the {QUAERO} Project,2013,34,4,5,1,27027,stephan peitz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes the joint submission of the QUAERO project for the German!English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTHxe2x80x99s system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation."
W13-2224,The {RWTH} {A}achen Machine Translation System for {WMT} 2013,2013,-1,-1,8,1,27027,stephan peitz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,None
W13-2238,Length-Incremental Phrase Training for {SMT},2013,27,2,2,1,7150,joern wuebker,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present an iterative technique to generate phrase tables for SMT, which is based on force-aligning the training data with a modified translation decoder. Different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation. During training, we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration. Experiments are carried out on the IWSLT 2011 Arabic-English task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method. The resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics. By interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5% BLEU and 0.5% TER."
W13-2258,A Phrase Orientation Model for Hierarchical Machine Translation,2013,31,17,4,1,5061,matthias huck,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST Chinese!English translation task. We achieve a significant improvement of 1.2 %BLEU over a typical hierarchical baseline setup and an improvement of 0.7 %BLEU over a syntax-augmented hierarchical setup. On a French!German translation task, we obtain a gain of up to 0.4 %BLEU."
W13-0804,A Performance Study of Cube Pruning for Large-Scale Hierarchical Machine Translation,2013,36,3,4,1,5061,matthias huck,"Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this paper, we empirically investigate the impact of critical configuration parameters in the popular cube pruning algorithm for decoding in hierarchical statistical machine translation. Specifically, we study how the choice of the k-best generation size affects translation quality and resource requirements in hierarchical search. We furthermore examine the influence of two different granularities of hypothesis recombination. Our experiments are conducted on the large-scale Chinese!English and Arabic!English NIST translation tasks. Besides standard hierarchical grammars, we also explore search with restricted recursion depth of hierarchical rules based on shallow-1 grammars."
P13-1032,Advancements in Reordering Models for Statistical Machine Translation,2013,31,10,3,1,36872,minwei feng,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported."
P13-1060,Decipherment Complexity in 1:1 Substitution Ciphers,2013,13,5,2,1,37443,malte nuhn,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper we show that even for the case of 1:1 substitution ciphersxe2x80x94which encipher plaintext symbols by exchanging them with a unique substitutexe2x80x94finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before."
P13-1154,Beam Search for Solving Substitution Ciphers,2013,11,16,3,1,37443,malte nuhn,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high ordern-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate."
N13-1074,Phrase Training Based Adaptation for Statistical Machine Translation,2013,11,9,2,1,2928,saab mansour,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a novel approach for translation model (TM) adaptation using phrase training. The proposed adaptation procedure is initialized with a standard general-domain TM, which is then used to perform phrase training on a smaller in-domain set. This way, we bias the probabilities of the general TM towards the in-domain distribution. Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones. Additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method."
D13-1138,Improving Statistical Machine Translation with Word Class Models,2013,17,30,4,1,7150,joern wuebker,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German!English and a larger French!German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French!German task and 0.3% BLEU and 1.1% TER on the German!English task."
2013.mtsummit-papers.19,(Hidden) Conditional Random Fields Using Intermediate Classes for Statistical Machine Translation,2013,-1,-1,4,1,2918,patrick lehnen,Proceedings of Machine Translation Summit XIV: Papers,0,None
2013.mtsummit-papers.20,Reverse Word Order Model,2013,-1,-1,5,1,3519,markus freitag,Proceedings of Machine Translation Summit XIV: Papers,0,None
2013.mtsummit-european.18,{SIGNSPEAK}: Scientific Understanding and Vision-based Technological Development for Continuous Sign Language Recognition and Translation,2013,-1,-1,3,1,39719,jens forster,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.iwslt-papers.1,Using viseme recognition to improve a sign language translation system,2013,16,13,3,1,18156,christoph schmidt,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"Sign language-to-text translation systems are similar to spoken language translation systems in that they consist of a recognition phase and a translation phase. First, the video of a person signing is transformed into a transcription of the signs, which is then translated into the text of a spoken language. One distinctive feature of sign languages is their multi-modal nature, as they can express meaning simultaneously via hand movements, body posture and facial expressions. In some sign languages, certain signs are accompanied by mouthings, i.e. the person silently pronounces the word while signing. In this work, we closely integrate a recognition and translation framework by adding a viseme recognizer ({``}lip reading system{''}) based on an active appearance model and by optimizing the recognition system to improve the translation output. The system outperforms the standard approach of separate recognition and translation."
2013.iwslt-evaluation.10,The {RWTH} {A}achen machine translation systems for {IWSLT} 2013,2013,-1,-1,7,1,7150,joern wuebker,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2013. We participated in the EnglishâFrench, EnglishâGerman, ArabicâEnglish, ChineseâEnglish and SlovenianâEnglish MT tracks and the EnglishâFrench and EnglishâGerman SLT tracks. We apply phrase-based and hierarchical SMT decoders, which are augmented by state-of-the-art extensions. The novel techniques we experimentally evaluate include discriminative phrase training, a continuous space language model, a hierarchical reordering model, a word class language model, domain adaptation via data selection and system combination of standard and reverse order models. By application of these methods we can show considerable improvements over the respective baseline systems."
2013.iwslt-evaluation.15,The {RWTH} {A}achen {G}erman and {E}nglish {LVCSR} systems for {IWSLT}-2013,2013,31,2,7,0,41945,ali shaik,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, German and English large vocabulary continuous speech recognition (LVCSR) systems developed by the RWTH Aachen University for the IWSLT-2013 evaluation campaign are presented. Good improvements are obtained with state-of-the-art monolingual and multilingual bottleneck features. In addition, an open vocabulary approach using morphemic sub-lexical units is investigated along with the language model adaptation for the German LVCSR. For both the languages, competitive WERs are achieved using system combination."
2013.iwslt-evaluation.16,{EU}-{BRIDGE} {MT}: text translation of talks in the {EU}-{BRIDGE} project,2013,52,8,4,1,3519,markus freitag,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, EnglishâFrench and GermanâEnglish. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems."
W12-5903,A Tagging-style Reordering Model for Phrase-based {SMT},2012,20,0,2,1,36872,minwei feng,Proceedings of the Workshop on Reordering for Statistical Machine Translation,0,"For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have significant word order differences. In this paper we propose a novel tagging-style reordering model. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically covered in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch long dependencies in the source sentence. The decoder makes use of the tagging information as soft constraints so that in the test phase (during translation) our model is very efficient. The model training on large scale tasks requests notably amounts of computational resources. We carried out experiments on five Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 0.98 BLEU 1.21 TER on average."
W12-3124,Review of Hypothesis Alignment Algorithms for {MT} System Combination via Confusion Network Decoding,2012,36,6,8,0,42249,anttiveikko rosti,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding. Controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented. Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences. All aligners yield significant BLEU score gains over the best individual system included in the combination. Incremental indirect hidden Markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small."
W12-3137,The {RWTH} {A}achen Machine Translation System for {WMT} 2012,2012,-1,-1,5,1,5061,matthias huck,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,None
W12-3140,Joint {WMT} 2012 Submission of the {QUAERO} Project,2012,37,5,4,1,3519,markus freitag,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system."
W12-3157,Phrase Model Training for Statistical Machine Translation with Word Lattices of Preprocessing Alternatives,2012,32,3,2,1,7150,joern wuebker,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. On the news-commentary portion of the Germanxe2x86x92English WMT 2011 task we can show moderate improvements of up to 0.6% Bleu over a state-of-the-art baseline system."
P12-2006,Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation,2012,13,4,2,1,7150,joern wuebker,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% Bleu, whereas our approach yields 20.0% with identical models."
P12-1017,Deciphering Foreign Language by Combining Language Models and Context Vectors,2012,18,34,3,1,37443,malte nuhn,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper we show how to train statistical machine translation systems on real-life tasks using only non-parallel monolingual data from two languages. We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model. The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus. We also report results using data from the monolingual French and English GIGAWORD corpora."
N12-1035,Insertion and Deletion Models for Statistical Machine Translation,2012,17,6,2,1,5061,matthias huck,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We investigate insertion and deletion models for hierarchical phrase-based statistical machine translation. Insertion and deletion models are designed as a means to avoid the omission of content words in the hypotheses. In our case, they are implemented as phrase-level feature functions which count the number of inserted or deleted words. An English word is considered inserted or deleted based on lexical probabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the NIST Chinesexe2x86x92English translation task."
mansour-ney-2012-arabic,{A}rabic-Segmentation Combination Strategies for Statistical Machine Translation,2012,16,0,2,1,2928,saab mansour,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Arabic segmentation was already applied successfully for the task of statistical machine translation (SMT). Yet, there is no consistent comparison of the effect of different techniques and methods over the final translation quality. In this work, we use existing tools and further re-implement and develop new methods for segmentation. We compare the resulting SMT systems based on the different segmentation methods over the small IWSLT 2010 BTEC and the large NIST 2009 Arabic-to-English translation tasks. Our results show that for both small and large training data, segmentation yields strong improvements, but, the differences between the top ranked segmenters are statistically insignificant. Due to the different methodologies that we apply for segmentation, we expect a complimentary variation in the results achieved by each method. As done in previous work, we combine several segmentation schemes of the same model but achieve modest improvements. Next, we try a different strategy, where we combine the different segmentation methods rather than the different segmentation schemes. In this case, we achieve stronger improvements over the best single system. Finally, combining schemes and methods has another slight gain over the best combination strategy."
forster-etal-2012-rwth,{RWTH}-{PHOENIX}-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus,2012,12,45,7,1,39719,jens forster,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrastto most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented."
C12-3061,{J}ane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation,2012,36,34,8,1,7150,joern wuebker,Proceedings of {COLING} 2012: Demonstration Papers,0,"We present Jane 2, an open source toolkit supporting both the phrase-based and the hierarchical phrase-based paradigm for statistical machine translation. It is implemented in C and provides efficient decoding algorithms and data structures. This work focuses on the description of its phrase-based functionality. In addition to the standard pipeline, including phrase extraction and parameter optimization, Jane 2 contains several state-of-the-art extensions and tools. Forced alignment phrase training can considerably reduce rule table size while learning the translation scores in a more principled manner. Word class language models can be used to integrate longer context with a reduced vocabulary size. Rule table interpolation is applicable for different tasks, e.g. domain adaptation. The decoder distinguishes between lexical and coverage pruning and applies reordering constraints for efficiency."
C12-2091,Forced Derivations for Hierarchical Machine Translation,2012,23,4,4,1,27027,stephan peitz,Proceedings of {COLING} 2012: Posters,0,"We present an efficient framework to estimate the rule probabilities for a hierarchical phrasebased statistical machine translation system from parallel data. In previous work, this was done with bilingual parsing. We use a more efficient approach splitting the bilingual parsing into two stages, which allows us to train a hierarchical translation model on larger tasks. Furthermore, we apply leave-one-out to counteract over-fitting and use the expected count from the inside-outside algorithm to prune the rule set. On the WMT12 Europarl Germanxe2x86x92English and Frenchxe2x86x92English tasks, we improve translation quality by up to 1.0 BLEU and 0.9 TER while simultaneously reducing the rule set to 5% of the original size."
C12-1053,Semantic Cohesion Model for Phrase-Based {SMT},2012,20,8,3,1,36872,minwei feng,Proceedings of {COLING} 2012,0,"In this paper, we propose a novel semantic cohesion model. Our model utilizes the predicateargument structures as soft constraints and plays the role as a reordering model in the phrasebased statistical machine translation system. We build a translation system with GALE data. Experimental results on the NIST02, NIST03, NIST04, NIST05 and NIST08 Chinese-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation."
2012.iwslt-papers.7,A simple and effective weighted phrase extraction for machine translation adaptation,2012,20,19,2,1,2928,saab mansour,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news) to maximize the performance on the test domain (e.g. weblogs). In previous work, weighting the training instances was used for filtering dissimilar data. We extend this by incorporating the weights directly into the standard phrase training procedure of statistical machine translation (SMT). This allows the SMT system to make the decision whether to use a phrase translation pair or not, a more methodological way than discarding phrase pairs completely when using filtering. Furthermore, we suggest a combined filtering and weighting procedure to achieve better results while reducing the phrase table size. The proposed methods are evaluated in the context of Arabicto-English translation on various conditions, where significant improvements are reported when using the suggested weighted phrase training. The weighting method also improves over filtering, and the combined filtering and weighting is better than a standalone filtering method. Finally, we experiment with mixture modeling, where additional improvements are reported when using weighted phrase extraction over a variety of baselines."
2012.iwslt-papers.16,Sequence labeling-based reordering model for phrase-based {SMT},2012,29,1,3,1,36872,minwei feng,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have significant word order differences. In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically implanted in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch the long dependency in the source sentence. Although the learning on large scale task requests notably amounts of computational resources, the decoder makes use of the tagging information as soft constraints. Therefore, the training procedure of our model is computationally expensive for large task while in the test phase (during translation) our model is very efficient. We carried out experiments on five Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 1.32 BLEU 1.53 TER on average."
2012.iwslt-papers.18,Spoken language translation using automatically transcribed text in training,2012,14,9,4,1,27027,stephan peitz,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,In spoken language translation a machine translation system takes speech as input and translates it into another language. A standard machine translation system is trained on written language data and expects written language as input. In this paper we propose an approach to close the gap between the output of automatic speech recognition and the input of machine translation by training the translation system on automatically transcribed speech. In our experiments we show improvements of up to 0.9 BLEU points on the IWSLT 2012 English-to-French speech translation task.
2012.iwslt-evaluation.7,The {RWTH} {A}achen speech recognition and machine translation system for {IWSLT} 2012,2012,36,2,9,1,27027,stephan peitz,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese-English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems."
2012.eamt-1.66,Discriminative Reordering Extensions for Hierarchical Phrase-Based Machine Translation,2012,17,12,4,1,5061,matthias huck,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"In this paper, we propose novel extensions of hierarchical phrase-based systems with a discriminative lexicalized reordering model. We compare different feature sets for the discriminative reordering model and investigate combinations with three types of non-lexicalized reordering rules which are added to the hierarchical grammar in order to allow for more reordering flexibility during decoding. All extensions are evaluated in standard hierarchical setups as well as in setups where the hierarchical recursion depth is restricted. We achieve improvements of up to 1.2 %BLEU on a large-scale Chinese!English translation task."
2012.amta-papers.8,Pivot Lightly-Supervised Training for Statistical Machine Translation,2012,-1,-1,2,1,5061,matthias huck,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this paper, we investigate large-scale lightly-supervised training with a pivot language: We augment a baseline statistical machine translation (SMT) system that has been trained on human-generated parallel training corpora with large amounts of additional unsupervised parallel data; but instead of creating this synthetic data from monolingual source language data with the baseline system itself, or from target language data with a reverse system, we employ a parallel corpus of target language data and data in a pivot language. The pivot language data is automatically translated into the source language, resulting in a trilingual corpus with unsupervised source language side. We augment our baseline system with the unsupervised source-target parallel data. Experiments are conducted for the German-French language pair using the standard WMT newstest sets for development and testing. We obtain the unsupervised data by translating the English side of the English-French 109 corpus to German. With careful system design, we are able to achieve improvements of up to +0.4 points BLEU / -0.7 points TER over the baseline."
W11-2211,Lightly-Supervised Training for Hierarchical Phrase-Based Machine Translation,2011,23,7,4,1,5061,matthias huck,Proceedings of the First workshop on Unsupervised Learning in {NLP},0,In this paper we apply lightly-supervised training to a hierarchical phrase-based statistical machine translation system. We employ bitexts that have been built by automatically translating large amounts of monolingual data as additional parallel training corpora. We explore different ways of using this additional data to improve our system.n n Our results show that integrating a second translation model with only non-hierarchical phrases extracted from the automatically generated bitexts is a reasonable approach. The translation performance matches the result we achieve with a joint extraction on all training bitexts while the system is kept smaller due to a considerably lower overall number of phrases.
W11-2118,The {RWTH} System Combination System for {WMT} 2011,2011,19,3,3,1,29312,gregor leusch,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"RWTH participated in the System Combination task of the Sixth Workshop on Statistical Machine Translation (WMT 2011).n n For three language pairs, we combined 6 to 14 systems into a single consensus translation. A three-level meta-combination scheme combining six different system combination setups with three different engines was applied on the French--English language pair. Depending on the language pair, improvements versus the best single system are in the range of 1.9% and 2.5% abs. on BLEU, and between xe2x88x921.8% and xe2x88x922.4% abs. on TER. Novel techniques compared with RWTH's submission to WMT 2010 include two additional system combination engines, an additional word alignment technique, meta combination, and additional optimization techniques."
W11-2142,Joint {WMT} Submission of the {QUAERO} Project,2011,25,1,5,1,3519,markus freitag,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach."
W11-2149,The {RWTH} {A}achen Machine Translation System for {WMT} 2011,2011,13,3,10,1,5061,matthias huck,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We describe our system for the news commentary translation task of WMT 2011. The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories. We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and Rover to combine n-best list of hypotheses output by different systems."
J11-4002,Towards Automatic Error Analysis of Machine Translation Output,2011,23,61,2,1,5059,maja popovic,Computational Linguistics,0,"Evaluation and error analysis of machine translation output are important but difficult tasks. In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems. The proposed approach enables the use of various types of linguistic knowledge in order to classify translation errors in many different ways. This work focuses on one possible set-up, namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices. For each of the categories, we analyze the contribution of various POS classes. We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors for a given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements. We used Arabic-English Newswire and Broadcast News and Chinese-English Newswire outputs created in the framework of the GALE project, several Spanish and English European Parliament outputs generated during the TC-Star project, and three German-English outputs generated in the framework of the fourth Machine Translation Workshop. We show that our results correlate very well with the results of a human error analysis, and that all our metrics except the extra words reflect well the differences between different versions of the same translation system as well as the differences between different translation systems."
2011.iwslt-papers.1,Lexicon models for hierarchical phrase-based machine translation,2011,26,7,4,1,5061,matthias huck,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"In this paper, we investigate lexicon models for hierarchical phrase-based statistical machine translation. We study five types of lexicon models: a model which is extracted from word-aligned training data and{---}given the word alignment matrix{---}relies on pure relative frequencies [1]; the IBM model 1 lexicon [2]; a regularized version of IBM model 1; a triplet lexicon model variant [3]; and a discriminatively trained word lexicon model [4]. We explore sourceto-target models with phrase-level as well as sentence-level scoring and target-to-source models with scoring on phrase level only. For the first two types of lexicon models, we compare several scoring variants. All models are used during search, i.e. they are incorporated directly into the log-linear model combination of the decoder. Phrase table smoothing with triplet lexicon models and with discriminative word lexicons are novel contributions. We also propose a new regularization technique for IBM model 1 by means of the Kullback-Leibler divergence with the empirical unigram distribution as regularization term. Experiments are carried out on the large-scale NIST ChineseâEnglish translation task and on the EnglishâFrench and ArabicâEnglish IWSLT TED tasks. For ChineseâEnglish and EnglishâFrench, we obtain the best results by using the discriminative word lexicon to smooth our phrase tables."
2011.iwslt-papers.5,Combining translation and language model scoring for domain-specific data filtering,2011,17,25,3,1,2928,saab mansour,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"The increasing popularity of statistical machine translation (SMT) systems is introducing new domains of translation that need to be tackled. As many resources are already available, domain adaptation methods can be applied to utilize these recourses in the most beneficial way for the new domain. We explore adaptation via filtering, using the crossentropy scores to discard irrelevant sentences. We focus on filtering for two important components of an SMT system, namely the language model (LM) and the translation model (TM). Previous work has already applied LM cross-entropy based scoring for filtering. We argue that LM cross-entropy might be appropriate for LM filtering, but not as much for TM filtering. We develop a novel filtering approach based on a combined TM and LM cross-entropy scores. We experiment with two large-scale translation tasks, the Arabic-to-English and English-to-French IWSLT 2011 TED Talks MT tasks. For LM filtering, we achieve strong perplexity improvements which carry over to the translation quality with improvements up to +0.4{\%} BLEU. For TM filtering, the combined method achieves small but consistent improvements over the standalone methods. As a side effect of adaptation via filtering, the fully fledged SMT system vocabulary size and phrase table size are reduced by a factor of at least 2 while up to +0.6{\%} BLEU improvement is observed."
2011.iwslt-papers.7,Modeling punctuation prediction as machine translation,2011,13,35,4,1,27027,stephan peitz,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"Punctuation prediction is an important task in Spoken Language Translation. The output of speech recognition systems does not typically contain punctuation marks. In this paper we analyze different methods for punctuation prediction and show improvements in the quality of the final translation output. In our experiments we compare the different approaches and show improvements of up to 0.8 BLEU points on the IWSLT 2011 English French Speech Translation of Talks task using a translation system to translate from unpunctuated to punctuated text instead of a language model based punctuation prediction method. Furthermore, we do a system combination of the hypotheses of all our different approaches and get an additional improvement of 0.4 points in BLEU."
2011.iwslt-papers.8,Soft string-to-dependency hierarchical machine translation,2011,0,7,3,1,30412,janthorsten peter,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"In this paper, we dissect the influence of several target-side dependency-based extensions to hierarchical machine translation, including a dependency language model (LM). We pursue a non-restrictive approach that does not prohibit the production of hypotheses with malformed dependency structures. Since many questions remained open from previous and related work, we offer in-depth analysis of the influence of the language model order, the impact of dependency-based restrictions on the search space, and the information to be gained from dependency tree building during decoding. The application of a non-restrictive approach together with an integrated dependency LM scoring is a novel contribution which yields significant improvements for two large-scale translation tasks for the language pairs Chinese{--}English and German{--}French."
2011.iwslt-evaluation.14,The {RWTH} {A}achen machine translation system for {IWSLT} 2011,2011,-1,-1,8,1,7150,joern wuebker,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2011 is presented. We participated in the MT (English-French, Arabic-English, ChineseEnglish) and SLT (English-French) tracks. Both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated, including domain adaptation via monolingual and bilingual data selection, phrase training, different lexical smoothing methods, additional reordering models for the hierarchical system, various Arabic and Chinese segmentation methods, punctuation prediction for speech recognition output, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,7,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
2011.iwslt-evaluation.16,Speech recognition for machine translation in Quaero,2011,25,27,9,0,14730,lori lamel,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the speech-to-text systems used to provide automatic transcriptions used in the Quaero 2010 evaluation of Machine Translation from speech. Quaero (www.quaero.org) is a large research and industrial innovation program focusing on technologies for automatic analysis and classification of multimedia and multilingual documents. The ASR transcript is the result of a Rover combination of systems from three teams ( KIT, RWTH, LIMSI+VR) for the French and German languages. The casesensitive word error rates (WER) of the combined systems were respectively 20.8{\%} and 18.1{\%} on the 2010 evaluation data, relative WER reductions of 14.6{\%} and 17.4{\%} respectively over the best component system."
2011.eamt-1.37,Advancements in {A}rabic-to-{E}nglish Hierarchical Machine Translation,2011,20,4,4,1,5061,matthias huck,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"In this paper we study several advanced techniques and models for Arabic-toEnglish statistical machine translation. We examine how the challenges imposed by this particular language pair and translation direction can be successfully tackled within the framework of hierarchical phrase-based translation. We extend the state-of-the-art with a novel cross-system and cross-paradigm lightlysupervised training approach. In addition, for following recently developed techniques we provide a concise review, an empirical evaluation, and an in-depth analysis: soft syntactic labels, a discriminative word lexicon model, additional reorderings, and shallow rules. We thus bring together complementary methods that previously have only been investigated in isolation and mostly on different language pairs. Combinations of the methods yield significant improvements over a baseline using a usual set of models. The resulting hierarchical systems perform competitive on the large-scale NIST Arabic-to-English translation task."
W10-1711,The {RWTH} {A}achen Machine Translation System for {WMT} 2010,2010,61,32,7,0,37774,carmen heger,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems."
W10-1738,"{J}ane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models",2010,29,67,4,1,5800,david vilar,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We present Jane, RWTH's hierarchical phrase-based translation system, which has been open sourced for the scientific community. This system has been in development at RWTH for the last two years and has been successfully applied in different machine translation evaluations. It includes extensions to the hierarchical approach developed by RWTH as well as other research institutions. In this paper we give an overview of its main features.n n We also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance, and analyze the effect some recent extended lexicon models have on the performance of the system."
W10-1747,The {RWTH} System Combination System for {WMT} 2010,2010,16,8,2,1,29312,gregor leusch,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"RWTH participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). For 7 of the 8 language pairs, we combine 5 to 13 systems into a single consensus translation, using additional n-best reranking techniques in two of these language pairs. Depending on the language pair, improvements versus the best single system are in the range of 0.5 and 1.7 on BLEU, and between -0.4 and -2.3 on TER. Novel techniques compared with RWTH's submission to WMT 2009 include the utilization of n-best reranking techniques, a consensus true casing approach, a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing."
P10-1049,Training Phrase Translation Models with Leaving-One-Out,2010,20,78,3,1,7150,joern wuebker,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%."
N10-1104,A Hybrid Morphologically Decomposed Factored Language Models for {A}rabic {LVCSR},2010,7,11,3,0,45815,amr eldesoky,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this work, we try a hybrid methodology for language modeling where both morphological decomposition and factored language modeling (FLM) are exploited to deal with the complex morphology of Arabic language. At the end, we are able to obtain from 3.5% to 7.0% relative reduction in word error rate (WER) with respect to a traditional full-words system, and from 1.0% to 2.0% relative WER reduction with respect to a non-factored decomposed system."
dreuw-etal-2010-signspeak,The {S}ign{S}peak Project - Bridging the Gap Between Signers and Speakers,2010,24,24,2,1,46030,philippe dreuw,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The SignSpeak project will be the first step to approach sign language recognition and translation at a scientific level already reached in similar research fields such as automatic speech recognition or statistical machine translation of spoken languages. Deaf communities revolve around sign languages as they are their natural means of communication. Although deaf, hard of hearing and hearing signers can communicate without problems amongst themselves, there is a serious challenge for the deaf community in trying to integrate into educational, social and work environments. The overall goal of SignSpeak is to develop a new vision-based technology for recognizing and translating continuous sign language to text. New knowledge about the nature of sign language structure from the perspective of machine recognition of continuous sign language will allow a subsequent breakthrough in the development of a new vision-based technology for continuous sign language recognition and translation. Existing and new publicly available corpora will be used to evaluate the research progress throughout the whole project."
2010.iwslt-papers.11,A combination of hierarchical systems with forced alignments from phrase-based systems,2010,14,3,4,0,37774,carmen heger,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Currently most state-of-the-art statistical machine translation systems present a mismatch between training and generation conditions. Word alignments are computed using the well known IBM models for single-word based translation. Afterwards phrases are extracted using extraction heuristics, unrelated to the stochastic models applied for finding the word alignment. In the last years, several research groups have tried to overcome this mismatch, but only with limited success. Recently, the technique of forced alignments has shown to improve translation quality for a phrase-based system, applying a more statistically sound approach to phrase extraction. In this work we investigate the first steps to combine forced alignment with a hierarchical model. Experimental results on IWSLT and WMT data show improvements in translation quality of up to 0.7{\%} BLEU and 1.0{\%} TER."
2010.iwslt-papers.12,Multi-pivot translation by system combination,2010,28,10,4,1,29312,gregor leusch,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"This paper describes a technique to exploit multiple pivot languages when using machine translation (MT) on language pairs with scarce bilingual resources, or where no translation system for a language pair is available. The principal idea is to generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using MT system combination techniques. Our technique can also be applied when a translation system for a language pair is available, but is limited in its translation accuracy because of scarce resources. Using statistical MT systems for the 11 different languages of Europarl, we show experimentally that a direct translation system can be replaced by this pivot approach without a loss in translation quality if about six pivot languages are available. Furthermore, we can already improve an existing MT system by adding two pivot systems to it. The maximum improvement was found to be 1.4{\%} abs. in BLEU in our experiments for 8 or more pivot languages."
2010.iwslt-papers.17,Sign language machine translation overkill,2010,15,9,3,1,32003,daniel stein,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Sign languages represent an interesting niche for statistical machine translation that is typically hampered by the scarceness of suitable data, and most papers in this area apply only a few, well-known techniques and do not adapt them to small-sized corpora. In this paper, we will propose new methods for common approaches like scaling factor optimization and alignment merging strategies which helped improve our baseline. We also conduct experiments with different decoders and employ state-of-the-art techniques like soft syntactic labels as well as trigger-based and discriminative word lexica and system combination. All methods are evaluated on one of the largest sign language corpora available."
2010.iwslt-papers.18,If {I} only had a parser: poor man{'}s syntax for hierarchical machine translation,2010,17,9,4,1,5800,david vilar,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"In the last few years, several enhancements for the hierarchical phrase-based translation model have been proposed. They aim to include additional syntactic information in the translation process in order to achieve better fluency in the generated output. In this work we review and compare three such methods: parsematch, soft syntactic labels and string-to-dependency. Our goal is to find out if these models complement each other of if they rather address the same deficiencies in the translation process. Furthermore, we present a novel method for extending the translation model in the same direction without the need for parse trees, since they may not be available for some languages. Our approach is based only on automatic clustering of phrases, without the need for additional information. Our findings show that we are able to achieve similar results as when applying syntax models."
2010.iwslt-evaluation.22,The {RWTH} {A}achen machine translation system for {IWSLT} 2010,2010,61,32,5,1,2928,saab mansour,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the IWSLT 2010. This year, we participated in the BTEC translation task for the Arabic to English language direction. We experimented with two state-of-theart decoders: phrase-based and hierarchical-based decoders. Extensions to the decoders included phrase training (as opposed to heuristic phrase extraction) for the phrase-based decoder, and soft syntactic features for the hierarchical decoder. Additionally, we experimented with various rule-based and statistical-based segmenters for Arabic. Due to the different decoders and the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each system. The next step would be to exploit these variations and achieve better results by combining the systems. We try different strategies for system combination and report significant improvements over the best single system."
2010.amta-papers.8,A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation,2010,-1,-1,4,1,32003,daniel stein,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this work we review and compare three additional syntactic enhancements for the hierarchical phrase-based translation model, which have been presented in the last few years. We compare their performance when applied separately and study whether the combination may yield additional improvements. Our findings show that the models are complementary, and their combination achieve an increase of 1{\%} in BLEU and a reduction of nearly 2{\%} in TER. The models presented in this work are made available as part of the Jane open source machine translation toolkit."
2010.amta-papers.22,A Source-side Decoding Sequence Model for Statistical Machine Translation,2010,-1,-1,3,1,36872,minwei feng,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,We propose a source-side decoding sequence language model for phrase-based statistical machine translation. This model is a reordering model in the sense that it helps the decoder find the correct decoding sequence. The model uses word-aligned bilingual training data. We show improved translation quality of up to 1.34{\%} BLEU and 0.54{\%} TER using this model compared to three other widely used reordering models.
2010.amta-papers.32,A Comparison of Various Types of Extended Lexicon Models for Statistical Machine Translation,2010,-1,-1,4,1,5061,matthias huck,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this work we give a detailed comparison of the impact of the integration of discriminative and trigger-based lexicon models in state-of-the-art hierarchical and conventional phrase-based statistical machine translation systems. As both types of extended lexicon models can grow very large, we apply certain restrictions to discard some of the less useful information. We show how these restrictions facilitate the training of the extended lexicon models. We finally evaluate systems that incorporate both types of models with different restrictions on a large-scale translation task for the Arabic-English language pair. Our results suggest that extended lexicon models can be substantially reduced in size while still giving clear improvements in translation performance."
W09-0402,Syntax-Oriented Evaluation Measures for Machine Translation Output,2009,6,31,2,1,5059,maja popovic,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the Bleu score on the detailed Part-of-Speech (pos) tags as well as the precision, recall and F-measure obtained on pos n-grams. We also introduced F-measure based on both word and pos n-grams. Correlations between the new metrics and human judgments were calculated on the data of the first, second and third shared task of the Statistical Machine Translation Workshop. Machine translation outputs in four different European languages were taken into account: English, Spanish, French and German. The results show that the new measures correlate very well with the human judgements and that they are competitive with the widely used BLEU, METEOR and TER metrics."
W09-0407,The {RWTH} System Combination System for {WMT} 2009,2009,18,20,3,1,29312,gregor leusch,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"RWTH participated in the System Combination task of the Fourth Workshop on Statistical Machine Translation (WMT 2009). Hypotheses from 9 Germanxe2x86x92English MT systems were combined into a consensus translation. This consensus translation scored 2.1% better in Bleu and 2.3% better in Ter (abs.) than the best single system. In addition, cross-lingual output from 10 French, German, and Spanishxe2x86x92English systems was combined into a consensus translation, which gave an improvement of 2.0% in Bleu/3.5% in Ter (abs.) over the best single system."
W09-0410,The {RWTH} Machine Translation System for {WMT} 2009,2009,40,10,5,1,5059,maja popovic,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"RWTH participated in the shared translation task of the Fourth Workshop of Statistical Machine Translation (WMT 2009) with the German-English, French-English and Spanish-English pair in each translation direction. The submissions were generated using a phrase-based and a hierarchical statistical machine translation systems with appropriate morpho-syntactic enhancements. pos-based reorderings of the source language for the phrase-based systems and splitting of German compounds for both systems were applied. For some tasks, a system combination was used to generate a final hypothesis. An additional English hypothesis was produced by combining all three final systems for translation into English."
W09-0438,A Deep Learning Approach to Machine Transliteration,2009,18,90,4,0,47114,thomas deselaers,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task."
N09-2005,Comparison of Extended Lexicon Models in Search and Rescoring for {SMT},2009,6,11,2,1,34439,savsa hasan,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,We show how the integration of an extended lexicon model into the decoder can improve translation performance. The model is based on lexical triggers that capture long-distance dependencies on the sentence level. The results are compared to variants of the model that are applied in reranking of n-best lists. We present how a combined application of these models in search and rescoring gives promising results. Experiments are reported on the GALE Chinese-English task with improvements of up to 0.9% BLEU and -1.5% TER absolute on a competitive baseline.
J09-1002,Statistical Approaches to Computer-Assisted Translation,2009,57,140,8,0,47362,sergio barrachina,Computational Linguistics,0,"Current machine translation (MT) systems are still not perfect. In practice, the output from these systems needs to be edited to correct errors. A way of increasing the productivity of the whole translation process (MT plus human work) is to incorporate the human correction activities within the translation process itself, thereby shifting the MT paradigm to that of computer-assisted translation. This model entails an iterative process in which the human translator activity is included in the loop: In each iteration, a prefix of the translation is validated (accepted or amended) by the human and the system computes its best (or n-best) translation suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statistical (or pattern recognition) framework. Interestingly, within this framework, the adaptation of MT systems to the interactive scenario affects mainly the search process, allowing a great reuse of successful techniques and models. In this article, alignment templates, phrase-based models, and stochastic finite-state transducers are used to develop computer-assisted translation systems. These systems were assessed in a European project (TransType2) in two real tasks: The translation of printer manuals; manuals and the translation of the Bulletin of the European Union. In each task, the following three pairs of languages were involved (in both translation directions): English-Spanish, English-German, and English-French."
D09-1022,Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models,2009,17,75,3,1,42680,arne mauser,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this work, we propose two extensions of standard word lexicons in statistical machine translation: A discriminative word lexicon that uses sentence-level source information to predict the target words and a trigger-based lexicon model that extends IBM model 1 with a second trigger, allowing for a more fine-grained lexical choice of target words. The models capture dependencies that go beyond the scope of conventional SMT models such as phrase-and language models. We show that the models improve translation quality by 1% in BLEU over a competitive baseline on a large-scale task."
2009.eamt-1.31,Are Unaligned Words Important for Machine Translation?,2009,13,9,3,1,7420,yuqi zhang,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"In this paper, we deal with the problem of a large number of unaligned words in automatically learned word alignments for machine translation (MT). These unaligned words are the reason for ambiguous phrase pairs extracted by a statistical phrase-based MT system. In translation, this phrase ambiguity causes deletion and insertion errors. We present hard and optional deletion approaches to remove the unaligned words in the source language sentences. Improvements in translation quality are achieved both on large and small vocabulary tasks with the presented methods."
2009.eamt-1.33,On {LM} Heuristics for the Cube Growing Algorithm,2009,13,15,2,1,5800,david vilar,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"Current approaches to statistical machine translation try to incorporate more structure into the translation process by including explicit syntactic information in form of a formal grammar (with a possible, but not necessary, correspondence to a linguistic motivated grammar). These more structured models incur into an increased generation cost, and efficient algorithms must be developed. In this paper we concentrate on the cube growing algorithm, a lazy version of the cube grow algorithm. The efficiency of this algorithm depends on a heuristic for language model computation, which is only scarcely discussed in the original paper. In this paper we investigate the effect of this heuristic on translation performance and efficiency and propose a new heuristic which efficiently decreases memory requirements and computation time, while maintaining translation performance."
hasan-ney-2008-multi,A Multi-Genre {SMT} System for {A}rabic to {F}rench,2008,8,5,2,1,34439,savsa hasan,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This work presents improvements of a large-scale Arabic to French statistical machine translation system over a period of three years. The development includes better preprocessing, more training data, additional genre-specific tuning for different domains, namely newswire text and broadcast news transcripts, and improved domain-dependent language models. Starting with an early prototype in 2005 that participated in the second CESTA evaluation, the system was further upgraded to achieve favorable BLEU scores of 44.8{\%} for the text and 41.1{\%} for the audio setting. These results are compared to a system based on the freely available Moses toolkit. We show significant gains both in terms of translation quality (up to +1.2{\%} BLEU absolute) and translation speed (up to 16 times faster) for comparable configuration settings."
mauser-etal-2008-automatic,Automatic Evaluation Measures for Statistical Machine Translation System Optimization,2008,8,17,3,1,42680,arne mauser,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Evaluation of machine translation (MT) output is a challenging task. In most cases, there is no single correct translation. In the extreme case, two translations of the same input can have completely different words and sentence structure while still both being perfectly valid. Large projects and competitions for MT research raised the need for reliable and efficient evaluation of MT systems. For the funding side, the obvious motivation is to measure performance and progress of research. This often results in a specific measure or metric taken as primarily evaluation criterion. Do improvements in one measure really lead to improved MT performance? How does a gain in one evaluation metric affect other measures? This paper is going to answer these questions by a number of experiments."
dreuw-etal-2008-benchmark,Benchmark Databases for Video-Based Automatic Sign Language Recognition,2008,20,57,5,1,46030,philippe dreuw,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"A new, linguistically annotated, video database for automatic sign language recognition is presented. The new RWTH-BOSTON-400 corpus, which consists of 843 sentences, several speakers and separate subsets for training, development, and testing is described in detail. For evaluation and benchmarking of automatic sign language recognition, large corpora are needed. Recent research has focused mainly on isolated sign language recognition methods using video sequences that have been recorded under lab conditions using special hardware like data gloves. Such databases have often consisted generally of only one speaker and thus have been speaker-dependent, and have had only small vocabularies. A new database access interface, which was designed and created to provide fast access to the database statistics and content, makes it possible to easily browse and retrieve particular subsets of the video database. Preliminary baseline results on the new corpora are presented. In contradistinction to other research in this area, all databases presented in this paper will be publicly available."
bungeroth-etal-2008-atis,The {ATIS} Sign Language Corpus,2008,5,26,4,1,48344,jan bungeroth,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Systems that automatically process sign language rely on appropriate data. We therefore present the ATIS sign language corpus that is based on the domain of air travel information. It is available for five languages, English, German, Irish sign language, German sign language and South African sign language. The corpus can be used for different tasks like automatic statistical translation and automatic sign language recognition and it allows the specific modeling of spatial references in signing space."
hahn-etal-2008-comparison,A Comparison of Various Methods for Concept Tagging for Spoken Language Understanding,2008,8,32,4,0,16073,stefan hahn,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The extraction of flat concepts out of a given word sequence is usually one of the first steps in building a spoken language understanding (SLU) or dialogue system. This paper explores five different modelling approaches for this task and presents results on a French state-of-the-art corpus, MEDIA. Additionally, two log-linear modelling approaches could be further improved by adding morphologic knowledge. This paper goes beyond what has been reported in the literature. We applied the models on the same training and testing data and used the NIST scoring toolkit to evaluate the experimental results to ensure identical conditions for each of the experiments and the comparability of the results. Using a model based on conditional random fields, we achieve a concept error rate of 11.8{\%} on the MEDIA evaluation corpus."
D08-1039,Triplet Lexicon Models for Statistical Machine Translation,2008,108,39,3,1,34439,savsa hasan,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,This paper describes a lexical trigger model for statistical machine translation. We present various methods using triplets incorporating long-distance dependencies that can go beyond the local context of phrases or n-gram based language models. We evaluate the presented methods on two translation tasks in a reranking framework and compare it to the related IBM model 1. We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speed up the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance.
D08-1088,Complexity of Finding the {BLEU}-optimal Hypothesis in a Confusion Network,2008,14,10,3,1,29312,gregor leusch,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Confusion networks are a simple representation of multiple speech recognition or translation hypotheses in a machine translation system. A typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric. In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU. This also holds for more complex representations like generic word graphs. In addition, we give an efficient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again.n n Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time."
C08-1128,{B}ayesian Semi-Supervised {C}hinese Word Segmentation for Statistical Machine Translation,2008,15,54,4,1,4017,jia xu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems. In MT, the widely used approach is to apply a Chinese word segmenter trained from manually annotated data, using a fixed lexicon. Such word segmentation is not necessarily optimal for translation. We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT. Experiments show that our method improves a state-of-the-art MT system in a small and a large data environment."
2008.iwslt-papers.7,Analysing soft syntax features and heuristics for hierarchical phrase based machine translation.,2008,18,27,3,1,5800,david vilar,Proceedings of the 5th International Workshop on Spoken Language Translation: Papers,0,"Similar to phrase-based machine translation, hierarchical systems produce a large proportion of phrases, most of which are supposedly junk and useless for the actual translation. For the hierarchical case, however, the amount of extracted rules is an order of magnitude bigger. In this paper, we investigate several soft constraints in the extraction of hierarchical phrases and whether these help as additional scores in the decoding to prune unneeded phrases. We show the methods that help best."
2008.iwslt-papers.8,Improvements in dynamic programming beam search for phrase-based statistical machine translation.,2008,16,35,2,1,30618,richard zens,Proceedings of the 5th International Workshop on Spoken Language Translation: Papers,0,"Search is a central component of any statistical machine translation system. We describe the search for phrase-based SMT in detail and show its importance for achieving good translation quality. We introduce an explicit distinction between reordering and lexical hypotheses and organize the pruning accordingly. We show that for the large Chinese-English NIST task already a small number of lexical alternatives is sufficient, whereas a large number of reordering hypotheses is required to achieve good translation quality. The resulting system compares favorably with the current stateof-the-art, in particular we perform a comparison with cube pruning as well as with Moses."
2008.iwslt-evaluation.16,The {RWTH} machine translation system for {IWSLT} 2008.,2008,0,6,8,1,5800,david vilar,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"RWTH{'}s system for the 2008 IWSLT evaluation consists of a combination of different phrase-based and hierarchical statistical machine translation systems. We participated in the translation tasks for the Chinese-to-English and Arabic-to-English language pairs. We investigated different preprocessing techniques, reordering methods for the phrase-based system, including reordering of speech lattices, and syntax-based enhancements for the hierarchical systems. We also tried the combination of the Arabic-to-English and Chinese-to-English outputs as an additional submission."
W07-0705,Can We Translate Letters?,2007,13,76,3,1,5800,david vilar,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols. Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data. In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters. We try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word suffixes and translation of unseen words. Experiments are carried out for the translation of Catalan to Spanish."
W07-0707,Word Error Rates: Decomposition over {POS} classes and Applications for Error Analysis,2007,16,66,2,1,5059,maja popovic,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Evaluation and error analysis of machine translation output are important but difficult tasks. In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (Wer) and Position independent word Error Rate (Per) over different Part-of-Speech (Pos) classes. Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over Pos classes. The obtained results are shown to correspond to the results of a human error analysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system."
W07-0713,Human Evaluation of Machine Translation Through Binary System Comparisons,2007,17,20,3,1,5800,david vilar,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We introduce a novel evaluation scheme for the human evaluation of different machine translation systems. Our method is based on direct comparison of two sentences at a time by human judges. These binary judgments are then used to decide between all possible rankings of the systems. The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign."
W07-0401,Chunk-Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation,2007,29,69,3,1,7420,yuqi zhang,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"In this paper, we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation. First, we shallow parse the source language sentences. Then, reordering rules are automatically learned from source-side chunks and word alignments. During translation, the rules are used to generate a reordering lattice for each sentence. Experimental results are reported for a Chinese-to-English task, showing an improvement of 0.5%--1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level."
P07-2026,Minimum {B}ayes Risk Decoding for {BLEU},2007,9,20,3,0,49154,nicola ehling,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"We present a Minimum Bayes Risk (MBR) decoder for statistical machine translation. The approach aims to minimize the expected loss of translation errors with regard to the BLEU score. We show that MBR decoding on N-best lists leads to an improvement of translation quality.n n We report the performance of the MBR decoder on four different tasks: the TC-STAR EPPS Spanish-English task 2006, the NIST Chinese-English task 2005 and the GALE Arabic-English and Chinese-English task 2006. The absolute improvement of the BLEU score is between 0.2% for the TC-STAR task and 1.1% for the GALE Chinese-English task."
N07-2015,Are Very Large {N}-Best Lists Useful for {SMT}?,2007,7,18,3,1,34439,savsa hasan,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"This paper describes an efficient method to extract large n-best lists from a word graph produced by a statistical machine translation system. The extraction is based on the k shortest paths algorithm which is efficient even for very large k. We show that, although we can generate large amounts of distinct translation hypotheses, these numerous candidates are not able to significantly improve overall system performance. We conclude that large n-best lists would benefit from better discriminating models."
N07-2017,i{ROVER}: Improving System Combination with Classification,2007,12,24,5,0,45790,dustin hillard,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We present an improved system combination technique, iROVER, Our approach obtains significant improvements over ROVER, and is consistently better across varying numbers of component systems. A classifier is trained on features from the system lattices, and selects the final word hypothesis by learning cues to choose the system that is most likely to be correct at each word location. This approach achieves the best result published to date on the TC-STAR 2006 English speech recognition evaluation set."
N07-2035,Analysis and System Combination of Phrase- and {N}-Gram-Based Statistical Machine Translation Systems,2007,10,9,6,0,5326,marta costajussa,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"In the framework of the Tc-Star project, we analyze and propose a combination of two Statistical Machine Translation systems: a phrase-based and an N-gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system's output. Additionally, we combine both systems, showing accuracy improvements."
N07-1062,Efficient Phrase-Table Representation for Machine Translation with Applications to Online {MT} and Speech Translation,2007,10,46,2,1,30618,richard zens,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"In phrase-based statistical machine translation, the phrase-table requires a large amount of memory. We will present an efficient representation with two key properties: on-demand loading and a prefix tree structure for the source phrases. We will show that this representation scales well to large data tasks and that we are able to store hundreds of millions of phrase pairs in the phrase-table. For the large Chinesexe2x80x90 English NIST task, the memory requirements of the phrase-table are reduced to less than 20MB using the new representation with no loss in translation quality and speed. Additionally, the new representation is not limited to a specific test set, which is important for online or real-time machine translation. One problem in speech translation is the matching of phrases in the input word graph and the phrase-table. We will describe a novel algorithm that effectively solves this combinatorial problem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality."
J07-1003,Word-Level Confidence Estimation for Machine Translation,2007,34,80,2,1,28492,nicola ueffing,Computational Linguistics,0,"This article introduces and evaluates several different word-level confidence measures for machine translation. These measures provide a method for labeling each word in an automatically generated translation as correct or incorrect. All approaches to confidence estimation presented here are based on word posterior probabilities. Different concepts of word posterior probabilities as well as different ways of calculating them will be introduced and compared. They can be divided into two categories: System-based methods that explore knowledge provided by the translation system that generated the translations, and direct methods that are independent of the translation system. The system-based techniques make use of system output, such as word graphs or N-best lists. The word posterior probability is determined by summing the probabilities of the sentences in the translation hypothesis space that contains the target word. The direct confidence measures take other knowledge sources, such as word or phrase lexica, into account. They can be applied to output from nonstatistical machine translation systems as well.n n Experimental assessment of the different confidence measures on various translation tasks and in several language pairs will be presented. Moreover,the application of confidence measures for rescoring of translation hypotheses will be investigated."
D07-1055,A Systematic Comparison of Training Criteria for Statistical Machine Translation,2007,21,30,3,1,30618,richard zens,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We address the problem of training the free parameters of a statistical machine translation system. We show significant improvements over a state-of-the-art minimum error rate training baseline on a large ChineseEnglish translation task. We present novel training criteria based on maximum likelihood estimation and expected loss computation. Additionally, we compare the maximum a-posteriori decision rule and the minimum Bayes risk decision rule. We show that, not only from a theoretical point of view but also in terms of translation quality, the minimum Bayes risk decision rule is preferable."
2007.tmi-plenaries.2,Statistical {MT} from {TMI}-1988 to {TMI}-2007: what has happened?,2007,-1,-1,1,1,3262,hermann ney,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Plenaries,0,None
2007.tmi-papers.26,Hand in hand: automatic sign language to {E}nglish translation,2007,9,12,3,1,32003,daniel stein,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"In this paper, we describe the first data-driven automatic sign-language-to- speech translation system. While both sign language (SL) recognition and translation techniques exist, both use an intermediate notation systemn not directly intelligible for untrained users. We combine a SL recognizing framework with a state-of-the-art phrase-based machine translation (MT) system, using corpora of both American Sign Language and Irish Sign Languagen data. In a set of experiments we show the overall results and also illustrate the importance of including an vision-based knowledge source in the development of a complete SL translation system."
2007.mtsummit-papers.44,Combining data-driven {MT} systems for improved sign language translation,2007,25,14,5,0,42572,sara morrissey,Proceedings of Machine Translation Summit XI: Papers,0,"In this paper, we investigate the feasibility of combining two data-driven machine translation (MT) systems for the translation of sign languages (SLs). We take the MT systems of two prominent data-driven research groups, the MaTrEx system developed at DCU and the Statistical Machinen Translation (SMT) system developed at RWTH Aachen University, and apply their respective approaches to the task of translating Irish Sign Language and German Sign Language into English and German. In a set of experiments supported by automatic evaluation results, we show thatn there is a definite value to the prospective merging of MaTrExxe2x80x99s Example-Based MT chunks and distortion limit increase with RWTHxe2x80x99s constraint reordering."
2007.mtsummit-papers.68,Domain dependent statistical machine translation,2007,-1,-1,4,1,4017,jia xu,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.iwslt-1.3,Improved chunk-level reordering for statistical machine translation,2007,20,30,3,1,7420,yuqi zhang,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"Inspired by previous chunk-level reordering approaches to statistical machine translation, this paper presents two methods to improve the reordering at the chunk level. By introducing a new lattice weighting factor and by reordering the training source data, an improvement is reported on TER and BLEU. Compared to the previous chunklevel reordering approach, the BLEU score improves 1.4{\%} absolutely. The translation results are reported on IWSLT Chinese-English task."
2007.iwslt-1.25,The {RWTH} machine translation system for {IWSLT} 2007,2007,0,5,5,1,42680,arne mauser,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"The RWTH system for the IWSLT 2007 evaluation is a combination of several statistical machine translation systems. The combination includes Phrase-Based models, a n-gram translation model and a hierarchical phrase model. We describe the individual systems and the method that was used for combining the system outputs. Compared to our 2006 system, we newly introduce a hierarchical phrase-based translation model and show improvements in system combination for Machine Translation. RWTH participated in the Italian-to-English and Chinese-to-English translation directions."
W06-3101,Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output,2006,16,33,5,1,5059,maja popovic,Proceedings on the Workshop on Statistical Machine Translation,0,"Evaluation of machine translation output is an important but difficult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements."
W06-3103,Morpho-syntactic {A}rabic Preprocessing for {A}rabic to {E}nglish Statistical Machine Translation,2006,15,32,4,0,49651,anas isbihani,Proceedings on the Workshop on Statistical Machine Translation,0,"The Arabic language has far richer systems of inflection and derivation than English which has very little morphology. This morphology difference causes a large gap between the vocabulary sizes in any given parallel training corpus. Segmentation of inflected Arabic words is a way to smooth its highly morphological nature. In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation. Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks."
W06-3108,Discriminative Reordering Models for Statistical Machine Translation,2006,23,104,2,1,30618,richard zens,Proceedings on the Workshop on Statistical Machine Translation,0,"We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system."
W06-3110,N-Gram Posterior Probabilities for Statistical Machine Translation,2006,17,68,2,1,30618,richard zens,Proceedings on the Workshop on Statistical Machine Translation,0,"Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation. We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality. Additionally, we will introduce a sentence length model based on posterior probabilities.n n We will show significant improvements on the Chinese-English NIST task. The absolute improvements of the BLEU score is between 1.1% and 1.6%."
W06-3111,Partitioning Parallel Documents Using Binary Segmentation,2006,11,8,3,1,4017,jia xu,Proceedings on the Workshop on Statistical Machine Translation,0,"In statistical machine translation, large numbers of parallel sentences are required to train the model parameters. However, plenty of the bilingual language resources available on web are aligned only at the document level. To exploit this data, we have to extract the bilingual sentences from these documents.n n The common method is to break the documents into segments using predefined anchor words, then these segments are aligned. This approach is not error free, incorrect alignments may decrease the translation quality.n n We present an alternative approach to extract the parallel sentences by partitioning a bilingual document into two pairs. This process is performed recursively until all the sub-pairs are short enough.n n In experiments on the Chinese-English FBIS data, our method was capable of producing translation results comparable to those of a state-of-the-art sentence aligner. Using a combination of the two approaches leads to better translation performance."
W06-2606,Reranking Translation Hypotheses Using Structural Properties,2006,16,16,3,1,34439,savsa hasan,Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,0,"We investigate methods that add syntactically motivated features to a statistical machine translation system in a reranking framework. The goal is to analyze whether shallow parsing techniques help in identifying ungrammatical hypotheses. We show that improvements are possible by utilizing supertagging, lightweight dependency analysis, a link grammar parser and a maximum-entropy based chunk parser. Adding features to n-best lists and discriminatively training the system on a development set increases the BLEU score up to 0.7% on the test set."
P06-2061,Integration of Speech to Computer-Assisted Translation Using Finite-State Automata,2006,24,9,3,0,5805,shahram khadivi,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"State-of-the-art computer-assisted translation engines are based on a statistical prediction engine, which interactively provides completions to what a human translator types. The integration of human speech into a computer-assisted system is also a challenging area and is the aim of this paper. So far, only a few methods for integrating statistical machine translation (MT) models with automatic speech recognition (ASR) models have been studied. They were mainly based on N-best rescoring approach. N-best rescoring is not an appropriate search method for building a real-time prediction engine. In this paper, we study the incorporation of MT models and ASR models using finite-state automata. We also propose some transducers based on MT models for rescoring the ASR word graphs."
mauser-etal-2006-training,Training a Statistical Machine Translation System without {GIZA}++,2006,11,3,3,1,42680,arne mauser,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The IBM Models (Brown et al., 1993) enjoy great popularity in the machine translation community because they offer high quality word alignments and a free implementation is available with the GIZA++ Toolkit (Och and Ney, 2003). Several methods have been developed to overcome the asymmetry of the alignment generated by the IBM Models. A remaining disadvantage, however, is the high model complexity. This paper describes a word alignment training procedure for statistical machine translation that uses a simple and clear statistical model, different from the IBM models. The main idea of the algorithm is to generate a symmetric and monotonic alignment between the target sentence and a permutation graph representing different reorderings of the words in the source sentence. The quality of the generated alignment is shown to be comparable to the standard GIZA++ training in an SMT setup."
hasan-etal-2006-creating,Creating a Large-Scale {A}rabic to {F}rench Statistical {M}achine{T}ranslation System,2006,6,9,3,1,34439,savsa hasan,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this work, the creation of a large-scale Arabic to French statistical machine translation system is presented. We introduce all necessary steps from corpus aquisition, preprocessing the data to training and optimizing the system and eventual evaluation. Since no corpora existed previously, we collected large amounts of data from the web. Arabic word segmentation was crucial to reduce the overall number of unknown words. We describe the phrase-based SMT system used for training and generation of the translation hypotheses. Results on the second CESTA evaluation campaign are reported. The setting was inthe medical domain. The prototype reaches a favorable BLEU score of40.8{\%}."
popovic-ney-2006-pos,{POS}-based Word Reorderings for Statistical Machine Translation,2006,6,82,2,1,5059,maja popovic,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Translation In this work we investigate new possibilities for improving the quality of statistical machine translation (SMT) by applying word reorderings of the source language sentences based on Part-of-Speech tags. Results are presented on the European Parliament corpus containing about 700k sentences and 15M running words. In order to investigate sparse training data scenarios, we also report results obtained on about 1{\textbackslash}{\%} of the original corpus. The source languages are Spanish and English and target languages are Spanish, English and German. We propose two types of reorderings depending on the language pair and the translation direction: local reorderings of nouns and adjectives for translation from and into Spanish and long-range reorderings of verbs for translation into German. For our best translation system, we achieve up to 2{\textbackslash}{\%} relative reduction of WER and up to 7{\textbackslash}{\%} relative increase of BLEU score. Improvements can be seen both on the reordered sentences as well as on the rest of the test corpus. Local reorderings are especially important for the translation systems trained on the small corpus whereas long-range reorderings are more effective for the larger corpus."
vilar-etal-2006-error,Error Analysis of Statistical Machine Translation Output,2006,3,176,4,1,5800,david vilar,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Evaluation of automatic translation output is a difficult task. Several performance measures like Word Error Rate, Position Independent Word Error Rate and the BLEU and NIST scores are widely use and provide a useful tool for comparing different systems and to evaluate improvements within a system. However the interpretation of all of these measures is not at all clear, and the identification of the most prominent source of errors in a given system using these measures alone is not possible. Therefore some analysis of the generated translations is needed in order to identify the main problems and to focus the research efforts. This area is however mostly unexplored and few works have dealt with it until now. In this paper we will present a framework for classification of the errors of a machine translation system and we will carry out an error analysis of the system used by the RWTH in the first TC-STAR evaluation."
bungeroth-etal-2006-german,A {G}erman {S}ign {L}anguage Corpus of the Domain Weather Report,2006,5,26,5,1,48344,jan bungeroth,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus. All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus."
E06-1005,Computing Consensus Translation for Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment,2006,14,161,3,1,5736,evgeny matusov,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative."
E06-1031,{CDER}: Efficient {MT} Evaluation Using Block Movements,2006,11,82,3,1,29312,gregor leusch,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs. The correlation of the new measure with human judgment has been investigated systematically on two different language pairs. The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation. Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment."
2006.iwslt-papers.1,Automatic sentence segmentation and punctuation prediction for spoken language translation,2006,13,56,3,1,5736,evgeny matusov,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"This paper studies the impact of automatic sentence segmentation and punctuation prediction on the quality of machine translation of automatically recognized speech. We present a novel sentence segmentation method which is specifically tailored to the requirements of machine translation algorithms and is competitive with state-of-the-art approaches for detecting sentence-like units. We also describe and compare three strategies for predicting punctuation in a machine translation framework, including the simple and effective implicit punctuation generation by a statistical phrase-based machine translation system. Our experiments show the robust performance of the proposed sentence segmentation and punctuation prediction approaches on the IWSLT Chinese-to-English and TC-STAR English-to-Spanish speech translation tasks in terms of translation quality."
2006.iwslt-papers.7,{AER}: do we need to {``}improve{''} our alignments?,2006,17,31,3,1,5800,david vilar,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"Currently most statistical machine translation systems make use of alignments as a first step in the process of training the actual translation models. Several researchers have investigated how to improve the alignment quality, with the (intuitive) assumption that better alignments increase the translation quality. In this paper we will investigate this assumption and show that this is not always the case."
2006.iwslt-evaluation.15,The {RWTH} statistical machine translation system for the {IWSLT} 2006 evaluation,2006,26,48,5,1,42680,arne mauser,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. The system was ranked first with respect to the BLEU measure in all language pairs it was used Using a two-pass aproach, we first generate the N best translation candidates. The second pass consists of rescoring and reranking these candidates. We will give a description of the search algorithm as well as of the models used in each pass. We will also describe our method for dealing with punctuation restoration, in order to overcome the difficulties of spoken language translation. This work also includes a brief description of the system combination done by the partners participating in the European TC-Star project."
2006.eamt-1.11,A Flexible Architecture for {CAT} Applications,2006,-1,-1,4,1,34439,savsa hasan,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
2006.eamt-1.21,Morpho-Syntax Based Statistical Methods for Automatic Sign Language Translation,2006,17,29,3,1,32003,daniel stein,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,"We present a novel approach for the automatic translation of written text into sign language. A new corpus focussing on the weather report domain for the language pair German and German Sign Language is introduced. We apply phrase-based statistical machine translation, enhanced by preand post-processing steps based on the morpho-syntactical analysis of German. Detailed results are given based on automatic and manual evaluation."
W05-0903,Preprocessing and Normalization for Automatic Evaluation of Machine Translation,2005,6,13,4,1,29312,gregor leusch,Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,0,"Evaluation measures for machine translation depend on several common methods, such as preprocessing, tokenization, handling of sentence boundaries, and the choice of a reference length. In this paper, we describe and review some new approaches to them and compare these to state-of-the-art methods. We experimentally look into their impact on four established evaluation measures. For this purpose, we study the correlation between automatic and human evaluation scores on three MT evaluation corpora. These experiments confirm that the tokenization method, the reference length selection scheme, and the use of sentence boundaries we introduce will increase the correlation between automatic and human evaluation scores. We find that ignoring case information and normalizing evaluator scores has a positive effect on the sentence level correlation as well."
W05-0806,Augmenting a Small Parallel Text with Morpho-Syntactic Language,2005,0,4,3,1,5059,maja popovic,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,None
W05-0831,Novel Reordering Approaches in Phrase-Based Statistical Machine Translation,2005,16,80,5,1,49189,stephan kanthak,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality."
W05-0834,Word Graphs for Statistical Machine Translation,2005,14,22,2,1,30618,richard zens,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"Word graphs have various applications in the field of machine translation. Therefore it is important for machine translation systems to produce compact word graphs of high quality. We will describe the generation of word graphs for state of the art phrase-based statistical machine translation. We will use these word graph to provide an analysis of the search process. We will evaluate the quality of the word graphs using the well-known graph word error rate. Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score.n n Experimental results are presented for two Chinese--English tasks: the small IWSLT task and the NIST large data track task. For both tasks, we achieve significant reductions of the graph error rate already with compact word graphs."
H05-1096,Word-Level Confidence Estimation for Machine Translation using Phrase-Based Translation Models,2005,19,35,2,1,28492,nicola ueffing,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Confidence measures for machine translation is a method for labeling each word in an automatically generated translation as correct or incorrect. In this paper, we will present a new approach to confidence estimation which has the advantage that it does not rely on system output such as N-best lists or word graphs as many other confidence measures do. It is, thus, applicable to any kind of machine translation system.Experimental evaluation has been performed on translation of technical manuals in three different language pairs. Results will be presented for different machine translation systems to show that the new approach is independent of the underlying machine translation system which generated the translations. To the best of our knowledge, the performance of the new confidence measure is better than that of any existing confidence measure."
2005.mtsummit-papers.34,Statistical Machine Translation of {E}uropean Parliamentary Speeches,2005,-1,-1,5,1,5800,david vilar,Proceedings of Machine Translation Summit X: Papers,0,"In this paper we present the ongoing work at RWTH Aachen University for building a speech-to-speech translation system within the TC-Star project. The corpus we work on consists of parliamentary speeches held in the European Plenary Sessions. To our knowledge, this is the first project that focuses on speech-to-speech translation applied to a real-life task. We describe the statistical approach used in the development of our system and analyze its performance under different conditions: dealing with syntactically correct input, dealing with the exact transcription of speech and dealing with the (noisy) output of an automatic speech recognition system. Experimental results show that our system is able to perform adequately in each of these conditions."
2005.mtsummit-invited.5,One Decade of Statistical Machine Translation: 1996-2005,2005,22,8,1,1,3262,hermann ney,Proceedings of Machine Translation Summit X: Invited papers,0,"In the last decade, the statistical approach has found widespread use in machine translation both for written and spoken language and has had a major impact on the translation accuracy. This paper will cover the principles of statistical machine translation and summarize the progress made so far."
2005.iwslt-1.18,Integrated {C}hinese Word Segmentation in Statistical Machine Translation,2005,12,36,4,1,4017,jia xu,Proceedings of the Second International Workshop on Spoken Language Translation,0,"A Chinese sentence is represented as a sequence of characters, and words are not separated from each other. In statistical machine translation, the conventional approach is to segment the Chinese character sequence into words during the pre-processing. The training and translation are performed afterwards. However, this method is not optimal for two reasons: 1. The segmentations may be erroneous. 2. For a given character sequence, the best segmentation depends on its context and translation. In order to minimize the translation errors, we take different segmentation alternatives instead of a single segmentation into account and integrate the segmentation process with the search for the best translation. The segmentation decision is only taken during the generation of the translation. With this method we are able to translate Chinese text at the character level. The experiments on the IWSLT 2005 task showed improvements in the translation performance using two translation systems: a phrase-based system and a finite state transducer based system. For the phrase-based system, the improvement of the BLEU score is 1.5% absolute."
2005.iwslt-1.19,Evaluating Machine Translation Output with Automatic Sentence Segmentation,2005,9,44,4,1,5736,evgeny matusov,Proceedings of the Second International Workshop on Spoken Language Translation,0,"This paper presents a novel automatic sentence segmentation method for evaluating machine translation output with possibly erroneous sentence boundaries. The algorithm can process translation hypotheses with segment boundaries which do not correspond to the reference segment boundaries, or a completely unsegmented text stream. Thus, the method is especially useful for evaluating translations of spoken language. The evaluation procedure takes advantage of the edit distance algorithm and is able to handle multiple reference translations. It efficiently produces an optimal automatic segmentation of the hypotheses and thus allows application of existing well-established evaluation measures. Experiments show that the evaluation measures based on the automatically produced segmentation correlate with the human judgement at least as well as the evaluation measures which are based on manual sentence boundaries."
2005.iwslt-1.20,The {RWTH} Phrase-based Statistical Machine Translation System,2005,24,46,8,1,30618,richard zens,Proceedings of the Second International Workshop on Spoken Language Translation,0,"We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation 2005. We use a two pass approach. In the first pass, we generate a list of the N best translation candidates. The second pass consists of rescoring and reranking this N -best list. We will give a description of the search algorithm as well as the models that are used in each pass. We participated in the supplied data tracks for manual transcriptions for the following translation directions: Arabic-English, Chinese-English, English-Chinese and Japanese-English. For Japanese-English, we also participated in the C-Star track. In addition, we performed translations of automatic speech recognition output for ChineseEnglish and Japanese-English. For both language pairs, we translated the single-best ASR hypotheses. Additionally, we translated Chinese ASR lattices."
2005.eamt-1.6,Comparison of generation strategies for interactive machine translation,2005,8,17,5,1,47115,oliver bender,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Fully automatic translations are far from being perfect. Non-grammatical sentences are often produced by automatic systems and there is even no guarantee that the meaning of the sentence is preserved. Nevertheless, automatic translation systems can be used to help human translators to produce high-quality translations. This is the goal of the TransType2 project, where an interactive translation tool is being developed that suggests, in real time, possible completions for the sentences that the human translator is typing. This leads to a modification of the generation strategy of the translation system, as now we are looking for the best translation of the given source sentence that is compatible with the prefix. In order to remain within the tight response time constraints of such a system, some simplifications have to be done. In this paper, we review possible generation strategies for an interactive statistical machine translation system and analyze what is the loss in performance when strict time constraints have to be met. Experiments are performed on the Spanish-English and German-English Xerox corpora, which consist of the translation of technical manuals, and the results show that the real time generation strategy causes only a small performance degradation."
2005.eamt-1.17,Clustered language models based on regular expressions for {SMT},2005,8,14,2,1,34439,savsa hasan,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In this paper, we present a language model based on clusters obtained by applying regular expressions to the training data and, thus, discriminating several different sentence types as, e.g. interrog- atives, imperatives or enumerations. The main motivation lies in the observation that different sentence types also underlie a different syntactic structure, and thus yield a varying distribution of n-grams reflect- ing their word order. We show that this assumption is valid by applying the models to English-Spanish bilingual corpora and obtaining good perplexity reductions of approximately 25%. In addition, we per- form an n-best rescoring experiment and show a relative improvement of 4-5% in word error rate. The models can be easily adapted to other translation tasks and do not need complicated training methods, thus being a valuable alternative for on-demand rescoring of sentence hypotheses such as they occur in the CAT framework."
2005.eamt-1.25,Efficient statistical machine translation with constrained reordering,2005,-1,-1,3,1,5736,evgeny matusov,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,None
2005.eamt-1.29,Exploiting phrasal lexica and additional morpho-syntactic language resources for statistical machine translation with scarce training data,2005,7,12,2,1,5059,maja popovic,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In this work, the use of a phrasal lexicon for statistical machine translation is proposed, and the relation between data acquisition costs and translation quality for different types and sizes of language resources has been analyzed. The language pairs are Spanish-English and Catalan-English, and the translation is performed in all directions. The phrasal lexicon is used to increase as well as to replace the original training corpus. The augmentation of the phrasal lexicon with the help of additional monolingual language resources containing morpho-syntactic information has been investigated for the translation with scarce training material. Using the augmented phrasal lexicon as additional training data, a reasonable translation quality can be achieved with only 1000 sentence pairs from the desired domain."
2005.eamt-1.35,Application of word-level confidence measures in interactive statistical machine translation,2005,11,31,2,1,28492,nicola ueffing,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In this paper, we will address the question of how to efficiently integrate word confidence measures into a state-of-the-art interactive statistical machine translation system and improve prediction performance. Different methods will be presented: the selection of words according to their confidence as well as the rejection which has not been investigated so far. Experimental evaluation with respect to prediction accuracy of the system and typing effort saved by a user will show the improved prediction quality. Additionally, we will describe novel methods exploiting knowledge about a correct prefix of the target sentence for confidence estimation. These further increase the interactive system's performance."
2005.eamt-1.37,Sentence segmentation using {IBM} word alignment model 1,2005,12,30,3,1,4017,jia xu,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In statistical machine translation, word alignment models are trained on bilingual corpora. Long sentences pose severe problems: 1. the high computational requirements; 2. the poor quality of the resulting word alignment. We present a sentence-segmentation method that solves these problems by splitting long sentence pairs. Our approach uses the lexicon information to locate the optimal split point. This method is evaluated on two Chinese-English translation tasks in the news domain. We show that the segmentation of long sentences before training significantly improves the final translation quality of a state-of-the-art machine translation system. In one of the tasks, we achieve an improvement of the BLEU score of more than 20% relative."
W04-3235,Error Measures and {B}ayes Decision Rules Revisited with Applications to {POS} Tagging,2004,12,4,1,1,3262,hermann ney,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Starting from first principles, we re-visit the statistical approach and study two forms of the Bayes decision rule: the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors. The Bayes decision rule for minimizing the number of string errors is widely used, e.g. in speech recognition, POS tagging and machine translation, but its justification is rarely questioned. To minimize the number of symbol errors as is more suitable for a task like POS tagging, we show that another form of the Bayes decision rule can be derived. The major purpose of this paper is to show that the form of the Bayes decision rule should not be taken for granted (as it is done in virtually all statistical NLP work), but should be adapted to the error measure being used. We present first experimental results for POS tagging tasks."
W04-1118,Do We Need {C}hinese Word Segmentation for Statistical Machine Translation?,2004,10,43,3,1,4017,jia xu,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"In Chinese texts, words are not separated by white spaces. This is problematic for many natural language processing tasks. The standard approach is to segment the Chinese character sequence into words. Here, we investigate Chinese word segmentation for statistical machine translation. We pursue two goals: the first one is the maximization of the final translation quality; the second is the minimization of the manual effort for building a translation system. The commonly used method for getting the word boundaries is based on a word segmentation tool and a predefined monolingual dictionary. To avoid the dependence of the translation system on an external dictionary, we have developed a system that learns a domainspecific dictionary from the parallel training corpus. This method produces results that are comparable with the predefined dictionary. Further more, our translation system is able to work without word segmentation with only a minor loss in translation quality."
P04-1065,{FSA}: An Efficient and Flexible {C}++ Toolkit for Finite State Automata Using On-Demand Computation,2004,16,43,2,1,49189,stephan kanthak,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"In this paper we present the RWTH FSA toolkit --- an efficient implementation of algorithms for creating and manipulating weighted finite-state automata. The toolkit has been designed using the principle of on-demand computation and offers a large range of widely used algorithms. To prove the superior efficiency of the toolkit, we compare the implementation to that of other publically available toolkits. We also show that on-demand computations help to reduce memory requirements significantly without any loss in speed. To increase its flexibility, the RWTH FSA toolkit supports high-level interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software."
N04-1033,Improvements in Phrase-Based Statistical Machine Translation,2004,14,182,2,1,30618,richard zens,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system."
popovic-ney-2004-towards,Towards the Use of Word Stems and Suffixes for Statistical Machine Translation,2004,9,61,2,1,5059,maja popovic,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present methods for improving the quality of translation from an inflected language into English by making use of part-of-speech tags and word stems and suffixes in the source language. Results for translations from Spanish and Catalan into English are presented on the LC-STAR trilingual corpus which consists of spontaneously spoken dialogues in the domain of travelling and appointment scheduling. Results for translation from Serbian into English are presented on the Assimil language course, the bilingual corpus from unrestricted domain. We achieve up to 5% relative reduction of error rates for Spanish and Catalan and about 8% for Serbian."
J04-4002,The Alignment Template Approach to Statistical Machine Translation,2004,43,829,2,0.941176,37712,franz och,Computational Linguistics,0,"A phrase-based statistical machine translation approach xe2x80x94 the alignment template approach xe2x80x94 is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used sourcexe2x80x93channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the Germanxe2x80x93English speech VERBMOBIL task, we analyze the effect of various system components. On the Frenchxe2x80x93English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinesexe2x80x93English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems."
J04-2003,Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information,2004,28,162,2,1,52339,sonja niessen,Computational Linguistics,0,"In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another. The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms. We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words. In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences. We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation. The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10p of the original corpus, while losing only 1.6p in translation quality. The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole! task."
C04-1006,Improved Word Alignment Using a Symmetric Lexicon Model,2004,11,22,3,1,30618,richard zens,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing. We improve the well-known IBM alignment models, as well as the Hidden-Markov alignment model using a symmetric lexicon model. This symmetrization takes not only the standard translation direction from source to target into account, but also the inverse translation direction from target to source. We present a theoretically sound derivation of these techniques. In addition to the symmetrization, we introduce a smoothed lexicon model. The standard lexicon model is based on full-form words only. We propose a lexicon smoothing method that takes the word base forms explicitly into account. Therefore, it is especially useful for highly inflected languages such as German. We evaluate these methods on the German-English Verbmobil task and the French-English Canadian Hansards task. We show statistically significant improvements of the alignment quality compared to the best system reported so far. For the Canadian Hansards task, we achieve an improvement of more than 30% relative."
C04-1030,Reordering Constraints for Phrase-Based Statistical Machine Translation,2004,17,105,2,1,30618,richard zens,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We investigate different reordering constraints for phrase-based statistical machine translation, namely the IBM constraints and the ITG constraints. We present efficient dynamic programming algorithms for both constraints. We evaluate the constraints with respect to translation quality on two Japanese-English tasks. We show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search."
C04-1032,Symmetric Word Alignments for Statistical Machine Translation,2004,11,68,3,1,5736,evgeny matusov,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we address the word alignment problem for statistical machine translation. We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships. We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models. Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair. Then, we use efficient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability). We evaluate the automatic alignments created in this way on the German--English Verbmobil task and the French--English Canadian Hansards task. We show statistically significant improvements of the alignment quality compared to the best results reported so far. On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%."
C04-1045,Improving Word Alignment Quality using Morpho-syntactic Information,2004,9,17,1,1,3262,hermann ney,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models. Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other. We propose a method which explicitly takes into account such in-terdependencies during the EM training of the statistical alignment models. The evaluation is done by comparing the obtained Viterbi alignments with a manually annotated reference alignment. The improvements of the alignment quality compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus."
2004.iwslt-papers.7,Statistical machine translation of spontaneous speech with scarce resources,2004,15,7,4,1,5736,evgeny matusov,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,This paper deals with the task of statistical machine translation of spontaneous speech using a limited amount of training data. We propose a method for selecting relevant additional training data from other sources that may come from other domains. We present two ways to solve the data sparseness problem by including morphological information into the EM training of word alignments. We show that the use of part-of-speech information for harmonizing word order between source and target sentences yields significant improvements in the BLEU score.
2004.iwslt-evaluation.13,Alignment templates: the {RWTH} {SMT} system,2004,18,33,4,1,47115,oliver bender,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe the RWTH statistical machine translation (SMT) system which is based on log-linear model combination. All knowledge sources are treated as feature functions which depend on the source language sentence, the target language sentence and possible hidden variables. The main feature of our approach are the alignment templates which take shallow phrase structures into account: a phrase level alignment between phrases and a word level alignment between single words within the phrases. Thereby, we directly consider word contexts and local reorderings. In order to incorporate additional models (the IBM-1 statistical lexicon model, a word deletion model, and higher order language models), we perform n-best list rescoring. Participating in the International Workshop on Spoken Language Translation (IWSLT 2004), we evaluate our system on the Basic Travel Expression Corpus (BTEC) Chinese-to-English and Japanese-to-English tasks."
W03-0420,Maximum Entropy Models for Named Entity Recognition,2003,4,114,3,1,47115,oliver bender,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER). Starting with an annotated corpus and a set of features which are easily obtainable for almost any language, we first build a baseline NE recognizer which is then used to extract the named entities and their context information from additional non-annotated data. In turn, these lists are incorporated into the final recognizer to further improve the recognition accuracy."
P03-1019,A Comparative Study on Reordering Constraints in Statistical Machine Translation,2003,17,121,2,1,30618,richard zens,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schroder numbers.We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%."
J03-1002,A Systematic Comparison of Various Statistical Alignment Models,2003,37,3252,2,0.769231,37712,franz och,Computational Linguistics,0,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."
J03-1005,Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation,2003,58,201,2,1,38270,christoph tillmann,Computational Linguistics,0,"In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article."
E03-1007,Using {POS} Information for {SMT} into Morphologically Rich Languages,2003,0,9,2,1,28492,nicola ueffing,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
E03-1032,Efficient Search for Interactive Statistical Machine Translation,2003,11,48,3,0.769231,37712,franz och,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The goal of interactive machine translation is to improve the productivity of human translators. An interactive machine translation system operates as follows: the automatic system proposes a translation. Now, the human user has two options: to accept the suggestion or to correct it. During the post-editing process, the human user is assisted by the interactive system in the following way: the system suggests an extension of the current translation prefix. Then, the user either accepts this extension (completely or partially) or ignores it. The two most important factors of such an interactive system are the quality of the proposed extensions and the response time. Here, we will use a fully fledged translation system to ensure the quality of the proposed extensions. To achieve fast response times, we will use word hypotheses graphs as an efficient search space representation. We will show results of our approach on the Verbmobil task and on the Canadian Hansards task."
E03-1055,Comparison of Alignment Templates and Maximum Entropy Models for {NLP},2003,0,0,4,1,47115,oliver bender,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
2003.mtsummit-plenaries.5,Have we found the Holy Grail?,2003,-1,-1,1,1,3262,hermann ney,Proceedings of Machine Translation Summit IX: Plenaries,0,None
2003.mtsummit-papers.32,A novel string-to-string distance measure with applications to machine translation evaluation,2003,-1,-1,3,1,29312,gregor leusch,Proceedings of Machine Translation Summit IX: Papers,0,"We introduce a string-to-string distance measure which extends the edit distance by block transpositions as constant cost edit operation. An algorithm for the calculation of this distance measure in polynomial time is presented. We then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. The correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. In general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level."
2003.mtsummit-papers.52,Confidence measures for statistical machine translation,2003,11,52,3,1,28492,nicola ueffing,Proceedings of Machine Translation Summit IX: Papers,0,"In this paper, we present several confidence measures for (statistical) machine translation. We introduce word posterior probabilities for words in the target sentence that can be determined either on a word graph or on an N best list. Two alternative confidence measures that can be calculated on N best lists are proposed. The performance of the measures is evaluated on two different translation tasks: on spontaneously spoken dialogues from the domain of appointment scheduling, and on a collection of technical manuals."
W02-1021,Generation of Word Graphs in Statistical Machine Translation,2002,7,143,3,1,28492,nicola ueffing,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,Statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. We describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. The advantage is that these hypotheses can be rescored using a refined language or translation model. Results are presented on the German-English Verbmobil corpus.
P02-1038,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,2002,15,974,2,1,37712,franz och,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach."
C02-1032,Improving Alignment Quality in Statistical Machine Translation Using Context-dependent Maximum Entropy Models,2002,9,16,3,0,49655,ismael varea,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Typically, statistical alignment models are based on single-word dependencies. These models do not include contextual information, which can lead to inadequate alignments. In this paper, we present an approach to include contextual dependencies in the statistical alignment model by using a refined lexicon model. Unlike previous work, we directly integrate this in the EM algorithm of statistical alignment models. Experimental results are given for the French-English Canadian Parliament Hansards task and the Verbmobil task. The evaluation is performed by comparing the obtained alignments with a manually annotated reference alignment."
garcia-varea-etal-2002-efficient,Efficient integration of maximum entropy lexicon models within the training of statistical alignment models,2002,10,5,3,1,43571,ismael garciavarea,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Maximum entropy (ME) models have been successfully applied to many natural language problems. In this paper, we show how to integrate ME models efficiently within a maximum likelihood training scheme of statistical machine translation models. Specifically, we define a set of context-dependent ME lexicon models and we present how to perform an efficient training of these ME models within the conventional expectation-maximization (EM) training of statistical translation models. Experimental results are also given in order to demonstrate how these ME models improve the results obtained with the traditional translation models. The results are presented by means of alignment quality comparing the resulting alignments with manually annotated reference alignments."
W01-1405,Stochastic Modelling: From Pattern Classification to Language Translation,2001,10,7,1,1,3262,hermann ney,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"This paper gives an overview of the stochastic modelling approach to machine translation. Starting with the Bayes decision rule as in pattern classification and speech recognition, we show how the resulting system architecture can be structured into three parts: the language model probability, the string translation model probability and the search procedure that generates the word sequence in the target language. We discuss the properties of the system components and report results on the translation of spoken dialogues in the VERBMOBIL project. The experience obtained in the VERBMOBIL project, in particular a large-scale end-to-end evaluation, showed that the stochastic modelling approach resulted in significantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches."
W01-1407,Toward hierarchical models for statistical machine translation of inflected languages,2001,13,53,2,1,52339,sonja niessen,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"In statistical machine translation, correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models. Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other. In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives. We do this along two directions: Usage of hierarchical lexicon models and the introduction of equivalence classes in order to ignore information not relevant for the translation task. The improvement of the translation results is demonstrated on a German-English corpus."
W01-1408,An Efficient {A}* Search Algorithm for Statistical Machine Translation,2001,10,87,3,1,37712,franz och,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"In this paper, we describe an efficient A* search algorithm for statistical machine translation. In contrary to beam-search or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various so-phisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search algorithm with a beam-search approach on the Hansards task."
P01-1027,Refined Lexicon Models for Statistical Machine Translation using a Maximum Entropy Approach,2001,17,29,3,1,43571,ismael garciavarea,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a correct word sense disambiguation. One way to deal with this problem within the statistical framework is to use maximum entropy methods. In this paper, we present how to use this type of information within a statistical machine translation system. We show that it is possible to significantly decrease training and test corpus perplexity of the translation models. In addition, we perform a rescoring of N-Best lists using our maximum entropy model and thereby yield an improvement in translation quality. Experimental results are presented on the so-called Verbmobil Task."
2001.mtsummit-road.6,What can machine translation learn from speech recognition?,2001,-1,-1,2,1,37712,franz och,Workshop on MT2010: Towards a Road Map for MT,0,"The performance of machine translation technology after 50 years of development leaves much to be desired. There is a high demand for well performing and cheap MT systems for many language pairs and domains, which automatically adapt to rapidly changing terminology. We argue that for successful MT systems it will be crucial to apply data-driven methods, especially statistical machine translation. In addition, it will be very important to establish common test environments. This includes the availability of large parallel training corpora, well defined test corpora and standardized evaluation criteria. Thereby research results can be compared and this will open the possibility for more competition in MT research."
2001.mtsummit-papers.45,Morpho-syntactic analysis for reordering in statistical machine translation,2001,7,38,2,1,52339,sonja niessen,Proceedings of Machine Translation Summit VIII,0,"In the framework of statistical machine translation (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models. Among other things these are meant to capture the differences in word order in different languages. In this paper we show that SMT can take advantage of the explicit introduction of some linguistic knowledge about the sentence structure in the languages under consideration. In contrast to previous publications dealing with the incorporation of morphological and syntactic information into SMT, we focus on two aspects of reordering for the language pair German and English, namely question inversion and detachable German verb prefixes. The results of systematic experiments are reported and demonstrate the applicability of the approach to both translation directions on a German-English corpus."
2001.mtsummit-papers.46,Statistical multi-source translation,2001,-1,-1,2,1,37712,franz och,Proceedings of Machine Translation Summit VIII,0,"We describe methods for translating a text given in multiple source languages into a single target language. The goal is to improve translation quality in applications where the ultimate goal is to translate the same document into many languages. We describe a statistical approach and two specific statistical models to deal with this problem. Our method is generally applicable as it is independent of specific models, languages or application domains. We evaluate the approach on a multilingual corpus covering all eleven official European Union languages that was collected automatically from the Internet. In various tests we show that these methods can significantly improve translation quality. As a side effect, we also compare the quality of statistical machine translation systems for many European languages in the same domain."
P00-1004,Translation with Cascaded Finite State Transducers,2000,10,14,2,1,29364,stephan vogel,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we discuss the use of cascaded finite state transducers for machine translation. A number of small, dedicated transducers is applied to convert sentence pairs from a bilingual corpus into generalized translation patterns. These patterns, together with the transducers are then used as a hierarchical translation memory for fully automatic translation. Results on the German--English VERBMOBIL corpus are given."
P00-1056,Improved Statistical Alignment Models,2000,8,930,2,1,37712,franz och,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies."
niessen-etal-2000-evaluation,An Evaluation Tool for Machine Translation: Fast Evaluation for {MT} Research,2000,6,234,4,1,52339,sonja niessen,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper we present a tool for the evaluation of translation quality. First, the typical requirements of such a tool in the framework of machine translation (MT) research are discussed. We define evaluation criteria which are more adequate than pure edit distance and we describe how the measurement along these quality criteria is performed semi-automatically in a fast, convenient and above all consistent way using our tool and the corresponding graphical user interface."
C00-2123,Word Re-ordering and {DP}-based Search in Statistical Machine Translation,2000,8,55,2,0.333333,38270,christoph tillmann,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an efficient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (German-English, 8000-word vocabulary), which is a limited-domain spoken-language task."
C00-2162,Improving {SMT} quality with morpho-syntactic analysis,2000,8,72,2,1,52339,sonja niessen,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In the framework of statistical machine translation (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models. Many of the statistical systems use little or no linguistic knowledge to structure the underlying models. In this paper we argue that training data is typically not large enough to sufficiently represent the range of different phenomena in natural languages and that SMT can take advantage of the explicit introduction of some knowledge about the languages under consideration. The improvement of the translation results is demonstrated on two different German-English corpora."
C00-2163,A Comparison of Alignment Models for Statistical Machine Translation,2000,8,197,2,1,37712,franz och,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we present and compare various alignment models for statistical machine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments. We also compare the impact of different alignment models on the translation quality of a statistical machine translation system."
2000.iwpt-1.23,On the Use of Grammar Based Language Models for Statistical Machine Translation,2000,-1,-1,3,0,18853,hassan sawaf,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"In this paper, we describe some concepts of language models beyond the usually used standard trigram and use such language models for statistical machine translation. In statistical machine translation the language model is the a-priori knowledge source of the system about the target language. One important requirement for the language model is the correct word order, given a certain choice of words, and to score the translations generated by the translation model $\textrm{Pr}(f_1^J/e^I_1)$, in view of the syntactic context. In addition to standard $m$-grams with long histories, we examine the use of Part-of-Speech based models as well as linguistically motivated grammars with stochastic parsing as a special type of language model. Translation results are given on the VERBMOBIL task, where translation is performed from German to English, with vocabulary sizes of 6500 and 4000 words, respectively."
2000.eamt-1.5,Statistical Machine Translation,2000,-1,-1,2,1,37712,franz och,5th EAMT Workshop: Harvesting Existing Resources,0,None
W99-0604,Improved Alignment Models for Statistical Machine Translation,1999,7,488,3,1,37712,franz och,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,None
W97-1014,Word Triggers and the {EM} Algorithm,1997,7,25,2,1,38270,christoph tillmann,{C}o{NLL}97: Computational Natural Language Learning,0,None
P97-1037,A {DP}-based Search Using Monotone Alignments in Statistical Translation,1997,10,112,3,1,38270,christoph tillmann,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation model, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1%."
C96-2141,{HMM}-Based Word Alignment in Statistical Translation,1996,6,748,2,0,29364,stephan vogel,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora."
