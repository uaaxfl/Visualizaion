2021.sustainlp-1.2,Evaluating the carbon footprint of {NLP} methods: a survey and analysis of existing tools,2021,-1,-1,3,0,861,nesrine bannour,Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing,0,"Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments."
2021.eacl-tutorials.4,Reviewing Natural Language Processing Research,2021,-1,-1,4,1,10471,kevin cohen,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"The reviewing procedure has been identified as one of the major issues in the current situation of the NLP field. While it is implicitly assumed that junior researcher learn reviewing during their PhD project, this might not always be the case. Additionally, with the growing NLP community and the efforts in the context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure."
2020.wmt-1.76,"Findings of the {WMT} 2020 Biomedical Translation Shared Task: {B}asque, {I}talian and {R}ussian as New Additional Languages",2020,-1,-1,8,0.425125,7687,rachel bawden,Proceedings of the Fifth Conference on Machine Translation,0,"Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight language pairs. Five language pairs were previously addressed in past editions of the shared task, namely, English/German, English/French, English/Spanish, English/Portuguese, and English/Chinese. Three additional languages pairs were also introduced this year: English/Russian, English/Italian, and English/Basque. The task addressed the evaluation of both scientific abstracts (all language pairs) and terminologies (English/Basque only). We received submissions from a total of 20 teams. For recurring language pairs, we observed an improvement in the translations in terms of automatic scores and qualitative evaluations, compared to previous years."
2020.lrec-1.453,"{MEDLINE} as a Parallel Corpus: a Survey to Gain Insight on {F}rench-, {S}panish- and {P}ortuguese-speaking Authors{'} Abstract Writing Practice",2020,-1,-1,1,1,863,aurelie neveol,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Background: Parallel corpora are used to train and evaluate machine translation systems. To alleviate the cost of producing parallel resources for evaluation campaigns, existing corpora are leveraged. However, little information may be available about the methods used for producing the corpus, including translation direction. Objective: To gain insight on MEDLINE parallel corpus used in the biomedical task at the Workshop on Machine Translation in 2019 (WMT 2019). Material and Methods: Contact information for the authors of MEDLINE articles included in the English/Spanish (EN/ES), English/French (EN/FR), and English/Portuguese (EN/PT) WMT 2019 test sets was obtained from PubMed and publisher websites. The authors were asked about their abstract writing practices in a survey. Results: The response rate was above 20{\%}. Authors reported that they are mainly native speakers of languages other than English. Although manual translation, sometimes via professional translation services, was commonly used for abstract translation, authors of articles in the EN/ES and EN/PT sets also relied on post-edited machine translation. Discussion: This study provides a characterization of MEDLINE authors{'} language skills and abstract writing practices. Conclusion: The information collected in this study will be used to inform test set design for the next WMT biomedical task."
2020.jeptalnrecital-taln.35,Mod{\\`e}le neuronal pour la r{\\'e}solution de la cor{\\'e}f{\\'e}rence dans les dossiers m{\\'e}dicaux {\\'e}lectroniques (Neural approach for coreference resolution in electronic health records ),2020,-1,-1,3,1,18601,julien tourille,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"La r{\'e}solution de la cor{\'e}f{\'e}rence est un {\'e}l{\'e}ment essentiel pour la constitution automatique de chronologies m{\'e}dicales {\`a} partir des dossiers m{\'e}dicaux {\'e}lectroniques. Dans ce travail, nous pr{\'e}sentons une approche neuronale pour la r{\'e}solution de la cor{\'e}f{\'e}rence dans des textes m{\'e}dicaux {\'e}crits en anglais pour les entit{\'e}s g{\'e}n{\'e}rales et cliniques en nous {\'e}valuant dans le cadre de r{\'e}f{\'e}rence pour cette t{\^a}che que constitue la t{\^a}che 1C de la campagne i2b2 2011."
2020.acl-tutorials.4,Reviewing Natural Language Processing Research,2020,-1,-1,4,1,10471,kevin cohen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"This tutorial will cover the theory and practice of reviewing research in natural language processing. Heavy reviewing burdens on natural language processing researchers have made it clear that our community needs to increase the size of our pool of potential reviewers. Simultaneously, notable {``}false negatives{''}---rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences{---}have raised awareness of the fact that our reviewing practices leave something to be desired. We do not often talk about {``}false positives{''} with respect to conference papers, but leaders in the field have noted that we seem to have a publication bias towards papers that report high performance, with perhaps not much else of interest in them. It need not be this way. Reviewing is a learnable skill, and you will learn it here via lectures and a considerable amount of hands-on practice."
W19-5403,Findings of the {WMT} 2019 Biomedical Translation Shared Task: Evaluation for {MEDLINE} Abstracts and Biomedical Terminologies,2019,0,2,8,0.425125,7687,rachel bawden,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"In the fourth edition of the WMT Biomedical Translation task, we considered a total of six languages, namely Chinese (zh), English (en), French (fr), German (de), Portuguese (pt), and Spanish (es). We performed an evaluation of automatic translations for a total of 10 language directions, namely, zh/en, en/zh, fr/en, en/fr, de/en, en/de, pt/en, en/pt, es/en, and en/es. We provided training data based on MEDLINE abstracts for eight of the 10 language pairs and test sets for all of them. In addition to that, we offered a new sub-task for the translation of terms in biomedical terminologies for the en/es language direction. Higher BLEU scores (close to 0.5) were obtained for the es/en, en/es and en/pt test sets, as well as for the terminology sub-task. After manual validation of the primary runs, some submissions were judged to be better than the reference translations, for instance, for de/en, en/es and es/en."
W19-5012,A distantly supervised dataset for automated data extraction from diagnostic studies,2019,0,1,5,1,23937,christopher norman,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Systematic reviews are important in evidence based medicine, but are expensive to produce. Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting systematic reviews of diagnostic test accuracy, but relevant training data is not available. We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation. We evaluate the performance of BioBERT and logistic regression for ranking the sentences, and compare the performance for distant and direct supervision. Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators."
R19-1089,Community Perspective on Replicability in Natural Language Processing,2019,0,0,3,0.346099,3132,margot mieskes,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"With recent efforts in drawing attention to the task of replicating and/or reproducing results, for example in the context of COLING 2018 and various LREC workshops, the question arises how the NLP community views the topic of replicability in general. Using a survey, in which we involve members of the NLP community, we investigate how our community perceives this topic, its relevance and options for improvement. Based on over two hundred participants, the survey results confirm earlier observations, that successful reproducibility requires more than having access to code and data. Additionally, the results show that the topic has to be tackled from the authors{'}, reviewers{'} and community{'}s side."
W18-6403,Findings of the {WMT} 2018 Biomedical Translation Shared Task: Evaluation on {M}edline test sets,2018,0,0,3,0.357143,13903,mariana neves,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish). We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year{'}s BLEU scores were somewhat higher that last year{'}s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish."
W18-5622,Evaluation of a Sequence Tagging Tool for Biomedical Texts,2018,0,2,4,1,18601,julien tourille,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts."
L18-1025,Three Dimensions of Reproducibility in Natural Language Processing,2018,0,3,8,0,29526,bretonnel cohen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Despite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about even the definition of the term. That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing. This paper proposes an ontology of reproducibility in that field. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of a conclusion, of a finding, and of a value. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions."
L18-1043,Parallel Corpora for the Biomedical Domain,2018,0,0,1,1,863,aurelie neveol,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"A vast amount of biomedical information is available in the form of scientific literature and government-authored patient information documents. While English is the most widely used language in many of these sources, there is a need to provide access to health information in languages other than English. Parallel corpora can be leveraged to implement cross-lingual information retrievaln or machine translation tools. Herein, we review the extent of parallel corpus coverage in the biomedical domain. Specifically, we perform a scoping review of existing resources and we describe the recent development of new datasets for scientific literature (the EDP dataset and an extension of the Scielo corpus) and clinical trials (the ReBEC corpus). These corpora are currently beingn used in the biomedical task in the Conference on Machine Translation (WMTxe2x80x9916 and WMTxe2x80x9917), which illustrates their potential for improving and evaluating biomedical machine translation systems. Furthermore, we suggest additional applications for multilingual natural language processing using these resources, and plan to extend resource coverage to additional text genres and language pairs."
L18-1582,Automating Document Discovery in the Systematic Review Process: How to Use Chaff to Extract Wheat,2018,0,2,4,1,23937,christopher norman,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Systematic reviews in e.g. empirical medicine address research questions by comprehensively examining the entire published literature. Conventionally, manual literature surveys decide inclusion in two steps, first based on abstracts and title, then by full text, yet currentn methods to automate the process make no distinction between gold data from these two stages. In this work we compare the impact different schemes for choosing positive and negative examples from the different screening stages have on the training of automated systems. We train a ranker using logistic regression and evaluate it on a new gold standard dataset for clinical NLP, and on an existing gold standard dataset for drug class efficacy. The classification and ranking achieves an average AUC of 0.803 and 0.768 when relying on gold standard decisions based on title and abstracts of articles, and an AUC of 0.625 and 0.839 when relying on gold standard decisions based on full text. Our results suggest that it makes little difference which screening stage the gold standard decisions are drawn from, and that the decisions need not be based on the full text. The results further suggest that common-off-the-shelf algorithms can reduce the amount of work required to retrieve relevant literature."
2018.jeptalnrecital-court.2,D{\\'e}tection automatique de phrases en domaine de sp{\\'e}cialit{\\'e} en fran{\\c{c}}ais (Sentence boundary detection for specialized domains in {F}rench ),2018,-1,-1,2,0,30982,arthur boyer,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"La d{\'e}tection de fronti{\`e}res de phrase est g{\'e}n{\'e}ralement consid{\'e}r{\'e} comme un probl{\`e}me r{\'e}solu. Cependant, les outils performant sur des textes en domaine g{\'e}n{\'e}ral, ne le sont pas forcement sur des domaines sp{\'e}cialis{\'e}s, ce qui peut engendrer des d{\'e}gradations de performance des outils intervenant en aval dans une cha{\^\i}ne de traitement automatique s{'}appuyant sur des textes d{\'e}coup{\'e}s en phrases. Dans cet article, nous {\'e}valuons 5 outils de segmentation en phrase sur 3 corpus issus de diff{\'e}rent domaines. Nous r{\'e}-entrainerons l{'}un de ces outils sur un corpus de sp{\'e}cialit{\'e} pour {\'e}tudier l{'}adaptation en domaine. Notamment, nous utilisons un nouveau corpus biom{\'e}dical annot{\'e} sp{\'e}cifiquement pour cette t{\^a}che. La detection de fronti{\`e}res de phrase {\`a} l{'}aide d{'}un mod{\`e}le OpenNLP entra{\^\i}n{\'e} sur un corpus clinique offre une F-mesure de .73, contre .66 pour la version standard de l{'}outil."
W17-4719,Findings of the {WMT} 2017 Biomedical Translation Shared Task,2017,9,7,2,0.99775,4214,antonio yepes,Proceedings of the Second Conference on Machine Translation,0,None
S17-2098,{LIMSI}-{COT} at {S}em{E}val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives,2017,0,5,4,1,18601,julien tourille,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks."
P17-2035,Neural Architecture for Temporal Relation Extraction: A {B}i-{LSTM} Approach for Detecting Narrative Containers,2017,20,23,3,1,18601,julien tourille,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best result reported on this corpus to date."
E17-2117,Temporal information extraction from clinical text,2017,15,5,4,1,18601,julien tourille,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages."
2017.jeptalnrecital-demo.11,Traitement automatique de la langue biom{\\'e}dicale au {LIMSI} (Biomedical language processing at {LIMSI}),2017,-1,-1,4,1,23937,christopher norman,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Nous proposons des d{\'e}monstrations de trois outils d{\'e}velopp{\'e}s par le LIMSI en traitement automatique des langues appliqu{\'e} au domaine biom{\'e}dical : la d{\'e}tection de concepts m{\'e}dicaux dans des textes courts, la cat{\'e}gorisation d{'}articles scientifiques pour l{'}assistance {\`a} l{'}{\'e}criture de revues syst{\'e}matiques, et l{'}anonymisation de textes cliniques."
2017.jeptalnrecital-court.29,Tri Automatique de la Litt{\\'e}rature pour les Revues Syst{\\'e}matiques (Automatically Ranking the Literature in Support of Systematic Reviews),2017,-1,-1,4,1,23937,christopher norman,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Les revues syst{\'e}matiques de la litt{\'e}rature dans le domaine biom{\'e}dical reposent essentiellement sur le travail bibliographique manuel d{'}experts. Nous {\'e}valuons les performances de la classification supervis{\'e}e pour la d{\'e}couverte automatique d{'}articles {\`a} l{'}aide de plusieurs d{\'e}finitions des crit{\`e}res d{'}inclusion. Nous appliquons un mod{\`e}le de regression logistique sur deux corpus issus de revues syst{\'e}matiques conduites dans le domaine du traitement automatique de la langue et de l{'}efficacit{\'e} des m{\'e}dicaments. La classification offre une aire sous la courbe moyenne (AUC) de 0.769 si le classifieur est contruit {\`a} partir des jugements experts port{\'e}s sur les titres et r{\'e}sum{\'e}s des articles, et de 0.835 si on utilise les jugements port{\'e}s sur le texte int{\'e}gral. Ces r{\'e}sultats indiquent l{'}importance des jugements port{\'e}s d{\`e}s le d{\'e}but du processus de s{\'e}lection pour d{\'e}velopper un classifieur efficace pour acc{\'e}l{\'e}rer l{'}{\'e}laboration des revues syst{\'e}matiques {\`a} l{'}aide d{'}un algorithme de classification standard."
W16-6110,Replicability of Research in Biomedical Natural Language Processing: a pilot evaluation for a coding task,2016,0,2,1,1,863,aurelie neveol,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
W16-5107,A Dataset for {ICD}-10 Coding of Death Certificates: Creation and Usage,2016,0,4,2,0.57045,8590,thomas lavergne,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English. This paper describes a large-scale dataset prepared from French death certificates, and the problems which needed to be solved to turn it into a dataset suitable for the application of machine learning and natural language processing methods of ICD-10 coding. The dataset includes the free-text statements written by medical doctors, the associated meta-data, the human coder-assigned codes for each statement, as well as the statement segments which supported the coder{'}s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task."
W16-5112,Detection of Text Reuse in {F}rench Medical Corpora,2016,12,0,3,1,32928,eva dhondt,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"Electronic Health Records (EHRs) are increasingly available in modern health care institutions either through the direct creation of electronic documents in hospitals{'} health information systems, or through the digitization of historical paper records. Each EHR creation method yields the need for sophisticated text reuse detection tools in order to prepare the EHR collections for efficient secondary use relying on Natural Language Processing methods. Herein, we address the detection of two types of text reuse in French EHRs: 1) the detection of updated versions of the same document and 2) the detection of document duplicates that still bear surface differences due to OCR or de-identification processing. We present a robust text reuse detection method to automatically identify redundant document pairs in two French EHR corpora that achieves an overall macro F-measure of 0.68 and 0.60, respectively and correctly identifies all redundant document pairs of interest."
W16-2301,Findings of the 2016 Conference on Machine Translation,2016,113,137,12,0,292,ondvrej bojar,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries."
S16-1175,{LIMSI}-{COT} at {S}em{E}val-2016 Task 12: Temporal relation identification using a pipeline of classifiers,2016,9,1,3,1,18601,julien tourille,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"SemEval 2016 Task 12 addresses temporal reasoning in the clinical domain. In this paper, we present our participation for relation extraction based on gold standard entities (subtasks DR and CR). We used a supervised approach comparing plain lexical features to word embeddings for temporal relation identification, and obtained above-median scores."
L16-1470,The Scielo Corpus: a Parallel Corpus of Scientific Publications for Biomedicine,2016,0,6,3,0.357143,13903,mariana neves,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The biomedical scientific literature is a rich source of information not only in the English language, for which it is more abundant, but also in other languages, such as Portuguese, Spanish and French. We present the first freely available parallel corpus of scientific publications for the biomedical domain. Documents from the {''}Biological Sciences{''} and {''}Health Sciences{''} categories were retrieved from the Scielo database and parallel titles and abstracts are available for the following language pairs: Portuguese/English (about 86,000 documents in total), Spanish/English (about 95,000 documents) and French/English (about 2,000 documents). Additionally, monolingual data was also collected for all four languages. Sentences in the parallel corpus were automatically aligned and a manual analysis of 200 documents by native experts found that a minimum of 79{\%} of sentences were correctly aligned in all language pairs. We demonstrate the utility of the corpus by running baseline machine translation experiments. We show that for all language pairs, a statistical machine translation system trained on the parallel corpora achieves performance that rivals or exceeds the state of the art in the biomedical domain. Furthermore, the corpora are currently being used in the biomedical task in the First Conference on Machine Translation (WMT{'}16)."
2016.jeptalnrecital-poster.19,Extraction de relations temporelles dans des dossiers {\\'e}lectroniques patient (Extracting Temporal Relations from Electronic Health Records),2016,-1,-1,3,1,18601,julien tourille,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"L{'}analyse temporelle des documents cliniques permet d{'}obtenir des repr{\'e}sentations riches des informations contenues dans les dossiers {\'e}lectroniques patient. Cette analyse repose sur l{'}extraction d{'}{\'e}v{\'e}nements, d{'}expressions temporelles et des relations entre eux. Dans ce travail, nous consid{\'e}rons que nous disposons des {\'e}v{\'e}nements et des expressions temporelles pertinents et nous nous int{\'e}ressons aux relations temporelles entre deux {\'e}v{\'e}nements ou entre un {\'e}v{\'e}nement et une expression temporelle. Nous pr{\'e}sentons des mod{\`e}les de classification supervis{\'e}e pour l{'}extraction de des relations en fran{\c{c}}ais et en anglais. Les performances obtenues sont comparables dans les deux langues, sugg{\'e}rant ainsi que diff{\'e}rents domaines cliniques et diff{\'e}rentes langues pourraient {\^e}tre abord{\'e}s de mani{\`e}re similaire."
W15-2603,Redundancy in {F}rench Electronic Health Records: A preliminary study,2015,16,1,3,1,32928,eva dhondt,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"The use of Electronic Health Records (EHRs) is becoming more prevalent in healthcare institutions world-wide. These digital records contain a wealth of information on patientsxe2x80x99 health in the form of Natural Language text. The electronic format of the clinical notes has evident advantages in terms of storage and shareability, but also makes it easy to duplicate information from one document to another through copy-pasting. Previous studies have shown that (copy-paste-induced) redundancy can reach high levels in American EHRs, and that these high levels of redundancy have a negative effect on the performance of Natural Language Processing (NLP) tools that are used to process EHRs automatically. In this paper, we present a preliminary study on the level of redundancy in French EHRs. We study the evolution of redundancy over time, and its occurrence in respect to different document types and sections in a small corpus comprising of three patient records (361 documents). We find that average redundancy levels in our subset are lower than those observed in U.S. corpora (respectively 33% vs. up to 78%), which may indicate different cultural practices between these two countries. Moreover, we find no evidence of the incremental increase (over time) of redundant text in clinical notes which has been found in American EHRs. These results suggest that redundancy mitigating strategies may not be needed when processing French EHRs."
W15-2604,Is it possible to recover personal health information from an automatically de-identified corpus of {F}rench {EHR}s?,2015,11,1,3,0.486119,5675,cyril grouin,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"De-identification aims at preserving patient confidentiality while enabling the use of clinical documents for furthering medical research. Herein, we aim to evaluate whether patient re-identification is possible on a corpus of de-identified clinical documents in French. Personal Health Identifiers are automatically marked by a de-identification system applied to the corpus, followed by reintroduction of plausible surrogates. The resulting documents are shown to individuals with varying knowledge of the documents and de-identification method. The individuals are asked to re-identify the patients. The amount of information recovered increases with familiarity with the documents and/or de-identification method. Surrogate re-introduction with localization from the same (vs. different) geographical area as the original documents is found more effective. The amount of information recovered was not sufficient to re-identify any of the patients, except when privileged access to the hospital health information system and several documents about the same patient were available."
D15-1055,Automatic Extraction of Time Expressions Accross Domains in {F}rench Narratives,2015,20,2,3,0,18809,mike nzali,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"The prevalence of temporal references across all types of natural language utterances makes temporal analysis a key issue in Natural Language Processing. This work adresses three research questions: 1/is temporal expression recognition specific to a particular domain? 2/if so, can we characterize domain specificity? and 3/how can subdomain specificity be integrated in a single tool for unified temporal expression extraction? Herein, we assess temporal expression recognition from documents written in French covering three domains. We present a new corpus of clinical narratives annotated for temporal expressions , and also use existing corpora in the newswire and historical domains. We show that temporal expressions can be extracted with high performance across domains (best F-measure 0.96 obtained with a CRF model on clinical narratives). We argue that domain adaptation for the extraction of temporal expressions can be done with limited efforts and should cover pre-processing as well as temporal specific tasks."
2015.jeptalnrecital-long.5,Analyse d{'}expressions temporelles dans les dossiers {\\'e}lectroniques patients,2015,-1,-1,2,0,18809,mike nzali,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les r{\'e}f{\'e}rences {\`a} des ph{\'e}nom{\`e}nes du monde r{\'e}el et {\`a} leur caract{\'e}risation temporelle se retrouvent dans beaucoup de types de discours en langue naturelle. Ainsi, l{'}analyse temporelle appara{\^\i}t comme un {\'e}l{\'e}ment important en traitement automatique de la langue. Cet article pr{\'e}sente une analyse de textes en domaine de sp{\'e}cialit{\'e} du point de vue temporel. En s{'}appuyant sur un corpus de documents issus de plusieurs dossiers {\'e}lectroniques patient d{\'e}sidentifi{\'e}s, nous d{\'e}crivons la construction d{'}une ressource annot{\'e}e en expressions temporelles selon la norme TimeML. Par suite, nous utilisons cette ressource pour {\'e}valuer plusieurs m{\'e}thodes d{'}extraction automatique d{'}expressions temporelles adapt{\'e}es au domaine m{\'e}dical. Notre meilleur syst{\`e}me statistique offre une performance de 0,91 de F-mesure, surpassant pour l{'}identification le syst{\`e}me {\'e}tat de l{'}art HeidelTime. La comparaison de notre corpus de travail avec le corpus journalistique FR-Timebank permet {\'e}galement de caract{\'e}riser les diff{\'e}rences d{'}utilisation des expressions temporelles dans deux domaines de sp{\'e}cialit{\'e}."
2015.jeptalnrecital-court.29,Etiquetage morpho-syntaxique en domaine de sp{\\'e}cialit{\\'e}: le domaine m{\\'e}dical,2015,-1,-1,3,0,37988,christelle rabary,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"L{'}{\'e}tiquetage morpho-syntaxique est une t{\^a}che fondamentale du Traitement Automatique de la Langue, sur laquelle reposent souvent des traitements plus complexes tels que l{'}extraction d{'}information ou la traduction automatique. L{'}{\'e}tiquetage en domaine de sp{\'e}cialit{\'e} est limit{\'e} par la disponibilit{\'e} d{'}outils et de corpus annot{\'e}s sp{\'e}cifiques au domaine. Dans cet article, nous pr{\'e}sentons le d{\'e}veloppement d{'}un corpus clinique du fran{\c{c}}ais annot{\'e} morpho-syntaxiquement {\`a} l{'}aide d{'}un jeu d{'}{\'e}tiquettes issus des guides d{'}annotation French Treebank et Multitag. L{'}analyse de ce corpus nous permet de caract{\'e}riser le domaine clinique et de d{\'e}gager les points cl{\'e}s pour l{'}adaptation d{'}outils d{'}analyse morpho-syntaxique {\`a} ce domaine. Nous montrons {\'e}galement les limites d{'}un outil entra{\^\i}n{\'e} sur un corpus journalistique appliqu{\'e} au domaine clinique. En perspective de ce travail, nous envisageons une application du corpus clinique annot{\'e} pour am{\'e}liorer l{'}{\'e}tiquetage morpho-syntaxique des documents cliniques en fran{\c{c}}ais."
W14-4907,Optimizing annotation efforts to build reliable annotated corpora for training statistical models,2014,17,5,3,0.486119,5675,cyril grouin,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"Creating high-quality manual annotations on text corpus is time-consuming and often requires the work of experts. In order to explore methods for optimizing annotation efforts, we study three key time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and (iii) careful annotations. Through a series of experiments using a corpus of clinical documents annotated for personally identifiable information written in French, we address each of these aspects and draw conclusions on how to make the most of an annotation effort."
deleger-etal-2014-annotation,Annotation of specialized corpora using a comprehensive entity and relation scheme,2014,15,8,5,0,17098,louise deleger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Annotated corpora are essential resources for many applications in Natural Language Processing. They provide insight on the linguistic and semantic characteristics of the genre and domain covered, and can be used for the training and evaluation of automatic tools. In the biomedical domain, annotated corpora of English texts have become available for several genres and subfields. However, very few similar resources are available for languages other than English. In this paper we present an effort to produce a high-quality corpus of clinical documents in French, annotated with a comprehensive scheme of entities and relations. We present the annotation scheme as well as the results of a pilot annotation study covering 35 clinical documents in a variety of subfields and genres. We show that high inter-annotator agreement can be achieved using a complex annotation scheme."
neveol-etal-2014-language,Language Resources for {F}rench in the Biomedical Domain,2014,18,15,1,1,863,aurelie neveol,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The biomedical domain offers a wealth of linguistic resources for Natural Language Processing, including terminologies and corpora. While many of these resources are prominently available for English, other languages including French benefit from substantial coverage thanks to the contribution of an active community over the past decades. However, access to terminological resources in languages other than English may not be as straight-forward as access to their English counterparts. Herein, we review the extent of resource coverage for French and give pointers to access French-language resources. We also discuss the sources and methods for making additional material available for French."
F14-2030,Automatic identification of document sections for designing a {F}rench clinical corpus (Identification automatique de zones dans des documents pour la constitution d{'}un corpus m{\\'e}dical en fran{\\c{c}}ais) [in {F}rench],2014,-1,-1,2,0,17098,louise deleger,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
W11-0213,Automatic extraction of data deposition statements: where do the research results go?,2011,3,0,1,1,863,aurelie neveol,Proceedings of {B}io{NLP} 2011 Workshop,0,"Research in the biomedical domain can have a major impact through open sharing of data produced. In this study, we use machine learning for the automatic identification of data deposition sentences in research articles. Articles containing deposition sentences are correctly identified with 73% f-measure. These results show the potential impact of our method for literature curation."
W09-1319,Exploring Two Biomedical Text Genres for Disease Recognition,2009,23,25,1,1,863,aurelie neveol,Proceedings of the {B}io{NLP} 2009 Workshop,0,"In the framework of contextual information retrieval in the biomedical domain, this paper reports on the automatic detection of disease concepts in two genres of biomedical text: sentences from the literature and PubMed user queries. A statistical model and a Natural Language Processing algorithm for disease recognition were applied on both corpora. While both methods show good performance (F=77% vs. F=76%) on the sentence corpus, results on the query corpus indicate that the statistical model is more robust (F=74% vs. F=70%)."
W08-0612,Automatic inference of indexing rules for {MEDLINE},2008,21,11,1,1,863,aurelie neveol,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"This paper describes the use and customization of Inductive Logic Programming (ILP) to infer indexing rules from MEDLINE citations. Preliminary results suggest this method may enhance the subheading attachment module of the Medical Text Indexer, a system for assisting MEDLINE indexers."
2008.jeptalnrecital-long.30,Apprentissage artificiel de r{\\`e}gles d{'}indexation pour {MEDLINE},2008,12,0,1,1,863,aurelie neveol,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}indexation est une composante importante de tout syst{\`e}me de recherche d{'}information. Dans MEDLINE, la base documentaire de r{\'e}f{\'e}rence pour la litt{\'e}rature du domaine biom{\'e}dical, le contenu des articles r{\'e}f{\'e}renc{\'e}s est index{\'e} {\`a} l{'}aide de descripteurs issus du th{\'e}saurus MeSH. Avec l{'}augmentation constante de publications {\`a} indexer pour maintenir la base {\`a} jour, le besoin d{'}outils automatiques se fait pressant pour les indexeurs. Dans cet article, nous d{\'e}crivons l{'}utilisation et l{'}adaptation de la Programmation Logique Inductive (PLI) pour d{\'e}couvrir des r{\`e}gles d{'}indexation permettant de g{\'e}n{\'e}rer automatiquement des recommandations d{'}indexation pour MEDLINE. Les r{\'e}sultats obtenus par cette approche originale sont tr{\`e}s satisfaisants compar{\'e}s {\`a} ceux obtenus {\`a} l{'}aide de r{\`e}gles manuelles lorsque celles-ci existent. Ainsi, les jeux de r{\`e}gles obtenus par PLI devraient {\^e}tre prochainement int{\'e}gr{\'e}s au syst{\`e}me produisant les recommandations d{'}indexation automatique pour MEDLINE."
W07-1014,From indexing the biomedical literature to coding clinical text: experience with {MTI} and machine learning approaches,2007,11,45,7,0,38556,alan aronson,"Biological, translational, and clinical language processing",0,"This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports. The basic methods used are: a modification of the NLM Medical Text Indexer system, SVM, k-NN and a simple pattern-matching method. The basic methods are combined using a variant of stacking. Evaluated in the context of a Medical NLP Challenge, fusion produced an F-score of 0.85 on the Challenge test set, which is considerably above the mean Challenge F-score of 0.77 for 44 participating groups."
W07-1026,Automatic Indexing of Specialized Documents: Using Generic vs. Domain-Specific Document Representations,2007,20,7,1,1,863,aurelie neveol,"Biological, translational, and clinical language processing",0,"The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized. In the biomedical domain, continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINExc2xae. In this paper, we evaluate two statistical methods of producing MeSHxc2xae indexing recommendations for the genetics literature, including recommendations involving subheadings, which is a novel application for the methods. We show that a generic representation of the documents yields both better precision and recall. We also find that a domain-specific representation of the documents can contribute to enhancing recall."
2005.jeptalnrecital-court.16,Indexation automatique de ressources de sant{\\'e} {\\`a} l{'}aide de paires de descripteurs {M}e{SH},2005,-1,-1,1,1,863,aurelie neveol,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Depuis quelques ann{\'e}es, m{\'e}decins et documentalistes doivent faire face {\`a} une demande croissante dans le domaine du codage m{\'e}dico-{\'e}conomique et de l{'}indexation des diverses sources d{'}information disponibles dans le domaine de la sant{\'e}. Il est donc n{\'e}cessaire de d{\'e}velopper des outils d{'}indexation automatique qui r{\'e}duisent les d{\'e}lais d{'}indexation et facilitent l{'}acc{\`e}s aux ressources m{\'e}dicales. Nous proposons deux m{\'e}thodes d{'}indexation automatique de ressources de sant{\'e} {\`a} l{'}aide de paires de descripteurs MeSH. La combinaison de ces deux m{\'e}thodes permet d{'}optimiser les r{\'e}sulats en exploitant la compl{\'e}mentarit{\'e} des approches. Les performances obtenues sont {\'e}quivalentes {\`a} celles des outils de la litt{\'e}rature pour une indexation {\`a} l{'}aide de descripteurs seuls."
2004.jeptalnrecital-recital.3,Indexation automatique de ressources de sant{\\'e} {\\`a} l{'}aide d{'}un vocabulaire contr{\\^o}l{\\'e},2004,-1,-1,1,1,863,aurelie neveol,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,"Nous pr{\'e}sentons ici le syst{\`e}me d{'}indexation automatique actuellement en cours de d{\'e}veloppement dans l{'}{\'e}quipe CISMeF afin d{'}aider les documentalistes lors de l{'}indexation de ressources de sant{\'e}. Nous d{\'e}taillons l{'}architecture du syst{\`e}me pour l{'}extraction de mots cl{\'e}s MeSH, et pr{\'e}sentons les r{\'e}sultats d{'}une premi{\`e}re {\'e}valuation. La strat{\'e}gie d{'}indexation choisie atteint une pr{\'e}cision comparable {\`a} celle des syst{\`e}mes existants. De plus, elle permet d{'}extraire des paires mot cl{\'e}/qualificatif, et non des termes isol{\'e}s, ce qui constitue une indexation beaucoup plus fine. Les travaux en cours s{'}attachent {\`a} {\'e}tendre la couverture des dictionnaires, et des tests {\`a} plus grande {\'e}chelle sont envisag{\'e}s afin de valider le syst{\`e}me et d{'}{\'e}valuer sa valeur ajout{\'e}e dans le travail quotidien des documentalistes."
